# Basic accelerator configuration
board: U280                    # Target FPGA
shell_flow_type: vitis_alveo   # Integration flow
synth_clk_period_ns: 10.0      # 100 MHz clock
target_fps: 1000               # Target throughput
verbose: true                  # Enable verbose output

# Output generation options
generate_outputs:
  - estimate_reports
  - bitfile
  - CPP_DRIVER
  - pynq_driver
  - deployment_package

verify_input_npy: outputs/inp.npy
verify_expected_output_npy: outputs/out.npy
verify_save_full_context: true
save_intermediate_models: true

verify_steps:
  - QONNX_TO_FINN_PYTHON
  - STREAMLINED_PYTHON
  - TIDY_UP_PYTHON
  - FOLDED_HLS_CPPSIM
  # - NODE_BY_NODE_RTLSIM
  # - STITCHED_IP_RTLSIM

auto_fifo_strategy: CHARACTERIZE
auto_fifo_depths:  False
# Build directory
output_dir: build_dir

steps:
    # Prepares the QONNX graph to be consumed by FINN: Cleanup, lowering
    # and Quant to MultiThreshold conversion
    - prepare_graph
    # Unified exhaustive streamlining of complex model topologies
    # including attention, residuals and splits
    - step_streamline
    # conversion of the scaled dot-product attention pattern to
    # hardware, including cleanup and data layout squeezing
    - step_convert_attention_to_hw
    # Convert the elementwise binary operations to hardware operators.
    # These include for example adding residual branches and positional
    # encoding
    - step_convert_elementwise_binary_to_hw
    # Convert Lookup layers, e.g., token embedding, to hardware custom
    # operators
    - step_convert_lookup_to_hw
    # Convert Split and Concat operators to hardware, e.g., splits
    # contained in the GLU activation
    - step_convert_split_concat_to_hw
    # Convert depth-wise convolution MatMuls to VVUs
    - step_convert_depth_wise_to_hw
    # Properly replicate the stream feeding the query, key and value
    # projections
    - step_replicate_streams
    # Convert most other layers supported by FINN to HW operators
    - step_convert_to_hw
    # Specialize HW layer implementations as either HLS or RTL
    - step_specialize_layers
    - step_create_dataflow_partition
    # Set the folding configuration to meet the cycles per sequence
    # target
    - set_target_parallelization(seq_len, emb_dim)
    # Apply folding configuration, specifying hardware implementation
    # details
    # Note: This triggers a verification step
    - step_apply_folding_config
    - step_minimize_bit_width
    # The ScaledDotProductAttention custom op does not define any
    # estimates
    - step_generate_estimate_reports
    - step_hw_codegen
    - step_hw_ipgen
    # Set the attention- and residual-related FIFO depths insert FIFOs
    # and apply folding configuration once again
    # Note: Implement all FIFOs with a depth at least as deep as the
    # sequence length in URAM.
    - set_fifo_depths(seq_len, emb_dim, uram_threshold=seq_len)
    # Run additional node-by-node verification in RTL simulation of the
    # model before creating the stitched IP
    # Note: end-to-end verification of the stitched IP in RTL simulation
    # is still not possible due to missing float IPs
    - node_by_node_cppsim
    # Only for debugging for now, does not work if "vivado" style
    # StreamingFIFOs are used
    # node_by_node_rtlsim,
    - step_create_stitched_ip
    # Attention does currently not support RTL simulation due to missing
    # float IPs.
    # "step_measure_rtlsim_performance",
    - step_out_of_context_synthesis
    - step_synthesize_bitfile
    - step_make_driver
    - step_deployment_package