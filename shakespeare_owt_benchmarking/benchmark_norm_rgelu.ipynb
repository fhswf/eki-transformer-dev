{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train openwebtext and shakespeare GPT2 models with either gelu or relu and layernorm or batchnorm and run inference on them\n",
    "### For openwebtext, 4 heads and 4 transformer blocks and for shakespeare, half are used\n",
    "### Tiktoken gpt2 Tokenization is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GPT2 with Shakespeare GELU BatchNorm, custom_ln is Identity layer\n",
    "### Params similiar to nanoGPT (https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py) except for gpt model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 5000, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 13:44:11.648799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:44:13,308 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,313 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,316 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-13_13:44:13\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,577 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,939 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,948 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,954 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,986 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,991 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,283 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,402 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,411 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,498 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,729 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.0814786911010743\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,962 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.028734016418458\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.417726612091064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.841564083099366\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.336726760864257\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,242 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.842315435409546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,564 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.42349648475647\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.046977567672729\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,188 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.75757155418396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.5278137683868405\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.345893001556396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,180 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.116252470016479\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.0132341384887695\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.878851127624512\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,109 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.804667854309082\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.695879507064819\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,760 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.600269079208374\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,083 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.538007020950317\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.44656195640564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.251713800430298\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.200865888595581\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.038680076599121\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.861286687850952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,037 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.634791707992553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.378215074539185\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,690 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.140535163879394\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,017 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.918185019493103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.59971444606781\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.332096314430237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.1224578857421874\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,306 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.891332507133484\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.6721574783325197\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4845260620117187\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.2823659896850588\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,583 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.1804298162460327\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,909 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.009354305267334\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.8618500351905822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,560 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.6786442637443542\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.5513007402420045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.4092130661010742\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.327401328086853\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.2216348052024841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.1183194756507873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.0603752493858338\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,828 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.9550049722194671\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,122 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.8526090204715728\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.8235867023468018\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.7416079103946686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,089 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.6766059696674347\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.623449730873108\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.5610007286071778\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,076 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.5322279185056686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,400 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.4816823750734329\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.4518648833036423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.4204030305147171\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,369 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.37947399616241456\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.35556192696094513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.3440380185842514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.29768029451370237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.28025399148464203\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.26523620039224627\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.2554860457777977\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.23417816907167435\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,937 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.21371423751115798\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.1988332137465477\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.19074835479259492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.18297459036111832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,240 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.17245704233646392\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.17341131418943406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.1595330536365509\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.15104628428816796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.14839548915624617\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.14153633862733841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.1328538939356804\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.12690432518720626\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.13007508590817451\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.125824736058712\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.12251279503107071\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,709 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.1187422126531601\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.11048740148544312\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,317 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.10810237675905228\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,641 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.10765649899840354\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,969 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.10224309861660004\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,292 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.10713213160634041\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.09892426878213882\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.10035960376262665\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.09816799089312553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.09627232924103737\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.09459140226244926\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,236 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.09456081539392472\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.09531142190098763\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.09356032237410546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.0900658719241619\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.09381850361824036\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.08983872383832932\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.089760223031044\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.08956152871251107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.08698741123080253\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.08797898441553116\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,475 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.08725197464227677\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.08584734052419662\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.08436140418052673\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.08557359799742699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,756 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.08337687328457832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.08435980305075645\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.08416210934519767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.08072719871997833\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.08021606504917145\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,268 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07771769538521767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.08004260808229446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.07954467162489891\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.08120676279067993\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.07973136603832245\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.08074760884046554\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.08167456313967705\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.08013327717781067\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.08386159390211105\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,168 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.08124819472432136\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,493 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.07815293446183205\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.08068719729781151\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,140 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.07527995184063911\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,460 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1210 loss: 0.0792896255850792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,777 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1220 loss: 0.07909879982471466\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,098 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1230 loss: 0.080222849547863\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,416 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1240 loss: 0.07985053658485412\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1250 loss: 0.07990247905254363\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1260 loss: 0.07967272847890854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1270 loss: 0.07683225870132446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,700 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1280 loss: 0.07696817442774773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1290 loss: 0.07847564071416854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1300 loss: 0.07635548561811448\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1310 loss: 0.07755953297019005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,980 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1320 loss: 0.08147314414381981\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,299 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1330 loss: 0.0745748072862625\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1340 loss: 0.07801730483770371\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1350 loss: 0.07652391642332076\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1360 loss: 0.07694418206810952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1370 loss: 0.07574513629078865\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1380 loss: 0.08034205883741379\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1390 loss: 0.07813942767679691\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1400 loss: 0.07715454623103142\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1410 loss: 0.07432392910122872\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,172 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1420 loss: 0.07728920727968216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,496 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1430 loss: 0.07314848452806473\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1440 loss: 0.07422819398343564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1450 loss: 0.07614408656954766\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,481 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1460 loss: 0.07296678051352501\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1470 loss: 0.07522793412208557\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,124 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1480 loss: 0.07485603466629982\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1490 loss: 0.07659061178565026\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,768 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1500 loss: 0.07550743147730828\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1510 loss: 0.07389946877956391\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1520 loss: 0.07652528509497643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1530 loss: 0.07560347318649292\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,056 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1540 loss: 0.07402549535036088\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,380 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1550 loss: 0.074910619109869\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1560 loss: 0.07472982481122017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1570 loss: 0.07554230615496635\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1580 loss: 0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/100: 0.133059561252594\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,172 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,348 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.005953484401106835\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07082101553678513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.07401572242379188\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.07121982723474503\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,635 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.07029064297676087\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.06943750269711017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.07120902799069881\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.06863513328135014\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,903 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.07117352485656739\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.06729949042201042\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,552 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07023796513676643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,871 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07243234217166901\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.0731443539261818\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.0650293543934822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.07004907317459583\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07083249166607856\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,487 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.06878587603569031\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.06674405559897423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,128 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.06930300667881965\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,433 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07282493859529496\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.0702348593622446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,074 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.06613621674478054\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.065633849427104\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07187155038118362\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07209198959171773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07056076899170875\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 0.0731478177011013\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 0.06732679381966591\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 0.06573506109416485\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,640 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 0.07029695585370063\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,964 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 0.06923590525984764\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 0.06766384989023208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,611 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 0.06799498610198498\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 0.06996757984161377\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,255 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 0.06888032145798206\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,580 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 0.06892801485955716\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 0.0686967846006155\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 0.07068811431527137\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,488 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 0.06987420693039895\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 0.06994570903480053\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 0.06992909088730812\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 0.06451450362801552\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 0.0671445544809103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 0.0680693306028843\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,430 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.07334598451852799\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,752 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.06892706342041492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.06757157891988755\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.07052591480314732\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.06981403641402721\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.06944975815713406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.07077371105551719\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.06996183507144452\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.0694230530411005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,345 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.06557997539639474\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.06862628795206546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.0700732484459877\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.06813231781125069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,618 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.06853835545480251\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.06956844478845596\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.06586511470377446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,582 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.07010632418096066\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.06835015043616295\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.06821108944714069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,557 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.06943667121231556\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.0673607874661684\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.06755397357046604\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.06880321577191353\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,840 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.06871512234210968\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.07124297022819519\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,482 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.06964963674545288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.07146930769085884\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.06947363130748271\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.06855200305581093\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,772 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.06658084094524383\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,090 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.06878070905804634\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.07016171887516975\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.07204182855784894\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,064 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.06919909864664078\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,359 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.0710775252431631\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,667 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.07189789228141308\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,986 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.07093326337635517\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.07051902897655964\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.07366400584578514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.0664932906627655\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,283 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.06879346258938313\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,608 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.06913681291043758\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,932 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.069281155616045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.06843326278030873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.06893916204571723\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,894 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.06852278523147107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,215 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.07043468467891216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.07027224525809288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.06835661269724369\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,189 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.07088794261217117\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,509 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.07069383189082146\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.06545544117689132\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,155 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.06914999186992646\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.06767004244029522\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.07045739553868771\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,120 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.0689571488648653\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.06914092190563678\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.06643678843975068\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,087 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.06644399277865887\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.06971023604273796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.07272332422435283\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.06667447723448276\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.06759239658713341\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,713 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.06913544759154319\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07417276315391064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,363 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.06717519052326679\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.06821254156529903\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.06578771919012069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.06996179968118668\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.066846938803792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.07002154067158699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.06956944465637208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,616 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.0701913632452488\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,935 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.07032300047576427\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.06583375632762908\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.06707629524171352\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.06304956637322903\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrun=train\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdevice=cuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[0;32m---> 16\u001b[0m qtransform\u001b[39m.\u001b[39;49mnotebook_run(args)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[39m=\u001b[39m compose(config_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, overrides\u001b[39m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m main(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m  __main__ \u001b[39mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m mn\u001b[39m.\u001b[39;49mmain(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m  train\u001b[39m.\u001b[39;49mrun(cfg)\n\u001b[1;32m     45\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbench\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[39m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[39m=\u001b[39m train(cfg\u001b[39m=\u001b[39;49mcfg, device\u001b[39m=\u001b[39;49mdevice, model\u001b[39m=\u001b[39;49mmodel, train_data_loader\u001b[39m=\u001b[39;49mtrain_dataloader, eval_data_loader\u001b[39m=\u001b[39;49meval_dataloader, optimizer\u001b[39m=\u001b[39;49moptimizer, scheduler\u001b[39m=\u001b[39;49mscheduler, timestamp\u001b[39m=\u001b[39;49mtimestamp)\n\u001b[1;32m    110\u001b[0m \u001b[39m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mFinished training model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    185\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEPOCH: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     metrics \u001b[39m=\u001b[39m train_one_epoch(cfg, device, model, train_data_loader, optimizer, mini_run)\n\u001b[1;32m    189\u001b[0m     \u001b[39m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39meval_epoch_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m eval_data_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:230\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    228\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    229\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(outputs, labels)\n\u001b[0;32m--> 230\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    231\u001b[0m \u001b[39m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgrad_clip\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mfloat\u001b[39m) \u001b[39mand\u001b[39;00m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mgrad_clip \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=64\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=5000\",\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=200\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write inference of Shakespeare GELU BatchNorm, custom_ln is Identity layer to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:52:43,381 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1'}}\n",
      "[ \u001b[36m2024-02-13 13:52:43,567 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,571 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,574 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,577 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,580 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,584 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,587 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,752 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,755 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,765 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,866 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,878 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,895 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,912 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,230 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,235 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,238 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,244 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,246 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,250 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,261 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,263 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   202,   317,   363,   385,   406,   563,   589,   879,   880,\n",
      "          962,  1010,  1152,  1267,  1369,  1408,  1487,  1860,  2143,  2186,\n",
      "         2244,  2266,  2283,  2394,  2566,  2889,  2988,  3209,  3297,  3385,\n",
      "         3599,  3708,  3765,  3770,  3843,  3889,  4252,  4256,  4388,  4393,\n",
      "         4410,  4611,  4692,  4891,  4930,  5398,  5399,  5507,  5675,  5785,\n",
      "         5801,  5933,  6006,  6100,  6111,  6458,  6481,  6596,  6625,  6760,\n",
      "         6783,  6821,  6868,  7134,  7248,  7251,  7491,  7555,  7840,  7845,\n",
      "         9030,  9041,  9067,  9194,  9341,  9352,  9388,  9477,  9839,  9855,\n",
      "         9912, 10180, 10458, 10546, 10605, 10643, 10687, 10827, 10873, 11037,\n",
      "        11325, 11415, 11481, 11507, 11520, 11717, 11748, 11769, 11788, 11793,\n",
      "        11872, 12002, 12476, 12579, 12659, 12694, 12711, 13030, 13072, 13091,\n",
      "        13178, 13201, 13216, 13255, 13346, 13451, 13509, 13562, 13563, 13635,\n",
      "        13675, 13816, 13858, 13903, 13905, 13949, 14001, 14073, 14202, 14327,\n",
      "        14383, 14505, 14515, 14532, 14725, 14840, 14855, 14921, 15073, 15096,\n",
      "        15121, 15222, 15304, 15314, 15433, 15794, 15860, 15861, 15989, 16035,\n",
      "        16067, 16392, 16416, 16437, 16527, 16824, 16862, 16999, 17013, 17108,\n",
      "        17113, 17318, 17556, 17574, 17602, 17667, 17672, 17856, 18135, 18198,\n",
      "        18250, 18301, 18333, 18362, 18371, 18471, 18734, 18926, 18948, 18976,\n",
      "        19104, 19145, 19296, 19306, 19393, 19524, 19762, 19899, 19949, 20126,\n",
      "        20128, 20188, 20326, 20517, 20518, 20627, 20656, 20706, 20856, 20893,\n",
      "        21354, 21369, 21381, 21567, 21694, 21710, 21722, 21910, 22076, 22425,\n",
      "        22530, 22568, 22591, 22634, 22968, 23201, 23230, 23253, 23480, 23530,\n",
      "        23678, 23689, 23738, 23852, 24144, 24208, 24403, 24513, 24758, 25452,\n",
      "        25492, 25580, 25810, 25867, 26008, 26248, 26268, 26476, 26770, 26944,\n",
      "        27156, 27611, 27620, 27800, 27858, 27884, 27923, 28165, 28205, 28563,\n",
      "        28682, 28699, 28705, 28992, 29037, 29094, 29116, 29445, 29483, 29485,\n",
      "        29557, 29746, 29963, 30178, 30365, 30468, 30521, 30872, 30962, 30966,\n",
      "        31103, 31107, 31456, 31510, 31643, 31673, 31736, 31788, 32060, 32336,\n",
      "        32493, 32728, 32780, 32829, 33010, 33161, 33174, 33280, 33324, 33535,\n",
      "        33727, 33769, 33871, 33877, 33883, 34004, 34042, 34227, 34273, 34278,\n",
      "        34311, 34373, 34412, 34571, 34626, 34937, 35030, 35075, 35418, 35814,\n",
      "        35903, 36082, 36114, 36220, 36315, 36492, 36673, 36697, 36873, 37061,\n",
      "        37090, 37206, 37691, 37728, 38118, 38356, 38385, 38432, 38535, 38538,\n",
      "        38687, 38798, 38965, 39075, 39232, 39377, 39384, 39403, 39698, 39700,\n",
      "        39781, 39801, 39805, 39984, 40009, 40062, 40078, 40388, 40427, 40430,\n",
      "        40508, 40599, 40604, 40693, 40701, 40713, 40755, 40850, 40869, 40901,\n",
      "        41318, 41534, 41550, 41736, 41762, 41924, 42139, 42192, 42248, 42259,\n",
      "        42452, 42585, 42598, 42834, 42993, 43034, 43045, 43189, 43218, 43289,\n",
      "        43426, 43503, 43865, 43888, 43901, 43956, 43984, 44153, 44159, 44205,\n",
      "        44360, 44624, 44678, 44679, 44730, 44822, 45186, 45208, 45384, 45479,\n",
      "        45529, 45567, 45589, 45627, 45638, 45674, 45754, 45789, 46087, 46153,\n",
      "        46241, 46253, 46304, 46403, 46414, 46581, 46696, 46807, 46912, 46944,\n",
      "        47316, 47470, 47653, 47746, 47871, 47892, 47894, 47992, 48013, 48112,\n",
      "        48305, 48377, 48479, 48614, 48664, 48752, 48763, 48999, 49026, 49100,\n",
      "        49143, 49153, 49533, 49615, 49792, 49945, 49969, 50158, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,207 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,705 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  153,   198,   251,   317,   650,   824,   829,   832,   878,   901,\n",
      "          919,   962,  1195,  1267,  1270,  1451,  1473,  1683,  1710,  1772,\n",
      "         1858,  1937,  2048,  2053,  2143,  2375,  2491,  2537,  2878,  2881,\n",
      "         2898,  2989,  3006,  3134,  3267,  3322,  3357,  3383,  3408,  3538,\n",
      "         3542,  3599,  3770,  3901,  3914,  3921,  4388,  4421,  4688,  4719,\n",
      "         4746,  4831,  5128,  5291,  5420,  5664,  5884,  5942,  6035,  6113,\n",
      "         6295,  6536,  6718,  6760,  6783,  6851,  6969,  6994,  7027,  7056,\n",
      "         7111,  7123,  7134,  7251,  7358,  7449,  7496,  7546,  7575,  7590,\n",
      "         7596,  8361,  8455,  8489,  8590,  8604,  8689,  8773,  8967,  9029,\n",
      "         9194,  9404,  9538,  9717,  9943,  9976, 10282, 10308, 10310, 10403,\n",
      "        10547, 10691, 10715, 10930, 10946, 11089, 11286, 11584, 11682, 11701,\n",
      "        11818, 11862, 11885, 11903, 12259, 12490, 12559, 12587, 12694, 12729,\n",
      "        12990, 13018, 13084, 13324, 13361, 13502, 13792, 13822, 13871, 13929,\n",
      "        13949, 14001, 14073, 14138, 14331, 14379, 14455, 14478, 14560, 14594,\n",
      "        14725, 14866, 14992, 15023, 15073, 15275, 15559, 15582, 15646, 15685,\n",
      "        15828, 16009, 16025, 16156, 16161, 16313, 16465, 16501, 16665, 16670,\n",
      "        16869, 16980, 16983, 17047, 17054, 17218, 17228, 17278, 17312, 17323,\n",
      "        17601, 17681, 17866, 17986, 18150, 18447, 18494, 18511, 18551, 18689,\n",
      "        18760, 18764, 18778, 18948, 18981, 19209, 19285, 19313, 19350, 19500,\n",
      "        19924, 20326, 20333, 20401, 20478, 20514, 20518, 20570, 20627, 20662,\n",
      "        20856, 20881, 20920, 21008, 21122, 21485, 21489, 21604, 21797, 22233,\n",
      "        22358, 22440, 22917, 22926, 22968, 23230, 23312, 23378, 23421, 23689,\n",
      "        23768, 23775, 23848, 24155, 24181, 24471, 24513, 24545, 24626, 24803,\n",
      "        25039, 25087, 25100, 25305, 25312, 25324, 25452, 25467, 25492, 25532,\n",
      "        25535, 25643, 25716, 25810, 25824, 25833, 26071, 26111, 26153, 26239,\n",
      "        26405, 26417, 26424, 26872, 27156, 27310, 27413, 27694, 27775, 27922,\n",
      "        27924, 27951, 28079, 28273, 28444, 28563, 28595, 28747, 28796, 29090,\n",
      "        29094, 29116, 29353, 29405, 29533, 29572, 29663, 29706, 29774, 30000,\n",
      "        30241, 30383, 30558, 30815, 30923, 30982, 31102, 31103, 31156, 31324,\n",
      "        31360, 31618, 31717, 31883, 32045, 32068, 32260, 32275, 32452, 33028,\n",
      "        33099, 33158, 33371, 33418, 33658, 33698, 33831, 33834, 34311, 34575,\n",
      "        34709, 34779, 34835, 34937, 34977, 35030, 35044, 35096, 35273, 35299,\n",
      "        35330, 35335, 35709, 35750, 35818, 35978, 36073, 36113, 36117, 36315,\n",
      "        36394, 36436, 36492, 36529, 36599, 36672, 36772, 36851, 36984, 37342,\n",
      "        37376, 37405, 37515, 37653, 37717, 37914, 37984, 38069, 38141, 38487,\n",
      "        38687, 38754, 38839, 38977, 39027, 39153, 39178, 39204, 39230, 39329,\n",
      "        39426, 39427, 39908, 39984, 39995, 40037, 40172, 40388, 40396, 40426,\n",
      "        40790, 40817, 41105, 41171, 41173, 41181, 41406, 41477, 41484, 41550,\n",
      "        41572, 41598, 41843, 41936, 41989, 41990, 42220, 42228, 42252, 42283,\n",
      "        42487, 42734, 42796, 42798, 42816, 42834, 42993, 43036, 43289, 43389,\n",
      "        43469, 43699, 43826, 43934, 43969, 43984, 44193, 44360, 44678, 44831,\n",
      "        44884, 45336, 45558, 45560, 45638, 45657, 45674, 45760, 45802, 45938,\n",
      "        45975, 46040, 46304, 46601, 46767, 46782, 47102, 47264, 47387, 47447,\n",
      "        47560, 47667, 47733, 47821, 47928, 48013, 48259, 48282, 48312, 48364,\n",
      "        48537, 48752, 48930, 48953, 49355, 49406, 49467, 49655, 49673, 49716,\n",
      "        49798, 50118]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,711 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,693 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   369,   784,   788,   821,   847,   869,   901,   962,   998,\n",
      "         1020,  1025,  1147,  1179,  1193,  1201,  1267,  1299,  1370,  1408,\n",
      "         1433,  1508,  1563,  1729,  1783,  1903,  2173,  2288,  2300,  2334,\n",
      "         2566,  2621,  2690,  2878,  2898,  2950,  3134,  3396,  3478,  3631,\n",
      "         3652,  3726,  3757,  3925,  3976,  4096,  4157,  4268,  4367,  4388,\n",
      "         4421,  4423,  4624,  4704,  4898,  5227,  5720,  5831,  5933,  5942,\n",
      "         5994,  6035,  6222,  6229,  6395,  6830,  6921,  6922,  6980,  6994,\n",
      "         7025,  7027,  7071,  7134,  7424,  7568,  7623,  7648,  7705,  8125,\n",
      "         8206,  8442,  8489,  8535,  8967,  9082,  9145,  9194,  9260,  9791,\n",
      "        10043, 10163, 10180, 10328, 10511, 10685, 10801, 10912, 10969, 11286,\n",
      "        11412, 11481, 11626, 11646, 11748, 11769, 11851, 12093, 12374, 13011,\n",
      "        13018, 13128, 13154, 13216, 13415, 13470, 13509, 13518, 13562, 13577,\n",
      "        13581, 13740, 13982, 14001, 14043, 14192, 14280, 14379, 14533, 14730,\n",
      "        14874, 14921, 15222, 15313, 15323, 15350, 15582, 15636, 15670, 15712,\n",
      "        15976, 15982, 16098, 16457, 16665, 16699, 16742, 16754, 16798, 17062,\n",
      "        17108, 17178, 17312, 17381, 17798, 17852, 17871, 17918, 17963, 18074,\n",
      "        18091, 18127, 18133, 18184, 18347, 18364, 18869, 18895, 18981, 18997,\n",
      "        19009, 19021, 19072, 19140, 19165, 19171, 19189, 19215, 19296, 19306,\n",
      "        19386, 19393, 19762, 19916, 19938, 19988, 20048, 20214, 20319, 20468,\n",
      "        20478, 20491, 21086, 21093, 21253, 21287, 21550, 21932, 22193, 22317,\n",
      "        22358, 22540, 22633, 22644, 22879, 23253, 23378, 23405, 23412, 23456,\n",
      "        23627, 23903, 24030, 24065, 24208, 24300, 24306, 24454, 24513, 24718,\n",
      "        24798, 24940, 25039, 25492, 25496, 25564, 25613, 25810, 26034, 26102,\n",
      "        26111, 26248, 26362, 26405, 26670, 26680, 26727, 26872, 26930, 27089,\n",
      "        27162, 27262, 27413, 27765, 27923, 28212, 28337, 28408, 28496, 28563,\n",
      "        28821, 28918, 29094, 29353, 29550, 29559, 29594, 29657, 29869, 30017,\n",
      "        30099, 30139, 30202, 30260, 30483, 30631, 30915, 30946, 31021, 31103,\n",
      "        31107, 31288, 31340, 31360, 31561, 31575, 31851, 31975, 32123, 32168,\n",
      "        32228, 32263, 32443, 32452, 32619, 32860, 33150, 33158, 33562, 33599,\n",
      "        33610, 33687, 33698, 33736, 33740, 34227, 34278, 34591, 34627, 34805,\n",
      "        34811, 34911, 34937, 35030, 35177, 35202, 35290, 35316, 35385, 35466,\n",
      "        35494, 35589, 35867, 35902, 35997, 36222, 36315, 36377, 36509, 36873,\n",
      "        37053, 37117, 37294, 37342, 37344, 37464, 37536, 37608, 37810, 37864,\n",
      "        37994, 38079, 38128, 38248, 38304, 38336, 38356, 38510, 38591, 38985,\n",
      "        39057, 39177, 39427, 39538, 39789, 39797, 39801, 39805, 39907, 39984,\n",
      "        39995, 40009, 40028, 40108, 40198, 40223, 40401, 40549, 40600, 40740,\n",
      "        40755, 40869, 40952, 40976, 41148, 41199, 41274, 41364, 41572, 41632,\n",
      "        41672, 41973, 41990, 42184, 42341, 42470, 42522, 42540, 42598, 42656,\n",
      "        42798, 42851, 43130, 43289, 43648, 43815, 43895, 43988, 44205, 44265,\n",
      "        44333, 44427, 44589, 44598, 44707, 44738, 44978, 45002, 45017, 45041,\n",
      "        45186, 45384, 45479, 45529, 45564, 45627, 45642, 45650, 45674, 45677,\n",
      "        45739, 45752, 45967, 46259, 46408, 46636, 46811, 46969, 47264, 47333,\n",
      "        47392, 47558, 47571, 47746, 47916, 47986, 48013, 48285, 48437, 48622,\n",
      "        48805, 48822, 48877, 48949, 49026, 49153, 49261, 49716, 49720, 49747,\n",
      "        49798, 49923, 49945, 50040, 50243]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,698 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,603 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   54,    96,   135,   198,   317,   343,   590,   694,   788,   824,\n",
      "          829,   879,   933,   962,  1010,  1020,  1113,  1267,  1416,  1620,\n",
      "         1645,  1656,  1733,  1926,  2334,  2451,  2616,  2872,  2874,  2889,\n",
      "         3063,  3073,  3298,  3322,  3478,  3510,  3534,  3546,  3629,  3726,\n",
      "         3759,  3770,  3929,  4062,  4283,  4294,  4422,  4641,  4671,  4716,\n",
      "         4719,  4795,  4930,  5065,  5359,  5476,  5533,  5578,  5720,  5748,\n",
      "         6013,  6087,  6336,  6760,  6783,  6810,  6932,  6968,  7027,  7134,\n",
      "         7160,  7226,  7552,  7568,  7672,  7943,  8035,  8489,  8978,  9260,\n",
      "         9388,  9439,  9538,  9674,  9717,  9746,  9870,  9912, 10282, 10503,\n",
      "        10573, 10607, 11182, 11265, 11481, 11544, 11584, 11588, 11647, 11748,\n",
      "        11769, 11780, 11793, 11870, 11872, 11885, 11974, 12346, 12407, 12489,\n",
      "        12587, 12703, 12723, 13444, 13451, 13460, 13502, 13656, 13684, 13725,\n",
      "        13807, 13822, 13939, 14073, 14105, 14186, 14268, 14614, 14725, 14836,\n",
      "        14992, 15073, 15222, 15350, 15501, 15609, 15872, 16031, 16067, 16457,\n",
      "        16634, 16665, 16722, 16941, 16992, 16998, 17013, 17098, 17210, 17228,\n",
      "        17233, 17247, 17323, 17452, 17575, 17913, 17945, 18356, 18556, 18577,\n",
      "        18659, 18699, 18758, 18789, 18839, 18948, 19064, 19145, 19402, 19480,\n",
      "        19485, 19601, 19751, 19762, 19949, 19990, 20048, 20060, 20326, 20478,\n",
      "        20627, 20751, 20893, 20920, 21020, 21079, 21117, 21246, 21253, 21404,\n",
      "        21420, 21459, 21544, 21567, 21714, 21723, 21819, 21864, 21910, 22080,\n",
      "        22089, 22317, 22451, 22568, 22633, 22880, 23194, 23316, 23412, 23678,\n",
      "        23689, 23695, 23798, 23821, 23868, 23982, 24065, 24431, 24513, 24550,\n",
      "        24666, 24798, 24872, 24917, 24991, 25089, 25100, 25452, 25492, 25546,\n",
      "        25595, 25734, 25755, 25810, 26119, 26153, 26343, 26476, 26493, 26660,\n",
      "        26920, 27085, 27277, 27319, 27775, 28003, 28252, 28256, 28563, 28583,\n",
      "        28821, 29127, 29405, 29440, 29502, 29565, 29594, 29908, 30000, 30241,\n",
      "        30260, 30315, 30381, 30558, 30571, 30641, 30716, 30812, 30837, 30886,\n",
      "        31103, 31288, 31353, 31618, 31705, 31851, 31974, 32228, 32262, 32443,\n",
      "        32650, 32652, 32674, 32876, 33174, 33216, 33520, 33687, 33769, 33780,\n",
      "        33905, 34004, 34051, 34213, 34373, 34433, 34586, 34604, 34706, 34761,\n",
      "        34766, 34801, 34895, 34899, 35044, 35316, 35335, 35466, 35494, 35559,\n",
      "        35589, 35749, 35997, 36017, 36110, 36418, 36439, 36453, 36486, 36492,\n",
      "        36570, 36673, 36694, 36857, 36998, 37061, 37116, 37163, 37282, 37536,\n",
      "        37637, 37653, 37835, 37887, 37898, 37914, 37988, 37994, 38118, 38125,\n",
      "        38128, 38131, 38139, 38244, 38461, 38518, 38520, 38640, 38687, 38819,\n",
      "        38839, 38889, 39067, 39074, 39109, 39189, 39258, 39311, 39315, 39427,\n",
      "        39775, 39984, 40062, 40300, 40323, 40388, 40401, 40604, 40783, 40817,\n",
      "        40828, 40976, 41265, 41442, 41762, 41906, 41957, 41962, 42120, 42259,\n",
      "        42298, 42413, 42457, 42834, 42845, 43058, 43295, 43298, 43320, 43815,\n",
      "        43826, 43851, 43863, 43895, 43910, 43960, 44024, 44265, 44427, 44477,\n",
      "        44598, 44738, 44796, 44856, 44970, 45186, 45384, 45389, 45477, 45479,\n",
      "        45558, 45599, 45805, 45829, 45831, 46304, 46345, 46355, 46420, 46807,\n",
      "        47130, 47227, 47287, 47335, 47698, 47894, 47990, 48000, 48119, 48212,\n",
      "        48215, 48312, 48479, 48629, 48659, 48789, 49100, 49119, 49214, 49655,\n",
      "        49792, 49798, 49813, 49823, 50087, 50216]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,609 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,214 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   590,   824,  1073,  1074,  1096,  1147,  1179,  1594,  1800,\n",
      "         1880,  2139,  2244,  2300,  2451,  2552,  2690,  2898,  3063,  3101,\n",
      "         3322,  3363,  3403,  3564,  3599,  3756,  3770,  3834,  3889,  4206,\n",
      "         4283,  4367,  4628,  4804,  5097,  5209,  5348,  5476,  5528,  5578,\n",
      "         5630,  5675,  5755,  5892,  5923,  5957,  5965,  6011,  6035,  6051,\n",
      "         6060,  6229,  6442,  6549,  6578,  6604,  6684,  6895,  6994,  7027,\n",
      "         7054,  7118,  7251,  7598,  7623,  7962,  8024,  8137,  8149,  8334,\n",
      "         8482,  8532,  8593,  8789,  9067,  9071,  9232,  9369,  9814,  9912,\n",
      "        10193, 10266, 10282, 10360, 10362, 10458, 10668, 10687, 10794, 10839,\n",
      "        10958, 10970, 11244, 11268, 11609, 11723, 11941, 12046, 12203, 12307,\n",
      "        12481, 12694, 12866, 13011, 13012, 13154, 13172, 13361, 13380, 13431,\n",
      "        13527, 13528, 13562, 13699, 13713, 13733, 13939, 14001, 14270, 14532,\n",
      "        14725, 14750, 14842, 14866, 14988, 15323, 15353, 15396, 15540, 15636,\n",
      "        15651, 15712, 15782, 15794, 15796, 15863, 16031, 16099, 16450, 16495,\n",
      "        16541, 16665, 16666, 16989, 17218, 17356, 17464, 17535, 17575, 17759,\n",
      "        17875, 17906, 17986, 18145, 18307, 18316, 18333, 18869, 18895, 18926,\n",
      "        18948, 18997, 19115, 19337, 19485, 19856, 19921, 19988, 20048, 20060,\n",
      "        20223, 20257, 20305, 20468, 20478, 20481, 20483, 20517, 20518, 20575,\n",
      "        20627, 20746, 20881, 20983, 21121, 21246, 21273, 21301, 21308, 21404,\n",
      "        21604, 21753, 21925, 21980, 22025, 22049, 22080, 22233, 22274, 22346,\n",
      "        22372, 22566, 22591, 22633, 22657, 22798, 22858, 23121, 23128, 23162,\n",
      "        23201, 23253, 23449, 23678, 23690, 23852, 23938, 23965, 24216, 24275,\n",
      "        24335, 24345, 24396, 24422, 24479, 24826, 25039, 25168, 25412, 25532,\n",
      "        25622, 25631, 25679, 25716, 25810, 25839, 25939, 25977, 26008, 26119,\n",
      "        26322, 26517, 26657, 26694, 26748, 26843, 27020, 27022, 27413, 27497,\n",
      "        27644, 27775, 28058, 28068, 28131, 28328, 28334, 28335, 28337, 28563,\n",
      "        28617, 28669, 28682, 28740, 29124, 29150, 29305, 29350, 29353, 29533,\n",
      "        29558, 29594, 29783, 29847, 30035, 30241, 30381, 30449, 30458, 30468,\n",
      "        30741, 30947, 31103, 31107, 31145, 31185, 31239, 31264, 31322, 31328,\n",
      "        31360, 31430, 31534, 31822, 31851, 31922, 32147, 32532, 32739, 33028,\n",
      "        33032, 33056, 33061, 33093, 33174, 33222, 33293, 33346, 33914, 34004,\n",
      "        34268, 34311, 34575, 34805, 34937, 35082, 35097, 35566, 35589, 36011,\n",
      "        36073, 36116, 36248, 36401, 36492, 36582, 36599, 36673, 36786, 36865,\n",
      "        36873, 36977, 37059, 37294, 37308, 37612, 38064, 38141, 38150, 38322,\n",
      "        38385, 38436, 39027, 39067, 39107, 39426, 39514, 39653, 39808, 39826,\n",
      "        39856, 39901, 39908, 39984, 40014, 40201, 40219, 40426, 40557, 40599,\n",
      "        40693, 40752, 40755, 40817, 41112, 41318, 41472, 41505, 41550, 41572,\n",
      "        41630, 41632, 41762, 42119, 42120, 42220, 42298, 42452, 42470, 42528,\n",
      "        42554, 42614, 42760, 42790, 42821, 42851, 42880, 42967, 43079, 43136,\n",
      "        43289, 43391, 43499, 43554, 43643, 43766, 43815, 43851, 43930, 43956,\n",
      "        43984, 44123, 44130, 44153, 44214, 44402, 44427, 44459, 44562, 44600,\n",
      "        44678, 44711, 44978, 45041, 45134, 45336, 45529, 45627, 45752, 45760,\n",
      "        45877, 45896, 46046, 46153, 46251, 46960, 47145, 47172, 47277, 47675,\n",
      "        47745, 47803, 47851, 47856, 48013, 48171, 48215, 48230, 48479, 48528,\n",
      "        48529, 48622, 49026, 49100, 49129, 49312, 49355, 49425, 49449, 49453,\n",
      "        49486, 49602, 49615, 49732, 49744, 49765, 49900, 49910, 49923, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,293 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 4/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,601 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   259,   285,   317,   406,   521,   653,   664,   829,   901,\n",
      "          962,   972,  1010,  1248,  1267,  1508,  1616,  1774,  1880,  2005,\n",
      "         2173,  2207,  2235,  2300,  2454,  2485,  2653,  2898,  2950,  2956,\n",
      "         3061,  3355,  3383,  3480,  3534,  3569,  3776,  3889,  4157,  4294,\n",
      "         4388,  4400,  4429,  4491,  4624,  4641,  4712,  4725,  4804,  4930,\n",
      "         5195,  5291,  5305,  5371,  5399,  5443,  5507,  5630,  5905,  5960,\n",
      "         5965,  6011,  6035,  6175,  6274,  6442,  6480,  6557,  6596,  6760,\n",
      "         6775,  7027,  7118,  7251,  7324,  7450,  8188,  8248,  8334,  8345,\n",
      "         8347,  8354,  8569,  8705,  8744,  8804,  8854,  8892,  9153,  9209,\n",
      "         9323,  9328,  9433,  9452,  9589,  9906,  9918, 10010, 10076, 10282,\n",
      "        10328, 10379, 10404, 11015, 11087, 11244, 11515, 11557, 11588, 11796,\n",
      "        11872, 11911, 11968, 12282, 12723, 12782, 12941, 13030, 13084, 13154,\n",
      "        13169, 13178, 13415, 13455, 13562, 13586, 13611, 13733, 13851, 13903,\n",
      "        14001, 14043, 14335, 14532, 14563, 14741, 14794, 14850, 14855, 14996,\n",
      "        15203, 15302, 15360, 15382, 15441, 15470, 15582, 15636, 16031, 16098,\n",
      "        16148, 16392, 16454, 16501, 16600, 16624, 16891, 16918, 17005, 17062,\n",
      "        17067, 17189, 17249, 17323, 17647, 17918, 17986, 18054, 18386, 18411,\n",
      "        18795, 18840, 18895, 18926, 18948, 18958, 19047, 19313, 19332, 19402,\n",
      "        19485, 19637, 19716, 19745, 19890, 20060, 20128, 20199, 20468, 20514,\n",
      "        20662, 20777, 20817, 20966, 21089, 21206, 21239, 21273, 21400, 21457,\n",
      "        21714, 21910, 22043, 22358, 22431, 22553, 22591, 22633, 22968, 23014,\n",
      "        23214, 23230, 23316, 23412, 23647, 23689, 23769, 23815, 23962, 24065,\n",
      "        24545, 24623, 24654, 25275, 25412, 25622, 25716, 25800, 25810, 26093,\n",
      "        26248, 26261, 26282, 26523, 26560, 26752, 26814, 26850, 26944, 27122,\n",
      "        27270, 27331, 27444, 27445, 27488, 27924, 28014, 28079, 28153, 28221,\n",
      "        28393, 28561, 28563, 28566, 28682, 28699, 28918, 29006, 29094, 29116,\n",
      "        29141, 29353, 29405, 29744, 29884, 29888, 30465, 30468, 30565, 30757,\n",
      "        30816, 30921, 30946, 31040, 31103, 31107, 31200, 31360, 31427, 31621,\n",
      "        31713, 32228, 32350, 32409, 32443, 32452, 32699, 32889, 33020, 33032,\n",
      "        33099, 33286, 33409, 33479, 33569, 33727, 33889, 33893, 34051, 34130,\n",
      "        34278, 34439, 34581, 34767, 35030, 35096, 35290, 35341, 35410, 35748,\n",
      "        35805, 35814, 35866, 36116, 36117, 36173, 36241, 36315, 36377, 36492,\n",
      "        36582, 36599, 36665, 36814, 37218, 37994, 38060, 38067, 38089, 38118,\n",
      "        38141, 38356, 38487, 38516, 38645, 38687, 38701, 38852, 39073, 39128,\n",
      "        39177, 39291, 39426, 39480, 39495, 39877, 39931, 39984, 40198, 40237,\n",
      "        40249, 40430, 40580, 40676, 40686, 40693, 40772, 40782, 40817, 40976,\n",
      "        41308, 41414, 41632, 41736, 41966, 42184, 42254, 42334, 42418, 42470,\n",
      "        42798, 42845, 43079, 43130, 43142, 43232, 43298, 43377, 43444, 43481,\n",
      "        43489, 43699, 43805, 43814, 43986, 44156, 44290, 44402, 44580, 44678,\n",
      "        44738, 44992, 45024, 45069, 45260, 45291, 45456, 45569, 45772, 45999,\n",
      "        46161, 46420, 46434, 46577, 46610, 46776, 46876, 46885, 46951, 46954,\n",
      "        47113, 47227, 47254, 47335, 47449, 47629, 47667, 47678, 47697, 47698,\n",
      "        47733, 47845, 47847, 47986, 48013, 48134, 48306, 48307, 48350, 48395,\n",
      "        48442, 48489, 48644, 48664, 48929, 48962, 48999, 49100, 49190, 49272,\n",
      "        49355, 49462, 49630, 49674, 49765, 49792, 49875, 49969, 50095, 50116]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,605 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 5/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,793 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   31,   153,   198,   211,   227,   317,   371,   433,   481,   486,\n",
      "          527,   589,   633,   857,   879,   880,   962,  1020,  1025,  1074,\n",
      "         1408,  1422,  1428,  1861,  2019,  2138,  2266,  2280,  2300,  2334,\n",
      "         2379,  2451,  2743,  2897,  2950,  3182,  3383,  3599,  3757,  3770,\n",
      "         4012,  4157,  4162,  4388,  4459,  4491,  4514,  4628,  4858,  4930,\n",
      "         4971,  5020,  5109,  5119,  5134,  5209,  5373,  5578,  5675,  5758,\n",
      "         5929,  6458,  6596,  6657,  6687,  6717,  6760,  6783,  6868,  6950,\n",
      "         6968,  6975,  6983,  7038,  7056,  7111,  7118,  7134,  7324,  7546,\n",
      "         8095,  8125,  8188,  8250,  8277,  8334,  8609,  8684,  8906,  9035,\n",
      "         9145,  9323,  9511,  9621,  9672,  9855,  9901,  9959, 10085, 10180,\n",
      "        10404, 10460, 10804, 10993, 11430, 11530, 11588, 11654, 11717, 12025,\n",
      "        12282, 12307, 12539, 12582, 12587, 12680, 12694, 12889, 13012, 13084,\n",
      "        13168, 13172, 13224, 13448, 13509, 13523, 13562, 13611, 13628, 13822,\n",
      "        13927, 14001, 14041, 14043, 14532, 14554, 14603, 14666, 14725, 14866,\n",
      "        14919, 15073, 15135, 15222, 15472, 15501, 15630, 15860, 15894, 15976,\n",
      "        16084, 16180, 16818, 16880, 16937, 17047, 17164, 17278, 17320, 17842,\n",
      "        17906, 18032, 18237, 18283, 18336, 18831, 18869, 18948, 19148, 19171,\n",
      "        19215, 19337, 19369, 19371, 19559, 19716, 19745, 20326, 20478, 20512,\n",
      "        20518, 20627, 20757, 20785, 20792, 20881, 20900, 21105, 21121, 21184,\n",
      "        21266, 21404, 21420, 21457, 21637, 21800, 21910, 21985, 22136, 22319,\n",
      "        22393, 22568, 22657, 22709, 22954, 23128, 23412, 23628, 24065, 24479,\n",
      "        24798, 24935, 24999, 25100, 25256, 25528, 25532, 25562, 25716, 25810,\n",
      "        25833, 25939, 26146, 26153, 26381, 26413, 26537, 26551, 26645, 26670,\n",
      "        26852, 27115, 27123, 27211, 27496, 27647, 27946, 28011, 28056, 28099,\n",
      "        28121, 28165, 28237, 28388, 28561, 28769, 29116, 29130, 29224, 29292,\n",
      "        29445, 29488, 29558, 29565, 29594, 29657, 29717, 29743, 30025, 30026,\n",
      "        30241, 30301, 30327, 30449, 30621, 30658, 30797, 30812, 30852, 31261,\n",
      "        31264, 31328, 31394, 31519, 31682, 31824, 31862, 31887, 32111, 32371,\n",
      "        32635, 32690, 32717, 32754, 32843, 33174, 33187, 33376, 33549, 33594,\n",
      "        33660, 34236, 34304, 34398, 34575, 34662, 34761, 34805, 34929, 35065,\n",
      "        35166, 35290, 35335, 35417, 35418, 35420, 35466, 35515, 35618, 36241,\n",
      "        36250, 36377, 36492, 36529, 36570, 36673, 36795, 37053, 37198, 37294,\n",
      "        37324, 37342, 37345, 37405, 37464, 37468, 37540, 37691, 37776, 37988,\n",
      "        38064, 38089, 38114, 38128, 38209, 38289, 38336, 38346, 38356, 38376,\n",
      "        38416, 38518, 38687, 38739, 38764, 38777, 39047, 39109, 39119, 39250,\n",
      "        39296, 39384, 39495, 39750, 39801, 39805, 39812, 40009, 40037, 40138,\n",
      "        40568, 40693, 40772, 40795, 40952, 40955, 40982, 41062, 41297, 41374,\n",
      "        41463, 41472, 41511, 41550, 41669, 41776, 41892, 42120, 42199, 42295,\n",
      "        42298, 42457, 42776, 42845, 42928, 42993, 43006, 43022, 43045, 43047,\n",
      "        43173, 43243, 43286, 43289, 43469, 43605, 43689, 43702, 43815, 43826,\n",
      "        44065, 44176, 44245, 44310, 44362, 44427, 44540, 44543, 44598, 44678,\n",
      "        44849, 45002, 45260, 45384, 45423, 45441, 45585, 45605, 45627, 45657,\n",
      "        45802, 45837, 45959, 46241, 46314, 46324, 46367, 46407, 46680, 46952,\n",
      "        47243, 47448, 47629, 47706, 47866, 48013, 48364, 48523, 48622, 48624,\n",
      "        48823, 48880, 48929, 49026, 49178, 49235, 49256, 49486, 49792, 49811,\n",
      "        49963, 49969, 50144, 50166, 50236]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,797 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 6/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,895 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,    47,   198,   208,   370,   476,   486,   518,   563,   590,\n",
      "          761,   801,   854,   879,   901,   962,  1005,  1077,  1127,  1264,\n",
      "         1267,  1533,  1563,  1772,  1860,  1880,  1887,  1992,  2055,  2127,\n",
      "         2457,  2660,  2690,  2694,  2832,  2874,  2950,  3196,  3322,  3341,\n",
      "         3355,  3363,  3444,  3510,  3593,  3665,  3770,  3843,  3877,  4223,\n",
      "         4256,  4328,  4333,  4367,  4388,  4491,  4620,  4624,  4641,  4692,\n",
      "         4740,  4831,  5072,  5276,  5320,  5373,  5385,  5399,  5456,  5630,\n",
      "         5905,  6229,  6760,  6783,  6830,  6994,  7027,  7111,  7134,  7256,\n",
      "         7330,  7344,  7553,  7648,  8124,  8555,  8590,  8609,  8648,  8689,\n",
      "         8874,  8915,  9190,  9194,  9323,  9352,  9369,  9450,  9694,  9795,\n",
      "         9855,  9901,  9925, 10403, 10435, 10669, 10691, 10719, 10769, 10858,\n",
      "        10946, 11089, 11143, 11286, 11872, 11885, 12207, 12283, 12490, 12673,\n",
      "        12739, 12955, 13027, 13283, 13346, 13548, 13618, 13635, 13695, 13822,\n",
      "        14013, 14064, 14073, 14280, 14464, 14532, 14643, 14725, 14988, 15222,\n",
      "        15244, 15314, 15559, 15636, 15737, 15794, 15860, 16035, 16081, 16410,\n",
      "        16614, 16716, 16998, 17046, 17067, 17101, 17151, 17191, 17218, 17278,\n",
      "        17575, 17654, 17759, 17829, 17852, 17875, 17883, 17913, 17928, 17986,\n",
      "        18032, 18237, 18307, 18331, 18347, 18731, 18849, 18895, 18948, 19072,\n",
      "        19215, 19274, 19313, 19374, 19384, 19409, 19524, 19548, 19593, 19813,\n",
      "        19916, 20017, 20048, 20060, 20303, 20360, 20393, 20402, 20518, 20627,\n",
      "        20706, 20882, 21103, 21117, 21239, 21345, 21395, 21457, 21496, 21824,\n",
      "        21841, 22182, 22233, 22633, 23194, 23230, 23320, 23347, 24275, 24349,\n",
      "        24431, 24872, 24991, 24999, 25306, 25357, 25468, 25535, 25612, 25677,\n",
      "        25716, 25810, 25904, 25934, 25939, 26008, 26111, 26362, 26405, 26471,\n",
      "        26493, 26500, 26670, 26791, 26890, 26944, 27037, 27331, 27479, 27488,\n",
      "        27496, 27498, 27576, 27775, 27898, 28252, 28264, 28332, 28379, 28489,\n",
      "        28491, 28563, 28682, 28844, 28919, 29094, 29116, 29233, 29313, 29353,\n",
      "        29485, 29488, 29509, 29550, 29678, 29774, 29869, 29894, 29984, 29988,\n",
      "        30000, 30041, 30103, 30274, 30808, 30858, 30946, 31040, 31103, 31433,\n",
      "        31547, 31676, 31686, 31815, 32068, 32266, 32324, 32443, 32588, 32591,\n",
      "        32650, 32860, 32900, 33032, 33187, 33199, 33475, 33827, 34064, 34150,\n",
      "        34171, 34278, 34311, 34507, 34575, 34825, 34937, 34951, 35030, 35220,\n",
      "        35335, 35631, 35709, 35723, 35790, 35814, 35929, 35962, 36082, 36116,\n",
      "        36117, 36255, 36259, 36599, 36673, 36779, 37053, 37344, 37405, 37536,\n",
      "        37568, 37588, 37994, 38128, 38172, 38210, 38253, 38438, 38516, 38585,\n",
      "        38586, 38598, 38839, 38883, 38903, 39027, 39081, 39311, 39348, 39426,\n",
      "        39495, 39622, 39696, 39789, 39826, 39984, 40198, 40388, 40422, 40676,\n",
      "        40693, 40817, 40907, 40911, 40960, 40976, 41435, 41572, 41681, 41863,\n",
      "        41962, 42350, 42550, 42635, 42702, 42784, 42821, 42834, 42919, 42991,\n",
      "        43045, 43173, 43513, 43664, 43699, 43788, 43805, 44105, 44214, 44427,\n",
      "        44738, 44953, 44976, 45024, 45327, 45372, 45585, 45752, 45764, 45849,\n",
      "        46241, 46304, 46678, 47110, 47200, 47264, 47821, 47913, 47936, 47951,\n",
      "        48000, 48013, 48108, 48309, 48479, 48523, 48612, 48664, 48935, 49026,\n",
      "        49045, 49085, 49091, 49105, 49715, 49783, 50156, 50160, 50195, 50232]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,899 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 7/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,707 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  107,   198,   220,   230,   608,   821,   879,   901,   948,   962,\n",
      "         1108,  1248,  1267,  1668,  1714,  1760,  1858,  1880,  1887,  1928,\n",
      "         1964,  2027,  2152,  2173,  2199,  2288,  2483,  2690,  2804,  2877,\n",
      "         2878,  2894,  2966,  2989,  3101,  3582,  3599,  3708,  3741,  3809,\n",
      "         3829,  4096,  4115,  4549,  4584,  4590,  4746,  4804,  5053,  5369,\n",
      "         5496,  5531,  5559,  5578,  5588,  5675,  5922,  6011,  6175,  6229,\n",
      "         6267,  6363,  6512,  6515,  6619,  6717,  6760,  6783,  6886,  6968,\n",
      "         6975,  6994,  7009,  7056,  7118,  7130,  7251,  7588,  7623,  7948,\n",
      "         8057,  8073,  8193,  8206,  8334,  8354,  8482,  8554,  8609,  8853,\n",
      "         9029,  9145,  9153,  9250,  9323,  9328,  9538,  9642,  9700,  9746,\n",
      "         9747, 10017, 10212, 10289, 10824, 11481, 11584, 11667, 11769, 11811,\n",
      "        11818, 11858, 11886, 12207, 12255, 12402, 12582, 12694, 12716, 12729,\n",
      "        12905, 12913, 13028, 13154, 13201, 13223, 13270, 13355, 13444, 13576,\n",
      "        13611, 13635, 13676, 13733, 13939, 14073, 14243, 14383, 14541, 14606,\n",
      "        14685, 14794, 14842, 14988, 14992, 15023, 15275, 15768, 15826, 15889,\n",
      "        15965, 15989, 16009, 16030, 16131, 16180, 16384, 16504, 16746, 16853,\n",
      "        17053, 17100, 17323, 17535, 17566, 17654, 17687, 17852, 18113, 18142,\n",
      "        18221, 18313, 18671, 18706, 18926, 18948, 19296, 19350, 19365, 19385,\n",
      "        19398, 19485, 19587, 19684, 19875, 19916, 19924, 19988, 20222, 20281,\n",
      "        20326, 20424, 20468, 20517, 20518, 20674, 20751, 20785, 20856, 20881,\n",
      "        21020, 21027, 21121, 21185, 21287, 21420, 21461, 21485, 21710, 21723,\n",
      "        21783, 21835, 22078, 22136, 22393, 22568, 22633, 22879, 22968, 23012,\n",
      "        23013, 23107, 23201, 23278, 23344, 23689, 23893, 24065, 24154, 24398,\n",
      "        24479, 24536, 24550, 24798, 25053, 25085, 25268, 25288, 25412, 25447,\n",
      "        25529, 25672, 25716, 25810, 26034, 26143, 26322, 26346, 26657, 26670,\n",
      "        26727, 26906, 27775, 27924, 28235, 28332, 28444, 28496, 28563, 28682,\n",
      "        28858, 28913, 28993, 29103, 29353, 29572, 29594, 29663, 30103, 30468,\n",
      "        30534, 30558, 30621, 30677, 30682, 30698, 30893, 31011, 31103, 31561,\n",
      "        31623, 31686, 31789, 32123, 32161, 32228, 32443, 32455, 32532, 32614,\n",
      "        32666, 32674, 32731, 33040, 33099, 33112, 33187, 33300, 33403, 33460,\n",
      "        33548, 33563, 33631, 33670, 33736, 33871, 34278, 34311, 34356, 34591,\n",
      "        34694, 34766, 34977, 34979, 35030, 35150, 35220, 35335, 35492, 35541,\n",
      "        35559, 35696, 35723, 35805, 35806, 35939, 36222, 36315, 36377, 36492,\n",
      "        36599, 36673, 36772, 37053, 37116, 37151, 37233, 37282, 37294, 37308,\n",
      "        37436, 37612, 37688, 37717, 37914, 38067, 38118, 38128, 38251, 38385,\n",
      "        38522, 38568, 38591, 38687, 39291, 39327, 39427, 39696, 39700, 39801,\n",
      "        39812, 39824, 39881, 39960, 40033, 40062, 40105, 40325, 40355, 40507,\n",
      "        40538, 40693, 40772, 40803, 40828, 41081, 41111, 41151, 41279, 41454,\n",
      "        41681, 41831, 42204, 42220, 42298, 42413, 42546, 42661, 42747, 42784,\n",
      "        42798, 42882, 43130, 43289, 43292, 43463, 43469, 43489, 43814, 43815,\n",
      "        43895, 43907, 43949, 44049, 44214, 44427, 44463, 44477, 44543, 44738,\n",
      "        44932, 45038, 45159, 45186, 45599, 45653, 45985, 46017, 46101, 46153,\n",
      "        46188, 46347, 46417, 46553, 46579, 46602, 46647, 46685, 46696, 46811,\n",
      "        46969, 47070, 47447, 47534, 47542, 47560, 47629, 47667, 47916, 47933,\n",
      "        48023, 48026, 48032, 48112, 48479, 48715, 48763, 48774, 48933, 48985,\n",
      "        49039, 49301, 49355, 49629, 49744, 49923, 49945, 50008, 50086, 50161]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,712 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 8/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,803 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,   122,   137,   153,   198,   415,   433,   589,   650,   655,\n",
      "          901,   919,  1025,  1030,  1044,  1074,  1187,  1267,  1418,  1599,\n",
      "         1641,  1714,  1903,  2138,  2300,  2334,  2451,  2470,  2566,  2690,\n",
      "         2890,  2905,  3006,  3478,  3652,  3708,  3741,  3757,  3770,  3793,\n",
      "         3925,  4160,  4430,  4491,  4611,  4730,  4804,  4818,  5482,  5507,\n",
      "         5578,  5675,  5708,  5785,  5809,  5957,  6317,  6456,  6619,  6760,\n",
      "         6907,  6968,  7027,  7056,  7160,  7334,  7369,  7568,  7583,  7623,\n",
      "         7712,  7739,  7753,  8082,  8216,  8248,  8334,  8347,  8832,  9145,\n",
      "         9155,  9167,  9194,  9260,  9323,  9538,  9541,  9627,  9683,  9943,\n",
      "        10060, 10076, 10180, 10282, 10404, 10622, 10719, 10738, 10800, 11286,\n",
      "        11788, 11796, 11872, 11885, 11970, 12046, 12307, 12396, 12476, 12514,\n",
      "        12582, 12641, 12729, 13158, 13172, 13173, 13204, 13346, 13364, 13415,\n",
      "        13542, 13650, 13939, 14001, 14073, 14243, 14268, 14725, 14730, 14842,\n",
      "        14855, 14879, 14988, 15169, 15222, 15226, 15275, 15636, 15737, 16040,\n",
      "        16180, 16457, 16504, 16600, 16631, 16877, 16918, 17098, 17256, 17320,\n",
      "        17323, 17416, 17483, 17558, 17842, 17866, 17949, 17951, 18092, 18375,\n",
      "        18681, 18869, 18948, 18979, 18989, 19044, 19277, 19296, 19645, 19687,\n",
      "        19746, 20060, 20202, 20257, 20303, 20457, 20468, 20478, 20517, 20706,\n",
      "        20741, 20881, 20893, 20998, 21155, 21400, 21472, 21481, 21614, 21636,\n",
      "        21800, 21910, 21998, 22136, 22317, 22442, 22591, 22633, 22657, 23024,\n",
      "        23087, 23102, 23201, 23297, 23347, 23428, 23530, 23553, 23769, 23788,\n",
      "        23852, 23901, 24065, 24361, 24506, 24513, 24550, 25039, 25452, 25492,\n",
      "        25532, 25535, 25683, 25716, 25734, 25755, 25782, 25786, 25810, 25904,\n",
      "        25962, 26008, 26176, 26385, 26670, 26791, 26900, 27136, 27455, 27474,\n",
      "        27488, 27516, 27891, 28068, 28271, 28563, 28682, 28705, 29047, 29116,\n",
      "        29458, 29543, 29546, 29565, 29572, 29663, 29777, 29975, 30009, 30182,\n",
      "        30558, 30757, 30851, 30946, 31040, 31043, 31215, 31365, 31398, 31510,\n",
      "        31571, 31575, 31581, 31822, 31845, 31975, 32129, 32443, 32452, 32514,\n",
      "        32532, 32674, 32776, 32829, 33148, 33161, 33687, 33736, 34064, 34222,\n",
      "        34321, 34339, 34373, 34472, 34561, 34592, 34726, 34761, 34937, 35004,\n",
      "        35007, 35030, 35098, 35130, 35166, 35174, 35284, 35290, 35324, 35335,\n",
      "        35418, 35468, 35814, 35929, 36082, 36116, 36134, 36217, 36220, 36236,\n",
      "        36280, 36305, 36315, 36316, 36372, 36394, 36582, 36769, 36821, 36835,\n",
      "        37318, 37319, 37405, 37495, 37539, 37838, 37887, 37895, 37914, 37988,\n",
      "        38009, 38118, 38121, 38128, 38169, 38336, 38356, 38436, 38481, 38516,\n",
      "        38777, 38798, 38839, 38885, 39067, 39176, 39209, 39427, 39801, 39872,\n",
      "        39895, 39960, 39984, 40009, 40014, 40062, 40118, 40125, 40153, 40330,\n",
      "        40401, 40521, 40599, 40828, 41025, 41135, 41171, 41318, 41414, 41721,\n",
      "        41736, 41744, 41951, 41962, 42120, 42155, 42328, 42503, 42575, 42595,\n",
      "        42598, 42798, 42805, 43045, 43130, 43189, 43251, 43287, 43532, 43895,\n",
      "        44017, 44104, 44176, 44190, 44310, 44362, 44378, 44402, 44752, 44778,\n",
      "        45291, 45336, 45347, 45384, 45441, 45564, 45585, 45627, 45676, 45849,\n",
      "        45938, 46331, 46379, 46608, 46701, 46939, 47165, 47388, 47489, 47614,\n",
      "        47651, 47891, 47916, 48013, 48268, 48420, 48479, 48573, 48622, 48739,\n",
      "        48929, 48952, 48953, 49026, 49076, 49156, 49223, 49299, 49339, 49732,\n",
      "        49923]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,809 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 9/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,812 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH=\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\"\n",
    "\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=\"+CHECKPOINT_PATH,\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make console cmd from args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmd_from_args(args: list[str]):\n",
    "    return \"python -m qtransform \" + ' '.join(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
