{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train openwebtext and shakespeare GPT2 models with either gelu or relu and layernorm or batchnorm and run inference on them\n",
    "### For openwebtext, 4 heads and 4 transformer blocks and for shakespeare, half are used\n",
    "### Tiktoken gpt2 Tokenization is used, currently gradient accumulation is unimplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the training and inference results, the overall loss for every shakespeare model trained with batchnorm was very low (under 1.0 at least), while the inference yielded poor results. Strangely however, the network did not seem to overfit as much with the checkpoints used for inference (eval loss was always under training loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GPT2 with Shakespeare GELU BatchNorm, custom_ln is Identity layer\n",
    "### Params similiar to nanoGPT (https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py) except for gpt model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 5000, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 13:44:11.648799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:44:13,308 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,313 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,316 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-13_13:44:13\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,577 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,939 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,948 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,954 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,986 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,991 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,283 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,402 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,411 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,498 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,729 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.0814786911010743\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,962 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.028734016418458\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.417726612091064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.841564083099366\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.336726760864257\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,242 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.842315435409546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,564 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.42349648475647\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.046977567672729\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,188 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.75757155418396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.5278137683868405\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.345893001556396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,180 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.116252470016479\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.0132341384887695\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.878851127624512\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,109 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.804667854309082\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.695879507064819\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,760 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.600269079208374\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,083 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.538007020950317\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.44656195640564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.251713800430298\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.200865888595581\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.038680076599121\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.861286687850952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,037 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.634791707992553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.378215074539185\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,690 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.140535163879394\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,017 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.918185019493103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.59971444606781\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.332096314430237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.1224578857421874\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,306 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.891332507133484\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.6721574783325197\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4845260620117187\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.2823659896850588\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,583 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.1804298162460327\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,909 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.009354305267334\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.8618500351905822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,560 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.6786442637443542\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.5513007402420045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.4092130661010742\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.327401328086853\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.2216348052024841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.1183194756507873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.0603752493858338\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,828 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.9550049722194671\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,122 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.8526090204715728\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.8235867023468018\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.7416079103946686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,089 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.6766059696674347\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.623449730873108\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.5610007286071778\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,076 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.5322279185056686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,400 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.4816823750734329\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.4518648833036423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.4204030305147171\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,369 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.37947399616241456\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.35556192696094513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.3440380185842514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.29768029451370237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.28025399148464203\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.26523620039224627\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.2554860457777977\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.23417816907167435\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,937 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.21371423751115798\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.1988332137465477\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.19074835479259492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.18297459036111832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,240 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.17245704233646392\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.17341131418943406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.1595330536365509\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.15104628428816796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.14839548915624617\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.14153633862733841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.1328538939356804\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.12690432518720626\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.13007508590817451\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.125824736058712\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.12251279503107071\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,709 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.1187422126531601\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.11048740148544312\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,317 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.10810237675905228\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,641 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.10765649899840354\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,969 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.10224309861660004\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,292 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.10713213160634041\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.09892426878213882\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.10035960376262665\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.09816799089312553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.09627232924103737\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.09459140226244926\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,236 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.09456081539392472\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.09531142190098763\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.09356032237410546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.0900658719241619\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.09381850361824036\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.08983872383832932\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.089760223031044\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.08956152871251107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.08698741123080253\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.08797898441553116\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,475 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.08725197464227677\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.08584734052419662\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.08436140418052673\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.08557359799742699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,756 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.08337687328457832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.08435980305075645\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.08416210934519767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.08072719871997833\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.08021606504917145\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,268 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07771769538521767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.08004260808229446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.07954467162489891\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.08120676279067993\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.07973136603832245\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.08074760884046554\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.08167456313967705\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.08013327717781067\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.08386159390211105\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,168 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.08124819472432136\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,493 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.07815293446183205\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.08068719729781151\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,140 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.07527995184063911\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,460 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1210 loss: 0.0792896255850792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,777 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1220 loss: 0.07909879982471466\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,098 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1230 loss: 0.080222849547863\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,416 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1240 loss: 0.07985053658485412\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1250 loss: 0.07990247905254363\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1260 loss: 0.07967272847890854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1270 loss: 0.07683225870132446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,700 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1280 loss: 0.07696817442774773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1290 loss: 0.07847564071416854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1300 loss: 0.07635548561811448\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1310 loss: 0.07755953297019005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,980 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1320 loss: 0.08147314414381981\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,299 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1330 loss: 0.0745748072862625\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1340 loss: 0.07801730483770371\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1350 loss: 0.07652391642332076\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1360 loss: 0.07694418206810952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1370 loss: 0.07574513629078865\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1380 loss: 0.08034205883741379\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1390 loss: 0.07813942767679691\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1400 loss: 0.07715454623103142\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1410 loss: 0.07432392910122872\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,172 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1420 loss: 0.07728920727968216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,496 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1430 loss: 0.07314848452806473\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1440 loss: 0.07422819398343564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1450 loss: 0.07614408656954766\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,481 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1460 loss: 0.07296678051352501\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1470 loss: 0.07522793412208557\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,124 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1480 loss: 0.07485603466629982\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1490 loss: 0.07659061178565026\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,768 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1500 loss: 0.07550743147730828\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1510 loss: 0.07389946877956391\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1520 loss: 0.07652528509497643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1530 loss: 0.07560347318649292\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,056 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1540 loss: 0.07402549535036088\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,380 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1550 loss: 0.074910619109869\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1560 loss: 0.07472982481122017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1570 loss: 0.07554230615496635\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1580 loss: 0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/100: 0.133059561252594\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,172 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,348 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.005953484401106835\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07082101553678513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.07401572242379188\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.07121982723474503\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,635 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.07029064297676087\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.06943750269711017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.07120902799069881\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.06863513328135014\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,903 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.07117352485656739\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.06729949042201042\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,552 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07023796513676643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,871 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07243234217166901\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.0731443539261818\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.0650293543934822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.07004907317459583\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07083249166607856\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,487 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.06878587603569031\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.06674405559897423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,128 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.06930300667881965\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,433 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07282493859529496\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.0702348593622446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,074 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.06613621674478054\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.065633849427104\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07187155038118362\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07209198959171773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07056076899170875\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 0.0731478177011013\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 0.06732679381966591\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 0.06573506109416485\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,640 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 0.07029695585370063\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,964 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 0.06923590525984764\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 0.06766384989023208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,611 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 0.06799498610198498\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 0.06996757984161377\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,255 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 0.06888032145798206\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,580 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 0.06892801485955716\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 0.0686967846006155\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 0.07068811431527137\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,488 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 0.06987420693039895\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 0.06994570903480053\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 0.06992909088730812\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 0.06451450362801552\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 0.0671445544809103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 0.0680693306028843\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,430 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.07334598451852799\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,752 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.06892706342041492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.06757157891988755\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.07052591480314732\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.06981403641402721\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.06944975815713406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.07077371105551719\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.06996183507144452\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.0694230530411005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,345 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.06557997539639474\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.06862628795206546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.0700732484459877\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.06813231781125069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,618 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.06853835545480251\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.06956844478845596\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.06586511470377446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,582 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.07010632418096066\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.06835015043616295\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.06821108944714069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,557 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.06943667121231556\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.0673607874661684\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.06755397357046604\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.06880321577191353\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,840 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.06871512234210968\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.07124297022819519\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,482 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.06964963674545288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.07146930769085884\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.06947363130748271\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.06855200305581093\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,772 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.06658084094524383\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,090 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.06878070905804634\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.07016171887516975\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.07204182855784894\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,064 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.06919909864664078\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,359 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.0710775252431631\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,667 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.07189789228141308\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,986 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.07093326337635517\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.07051902897655964\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.07366400584578514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.0664932906627655\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,283 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.06879346258938313\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,608 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.06913681291043758\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,932 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.069281155616045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.06843326278030873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.06893916204571723\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,894 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.06852278523147107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,215 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.07043468467891216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.07027224525809288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.06835661269724369\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,189 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.07088794261217117\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,509 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.07069383189082146\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.06545544117689132\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,155 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.06914999186992646\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.06767004244029522\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.07045739553868771\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,120 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.0689571488648653\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.06914092190563678\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.06643678843975068\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,087 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.06644399277865887\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.06971023604273796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.07272332422435283\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.06667447723448276\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.06759239658713341\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,713 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.06913544759154319\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07417276315391064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,363 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.06717519052326679\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.06821254156529903\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.06578771919012069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.06996179968118668\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.066846938803792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.07002154067158699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.06956944465637208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,616 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.0701913632452488\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,935 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.07032300047576427\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.06583375632762908\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.06707629524171352\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.06304956637322903\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrun=train\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdevice=cuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[0;32m---> 16\u001b[0m qtransform\u001b[39m.\u001b[39;49mnotebook_run(args)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[39m=\u001b[39m compose(config_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, overrides\u001b[39m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m main(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m  __main__ \u001b[39mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m mn\u001b[39m.\u001b[39;49mmain(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m  train\u001b[39m.\u001b[39;49mrun(cfg)\n\u001b[1;32m     45\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbench\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[39m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[39m=\u001b[39m train(cfg\u001b[39m=\u001b[39;49mcfg, device\u001b[39m=\u001b[39;49mdevice, model\u001b[39m=\u001b[39;49mmodel, train_data_loader\u001b[39m=\u001b[39;49mtrain_dataloader, eval_data_loader\u001b[39m=\u001b[39;49meval_dataloader, optimizer\u001b[39m=\u001b[39;49moptimizer, scheduler\u001b[39m=\u001b[39;49mscheduler, timestamp\u001b[39m=\u001b[39;49mtimestamp)\n\u001b[1;32m    110\u001b[0m \u001b[39m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mFinished training model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    185\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEPOCH: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     metrics \u001b[39m=\u001b[39m train_one_epoch(cfg, device, model, train_data_loader, optimizer, mini_run)\n\u001b[1;32m    189\u001b[0m     \u001b[39m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39meval_epoch_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m eval_data_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:230\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    228\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    229\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(outputs, labels)\n\u001b[0;32m--> 230\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    231\u001b[0m \u001b[39m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgrad_clip\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mfloat\u001b[39m) \u001b[39mand\u001b[39;00m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mgrad_clip \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=64\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=5000\",\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=200\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using karpathy's script\n",
    "### To check if our script is faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample.py from nanoGPT, adjusted for our models\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from qtransform.model.gpt import GPTConfig, GPT\n",
    "import omegaconf\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "\n",
    "\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "def inference_karpathy(ckpt_path: str, start: str = \"\\n\"):\n",
    "\n",
    "    # model\n",
    "    if init_from == 'resume':\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        print(checkpoint.keys())\n",
    "        gptconf = GPTConfig(**checkpoint['model_cfg'][\"args\"])\n",
    "        model = GPT(gptconf)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "    elif init_from.startswith('gpt2'):\n",
    "        # init from a given GPT-2 model\n",
    "        model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "    # look for the meta pickle in case it is available in the dataset folder\n",
    "    load_meta = False\n",
    "    if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "        meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "        load_meta = os.path.exists(meta_path)\n",
    "    if load_meta:\n",
    "        print(f\"Loading meta from {meta_path}...\")\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    else:\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write inference of Shakespeare GELU BatchNorm, custom_ln is Identity layer to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:52:43,381 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1'}}\n",
      "[ \u001b[36m2024-02-13 13:52:43,567 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,571 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,574 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,577 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,580 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,584 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,587 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,752 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,755 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,765 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,866 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,878 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,895 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,912 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,230 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,235 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,238 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,244 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,246 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,250 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,261 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,263 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   202,   317,   363,   385,   406,   563,   589,   879,   880,\n",
      "          962,  1010,  1152,  1267,  1369,  1408,  1487,  1860,  2143,  2186,\n",
      "         2244,  2266,  2283,  2394,  2566,  2889,  2988,  3209,  3297,  3385,\n",
      "         3599,  3708,  3765,  3770,  3843,  3889,  4252,  4256,  4388,  4393,\n",
      "         4410,  4611,  4692,  4891,  4930,  5398,  5399,  5507,  5675,  5785,\n",
      "         5801,  5933,  6006,  6100,  6111,  6458,  6481,  6596,  6625,  6760,\n",
      "         6783,  6821,  6868,  7134,  7248,  7251,  7491,  7555,  7840,  7845,\n",
      "         9030,  9041,  9067,  9194,  9341,  9352,  9388,  9477,  9839,  9855,\n",
      "         9912, 10180, 10458, 10546, 10605, 10643, 10687, 10827, 10873, 11037,\n",
      "        11325, 11415, 11481, 11507, 11520, 11717, 11748, 11769, 11788, 11793,\n",
      "        11872, 12002, 12476, 12579, 12659, 12694, 12711, 13030, 13072, 13091,\n",
      "        13178, 13201, 13216, 13255, 13346, 13451, 13509, 13562, 13563, 13635,\n",
      "        13675, 13816, 13858, 13903, 13905, 13949, 14001, 14073, 14202, 14327,\n",
      "        14383, 14505, 14515, 14532, 14725, 14840, 14855, 14921, 15073, 15096,\n",
      "        15121, 15222, 15304, 15314, 15433, 15794, 15860, 15861, 15989, 16035,\n",
      "        16067, 16392, 16416, 16437, 16527, 16824, 16862, 16999, 17013, 17108,\n",
      "        17113, 17318, 17556, 17574, 17602, 17667, 17672, 17856, 18135, 18198,\n",
      "        18250, 18301, 18333, 18362, 18371, 18471, 18734, 18926, 18948, 18976,\n",
      "        19104, 19145, 19296, 19306, 19393, 19524, 19762, 19899, 19949, 20126,\n",
      "        20128, 20188, 20326, 20517, 20518, 20627, 20656, 20706, 20856, 20893,\n",
      "        21354, 21369, 21381, 21567, 21694, 21710, 21722, 21910, 22076, 22425,\n",
      "        22530, 22568, 22591, 22634, 22968, 23201, 23230, 23253, 23480, 23530,\n",
      "        23678, 23689, 23738, 23852, 24144, 24208, 24403, 24513, 24758, 25452,\n",
      "        25492, 25580, 25810, 25867, 26008, 26248, 26268, 26476, 26770, 26944,\n",
      "        27156, 27611, 27620, 27800, 27858, 27884, 27923, 28165, 28205, 28563,\n",
      "        28682, 28699, 28705, 28992, 29037, 29094, 29116, 29445, 29483, 29485,\n",
      "        29557, 29746, 29963, 30178, 30365, 30468, 30521, 30872, 30962, 30966,\n",
      "        31103, 31107, 31456, 31510, 31643, 31673, 31736, 31788, 32060, 32336,\n",
      "        32493, 32728, 32780, 32829, 33010, 33161, 33174, 33280, 33324, 33535,\n",
      "        33727, 33769, 33871, 33877, 33883, 34004, 34042, 34227, 34273, 34278,\n",
      "        34311, 34373, 34412, 34571, 34626, 34937, 35030, 35075, 35418, 35814,\n",
      "        35903, 36082, 36114, 36220, 36315, 36492, 36673, 36697, 36873, 37061,\n",
      "        37090, 37206, 37691, 37728, 38118, 38356, 38385, 38432, 38535, 38538,\n",
      "        38687, 38798, 38965, 39075, 39232, 39377, 39384, 39403, 39698, 39700,\n",
      "        39781, 39801, 39805, 39984, 40009, 40062, 40078, 40388, 40427, 40430,\n",
      "        40508, 40599, 40604, 40693, 40701, 40713, 40755, 40850, 40869, 40901,\n",
      "        41318, 41534, 41550, 41736, 41762, 41924, 42139, 42192, 42248, 42259,\n",
      "        42452, 42585, 42598, 42834, 42993, 43034, 43045, 43189, 43218, 43289,\n",
      "        43426, 43503, 43865, 43888, 43901, 43956, 43984, 44153, 44159, 44205,\n",
      "        44360, 44624, 44678, 44679, 44730, 44822, 45186, 45208, 45384, 45479,\n",
      "        45529, 45567, 45589, 45627, 45638, 45674, 45754, 45789, 46087, 46153,\n",
      "        46241, 46253, 46304, 46403, 46414, 46581, 46696, 46807, 46912, 46944,\n",
      "        47316, 47470, 47653, 47746, 47871, 47892, 47894, 47992, 48013, 48112,\n",
      "        48305, 48377, 48479, 48614, 48664, 48752, 48763, 48999, 49026, 49100,\n",
      "        49143, 49153, 49533, 49615, 49792, 49945, 49969, 50158, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,207 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,705 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  153,   198,   251,   317,   650,   824,   829,   832,   878,   901,\n",
      "          919,   962,  1195,  1267,  1270,  1451,  1473,  1683,  1710,  1772,\n",
      "         1858,  1937,  2048,  2053,  2143,  2375,  2491,  2537,  2878,  2881,\n",
      "         2898,  2989,  3006,  3134,  3267,  3322,  3357,  3383,  3408,  3538,\n",
      "         3542,  3599,  3770,  3901,  3914,  3921,  4388,  4421,  4688,  4719,\n",
      "         4746,  4831,  5128,  5291,  5420,  5664,  5884,  5942,  6035,  6113,\n",
      "         6295,  6536,  6718,  6760,  6783,  6851,  6969,  6994,  7027,  7056,\n",
      "         7111,  7123,  7134,  7251,  7358,  7449,  7496,  7546,  7575,  7590,\n",
      "         7596,  8361,  8455,  8489,  8590,  8604,  8689,  8773,  8967,  9029,\n",
      "         9194,  9404,  9538,  9717,  9943,  9976, 10282, 10308, 10310, 10403,\n",
      "        10547, 10691, 10715, 10930, 10946, 11089, 11286, 11584, 11682, 11701,\n",
      "        11818, 11862, 11885, 11903, 12259, 12490, 12559, 12587, 12694, 12729,\n",
      "        12990, 13018, 13084, 13324, 13361, 13502, 13792, 13822, 13871, 13929,\n",
      "        13949, 14001, 14073, 14138, 14331, 14379, 14455, 14478, 14560, 14594,\n",
      "        14725, 14866, 14992, 15023, 15073, 15275, 15559, 15582, 15646, 15685,\n",
      "        15828, 16009, 16025, 16156, 16161, 16313, 16465, 16501, 16665, 16670,\n",
      "        16869, 16980, 16983, 17047, 17054, 17218, 17228, 17278, 17312, 17323,\n",
      "        17601, 17681, 17866, 17986, 18150, 18447, 18494, 18511, 18551, 18689,\n",
      "        18760, 18764, 18778, 18948, 18981, 19209, 19285, 19313, 19350, 19500,\n",
      "        19924, 20326, 20333, 20401, 20478, 20514, 20518, 20570, 20627, 20662,\n",
      "        20856, 20881, 20920, 21008, 21122, 21485, 21489, 21604, 21797, 22233,\n",
      "        22358, 22440, 22917, 22926, 22968, 23230, 23312, 23378, 23421, 23689,\n",
      "        23768, 23775, 23848, 24155, 24181, 24471, 24513, 24545, 24626, 24803,\n",
      "        25039, 25087, 25100, 25305, 25312, 25324, 25452, 25467, 25492, 25532,\n",
      "        25535, 25643, 25716, 25810, 25824, 25833, 26071, 26111, 26153, 26239,\n",
      "        26405, 26417, 26424, 26872, 27156, 27310, 27413, 27694, 27775, 27922,\n",
      "        27924, 27951, 28079, 28273, 28444, 28563, 28595, 28747, 28796, 29090,\n",
      "        29094, 29116, 29353, 29405, 29533, 29572, 29663, 29706, 29774, 30000,\n",
      "        30241, 30383, 30558, 30815, 30923, 30982, 31102, 31103, 31156, 31324,\n",
      "        31360, 31618, 31717, 31883, 32045, 32068, 32260, 32275, 32452, 33028,\n",
      "        33099, 33158, 33371, 33418, 33658, 33698, 33831, 33834, 34311, 34575,\n",
      "        34709, 34779, 34835, 34937, 34977, 35030, 35044, 35096, 35273, 35299,\n",
      "        35330, 35335, 35709, 35750, 35818, 35978, 36073, 36113, 36117, 36315,\n",
      "        36394, 36436, 36492, 36529, 36599, 36672, 36772, 36851, 36984, 37342,\n",
      "        37376, 37405, 37515, 37653, 37717, 37914, 37984, 38069, 38141, 38487,\n",
      "        38687, 38754, 38839, 38977, 39027, 39153, 39178, 39204, 39230, 39329,\n",
      "        39426, 39427, 39908, 39984, 39995, 40037, 40172, 40388, 40396, 40426,\n",
      "        40790, 40817, 41105, 41171, 41173, 41181, 41406, 41477, 41484, 41550,\n",
      "        41572, 41598, 41843, 41936, 41989, 41990, 42220, 42228, 42252, 42283,\n",
      "        42487, 42734, 42796, 42798, 42816, 42834, 42993, 43036, 43289, 43389,\n",
      "        43469, 43699, 43826, 43934, 43969, 43984, 44193, 44360, 44678, 44831,\n",
      "        44884, 45336, 45558, 45560, 45638, 45657, 45674, 45760, 45802, 45938,\n",
      "        45975, 46040, 46304, 46601, 46767, 46782, 47102, 47264, 47387, 47447,\n",
      "        47560, 47667, 47733, 47821, 47928, 48013, 48259, 48282, 48312, 48364,\n",
      "        48537, 48752, 48930, 48953, 49355, 49406, 49467, 49655, 49673, 49716,\n",
      "        49798, 50118]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,711 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,693 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   369,   784,   788,   821,   847,   869,   901,   962,   998,\n",
      "         1020,  1025,  1147,  1179,  1193,  1201,  1267,  1299,  1370,  1408,\n",
      "         1433,  1508,  1563,  1729,  1783,  1903,  2173,  2288,  2300,  2334,\n",
      "         2566,  2621,  2690,  2878,  2898,  2950,  3134,  3396,  3478,  3631,\n",
      "         3652,  3726,  3757,  3925,  3976,  4096,  4157,  4268,  4367,  4388,\n",
      "         4421,  4423,  4624,  4704,  4898,  5227,  5720,  5831,  5933,  5942,\n",
      "         5994,  6035,  6222,  6229,  6395,  6830,  6921,  6922,  6980,  6994,\n",
      "         7025,  7027,  7071,  7134,  7424,  7568,  7623,  7648,  7705,  8125,\n",
      "         8206,  8442,  8489,  8535,  8967,  9082,  9145,  9194,  9260,  9791,\n",
      "        10043, 10163, 10180, 10328, 10511, 10685, 10801, 10912, 10969, 11286,\n",
      "        11412, 11481, 11626, 11646, 11748, 11769, 11851, 12093, 12374, 13011,\n",
      "        13018, 13128, 13154, 13216, 13415, 13470, 13509, 13518, 13562, 13577,\n",
      "        13581, 13740, 13982, 14001, 14043, 14192, 14280, 14379, 14533, 14730,\n",
      "        14874, 14921, 15222, 15313, 15323, 15350, 15582, 15636, 15670, 15712,\n",
      "        15976, 15982, 16098, 16457, 16665, 16699, 16742, 16754, 16798, 17062,\n",
      "        17108, 17178, 17312, 17381, 17798, 17852, 17871, 17918, 17963, 18074,\n",
      "        18091, 18127, 18133, 18184, 18347, 18364, 18869, 18895, 18981, 18997,\n",
      "        19009, 19021, 19072, 19140, 19165, 19171, 19189, 19215, 19296, 19306,\n",
      "        19386, 19393, 19762, 19916, 19938, 19988, 20048, 20214, 20319, 20468,\n",
      "        20478, 20491, 21086, 21093, 21253, 21287, 21550, 21932, 22193, 22317,\n",
      "        22358, 22540, 22633, 22644, 22879, 23253, 23378, 23405, 23412, 23456,\n",
      "        23627, 23903, 24030, 24065, 24208, 24300, 24306, 24454, 24513, 24718,\n",
      "        24798, 24940, 25039, 25492, 25496, 25564, 25613, 25810, 26034, 26102,\n",
      "        26111, 26248, 26362, 26405, 26670, 26680, 26727, 26872, 26930, 27089,\n",
      "        27162, 27262, 27413, 27765, 27923, 28212, 28337, 28408, 28496, 28563,\n",
      "        28821, 28918, 29094, 29353, 29550, 29559, 29594, 29657, 29869, 30017,\n",
      "        30099, 30139, 30202, 30260, 30483, 30631, 30915, 30946, 31021, 31103,\n",
      "        31107, 31288, 31340, 31360, 31561, 31575, 31851, 31975, 32123, 32168,\n",
      "        32228, 32263, 32443, 32452, 32619, 32860, 33150, 33158, 33562, 33599,\n",
      "        33610, 33687, 33698, 33736, 33740, 34227, 34278, 34591, 34627, 34805,\n",
      "        34811, 34911, 34937, 35030, 35177, 35202, 35290, 35316, 35385, 35466,\n",
      "        35494, 35589, 35867, 35902, 35997, 36222, 36315, 36377, 36509, 36873,\n",
      "        37053, 37117, 37294, 37342, 37344, 37464, 37536, 37608, 37810, 37864,\n",
      "        37994, 38079, 38128, 38248, 38304, 38336, 38356, 38510, 38591, 38985,\n",
      "        39057, 39177, 39427, 39538, 39789, 39797, 39801, 39805, 39907, 39984,\n",
      "        39995, 40009, 40028, 40108, 40198, 40223, 40401, 40549, 40600, 40740,\n",
      "        40755, 40869, 40952, 40976, 41148, 41199, 41274, 41364, 41572, 41632,\n",
      "        41672, 41973, 41990, 42184, 42341, 42470, 42522, 42540, 42598, 42656,\n",
      "        42798, 42851, 43130, 43289, 43648, 43815, 43895, 43988, 44205, 44265,\n",
      "        44333, 44427, 44589, 44598, 44707, 44738, 44978, 45002, 45017, 45041,\n",
      "        45186, 45384, 45479, 45529, 45564, 45627, 45642, 45650, 45674, 45677,\n",
      "        45739, 45752, 45967, 46259, 46408, 46636, 46811, 46969, 47264, 47333,\n",
      "        47392, 47558, 47571, 47746, 47916, 47986, 48013, 48285, 48437, 48622,\n",
      "        48805, 48822, 48877, 48949, 49026, 49153, 49261, 49716, 49720, 49747,\n",
      "        49798, 49923, 49945, 50040, 50243]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,698 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,603 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   54,    96,   135,   198,   317,   343,   590,   694,   788,   824,\n",
      "          829,   879,   933,   962,  1010,  1020,  1113,  1267,  1416,  1620,\n",
      "         1645,  1656,  1733,  1926,  2334,  2451,  2616,  2872,  2874,  2889,\n",
      "         3063,  3073,  3298,  3322,  3478,  3510,  3534,  3546,  3629,  3726,\n",
      "         3759,  3770,  3929,  4062,  4283,  4294,  4422,  4641,  4671,  4716,\n",
      "         4719,  4795,  4930,  5065,  5359,  5476,  5533,  5578,  5720,  5748,\n",
      "         6013,  6087,  6336,  6760,  6783,  6810,  6932,  6968,  7027,  7134,\n",
      "         7160,  7226,  7552,  7568,  7672,  7943,  8035,  8489,  8978,  9260,\n",
      "         9388,  9439,  9538,  9674,  9717,  9746,  9870,  9912, 10282, 10503,\n",
      "        10573, 10607, 11182, 11265, 11481, 11544, 11584, 11588, 11647, 11748,\n",
      "        11769, 11780, 11793, 11870, 11872, 11885, 11974, 12346, 12407, 12489,\n",
      "        12587, 12703, 12723, 13444, 13451, 13460, 13502, 13656, 13684, 13725,\n",
      "        13807, 13822, 13939, 14073, 14105, 14186, 14268, 14614, 14725, 14836,\n",
      "        14992, 15073, 15222, 15350, 15501, 15609, 15872, 16031, 16067, 16457,\n",
      "        16634, 16665, 16722, 16941, 16992, 16998, 17013, 17098, 17210, 17228,\n",
      "        17233, 17247, 17323, 17452, 17575, 17913, 17945, 18356, 18556, 18577,\n",
      "        18659, 18699, 18758, 18789, 18839, 18948, 19064, 19145, 19402, 19480,\n",
      "        19485, 19601, 19751, 19762, 19949, 19990, 20048, 20060, 20326, 20478,\n",
      "        20627, 20751, 20893, 20920, 21020, 21079, 21117, 21246, 21253, 21404,\n",
      "        21420, 21459, 21544, 21567, 21714, 21723, 21819, 21864, 21910, 22080,\n",
      "        22089, 22317, 22451, 22568, 22633, 22880, 23194, 23316, 23412, 23678,\n",
      "        23689, 23695, 23798, 23821, 23868, 23982, 24065, 24431, 24513, 24550,\n",
      "        24666, 24798, 24872, 24917, 24991, 25089, 25100, 25452, 25492, 25546,\n",
      "        25595, 25734, 25755, 25810, 26119, 26153, 26343, 26476, 26493, 26660,\n",
      "        26920, 27085, 27277, 27319, 27775, 28003, 28252, 28256, 28563, 28583,\n",
      "        28821, 29127, 29405, 29440, 29502, 29565, 29594, 29908, 30000, 30241,\n",
      "        30260, 30315, 30381, 30558, 30571, 30641, 30716, 30812, 30837, 30886,\n",
      "        31103, 31288, 31353, 31618, 31705, 31851, 31974, 32228, 32262, 32443,\n",
      "        32650, 32652, 32674, 32876, 33174, 33216, 33520, 33687, 33769, 33780,\n",
      "        33905, 34004, 34051, 34213, 34373, 34433, 34586, 34604, 34706, 34761,\n",
      "        34766, 34801, 34895, 34899, 35044, 35316, 35335, 35466, 35494, 35559,\n",
      "        35589, 35749, 35997, 36017, 36110, 36418, 36439, 36453, 36486, 36492,\n",
      "        36570, 36673, 36694, 36857, 36998, 37061, 37116, 37163, 37282, 37536,\n",
      "        37637, 37653, 37835, 37887, 37898, 37914, 37988, 37994, 38118, 38125,\n",
      "        38128, 38131, 38139, 38244, 38461, 38518, 38520, 38640, 38687, 38819,\n",
      "        38839, 38889, 39067, 39074, 39109, 39189, 39258, 39311, 39315, 39427,\n",
      "        39775, 39984, 40062, 40300, 40323, 40388, 40401, 40604, 40783, 40817,\n",
      "        40828, 40976, 41265, 41442, 41762, 41906, 41957, 41962, 42120, 42259,\n",
      "        42298, 42413, 42457, 42834, 42845, 43058, 43295, 43298, 43320, 43815,\n",
      "        43826, 43851, 43863, 43895, 43910, 43960, 44024, 44265, 44427, 44477,\n",
      "        44598, 44738, 44796, 44856, 44970, 45186, 45384, 45389, 45477, 45479,\n",
      "        45558, 45599, 45805, 45829, 45831, 46304, 46345, 46355, 46420, 46807,\n",
      "        47130, 47227, 47287, 47335, 47698, 47894, 47990, 48000, 48119, 48212,\n",
      "        48215, 48312, 48479, 48629, 48659, 48789, 49100, 49119, 49214, 49655,\n",
      "        49792, 49798, 49813, 49823, 50087, 50216]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,609 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,214 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   590,   824,  1073,  1074,  1096,  1147,  1179,  1594,  1800,\n",
      "         1880,  2139,  2244,  2300,  2451,  2552,  2690,  2898,  3063,  3101,\n",
      "         3322,  3363,  3403,  3564,  3599,  3756,  3770,  3834,  3889,  4206,\n",
      "         4283,  4367,  4628,  4804,  5097,  5209,  5348,  5476,  5528,  5578,\n",
      "         5630,  5675,  5755,  5892,  5923,  5957,  5965,  6011,  6035,  6051,\n",
      "         6060,  6229,  6442,  6549,  6578,  6604,  6684,  6895,  6994,  7027,\n",
      "         7054,  7118,  7251,  7598,  7623,  7962,  8024,  8137,  8149,  8334,\n",
      "         8482,  8532,  8593,  8789,  9067,  9071,  9232,  9369,  9814,  9912,\n",
      "        10193, 10266, 10282, 10360, 10362, 10458, 10668, 10687, 10794, 10839,\n",
      "        10958, 10970, 11244, 11268, 11609, 11723, 11941, 12046, 12203, 12307,\n",
      "        12481, 12694, 12866, 13011, 13012, 13154, 13172, 13361, 13380, 13431,\n",
      "        13527, 13528, 13562, 13699, 13713, 13733, 13939, 14001, 14270, 14532,\n",
      "        14725, 14750, 14842, 14866, 14988, 15323, 15353, 15396, 15540, 15636,\n",
      "        15651, 15712, 15782, 15794, 15796, 15863, 16031, 16099, 16450, 16495,\n",
      "        16541, 16665, 16666, 16989, 17218, 17356, 17464, 17535, 17575, 17759,\n",
      "        17875, 17906, 17986, 18145, 18307, 18316, 18333, 18869, 18895, 18926,\n",
      "        18948, 18997, 19115, 19337, 19485, 19856, 19921, 19988, 20048, 20060,\n",
      "        20223, 20257, 20305, 20468, 20478, 20481, 20483, 20517, 20518, 20575,\n",
      "        20627, 20746, 20881, 20983, 21121, 21246, 21273, 21301, 21308, 21404,\n",
      "        21604, 21753, 21925, 21980, 22025, 22049, 22080, 22233, 22274, 22346,\n",
      "        22372, 22566, 22591, 22633, 22657, 22798, 22858, 23121, 23128, 23162,\n",
      "        23201, 23253, 23449, 23678, 23690, 23852, 23938, 23965, 24216, 24275,\n",
      "        24335, 24345, 24396, 24422, 24479, 24826, 25039, 25168, 25412, 25532,\n",
      "        25622, 25631, 25679, 25716, 25810, 25839, 25939, 25977, 26008, 26119,\n",
      "        26322, 26517, 26657, 26694, 26748, 26843, 27020, 27022, 27413, 27497,\n",
      "        27644, 27775, 28058, 28068, 28131, 28328, 28334, 28335, 28337, 28563,\n",
      "        28617, 28669, 28682, 28740, 29124, 29150, 29305, 29350, 29353, 29533,\n",
      "        29558, 29594, 29783, 29847, 30035, 30241, 30381, 30449, 30458, 30468,\n",
      "        30741, 30947, 31103, 31107, 31145, 31185, 31239, 31264, 31322, 31328,\n",
      "        31360, 31430, 31534, 31822, 31851, 31922, 32147, 32532, 32739, 33028,\n",
      "        33032, 33056, 33061, 33093, 33174, 33222, 33293, 33346, 33914, 34004,\n",
      "        34268, 34311, 34575, 34805, 34937, 35082, 35097, 35566, 35589, 36011,\n",
      "        36073, 36116, 36248, 36401, 36492, 36582, 36599, 36673, 36786, 36865,\n",
      "        36873, 36977, 37059, 37294, 37308, 37612, 38064, 38141, 38150, 38322,\n",
      "        38385, 38436, 39027, 39067, 39107, 39426, 39514, 39653, 39808, 39826,\n",
      "        39856, 39901, 39908, 39984, 40014, 40201, 40219, 40426, 40557, 40599,\n",
      "        40693, 40752, 40755, 40817, 41112, 41318, 41472, 41505, 41550, 41572,\n",
      "        41630, 41632, 41762, 42119, 42120, 42220, 42298, 42452, 42470, 42528,\n",
      "        42554, 42614, 42760, 42790, 42821, 42851, 42880, 42967, 43079, 43136,\n",
      "        43289, 43391, 43499, 43554, 43643, 43766, 43815, 43851, 43930, 43956,\n",
      "        43984, 44123, 44130, 44153, 44214, 44402, 44427, 44459, 44562, 44600,\n",
      "        44678, 44711, 44978, 45041, 45134, 45336, 45529, 45627, 45752, 45760,\n",
      "        45877, 45896, 46046, 46153, 46251, 46960, 47145, 47172, 47277, 47675,\n",
      "        47745, 47803, 47851, 47856, 48013, 48171, 48215, 48230, 48479, 48528,\n",
      "        48529, 48622, 49026, 49100, 49129, 49312, 49355, 49425, 49449, 49453,\n",
      "        49486, 49602, 49615, 49732, 49744, 49765, 49900, 49910, 49923, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,293 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 4/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,601 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   259,   285,   317,   406,   521,   653,   664,   829,   901,\n",
      "          962,   972,  1010,  1248,  1267,  1508,  1616,  1774,  1880,  2005,\n",
      "         2173,  2207,  2235,  2300,  2454,  2485,  2653,  2898,  2950,  2956,\n",
      "         3061,  3355,  3383,  3480,  3534,  3569,  3776,  3889,  4157,  4294,\n",
      "         4388,  4400,  4429,  4491,  4624,  4641,  4712,  4725,  4804,  4930,\n",
      "         5195,  5291,  5305,  5371,  5399,  5443,  5507,  5630,  5905,  5960,\n",
      "         5965,  6011,  6035,  6175,  6274,  6442,  6480,  6557,  6596,  6760,\n",
      "         6775,  7027,  7118,  7251,  7324,  7450,  8188,  8248,  8334,  8345,\n",
      "         8347,  8354,  8569,  8705,  8744,  8804,  8854,  8892,  9153,  9209,\n",
      "         9323,  9328,  9433,  9452,  9589,  9906,  9918, 10010, 10076, 10282,\n",
      "        10328, 10379, 10404, 11015, 11087, 11244, 11515, 11557, 11588, 11796,\n",
      "        11872, 11911, 11968, 12282, 12723, 12782, 12941, 13030, 13084, 13154,\n",
      "        13169, 13178, 13415, 13455, 13562, 13586, 13611, 13733, 13851, 13903,\n",
      "        14001, 14043, 14335, 14532, 14563, 14741, 14794, 14850, 14855, 14996,\n",
      "        15203, 15302, 15360, 15382, 15441, 15470, 15582, 15636, 16031, 16098,\n",
      "        16148, 16392, 16454, 16501, 16600, 16624, 16891, 16918, 17005, 17062,\n",
      "        17067, 17189, 17249, 17323, 17647, 17918, 17986, 18054, 18386, 18411,\n",
      "        18795, 18840, 18895, 18926, 18948, 18958, 19047, 19313, 19332, 19402,\n",
      "        19485, 19637, 19716, 19745, 19890, 20060, 20128, 20199, 20468, 20514,\n",
      "        20662, 20777, 20817, 20966, 21089, 21206, 21239, 21273, 21400, 21457,\n",
      "        21714, 21910, 22043, 22358, 22431, 22553, 22591, 22633, 22968, 23014,\n",
      "        23214, 23230, 23316, 23412, 23647, 23689, 23769, 23815, 23962, 24065,\n",
      "        24545, 24623, 24654, 25275, 25412, 25622, 25716, 25800, 25810, 26093,\n",
      "        26248, 26261, 26282, 26523, 26560, 26752, 26814, 26850, 26944, 27122,\n",
      "        27270, 27331, 27444, 27445, 27488, 27924, 28014, 28079, 28153, 28221,\n",
      "        28393, 28561, 28563, 28566, 28682, 28699, 28918, 29006, 29094, 29116,\n",
      "        29141, 29353, 29405, 29744, 29884, 29888, 30465, 30468, 30565, 30757,\n",
      "        30816, 30921, 30946, 31040, 31103, 31107, 31200, 31360, 31427, 31621,\n",
      "        31713, 32228, 32350, 32409, 32443, 32452, 32699, 32889, 33020, 33032,\n",
      "        33099, 33286, 33409, 33479, 33569, 33727, 33889, 33893, 34051, 34130,\n",
      "        34278, 34439, 34581, 34767, 35030, 35096, 35290, 35341, 35410, 35748,\n",
      "        35805, 35814, 35866, 36116, 36117, 36173, 36241, 36315, 36377, 36492,\n",
      "        36582, 36599, 36665, 36814, 37218, 37994, 38060, 38067, 38089, 38118,\n",
      "        38141, 38356, 38487, 38516, 38645, 38687, 38701, 38852, 39073, 39128,\n",
      "        39177, 39291, 39426, 39480, 39495, 39877, 39931, 39984, 40198, 40237,\n",
      "        40249, 40430, 40580, 40676, 40686, 40693, 40772, 40782, 40817, 40976,\n",
      "        41308, 41414, 41632, 41736, 41966, 42184, 42254, 42334, 42418, 42470,\n",
      "        42798, 42845, 43079, 43130, 43142, 43232, 43298, 43377, 43444, 43481,\n",
      "        43489, 43699, 43805, 43814, 43986, 44156, 44290, 44402, 44580, 44678,\n",
      "        44738, 44992, 45024, 45069, 45260, 45291, 45456, 45569, 45772, 45999,\n",
      "        46161, 46420, 46434, 46577, 46610, 46776, 46876, 46885, 46951, 46954,\n",
      "        47113, 47227, 47254, 47335, 47449, 47629, 47667, 47678, 47697, 47698,\n",
      "        47733, 47845, 47847, 47986, 48013, 48134, 48306, 48307, 48350, 48395,\n",
      "        48442, 48489, 48644, 48664, 48929, 48962, 48999, 49100, 49190, 49272,\n",
      "        49355, 49462, 49630, 49674, 49765, 49792, 49875, 49969, 50095, 50116]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,605 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 5/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,793 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   31,   153,   198,   211,   227,   317,   371,   433,   481,   486,\n",
      "          527,   589,   633,   857,   879,   880,   962,  1020,  1025,  1074,\n",
      "         1408,  1422,  1428,  1861,  2019,  2138,  2266,  2280,  2300,  2334,\n",
      "         2379,  2451,  2743,  2897,  2950,  3182,  3383,  3599,  3757,  3770,\n",
      "         4012,  4157,  4162,  4388,  4459,  4491,  4514,  4628,  4858,  4930,\n",
      "         4971,  5020,  5109,  5119,  5134,  5209,  5373,  5578,  5675,  5758,\n",
      "         5929,  6458,  6596,  6657,  6687,  6717,  6760,  6783,  6868,  6950,\n",
      "         6968,  6975,  6983,  7038,  7056,  7111,  7118,  7134,  7324,  7546,\n",
      "         8095,  8125,  8188,  8250,  8277,  8334,  8609,  8684,  8906,  9035,\n",
      "         9145,  9323,  9511,  9621,  9672,  9855,  9901,  9959, 10085, 10180,\n",
      "        10404, 10460, 10804, 10993, 11430, 11530, 11588, 11654, 11717, 12025,\n",
      "        12282, 12307, 12539, 12582, 12587, 12680, 12694, 12889, 13012, 13084,\n",
      "        13168, 13172, 13224, 13448, 13509, 13523, 13562, 13611, 13628, 13822,\n",
      "        13927, 14001, 14041, 14043, 14532, 14554, 14603, 14666, 14725, 14866,\n",
      "        14919, 15073, 15135, 15222, 15472, 15501, 15630, 15860, 15894, 15976,\n",
      "        16084, 16180, 16818, 16880, 16937, 17047, 17164, 17278, 17320, 17842,\n",
      "        17906, 18032, 18237, 18283, 18336, 18831, 18869, 18948, 19148, 19171,\n",
      "        19215, 19337, 19369, 19371, 19559, 19716, 19745, 20326, 20478, 20512,\n",
      "        20518, 20627, 20757, 20785, 20792, 20881, 20900, 21105, 21121, 21184,\n",
      "        21266, 21404, 21420, 21457, 21637, 21800, 21910, 21985, 22136, 22319,\n",
      "        22393, 22568, 22657, 22709, 22954, 23128, 23412, 23628, 24065, 24479,\n",
      "        24798, 24935, 24999, 25100, 25256, 25528, 25532, 25562, 25716, 25810,\n",
      "        25833, 25939, 26146, 26153, 26381, 26413, 26537, 26551, 26645, 26670,\n",
      "        26852, 27115, 27123, 27211, 27496, 27647, 27946, 28011, 28056, 28099,\n",
      "        28121, 28165, 28237, 28388, 28561, 28769, 29116, 29130, 29224, 29292,\n",
      "        29445, 29488, 29558, 29565, 29594, 29657, 29717, 29743, 30025, 30026,\n",
      "        30241, 30301, 30327, 30449, 30621, 30658, 30797, 30812, 30852, 31261,\n",
      "        31264, 31328, 31394, 31519, 31682, 31824, 31862, 31887, 32111, 32371,\n",
      "        32635, 32690, 32717, 32754, 32843, 33174, 33187, 33376, 33549, 33594,\n",
      "        33660, 34236, 34304, 34398, 34575, 34662, 34761, 34805, 34929, 35065,\n",
      "        35166, 35290, 35335, 35417, 35418, 35420, 35466, 35515, 35618, 36241,\n",
      "        36250, 36377, 36492, 36529, 36570, 36673, 36795, 37053, 37198, 37294,\n",
      "        37324, 37342, 37345, 37405, 37464, 37468, 37540, 37691, 37776, 37988,\n",
      "        38064, 38089, 38114, 38128, 38209, 38289, 38336, 38346, 38356, 38376,\n",
      "        38416, 38518, 38687, 38739, 38764, 38777, 39047, 39109, 39119, 39250,\n",
      "        39296, 39384, 39495, 39750, 39801, 39805, 39812, 40009, 40037, 40138,\n",
      "        40568, 40693, 40772, 40795, 40952, 40955, 40982, 41062, 41297, 41374,\n",
      "        41463, 41472, 41511, 41550, 41669, 41776, 41892, 42120, 42199, 42295,\n",
      "        42298, 42457, 42776, 42845, 42928, 42993, 43006, 43022, 43045, 43047,\n",
      "        43173, 43243, 43286, 43289, 43469, 43605, 43689, 43702, 43815, 43826,\n",
      "        44065, 44176, 44245, 44310, 44362, 44427, 44540, 44543, 44598, 44678,\n",
      "        44849, 45002, 45260, 45384, 45423, 45441, 45585, 45605, 45627, 45657,\n",
      "        45802, 45837, 45959, 46241, 46314, 46324, 46367, 46407, 46680, 46952,\n",
      "        47243, 47448, 47629, 47706, 47866, 48013, 48364, 48523, 48622, 48624,\n",
      "        48823, 48880, 48929, 49026, 49178, 49235, 49256, 49486, 49792, 49811,\n",
      "        49963, 49969, 50144, 50166, 50236]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,797 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 6/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,895 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,    47,   198,   208,   370,   476,   486,   518,   563,   590,\n",
      "          761,   801,   854,   879,   901,   962,  1005,  1077,  1127,  1264,\n",
      "         1267,  1533,  1563,  1772,  1860,  1880,  1887,  1992,  2055,  2127,\n",
      "         2457,  2660,  2690,  2694,  2832,  2874,  2950,  3196,  3322,  3341,\n",
      "         3355,  3363,  3444,  3510,  3593,  3665,  3770,  3843,  3877,  4223,\n",
      "         4256,  4328,  4333,  4367,  4388,  4491,  4620,  4624,  4641,  4692,\n",
      "         4740,  4831,  5072,  5276,  5320,  5373,  5385,  5399,  5456,  5630,\n",
      "         5905,  6229,  6760,  6783,  6830,  6994,  7027,  7111,  7134,  7256,\n",
      "         7330,  7344,  7553,  7648,  8124,  8555,  8590,  8609,  8648,  8689,\n",
      "         8874,  8915,  9190,  9194,  9323,  9352,  9369,  9450,  9694,  9795,\n",
      "         9855,  9901,  9925, 10403, 10435, 10669, 10691, 10719, 10769, 10858,\n",
      "        10946, 11089, 11143, 11286, 11872, 11885, 12207, 12283, 12490, 12673,\n",
      "        12739, 12955, 13027, 13283, 13346, 13548, 13618, 13635, 13695, 13822,\n",
      "        14013, 14064, 14073, 14280, 14464, 14532, 14643, 14725, 14988, 15222,\n",
      "        15244, 15314, 15559, 15636, 15737, 15794, 15860, 16035, 16081, 16410,\n",
      "        16614, 16716, 16998, 17046, 17067, 17101, 17151, 17191, 17218, 17278,\n",
      "        17575, 17654, 17759, 17829, 17852, 17875, 17883, 17913, 17928, 17986,\n",
      "        18032, 18237, 18307, 18331, 18347, 18731, 18849, 18895, 18948, 19072,\n",
      "        19215, 19274, 19313, 19374, 19384, 19409, 19524, 19548, 19593, 19813,\n",
      "        19916, 20017, 20048, 20060, 20303, 20360, 20393, 20402, 20518, 20627,\n",
      "        20706, 20882, 21103, 21117, 21239, 21345, 21395, 21457, 21496, 21824,\n",
      "        21841, 22182, 22233, 22633, 23194, 23230, 23320, 23347, 24275, 24349,\n",
      "        24431, 24872, 24991, 24999, 25306, 25357, 25468, 25535, 25612, 25677,\n",
      "        25716, 25810, 25904, 25934, 25939, 26008, 26111, 26362, 26405, 26471,\n",
      "        26493, 26500, 26670, 26791, 26890, 26944, 27037, 27331, 27479, 27488,\n",
      "        27496, 27498, 27576, 27775, 27898, 28252, 28264, 28332, 28379, 28489,\n",
      "        28491, 28563, 28682, 28844, 28919, 29094, 29116, 29233, 29313, 29353,\n",
      "        29485, 29488, 29509, 29550, 29678, 29774, 29869, 29894, 29984, 29988,\n",
      "        30000, 30041, 30103, 30274, 30808, 30858, 30946, 31040, 31103, 31433,\n",
      "        31547, 31676, 31686, 31815, 32068, 32266, 32324, 32443, 32588, 32591,\n",
      "        32650, 32860, 32900, 33032, 33187, 33199, 33475, 33827, 34064, 34150,\n",
      "        34171, 34278, 34311, 34507, 34575, 34825, 34937, 34951, 35030, 35220,\n",
      "        35335, 35631, 35709, 35723, 35790, 35814, 35929, 35962, 36082, 36116,\n",
      "        36117, 36255, 36259, 36599, 36673, 36779, 37053, 37344, 37405, 37536,\n",
      "        37568, 37588, 37994, 38128, 38172, 38210, 38253, 38438, 38516, 38585,\n",
      "        38586, 38598, 38839, 38883, 38903, 39027, 39081, 39311, 39348, 39426,\n",
      "        39495, 39622, 39696, 39789, 39826, 39984, 40198, 40388, 40422, 40676,\n",
      "        40693, 40817, 40907, 40911, 40960, 40976, 41435, 41572, 41681, 41863,\n",
      "        41962, 42350, 42550, 42635, 42702, 42784, 42821, 42834, 42919, 42991,\n",
      "        43045, 43173, 43513, 43664, 43699, 43788, 43805, 44105, 44214, 44427,\n",
      "        44738, 44953, 44976, 45024, 45327, 45372, 45585, 45752, 45764, 45849,\n",
      "        46241, 46304, 46678, 47110, 47200, 47264, 47821, 47913, 47936, 47951,\n",
      "        48000, 48013, 48108, 48309, 48479, 48523, 48612, 48664, 48935, 49026,\n",
      "        49045, 49085, 49091, 49105, 49715, 49783, 50156, 50160, 50195, 50232]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,899 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 7/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,707 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  107,   198,   220,   230,   608,   821,   879,   901,   948,   962,\n",
      "         1108,  1248,  1267,  1668,  1714,  1760,  1858,  1880,  1887,  1928,\n",
      "         1964,  2027,  2152,  2173,  2199,  2288,  2483,  2690,  2804,  2877,\n",
      "         2878,  2894,  2966,  2989,  3101,  3582,  3599,  3708,  3741,  3809,\n",
      "         3829,  4096,  4115,  4549,  4584,  4590,  4746,  4804,  5053,  5369,\n",
      "         5496,  5531,  5559,  5578,  5588,  5675,  5922,  6011,  6175,  6229,\n",
      "         6267,  6363,  6512,  6515,  6619,  6717,  6760,  6783,  6886,  6968,\n",
      "         6975,  6994,  7009,  7056,  7118,  7130,  7251,  7588,  7623,  7948,\n",
      "         8057,  8073,  8193,  8206,  8334,  8354,  8482,  8554,  8609,  8853,\n",
      "         9029,  9145,  9153,  9250,  9323,  9328,  9538,  9642,  9700,  9746,\n",
      "         9747, 10017, 10212, 10289, 10824, 11481, 11584, 11667, 11769, 11811,\n",
      "        11818, 11858, 11886, 12207, 12255, 12402, 12582, 12694, 12716, 12729,\n",
      "        12905, 12913, 13028, 13154, 13201, 13223, 13270, 13355, 13444, 13576,\n",
      "        13611, 13635, 13676, 13733, 13939, 14073, 14243, 14383, 14541, 14606,\n",
      "        14685, 14794, 14842, 14988, 14992, 15023, 15275, 15768, 15826, 15889,\n",
      "        15965, 15989, 16009, 16030, 16131, 16180, 16384, 16504, 16746, 16853,\n",
      "        17053, 17100, 17323, 17535, 17566, 17654, 17687, 17852, 18113, 18142,\n",
      "        18221, 18313, 18671, 18706, 18926, 18948, 19296, 19350, 19365, 19385,\n",
      "        19398, 19485, 19587, 19684, 19875, 19916, 19924, 19988, 20222, 20281,\n",
      "        20326, 20424, 20468, 20517, 20518, 20674, 20751, 20785, 20856, 20881,\n",
      "        21020, 21027, 21121, 21185, 21287, 21420, 21461, 21485, 21710, 21723,\n",
      "        21783, 21835, 22078, 22136, 22393, 22568, 22633, 22879, 22968, 23012,\n",
      "        23013, 23107, 23201, 23278, 23344, 23689, 23893, 24065, 24154, 24398,\n",
      "        24479, 24536, 24550, 24798, 25053, 25085, 25268, 25288, 25412, 25447,\n",
      "        25529, 25672, 25716, 25810, 26034, 26143, 26322, 26346, 26657, 26670,\n",
      "        26727, 26906, 27775, 27924, 28235, 28332, 28444, 28496, 28563, 28682,\n",
      "        28858, 28913, 28993, 29103, 29353, 29572, 29594, 29663, 30103, 30468,\n",
      "        30534, 30558, 30621, 30677, 30682, 30698, 30893, 31011, 31103, 31561,\n",
      "        31623, 31686, 31789, 32123, 32161, 32228, 32443, 32455, 32532, 32614,\n",
      "        32666, 32674, 32731, 33040, 33099, 33112, 33187, 33300, 33403, 33460,\n",
      "        33548, 33563, 33631, 33670, 33736, 33871, 34278, 34311, 34356, 34591,\n",
      "        34694, 34766, 34977, 34979, 35030, 35150, 35220, 35335, 35492, 35541,\n",
      "        35559, 35696, 35723, 35805, 35806, 35939, 36222, 36315, 36377, 36492,\n",
      "        36599, 36673, 36772, 37053, 37116, 37151, 37233, 37282, 37294, 37308,\n",
      "        37436, 37612, 37688, 37717, 37914, 38067, 38118, 38128, 38251, 38385,\n",
      "        38522, 38568, 38591, 38687, 39291, 39327, 39427, 39696, 39700, 39801,\n",
      "        39812, 39824, 39881, 39960, 40033, 40062, 40105, 40325, 40355, 40507,\n",
      "        40538, 40693, 40772, 40803, 40828, 41081, 41111, 41151, 41279, 41454,\n",
      "        41681, 41831, 42204, 42220, 42298, 42413, 42546, 42661, 42747, 42784,\n",
      "        42798, 42882, 43130, 43289, 43292, 43463, 43469, 43489, 43814, 43815,\n",
      "        43895, 43907, 43949, 44049, 44214, 44427, 44463, 44477, 44543, 44738,\n",
      "        44932, 45038, 45159, 45186, 45599, 45653, 45985, 46017, 46101, 46153,\n",
      "        46188, 46347, 46417, 46553, 46579, 46602, 46647, 46685, 46696, 46811,\n",
      "        46969, 47070, 47447, 47534, 47542, 47560, 47629, 47667, 47916, 47933,\n",
      "        48023, 48026, 48032, 48112, 48479, 48715, 48763, 48774, 48933, 48985,\n",
      "        49039, 49301, 49355, 49629, 49744, 49923, 49945, 50008, 50086, 50161]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,712 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 8/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,803 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,   122,   137,   153,   198,   415,   433,   589,   650,   655,\n",
      "          901,   919,  1025,  1030,  1044,  1074,  1187,  1267,  1418,  1599,\n",
      "         1641,  1714,  1903,  2138,  2300,  2334,  2451,  2470,  2566,  2690,\n",
      "         2890,  2905,  3006,  3478,  3652,  3708,  3741,  3757,  3770,  3793,\n",
      "         3925,  4160,  4430,  4491,  4611,  4730,  4804,  4818,  5482,  5507,\n",
      "         5578,  5675,  5708,  5785,  5809,  5957,  6317,  6456,  6619,  6760,\n",
      "         6907,  6968,  7027,  7056,  7160,  7334,  7369,  7568,  7583,  7623,\n",
      "         7712,  7739,  7753,  8082,  8216,  8248,  8334,  8347,  8832,  9145,\n",
      "         9155,  9167,  9194,  9260,  9323,  9538,  9541,  9627,  9683,  9943,\n",
      "        10060, 10076, 10180, 10282, 10404, 10622, 10719, 10738, 10800, 11286,\n",
      "        11788, 11796, 11872, 11885, 11970, 12046, 12307, 12396, 12476, 12514,\n",
      "        12582, 12641, 12729, 13158, 13172, 13173, 13204, 13346, 13364, 13415,\n",
      "        13542, 13650, 13939, 14001, 14073, 14243, 14268, 14725, 14730, 14842,\n",
      "        14855, 14879, 14988, 15169, 15222, 15226, 15275, 15636, 15737, 16040,\n",
      "        16180, 16457, 16504, 16600, 16631, 16877, 16918, 17098, 17256, 17320,\n",
      "        17323, 17416, 17483, 17558, 17842, 17866, 17949, 17951, 18092, 18375,\n",
      "        18681, 18869, 18948, 18979, 18989, 19044, 19277, 19296, 19645, 19687,\n",
      "        19746, 20060, 20202, 20257, 20303, 20457, 20468, 20478, 20517, 20706,\n",
      "        20741, 20881, 20893, 20998, 21155, 21400, 21472, 21481, 21614, 21636,\n",
      "        21800, 21910, 21998, 22136, 22317, 22442, 22591, 22633, 22657, 23024,\n",
      "        23087, 23102, 23201, 23297, 23347, 23428, 23530, 23553, 23769, 23788,\n",
      "        23852, 23901, 24065, 24361, 24506, 24513, 24550, 25039, 25452, 25492,\n",
      "        25532, 25535, 25683, 25716, 25734, 25755, 25782, 25786, 25810, 25904,\n",
      "        25962, 26008, 26176, 26385, 26670, 26791, 26900, 27136, 27455, 27474,\n",
      "        27488, 27516, 27891, 28068, 28271, 28563, 28682, 28705, 29047, 29116,\n",
      "        29458, 29543, 29546, 29565, 29572, 29663, 29777, 29975, 30009, 30182,\n",
      "        30558, 30757, 30851, 30946, 31040, 31043, 31215, 31365, 31398, 31510,\n",
      "        31571, 31575, 31581, 31822, 31845, 31975, 32129, 32443, 32452, 32514,\n",
      "        32532, 32674, 32776, 32829, 33148, 33161, 33687, 33736, 34064, 34222,\n",
      "        34321, 34339, 34373, 34472, 34561, 34592, 34726, 34761, 34937, 35004,\n",
      "        35007, 35030, 35098, 35130, 35166, 35174, 35284, 35290, 35324, 35335,\n",
      "        35418, 35468, 35814, 35929, 36082, 36116, 36134, 36217, 36220, 36236,\n",
      "        36280, 36305, 36315, 36316, 36372, 36394, 36582, 36769, 36821, 36835,\n",
      "        37318, 37319, 37405, 37495, 37539, 37838, 37887, 37895, 37914, 37988,\n",
      "        38009, 38118, 38121, 38128, 38169, 38336, 38356, 38436, 38481, 38516,\n",
      "        38777, 38798, 38839, 38885, 39067, 39176, 39209, 39427, 39801, 39872,\n",
      "        39895, 39960, 39984, 40009, 40014, 40062, 40118, 40125, 40153, 40330,\n",
      "        40401, 40521, 40599, 40828, 41025, 41135, 41171, 41318, 41414, 41721,\n",
      "        41736, 41744, 41951, 41962, 42120, 42155, 42328, 42503, 42575, 42595,\n",
      "        42598, 42798, 42805, 43045, 43130, 43189, 43251, 43287, 43532, 43895,\n",
      "        44017, 44104, 44176, 44190, 44310, 44362, 44378, 44402, 44752, 44778,\n",
      "        45291, 45336, 45347, 45384, 45441, 45564, 45585, 45627, 45676, 45849,\n",
      "        45938, 46331, 46379, 46608, 46701, 46939, 47165, 47388, 47489, 47614,\n",
      "        47651, 47891, 47916, 48013, 48268, 48420, 48479, 48573, 48622, 48739,\n",
      "        48929, 48952, 48953, 49026, 49076, 49156, 49223, 49299, 49339, 49732,\n",
      "        49923]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,809 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 9/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,812 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH=\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\"\n",
    "\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=\"+CHECKPOINT_PATH,\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 08:44:40,317 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 08:44:40,322 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 08:44:40,326 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test = torch.load(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a shakespeare model from nanoGPT to check if inference script is faulty\n",
    "#### Model had a loss of around 0.7 after training and predicted words that resembled shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_NANOGPT_PATH = \"/home/mabot004/nanoGPT/out-shakespeare/ckpt.pt\"\n",
    "checkpoint = torch.load(CHECKPOINT_NANOGPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_cfg instead of model_args, model_state_dict instead of model\n",
    "#no tokenizer config -> specify in hydra config\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'iter_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miter_num\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'iter_num'"
     ]
    }
   ],
   "source": [
    "checkpoint[\"epoch\"] = checkpoint[\"iter_num\"]\n",
    "checkpoint[\"model_state_dict\"] = checkpoint[\"model\"]\n",
    "checkpoint[\"model_cfg\"] = checkpoint[\"model_args\"]\n",
    "del checkpoint[\"iter_num\"]\n",
    "del checkpoint[\"model\"]\n",
    "del checkpoint[\"model_args\"]\n",
    "checkpoint[\"tokenizer_cfg\"] = {'dtype': 'float32', \n",
    "                               'meta_file': 'meta.pkl', \n",
    "                               'wrapper': 'TikTokenizer', \n",
    "                               'encoding': 'gpt2', \n",
    "                               'module': 'tiktoken', \n",
    "                               'meta': {\n",
    "                                   'max_token_value': 50256, \n",
    "                                   'encoding': 'gpt2', \n",
    "                                   'dtype': 'float32', \n",
    "                                   'num_tokens': 338027, \n",
    "                                   'module': 'tiktoken'\n",
    "                                }\n",
    "                              }\n",
    "checkpoint[\"model_cfg\"] = {\n",
    "    \"cls\": \"GPT\",\n",
    "    \"calc_loss_in_model\": True,\n",
    "    \"args\": {\n",
    "      \"n_layer\" : checkpoint[\"model_cfg\"][\"n_layer\"],\n",
    "      \"n_head\" : checkpoint[\"model_cfg\"][\"n_head\"],\n",
    "      \"n_embd\" : checkpoint[\"model_cfg\"][\"n_embd\"],\n",
    "      \"dropout\" : checkpoint[\"model_cfg\"][\"dropout\"],\n",
    "      \"bias\" :  checkpoint[\"model_cfg\"][\"bias\"],\n",
    "      \"block_size\" : checkpoint[\"model_cfg\"][\"block_size\"],\n",
    "      \"vocab_size\" : checkpoint[\"model_cfg\"][\"vocab_size\"],\n",
    "      \"transformer_active_func\": \"GELU\",\n",
    "      \"norm_layer\": \"LayerNorm\",\n",
    "      \"flash\": False \n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(checkpoint, \"karpathy_shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since karpathy used a larger vocabulary than the tokenizer, some tokens could not be encoded\n",
    "#### Even though karpathy's inference generated good sentences, ours does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 09:17:24,190 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare'}}\n",
      "[ \u001b[36m2024-02-14 09:17:24,383 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,388 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,391 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,395 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,399 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,403 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,408 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,711 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mNo info specified if checkpoint is quantized. Assuming false.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,719 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'cls': 'GPT', 'calc_loss_in_model': True, 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': False, 'block_size': 256, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,721 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,760 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=256, vocab_size=50304, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,991 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,010 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,096 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,113 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,202 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,213 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,295 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,313 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,390 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,401 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,423 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,501 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,970 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.88M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,977 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': 'float32',\n",
      " 'encoding': 'gpt2',\n",
      " 'meta': {'dtype': 'float32',\n",
      "          'encoding': 'gpt2',\n",
      "          'max_token_value': 50256,\n",
      "          'module': 'tiktoken',\n",
      "          'num_tokens': 338027},\n",
      " 'meta_file': 'meta.pkl',\n",
      " 'module': 'tiktoken',\n",
      " 'wrapper': 'TikTokenizer'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,981 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,990 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': 'float32', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,993 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenizer config is of type dict. Creating DictConfig object.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,999 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': 'float32', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,498 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,523 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating infer dir: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,530 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_09:17:26_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,534 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:18:54,495 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   113,   154,   198,   201,   394,   496,   702,   760,   763,\n",
      "         1004,  1286,  1512,  1634,  1765,  2001,  2326,  2347,  2418,  2676,\n",
      "         2894,  3497,  3736,  3802,  4071,  4133,  4206,  4262,  4337,  4367,\n",
      "         4368,  4459,  4592,  4633,  4712,  4803,  4994,  5073,  5130,  5210,\n",
      "         5511,  5557,  5632,  5929,  6072,  6491,  6597,  6733,  6772,  6924,\n",
      "         6995,  7358,  7485,  7668,  8022,  8085,  8124,  8248,  8266,  8300,\n",
      "         8346,  8374,  8488,  8512,  8675,  8740,  8772,  8852,  8911,  8923,\n",
      "         9047,  9162,  9171,  9172,  9225,  9670,  9680,  9895,  9938,  9980,\n",
      "        10020, 10175, 10319, 10355, 10391, 10556, 10608, 10628, 10802, 10840,\n",
      "        11002, 11134, 11160, 11168, 11236, 11299, 11340, 11414, 11456, 11588,\n",
      "        11762, 11782, 12137, 12469, 12592, 12702, 12858, 12977, 13102, 13134,\n",
      "        13542, 13553, 13678, 13868, 14166, 14345, 14347, 14396, 14469, 14557,\n",
      "        14658, 14659, 14847, 14872, 15278, 15336, 15521, 15586, 15657, 16276,\n",
      "        16400, 16402, 16525, 16720, 16777, 16864, 16919, 17083, 17084, 17224,\n",
      "        17372, 17426, 17540, 17958, 18069, 18233, 18243, 18293, 18526, 18551,\n",
      "        18686, 18701, 18890, 19126, 19157, 19583, 20096, 20681, 21208, 21340,\n",
      "        21379, 21487, 21493, 21682, 21710, 21729, 22057, 22081, 22198, 22245,\n",
      "        22382, 22415, 22437, 22605, 22651, 22822, 23068, 23098, 23166, 23259,\n",
      "        23498, 23586, 24040, 24111, 24148, 24408, 24434, 24695, 24751, 25159,\n",
      "        25171, 25684, 25788, 25817, 25925, 26128, 26217, 26249, 26540, 26603,\n",
      "        26713, 26769, 26869, 26921, 27290, 27305, 27328, 27681, 27682, 27740,\n",
      "        27850, 27934, 28114, 28436, 28588, 28836, 28876, 28913, 29077, 29456,\n",
      "        29544, 29713, 29806, 29851, 30087, 30271, 30424, 30475, 30584, 30712,\n",
      "        30715, 30967, 31121, 31194, 31223, 31647, 31720, 31852, 31862, 31906,\n",
      "        31945, 32072, 32231, 32325, 32455, 32680, 32752, 32837, 32955, 32956,\n",
      "        33055, 33085, 33135, 33176, 33281, 33316, 33449, 33458, 33464, 33627,\n",
      "        33671, 33782, 33926, 34044, 34686, 34807, 35171, 35440, 35445, 35514,\n",
      "        35548, 35639, 35944, 36152, 36295, 36668, 36680, 36738, 36949, 37048,\n",
      "        37143, 37366, 37555, 37744, 37993, 38148, 38322, 38437, 38501, 38725,\n",
      "        38766, 38963, 39497, 39524, 39591, 39631, 39659, 39707, 39928, 40295,\n",
      "        40332, 40376, 40461, 40477, 40808, 40815, 40976, 41027, 41074, 41166,\n",
      "        41179, 41440, 41559, 41562, 41673, 41845, 42200, 42211, 42394, 42734,\n",
      "        42750, 42808, 42835, 42894, 42962, 43449, 43620, 43878, 43916, 44025,\n",
      "        44040, 44058, 44472, 44500, 44691, 44826, 44913, 45707, 45914, 46066,\n",
      "        46158, 46294, 46310, 46312, 46470, 46856, 46951, 46989, 47112, 47198,\n",
      "        47276, 47343, 47753, 47793, 47968, 48052, 48138, 48459, 48532, 48583,\n",
      "        48679, 48834, 49075, 49159, 49209, 49281, 49794, 49841, 50096, 50186]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:18:54,500 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:20:23,594 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   77,   198,   208,   817,  1051,  1653,  1791,  1997,  2169,  2767,\n",
      "         2807,  3032,  3336,  3350,  3393,  3736,  3997,  4007,  4136,  4331,\n",
      "         4343,  4414,  4459,  4468,  4528,  4615,  4803,  5063,  5216,  5252,\n",
      "         5421,  5540,  5569,  5657,  5929,  6586,  6750,  6839,  6844,  7723,\n",
      "         7761,  7983,  8079,  8124,  8225,  8250,  8374,  8512,  8577,  8610,\n",
      "         8768,  9047,  9159,  9172,  9245,  9387,  9591,  9612,  9985,  9989,\n",
      "        10020, 10022, 10260, 10271, 10319, 10398, 10574, 10827, 10907, 11134,\n",
      "        11216, 11224, 11540, 11551, 11601, 11699, 11976, 12160, 12281, 12376,\n",
      "        12930, 12969, 13076, 13417, 13595, 13641, 13893, 13904, 13964, 13966,\n",
      "        14014, 14102, 14129, 14196, 14250, 14277, 14579, 14850, 14857, 15196,\n",
      "        15232, 15354, 15459, 15464, 15666, 15716, 15948, 15978, 16276, 16429,\n",
      "        16470, 17010, 17199, 17282, 17330, 17464, 17976, 18206, 18220, 18302,\n",
      "        18365, 18485, 18526, 18551, 18731, 18890, 19036, 19192, 19230, 19260,\n",
      "        19316, 19537, 19552, 19583, 19807, 20104, 20148, 20280, 20281, 20300,\n",
      "        20382, 20525, 20637, 20681, 20808, 21002, 21017, 21118, 21229, 21409,\n",
      "        21549, 21578, 21666, 21713, 21831, 22092, 22162, 22172, 22532, 22832,\n",
      "        22887, 23058, 23131, 23178, 23257, 23269, 23586, 23933, 24040, 24785,\n",
      "        25148, 25246, 25336, 25554, 25595, 26119, 26190, 26217, 26364, 26389,\n",
      "        26663, 26687, 27394, 27416, 27653, 27798, 28436, 28466, 28641, 28842,\n",
      "        28876, 28967, 29164, 29170, 29544, 29657, 29802, 29805, 29905, 30055,\n",
      "        30216, 30303, 30390, 30584, 30660, 30712, 30715, 30813, 31223, 31480,\n",
      "        31583, 31705, 31761, 31998, 32114, 32497, 32653, 32752, 32759, 33050,\n",
      "        33224, 33316, 33348, 33352, 33421, 33627, 33724, 33812, 34227, 34538,\n",
      "        34818, 35440, 35578, 35666, 35724, 35753, 35942, 35944, 36077, 36161,\n",
      "        36194, 36492, 36668, 36895, 37378, 37539, 37689, 37707, 37744, 38008,\n",
      "        38029, 38129, 38473, 38559, 38600, 38633, 38703, 38871, 39100, 39241,\n",
      "        39248, 39361, 39428, 39496, 39707, 39752, 39888, 39932, 40200, 40332,\n",
      "        40363, 40371, 40376, 40528, 40587, 40709, 40737, 40747, 40980, 41253,\n",
      "        41312, 41470, 41474, 41825, 42094, 42407, 42437, 42456, 42709, 42757,\n",
      "        43194, 43569, 43848, 43878, 44040, 44115, 44500, 44573, 44636, 44817,\n",
      "        44875, 44903, 44908, 44919, 45312, 45714, 45725, 45952, 46060, 46110,\n",
      "        46123, 46815, 46951, 47002, 47114, 47276, 47299, 47819, 47956, 48176,\n",
      "        48329, 48765, 48834, 48838, 48883, 49182, 49197, 49248, 49530, 49962,\n",
      "        50098, 50108, 50173, 50227]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:20:23,599 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:22:36,013 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   90,   172,   198,   325,   496,   826,   860,  1003,  1150,  1335,\n",
      "         1541,  1595,  1689,  1828,  1938,  1957,  1998,  2018,  2342,  2553,\n",
      "         2844,  2894,  3033,  3336,  3440,  3462,  4007,  4012,  4157,  4390,\n",
      "         4406,  4582,  4667,  4803,  5073,  5075,  5130,  5144,  5289,  5340,\n",
      "         5714,  6076,  6370,  6386,  6431,  6488,  6505,  6625,  6668,  6728,\n",
      "         6744,  6810,  7051,  7252,  7405,  7437,  7559,  7668,  7704,  7723,\n",
      "         7935,  7985,  8079,  8240,  8250,  8374,  8419,  8518,  8610,  8663,\n",
      "         8740,  8809,  9025,  9075,  9298,  9334,  9640,  9789,  9820, 10020,\n",
      "        10227, 10370, 10417, 10669, 10827, 10900, 10907, 10958, 10985, 10991,\n",
      "        11107, 11183, 11333, 11382, 11630, 12264, 12445, 13054, 13104, 13486,\n",
      "        13595, 13620, 13678, 13760, 13833, 13904, 14156, 14307, 14460, 14790,\n",
      "        14795, 14842, 14872, 14931, 15104, 15213, 15315, 15396, 15569, 16174,\n",
      "        16276, 16326, 16598, 16864, 17063, 17226, 17248, 17282, 17409, 17528,\n",
      "        17670, 17808, 17909, 18180, 18266, 18426, 18551, 18688, 18878, 19263,\n",
      "        19292, 19422, 19800, 19915, 19937, 20052, 20255, 20281, 20668, 20681,\n",
      "        21033, 21130, 21156, 21229, 21289, 21487, 21573, 21710, 21986, 21996,\n",
      "        22063, 22198, 22296, 22300, 22370, 22415, 22824, 22990, 23056, 23068,\n",
      "        23184, 23265, 23269, 23529, 23662, 23796, 24111, 24202, 24281, 24527,\n",
      "        24747, 24797, 24829, 24951, 25084, 25273, 25562, 25638, 25712, 25762,\n",
      "        25820, 25983, 26030, 26190, 26217, 26296, 26355, 26384, 26394, 26585,\n",
      "        26712, 26800, 27002, 27249, 27265, 27330, 27407, 27826, 27899, 28012,\n",
      "        28323, 28466, 28641, 28718, 29013, 29015, 29513, 30238, 30316, 30353,\n",
      "        30374, 30474, 30618, 30712, 31223, 31554, 31583, 31669, 31758, 31852,\n",
      "        32236, 32653, 32713, 32752, 33095, 33181, 33264, 33415, 33864, 34044,\n",
      "        34133, 34212, 34459, 35099, 35156, 35305, 35440, 35445, 35666, 35870,\n",
      "        35944, 36152, 36510, 36676, 36680, 36787, 36895, 37048, 37106, 37144,\n",
      "        37201, 37205, 37241, 37490, 37744, 37765, 37948, 38332, 38405, 38533,\n",
      "        38579, 38600, 38630, 38703, 38733, 38772, 38872, 39015, 39059, 39065,\n",
      "        39149, 39241, 39307, 39611, 39707, 39711, 39723, 39734, 39754, 39836,\n",
      "        39978, 40141, 40371, 40668, 40787, 41090, 41463, 41557, 41819, 41845,\n",
      "        42016, 42211, 42479, 42646, 43299, 43302, 43659, 43878, 43920, 43956,\n",
      "        43993, 44025, 44040, 44058, 44590, 44647, 44685, 44857, 44913, 45236,\n",
      "        45480, 45495, 45714, 45971, 45972, 46150, 46167, 46278, 46539, 46588,\n",
      "        46685, 46770, 46814, 46951, 46954, 47169, 47276, 47679, 47777, 47819,\n",
      "        48054, 48075, 48249, 48312, 48432, 49000, 49166, 49181, 49746, 49776,\n",
      "        49787, 49794, 49843, 50098, 50141, 50224]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:22:36,017 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:24:35,904 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   65,   198,   456,   558,   689,   858,   937,  1221,  1485,  1512,\n",
      "         1757,  1938,  1950,  1952,  2406,  2418,  2585,  2767,  3280,  3477,\n",
      "         3707,  3769,  4071,  4331,  4509,  4649,  4769,  4803,  5063,  5073,\n",
      "         5160,  5200,  5235,  5328,  5829,  6214,  6399,  6625,  6694,  7440,\n",
      "         7723,  7890,  7983,  8300,  8456,  8585,  8591,  8931,  9140,  9184,\n",
      "         9492,  9578,  9612,  9652,  9680,  9699,  9799,  9867, 10022, 10238,\n",
      "        10319, 10373, 10398, 10462, 10508, 10574, 11002, 11156, 11224, 11331,\n",
      "        11382, 11540, 11630, 11844, 12257, 12469, 12730, 12964, 13018, 13323,\n",
      "        13333, 13417, 13462, 13654, 13706, 14056, 14307, 14336, 14384, 14404,\n",
      "        14893, 14931, 14995, 15117, 15196, 15509, 15555, 15994, 16065, 16326,\n",
      "        16429, 16470, 16537, 16787, 17010, 17064, 17282, 17311, 17393, 17666,\n",
      "        18069, 18320, 18356, 18451, 18525, 18832, 18890, 19120, 19554, 19670,\n",
      "        19729, 19800, 19905, 20084, 20382, 20495, 20525, 20681, 20842, 21156,\n",
      "        21310, 21548, 21549, 21713, 21729, 21888, 21957, 21970, 21986, 22109,\n",
      "        22242, 22245, 22269, 22296, 22807, 22900, 22960, 22972, 23295, 23395,\n",
      "        23792, 23794, 24065, 24111, 24685, 24694, 24695, 24829, 24850, 24872,\n",
      "        24908, 25076, 25084, 25182, 25231, 25348, 25435, 25595, 25878, 26044,\n",
      "        26047, 26120, 26217, 26384, 26673, 26798, 27233, 27798, 27807, 28124,\n",
      "        28273, 28436, 28466, 28487, 28538, 28647, 28718, 28876, 28967, 29015,\n",
      "        29017, 29209, 29216, 29390, 29669, 29880, 29922, 30250, 30424, 30450,\n",
      "        30712, 30715, 30967, 31198, 31206, 31420, 31480, 31626, 31693, 31705,\n",
      "        31847, 31852, 32353, 32497, 32832, 32925, 33050, 33204, 33224, 33421,\n",
      "        33627, 33728, 33924, 33949, 33950, 34049, 34374, 34545, 34828, 34965,\n",
      "        35196, 35262, 35461, 35667, 35825, 35856, 35905, 36443, 36529, 36540,\n",
      "        36676, 37102, 37479, 37718, 37744, 37792, 37908, 37959, 38012, 38029,\n",
      "        38134, 38388, 38490, 38703, 38733, 38810, 38939, 39058, 39707, 39708,\n",
      "        39711, 39718, 39752, 39767, 39888, 39932, 40011, 40029, 40073, 40281,\n",
      "        40587, 40729, 40840, 40974, 41000, 41074, 41116, 41117, 41162, 41253,\n",
      "        41269, 41303, 41652, 41845, 42094, 42200, 42437, 42528, 43159, 43235,\n",
      "        43302, 43596, 43620, 43669, 43804, 44043, 44353, 44573, 44875, 44908,\n",
      "        45312, 45714, 45736, 45952, 46023, 46222, 46310, 46317, 46466, 46794,\n",
      "        46875, 46931, 46951, 47002, 47006, 47276, 47317, 47733, 47819, 47935,\n",
      "        48120, 48249, 48294, 48472, 48765, 49020, 49183, 49209, 49299, 49315,\n",
      "        49408, 49530, 49811, 49997, 50113, 50179, 50186, 50228]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:24:35,914 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:26:31,805 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   201,   208,   304,   394,   554,   558,   630,   689,   991,\n",
      "         1329,  1375,  1503,  1705,  1757,  1788,  1791,  1957,  1962,  1977,\n",
      "         1997,  2079,  2560,  2568,  2742,  2767,  3012,  3118,  3280,  3349,\n",
      "         3350,  3704,  3846,  3997,  4117,  4274,  4421,  4468,  4655,  5063,\n",
      "         5073,  5235,  5340,  5375,  5413,  5479,  5714,  5821,  5929,  6399,\n",
      "         6418,  6582,  6597,  7130,  7301,  7597,  7935,  7983,  8040,  8079,\n",
      "         8188,  8225,  8242,  8374,  8488,  8610,  8675,  8809,  8931,  9047,\n",
      "         9100,  9225,  9325,  9347,  9459,  9569, 10020, 10022, 10054, 10278,\n",
      "        10373, 10398, 10827, 10964, 10991, 11002, 11224, 11700, 11940, 12160,\n",
      "        12257, 12296, 12420, 12534, 12581, 12785, 12900, 12930, 13018, 13036,\n",
      "        13104, 13125, 13221, 13264, 13282, 13417, 13654, 13709, 13904, 13948,\n",
      "        14019, 14062, 14307, 14743, 14829, 14872, 15045, 15078, 15117, 15196,\n",
      "        15341, 15701, 15865, 16288, 16484, 16864, 17171, 17282, 17323, 17444,\n",
      "        17464, 17811, 18266, 18526, 18890, 18946, 19230, 19260, 19352, 19357,\n",
      "        19579, 19607, 19729, 19862, 19983, 20084, 20201, 20396, 20405, 20681,\n",
      "        20797, 20808, 20842, 20972, 20999, 21002, 21033, 21038, 21229, 21825,\n",
      "        21967, 22109, 22269, 22296, 22532, 22627, 22957, 23392, 24031, 24111,\n",
      "        24195, 24451, 24695, 24756, 25244, 25274, 25336, 25389, 25449, 25638,\n",
      "        26030, 26217, 26364, 26384, 26537, 26673, 26713, 27057, 27223, 27225,\n",
      "        27269, 27416, 27807, 27837, 27869, 28085, 28439, 28452, 28641, 28752,\n",
      "        29337, 29496, 29501, 29827, 29841, 29848, 30250, 30320, 30374, 30509,\n",
      "        30584, 30660, 30712, 30774, 30849, 31205, 31480, 31496, 31626, 31907,\n",
      "        32049, 32270, 32357, 32448, 32499, 32653, 32752, 33204, 33263, 33316,\n",
      "        33415, 33458, 33627, 33787, 33860, 33971, 34049, 34109, 34165, 34502,\n",
      "        34519, 34536, 34630, 34785, 34828, 34903, 35077, 35216, 35440, 35445,\n",
      "        35461, 35944, 36194, 36317, 36387, 36540, 36676, 36738, 36787, 36929,\n",
      "        36946, 37042, 37099, 37102, 37191, 37539, 37718, 37792, 38004, 38129,\n",
      "        38362, 38490, 38565, 38796, 38835, 38963, 39052, 39248, 39428, 39675,\n",
      "        39718, 39784, 39928, 40073, 40329, 40339, 40371, 40587, 40877, 40958,\n",
      "        41067, 41166, 41174, 41591, 41683, 41747, 41765, 41870, 42089, 42315,\n",
      "        42479, 42550, 42750, 43123, 43146, 43576, 43582, 43738, 43795, 43834,\n",
      "        44025, 44040, 44550, 44573, 44898, 45114, 45312, 45359, 45714, 45898,\n",
      "        45952, 45972, 46023, 46310, 47002, 47062, 47169, 47276, 47299, 47317,\n",
      "        47365, 47377, 47679, 47968, 48120, 48604, 48765, 48834, 48966, 49009,\n",
      "        49277, 49594, 49752, 49787, 50287]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344]))\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at src/lib.rs:201:64:\n",
      "no entry found for key\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "no entry found for key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=infer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferonnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inferonnx\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:52\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     51\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:230\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(cfg, device)\u001b[0m\n\u001b[1;32m    227\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_new_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mhex\u001b[39m(\u001b[38;5;28mord\u001b[39m(x))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstart]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    229\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- BEGIN INFERENCE -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_infer):\n\u001b[1;32m    231\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:203\u001b[0m, in \u001b[0;36minfer.<locals>.write_inference\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m    200\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniquely generated tokens, sorted in ascending order: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39msort()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#TODO: model could have larger vocabulary size than the tokenizer's max_token_value\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#      for character tokenization, a sequence of <UNKNOWN> chars will be printed. for tiktoken, inference crashes\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/dataset/tokenizer/tiktoken.py:35\u001b[0m, in \u001b[0;36mTikTokenizer.decode\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tiktoken/core.py:254\u001b[0m, in \u001b[0;36mEncoding.decode\u001b[0;34m(self, tokens, errors)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;31mPanicException\u001b[0m: no entry found for key"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\",\n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"device=cuda\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make our checkpoint compatible with karpathy's inference script and see if inference is bettereki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAKESPEARE_QTRANSFORM_PATH = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\"\n",
    "checkpoint_qtransform = torch.load(SHAKESPEARE_QTRANSFORM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_qtransform[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_qtransform[\"model_args\"] = dict(checkpoint_qtransform[\"model_cfg\"][\"args\"])\n",
    "del checkpoint_qtransform[\"model_args\"][\"transformer_active_func\"]\n",
    "del checkpoint_qtransform[\"model_args\"][\"norm_layer\"]\n",
    "del checkpoint_qtransform[\"model_args\"][\"flash\"]\n",
    "checkpoint_qtransform[\"iter_num\"] = checkpoint_qtransform[\"epoch\"]\n",
    "checkpoint_qtransform[\"optimizer\"] = checkpoint_qtransform[\"optimizer_state_dict\"]\n",
    "checkpoint_qtransform[\"model\"] = checkpoint_qtransform[\"model_state_dict\"]\n",
    "checkpoint_qtransform[\"best_val_loss\"] = checkpoint_qtransform[\"metrics\"]\n",
    "checkpoint_qtransform[\"config\"] = {\n",
    "    'out_dir': 'out-shakespeare',\n",
    "    'eval_interval': 250,\n",
    "     'log_interval': 10,\n",
    "     'eval_iters': 200,\n",
    "     'eval_only': False,\n",
    "     'always_save_checkpoint': False,\n",
    "     'init_from': 'scratch',\n",
    "     'wandb_log': False,\n",
    "     'wandb_project': 'shakespeare',\n",
    "     'wandb_run_name': 'mini-gpt',\n",
    "     'dataset': 'shakespeare',\n",
    "     'gradient_accumulation_steps': 1,\n",
    "     'batch_size': 64,\n",
    "     'block_size': 256,\n",
    "     'n_layer': 2,\n",
    "     'n_head': 2,\n",
    "     'n_embd': 256,\n",
    "     'dropout': 0.0,\n",
    "     'bias': True,\n",
    "     'learning_rate': 0.001,\n",
    "     'max_iters': 5000,\n",
    "     'weight_decay': 0.1,\n",
    "     'beta1': 0.9,\n",
    "     'beta2': 0.99,\n",
    "     'grad_clip': 1.0,\n",
    "     'decay_lr': True,\n",
    "     'warmup_iters': 100,\n",
    "     'lr_decay_iters': 5000,\n",
    "     'min_lr': 0.0001,\n",
    "     'backend': 'nccl',\n",
    "     'device': 'cuda',\n",
    "     'dtype': 'bfloat16',\n",
    "     'compile': True}\n",
    "del checkpoint_qtransform[\"model_state_dict\"]\n",
    "del checkpoint_qtransform[\"optimizer_state_dict\"]\n",
    "del checkpoint_qtransform[\"epoch\"]\n",
    "del checkpoint_qtransform[\"model_cfg\"]\n",
    "del checkpoint_qtransform[\"tokenizer_cfg\"]\n",
    "del checkpoint_qtransform[\"metrics\"]\n",
    "del checkpoint_qtransform[\"quant_cfg\"]\n",
    "del checkpoint_qtransform[\"quantized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_args', 'iter_num', 'optimizer', 'model', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_qtransform.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(checkpoint_qtransform, \"/home/mabot004/nanoGPT/out-shakespeare/qtransform_shakespeare_karpathy.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despite using karpathy's inference script, our model still generates nonsense sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with karpathy's params for Shakespeare (https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py)\n",
    "#### Eval loss is lower than training loss, so it does not seem to overfit\n",
    "#### Loss does not seem to lower significantly after the 3rd/4th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': [{'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}], 'milestones': None, 'warmup_iters': 100, 'min_lr': 6e-05}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 10, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-20 13:11:42,948 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:42,950 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:42,952 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:11:43.581820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-20 13:11:44,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,001 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-20_13:11:45\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,008 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,014 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,304 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,311 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,315 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,318 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,320 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 118309 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,322 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,324 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,358 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,535 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,582 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,606 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,616 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,690 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,699 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,796 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,805 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,880 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,889 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,905 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:45,915 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:46,412 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:48,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-20 13:11:48,548 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/10\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     24\u001b[0m beta2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m \u001b[38;5;66;03m# make a bit bigger because number of tokens per iter is small\u001b[39;00m\n\u001b[1;32m     26\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:108\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    105\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    111\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#dataloader always returns the same tensors after each epoch because it is casted inside of function call\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#therefore, cast it before training\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#TODO: find a more elegant solution, maybe by manipulating its seed with a torch.Generator?\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:222\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    220\u001b[0m gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_accumulation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m#dataloader already iterable, refer to TODO from train function for randomness in samples\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_data, \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataLoader):\n\u001b[1;32m    223\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCasting dataloader to iterable.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    224\u001b[0m     train_data: data\u001b[38;5;241m.\u001b[39mdataloader\u001b[38;5;241m.\u001b[39m_MultiProcessingDataLoaderIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_data)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py\n",
    "#karpathy evaluates after 250 iterations, we implemented eval to do so after every epoch -> max_iters = 5000 / 200\n",
    "eval_epoch_interval = 1 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "max_iters = 250\n",
    "epochs = 10 #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = 1 #one large batch, potentially do gradient_accumulation_steps = 8 and batch_size = 8\n",
    "batch_size = 64\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "# baby GPT model :)\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "\n",
    "#not implemented currently\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "\n",
    "#not used currently\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if custom_ln layers had their params back propagated\n",
    "import re\n",
    "ckpt_shakespeare = torch.load(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\")\n",
    "#custom_ln are identity layers in this case\n",
    "list(filter(lambda x: re.search(r'custom_ln[1-2]', x), ckpt_shakespeare[\"model_state_dict\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3'}}\n",
      "[ \u001b[36m2024-02-20 09:19:20,966 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,980 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,984 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,987 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,990 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,993 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:20,997 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,328 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,330 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,336 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,483 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,515 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,598 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,676 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,696 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,776 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,794 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,884 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,909 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:21,995 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,076 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,496 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,502 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,505 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,512 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,514 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,518 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,546 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-20_09:19:22_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:19:22,549 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:20:18,879 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 49815\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:20:18,887 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:21:13,284 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50190\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:21:13,292 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:22:07,883 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50190\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:22:07,889 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:22:07,892 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-20_09:19:22_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#run inference again, this time with karpathy's params\n",
    "#generated file: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_11:29:32_CHECKPOINT.out\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        #\"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3\",\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run perplexity benchmark\n",
    "#### Inference generates nonsense, perplexity very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-20 09:23:50,763 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 64}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 100, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-20 09:23:50,987 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:50,991 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:50,994 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:50,997 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-20_09:23:50\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,000 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,005 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,008 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,014 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,021 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,024 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,028 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,031 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,035 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,041 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,044 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,046 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,050 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,054 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,057 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 77744, start is 0.23, end is 0.28\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,060 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 77744\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,063 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,066 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,068 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.4\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,070 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,073 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 135210 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,076 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,080 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,494 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,497 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,505 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,652 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,664 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,686 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,700 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,805 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,816 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,903 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:51,915 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,005 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,015 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,100 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,118 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,606 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,634 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,637 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,643 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,645 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:52,648 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:23:54,561 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m\u001b[0m\n",
      "\n",
      "   avg_ppl \n",
      "\n",
      "   50106.7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3\",\n",
    "    \"run.num_samples=100\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=tiny_shakespeare\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=64\",\n",
    "    \"dataset.sizes.bench=0.4\"\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use karpathy's inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])\n",
      "[ \u001b[36m2024-02-20 09:36:30,098 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,248 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,267 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,296 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,376 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,393 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,476 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,497 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,507 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,597 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,676 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,709 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:30,780 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:36:31,222 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " man hands\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " war\n",
      "\n",
      "\n",
      "rea\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " be you\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " lips\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "::\n",
      " of or: child\n",
      "\n",
      "D all\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " known\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      " worthy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Like\n",
      "\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " worthy\n",
      "\n",
      "\n",
      " and and\n",
      "\n",
      " ' what now\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " CorP but\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ye\n",
      "\n",
      "\n",
      " and and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and or\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      ":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::;::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and,\n",
      "\n",
      "\n",
      " O\n",
      "\n",
      "\n",
      "\n",
      " traitor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you:\n",
      "\n",
      "\n",
      " very\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      " and what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " as\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      " if\n",
      "\n",
      " for\n",
      "\n",
      " dispatch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " like\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " there\n",
      "\n",
      "\n",
      "Were\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "he but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " being\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "he but be up\n",
      ":\n",
      "the\n",
      " where\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ' ' i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " '\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      " for or:\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ile\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for as for if for for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " whose\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      " you where for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      " for for:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " renowned\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Though\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "With which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "!\n",
      "\n",
      " as\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Within or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "Like I\n",
      " by has and but of for but--\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " i what or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      " and sir here you as\n",
      " or or there for for now I but and\n",
      "\n",
      "\n",
      "\n",
      " o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Most\n",
      "\n",
      " against and\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "b there at for then\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " these\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and I then--\n",
      "\n",
      "\n",
      " you o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      " then\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      " which\n",
      "\n",
      " against\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      " or\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      " you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference_karpathy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 91\u001b[0m, in \u001b[0;36minference_karpathy\u001b[0;34m(ckpt_path, start)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 91\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:281\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    279\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    283\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:141\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:264\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:192\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!HELP ME\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# this if block is needed for toprch <2.21 where flash attention onnx export does not work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m#QuantMultiheadAttention does not have is_causal in constructor -> use attention mask instead\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5337\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5335\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m-> 5337\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5338\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   5339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5340\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inference_karpathy('/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint on our own and forward pass it some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-20 09:43:16,617 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,743 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,754 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,773 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,785 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,799 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,876 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,890 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,898 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:16,995 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:17,010 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:17,091 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:17,105 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 09:43:17,586 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 2D attn_mask is torch.Size([64, 64]), but should be (1, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(gpt2_encoding\u001b[38;5;241m.\u001b[39mencode_ordinary(start))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 11\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:141\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:264\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:192\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!HELP ME\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# this if block is needed for toprch <2.21 where flash attention onnx export does not work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m#QuantMultiheadAttention does not have is_causal in constructor -> use attention mask instead\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5215\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5213\u001b[0m     correct_2d_size \u001b[38;5;241m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   5214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 5215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 2D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_2d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5216\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   5217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([64, 64]), but should be (1, 1)."
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_09:03:43__epoch:3'\n",
    "checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "gptconf = GPTConfig(**checkpoint['model_cfg'][\"args\"])\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = GPT(gptconf).to(device=device)\n",
    "model.eval()\n",
    "start = \"\\n\"\n",
    "from tiktoken import get_encoding\n",
    "gpt2_encoding = get_encoding(\"gpt2\")\n",
    "input = torch.Tensor(gpt2_encoding.encode_ordinary(start)).to(device=device, dtype=torch.long).unsqueeze(dim=0)\n",
    "logits = model(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LayerNorm and check if inference is going better\n",
    "#### The loss after the first epoch is way higher than with batchnorm (4.5 with layernorm compared to 1.7 with batchnorm)\n",
    "#### Using layernorm for inference did not help either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:45:55,161 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 200, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-14 11:45:55,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,377 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,379 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-14_11:45:55\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,380 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,381 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,382 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,387 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,393 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,394 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,397 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,398 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,400 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,402 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,404 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,405 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,407 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,409 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,410 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 77744, start is 0.23, end is 0.28\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,411 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 77744\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,412 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,415 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,417 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,423 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,424 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,429 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,570 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,592 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,611 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,620 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,714 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,794 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,817 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,890 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,918 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,991 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,015 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,097 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,538 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,559 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,562 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,565 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'weight_decay', 'betas', 'lr'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,567 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,571 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.098379898071289\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.4451171875\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.493041849136352\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.343642950057983\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,955 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 6.271537733078003\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 6.260971879959106\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 6.223812294006348\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 6.129550313949585\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,083 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 5.998541307449341\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.842705154418946\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.726846313476562\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.6491515159606935\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,206 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 5.490401029586792\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,479 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.358382272720337\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,757 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.272852039337158\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,035 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.118131446838379\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,314 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.096799993515015\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.945697975158692\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,867 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.849489974975586\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,149 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.819471025466919\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.774138307571411\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.759201622009277\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.679498815536499\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.632328081130981\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,539 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.531281566619873\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,811 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.531898069381714\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:02,782 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/200: 4.566741943359375\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:02,784 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.531898069381714\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,356 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,568 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.45096392631530763\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.449897623062133\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.47121376991272\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.357682180404663\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.440573263168335\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,966 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.40941891670227\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.347307109832764\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.344051218032837\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.370062971115113\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,077 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.4027611255645756\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.366365909576416\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,632 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.328843879699707\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,915 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.340453290939331\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,193 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.270555067062378\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,470 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.294016790390015\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.271436929702759\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.313546085357666\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.2667933940887455\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,581 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.268800735473633\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,866 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.229130458831787\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,147 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.261798858642578\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,427 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.209460020065308\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,703 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.269817972183228\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,981 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.221245336532593\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:10,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.170434999465942\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:10,539 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.249877405166626\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:12,575 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/200: 4.283267974853516\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:12,578 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.249877405166626\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,164 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,166 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.41936407089233396\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.1783356189727785\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,959 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.227576971054077\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,238 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.184141683578491\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.19386739730835\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,790 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.15916690826416\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,067 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.233234643936157\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,360 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.212756824493408\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,638 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.195131778717041\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.193507337570191\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.17202582359314\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,477 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.129647874832154\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.204125595092774\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,034 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.137046575546265\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.209381437301635\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.1829833984375\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,873 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.190350818634033\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.191827774047852\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.201117897033692\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.1918213844299315\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.1793681383132935\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,160 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.164639210700988\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.1492725849151615\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,718 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.193277215957641\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,993 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.156240749359131\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:20,269 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.184376573562622\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/200: 4.248761177062988\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.184376573562622\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,939 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,146 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.41558589935302737\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,435 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.14961838722229\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.181100654602051\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.149146556854248\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.20280351638794\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.186082553863526\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,825 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.152220773696899\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.156947660446167\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,383 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.098524475097657\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,659 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.16286678314209\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.129656267166138\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.194824528694153\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.150462031364441\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,753 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.187245082855225\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,027 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.134180188179016\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,307 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.180058479309082\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,587 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.152106857299804\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.163632011413574\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,142 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.188905572891235\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.136766052246093\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.1829733610153195\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,971 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.183438682556153\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.165262365341187\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,531 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.162627553939819\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,806 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.150086116790772\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:26,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.154836225509643\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5dc88ad3f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5dc88ad3f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:50:25,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/200: 4.248138904571533\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.154836225509643\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,648 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.42351655960083007\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.157746601104736\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.187510585784912\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.150379395484924\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.159930467605591\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.140870571136475\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.117836356163025\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.203375673294067\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.2219672203063965\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,381 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.129686570167541\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,662 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.113000202178955\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeLN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[0;32m---> 22\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    185\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:226\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    224\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss_in_model:\n\u001b[0;32m--> 226\u001b[0m     outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:141\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:264\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:192\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!HELP ME\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# this if block is needed for toprch <2.21 where flash attention onnx export does not work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m#QuantMultiheadAttention does not have is_causal in constructor -> use attention mask instead\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5187\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5188\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[1;32m   4763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[1;32m   4764\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4765\u001b[0m         proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4766\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4767\u001b[0m         proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeLN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:51:24,747 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'None', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4'}}\n",
      "[ \u001b[36m2024-02-14 11:51:24,939 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,941 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,943 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,945 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,946 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,948 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,950 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,251 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,253 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,260 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,402 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,498 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,535 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,606 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,623 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,706 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,723 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,802 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,820 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,890 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,916 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,925 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,429 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,433 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,434 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,440 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,442 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,444 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,463 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating infer dir: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,469 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None/INFER_2024-02-14_11:51:26_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,471 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:52:25,807 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   45,    62,    98,   198,   220,   252,   309,   595,   960,  1016,\n",
      "         1132,  1247,  1438,  1786,  1879,  2118,  2122,  2260,  2346,  2497,\n",
      "         2630,  2990,  2994,  3109,  3373,  3536,  3579,  3582,  3588,  3833,\n",
      "         3862,  3906,  4021,  4120,  4357,  4696,  4873,  5210,  5274,  5558,\n",
      "         5773,  5899,  5949,  6047,  6129,  6231,  6270,  6329,  6337,  6375,\n",
      "         6383,  6441,  6475,  6557,  6576,  6743,  6822,  7145,  7169,  7180,\n",
      "         7260,  7391,  7434,  7502,  8214,  8284,  8767,  8830,  8860,  9087,\n",
      "         9216,  9243,  9299,  9569,  9586,  9700,  9904, 10086, 10329, 10363,\n",
      "        10494, 10565, 10627, 10840, 10880, 10900, 10949, 10969, 11164, 11247,\n",
      "        11263, 11290, 11719, 11778, 11962, 12406, 12459, 12725, 12772, 12869,\n",
      "        12994, 13004, 13215, 13544, 13682, 13813, 13884, 13934, 14038, 14194,\n",
      "        14200, 14302, 14311, 14452, 14787, 14898, 15035, 15097, 15195, 15869,\n",
      "        15902, 16112, 16287, 16375, 16520, 16705, 16916, 16985, 17054, 17090,\n",
      "        17291, 17518, 17531, 17653, 17703, 17768, 17793, 17876, 18401, 18516,\n",
      "        18574, 18601, 18674, 18771, 18808, 18871, 18971, 19111, 19237, 19682,\n",
      "        19686, 19722, 19802, 19820, 20133, 20142, 20283, 20560, 20683, 20690,\n",
      "        20762, 21464, 21610, 21753, 21901, 22092, 22130, 22146, 22277, 22278,\n",
      "        22308, 22338, 22566, 22640, 23459, 23479, 23493, 23755, 23835, 24299,\n",
      "        24334, 24423, 24582, 24634, 24699, 24819, 25056, 25173, 25292, 25528,\n",
      "        25605, 25695, 25759, 25791, 25915, 25997, 26150, 26162, 26204, 26298,\n",
      "        26414, 26473, 26531, 26575, 26801, 26836, 26838, 26842, 26922, 27065,\n",
      "        27286, 27403, 27460, 27519, 27574, 27678, 28040, 28136, 28367, 28414,\n",
      "        28471, 28550, 28627, 28739, 28751, 28788, 29063, 29304, 29384, 29561,\n",
      "        29814, 29861, 29992, 30064, 30562, 30602, 30686, 30718, 30730, 30930,\n",
      "        31274, 31381, 31419, 31496, 31513, 31552, 31794, 31816, 31997, 32006,\n",
      "        32043, 32248, 32286, 32328, 32457, 32732, 32903, 32927, 33114, 33471,\n",
      "        33501, 33970, 33992, 34286, 34289, 34350, 34702, 34819, 34844, 34881,\n",
      "        34950, 35114, 35121, 35125, 35182, 35297, 35487, 35592, 35624, 35654,\n",
      "        35870, 35878, 35911, 36004, 36184, 36297, 36318, 36364, 36382, 36784,\n",
      "        37038, 37045, 37356, 37456, 37502, 37533, 37615, 37674, 37748, 38137,\n",
      "        38419, 38436, 38485, 38536, 38556, 38623, 38683, 38983, 39061, 39141,\n",
      "        39183, 39462, 39524, 40006, 40015, 40076, 40120, 40249, 40519, 40847,\n",
      "        41087, 41093, 41179, 41286, 41328, 41563, 41783, 41914, 41939, 41972,\n",
      "        42012, 42524, 42791, 42880, 42944, 42955, 43168, 43173, 43245, 43302,\n",
      "        43386, 43520, 43586, 43840, 43883, 44221, 44402, 44454, 44484, 44561,\n",
      "        44653, 44829, 44939, 44945, 45076, 45131, 45463, 45849, 45863, 45945,\n",
      "        45999, 46042, 46045, 46149, 47054, 47169, 47239, 47309, 47411, 47421,\n",
      "        47703, 47735, 48198, 48275, 48362, 48395, 48431, 48533, 48690, 48742,\n",
      "        48797, 49050, 49113, 49153, 49218, 49325, 49739, 49939, 50002, 50044]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:52:25,812 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:53:26,197 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   141,   198,   220,   486,   500,   684,   757,   780,   795,\n",
      "          967,  1016,  1043,  1438,  1523,  1528,  1601,  1670,  1709,  1819,\n",
      "         2030,  2032,  2124,  2630,  2843,  2848,  2914,  3032,  3072,  3109,\n",
      "         3337,  3359,  3536,  3588,  3694,  3708,  3906,  4107,  4183,  4357,\n",
      "         4477,  4490,  4532,  4769,  4952,  5059,  5161,  5232,  5250,  5391,\n",
      "         5439,  5586,  5614,  5748,  5765,  5782,  5790,  6006,  6158,  6181,\n",
      "         6231,  6329,  6333,  6375,  6397,  6441,  6518,  6549,  6551,  6754,\n",
      "         6807,  6905,  6907,  7139,  7169,  7709,  7893,  8020,  8767,  8815,\n",
      "         8931,  9114,  9431, 10086, 10128, 10436, 10501, 10565, 10644, 10694,\n",
      "        10709, 10868, 10926, 10969, 11006, 11577, 11719, 11812, 11913, 11962,\n",
      "        12157, 12367, 12680, 12869, 12963, 13098, 13132, 13682, 13811, 13813,\n",
      "        13934, 14351, 14415, 14604, 14759, 14857, 15035, 15122, 15131, 15275,\n",
      "        15376, 15902, 16287, 16700, 16705, 16723, 16927, 17054, 17090, 17245,\n",
      "        17336, 17365, 17392, 17496, 17586, 17696, 17738, 17943, 18081, 18269,\n",
      "        18411, 18564, 18682, 18706, 18771, 19131, 19544, 19573, 19616, 19794,\n",
      "        19802, 19878, 20283, 20459, 20714, 20758, 20873, 21163, 21468, 22000,\n",
      "        22092, 22231, 22385, 22490, 22566, 22804, 22839, 23023, 23158, 23203,\n",
      "        23235, 23493, 23596, 23748, 23926, 23997, 24129, 24169, 24299, 24311,\n",
      "        24334, 24350, 24423, 24582, 24634, 24675, 24849, 24955, 25528, 25617,\n",
      "        25714, 25745, 25759, 26150, 26204, 26298, 26443, 26473, 26480, 26836,\n",
      "        26838, 26842, 27065, 27281, 27485, 27574, 27599, 27672, 27809, 27926,\n",
      "        28025, 28147, 28152, 28414, 28451, 28471, 28831, 29141, 29167, 29304,\n",
      "        29357, 29705, 29992, 30431, 30518, 30686, 30718, 30730, 31017, 31064,\n",
      "        31086, 31274, 31379, 31387, 31388, 31496, 31513, 31550, 32043, 32162,\n",
      "        32283, 32285, 32357, 32600, 32770, 32826, 32997, 33075, 33114, 33379,\n",
      "        33432, 33536, 33787, 33848, 33992, 34101, 34219, 34289, 34330, 34350,\n",
      "        34474, 34611, 34673, 34983, 35059, 35297, 36147, 36382, 36537, 36784,\n",
      "        36881, 37130, 37356, 37414, 37448, 37502, 37834, 38058, 38242, 38519,\n",
      "        38619, 38683, 38823, 38832, 38983, 39021, 39086, 39141, 39236, 39237,\n",
      "        39270, 39462, 39610, 39690, 39847, 39898, 40249, 40349, 40613, 40751,\n",
      "        40761, 40847, 41151, 41183, 41563, 42502, 42524, 42853, 42908, 42984,\n",
      "        43173, 43390, 43449, 43478, 43672, 43840, 43856, 44136, 44295, 44394,\n",
      "        44404, 44619, 44653, 44939, 44977, 45418, 45464, 45580, 45731, 45818,\n",
      "        46045, 46098, 46149, 46524, 46917, 47018, 47026, 47054, 47105, 47168,\n",
      "        47169, 47238, 47290, 47320, 47553, 47634, 47741, 47835, 47928, 48122,\n",
      "        48275, 48341, 48376, 48403, 48481, 48619, 48676, 48690, 48723, 49141,\n",
      "        49218, 49579, 49688, 50244]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:53:26,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,494 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   136,   191,   198,   472,   473,   486,   549,   638,  1016,\n",
      "         1117,  1132,  1190,  1449,  1523,  1622,  1786,  1813,  2144,  2424,\n",
      "         2751,  2848,  3359,  3373,  3492,  3536,  3582,  3588,  3716,  3842,\n",
      "         3939,  3983,  4053,  4089,  4357,  4383,  4491,  4612,  4906,  5232,\n",
      "         5468,  5708,  5748,  5790,  6047,  6162,  6181,  6231,  6270,  6274,\n",
      "         6281,  6375,  6402,  6421,  6475,  6549,  6551,  6655,  6761,  6897,\n",
      "         6905,  6934,  7034,  7139,  7169,  7249,  7260,  7280,  7353,  7873,\n",
      "         8005,  8020,  8203,  8279,  8346,  8367,  8665,  8734,  8741,  8931,\n",
      "         9087,  9443,  9619,  9664,  9830,  9982, 10015, 10086, 10267, 10562,\n",
      "        10752, 10766, 10785, 10844, 10851, 10868, 10880, 10969, 10997, 11014,\n",
      "        11167, 11299, 11577, 11719, 11907, 11913, 11962, 12115, 12139, 12157,\n",
      "        12260, 12313, 12589, 12680, 12844, 12869, 13003, 13132, 13218, 13252,\n",
      "        13362, 13421, 13682, 13813, 13933, 13934, 14095, 14183, 14415, 14726,\n",
      "        15035, 15097, 15195, 15215, 15337, 15376, 15412, 15483, 15503, 15876,\n",
      "        16158, 16217, 16227, 16323, 16891, 16916, 17054, 17518, 18161, 18260,\n",
      "        18268, 18296, 18390, 18411, 18516, 18682, 18894, 19000, 19130, 19264,\n",
      "        19277, 19651, 19780, 19794, 19797, 19802, 19963, 20487, 20503, 20558,\n",
      "        20560, 20683, 20721, 20839, 20864, 20873, 21163, 21883, 22278, 22572,\n",
      "        22621, 22839, 22993, 23235, 23408, 23448, 23721, 23835, 23926, 23948,\n",
      "        23997, 24325, 24334, 24423, 25089, 25240, 25528, 25621, 25714, 25759,\n",
      "        25851, 25908, 25982, 25989, 26107, 26150, 26178, 26204, 26298, 26567,\n",
      "        26587, 26698, 27388, 27523, 27926, 27935, 28292, 28344, 28350, 28365,\n",
      "        28378, 28414, 28451, 28471, 28850, 28913, 29000, 29107, 29167, 29474,\n",
      "        29518, 29577, 29673, 29724, 29851, 29961, 29992, 30379, 30439, 30442,\n",
      "        30551, 30569, 30686, 30718, 30750, 30989, 30995, 31017, 31085, 31304,\n",
      "        31381, 31387, 31548, 31552, 31846, 32162, 32164, 32248, 32412, 32986,\n",
      "        32997, 33075, 33114, 33118, 33459, 33632, 33706, 33764, 34286, 34289,\n",
      "        34330, 34350, 34493, 34568, 35059, 35114, 35297, 35405, 35492, 35597,\n",
      "        35620, 35870, 35909, 36294, 36297, 36316, 36382, 36516, 36537, 36717,\n",
      "        36784, 37130, 37264, 37358, 37533, 37615, 37676, 37758, 37918, 38058,\n",
      "        38159, 38172, 38283, 39004, 39141, 39237, 39253, 39462, 39690, 39756,\n",
      "        39898, 40053, 40061, 40097, 40131, 40155, 40273, 40683, 40686, 40761,\n",
      "        40829, 40914, 41088, 41231, 41328, 41380, 41532, 41563, 41891, 41893,\n",
      "        41914, 42021, 42502, 42557, 42853, 42955, 43217, 43393, 43432, 43449,\n",
      "        43478, 43542, 43587, 43672, 43771, 43840, 43911, 44008, 44455, 44488,\n",
      "        44561, 44829, 45076, 45403, 45464, 45628, 45818, 45938, 46045, 46421,\n",
      "        47018, 47101, 47168, 47386, 47703, 48122, 48359, 48376, 48481, 48611,\n",
      "        48619, 49593, 49667, 49819, 49967, 50022]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,498 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,499 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None/INFER_2024-02-14_11:51:26_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#layernorm did not help either\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\",\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if inference changes when we use karpathy's script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strangely, it complains that the padding tokens used in batchnorm arent on the gpu, but it did not complain in our script\n",
    "#### The text also contains a large amount of newline tokens, but semantically the words make more sense than in our infer script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 13:04:20,749 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:20,753 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:20,757 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])\n",
      "[ \u001b[36m2024-02-14 13:04:21,465 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,622 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,705 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,813 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,833 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,926 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:21,990 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,015 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,091 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,116 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,133 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,219 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,290 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 13:04:22,808 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      ":\n",
      " man\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " bid\n",
      "\n",
      "\n",
      "!'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!':\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      "US;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "An\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " till\n",
      "\n",
      "Are\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " o\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " say\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " whilst\n",
      "\n",
      " go?\n",
      "\n",
      ":\n",
      "Only\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " merry:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "With bring\n",
      "\n",
      " no\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " would,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " when\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- He\n",
      "\n",
      "Mes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " worthy\n",
      " why\n",
      "\n",
      "\n",
      "\n",
      " my\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      "\n",
      "Are\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Were\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and\n",
      "\n",
      "\n",
      " he\n",
      " worthy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      "\n",
      " where worthy\n",
      "\n",
      "\n",
      " Citizen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " my--:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Were\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--VOL\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      " under\n",
      "\n",
      "\n",
      "\n",
      " or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "With\n",
      "\n",
      "\n",
      " say\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " though\n",
      "\n",
      " one:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Let\n",
      "\n",
      "\n",
      "\n",
      " or or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That:\n",
      "\n",
      "\n",
      " But:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " one:--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " how\n",
      "\n",
      "\n",
      "\n",
      " if vent? Sir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " say\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " my\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " What\n",
      "\n",
      "\n",
      "\n",
      "As\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--MostUS\n",
      "\n",
      "\n",
      "\n",
      "Good\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the noWhen\n",
      " where\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Second\n",
      "\n",
      "\n",
      "::If\n",
      "\n",
      "\n",
      "\n",
      " you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      " when--\n",
      "\n",
      "\n",
      "\n",
      " most\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " when\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " how\n",
      "\n",
      "\n",
      " or\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  wherein\n",
      ":\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If E::\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " defend heed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " your cons\n",
      "\n",
      "\n",
      "\n",
      " ' get therefore--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " if\n",
      " he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and,\n",
      "\n",
      " sink O\n",
      "\n",
      "\n",
      " cons traitor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " like\n",
      " bid\n",
      " you:\n",
      "\n",
      "\n",
      " very\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?All\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " would what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " as\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BRard\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " if\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " renowned\n",
      "\n",
      " learn--\n",
      " who\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " therefore\n",
      "\n",
      " like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " state\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "Mess he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " there\n",
      "Roman\n",
      "Were\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " being:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " would\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      ":\n",
      "the\n",
      " where\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      " where i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      " till your take\n",
      "\n",
      "\n",
      "Are?\n",
      "\n",
      "Both\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "US thus\n",
      " but then\n",
      "\n",
      "--\n",
      " come\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "? most,\n",
      "\n",
      "\n",
      "\n",
      " and he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      " hang\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A till which since\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " good which the\n",
      "\n",
      "\n",
      " which?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " i\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " traitor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " where\n",
      "\n",
      "\n",
      "\n",
      " make\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most traitor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      ",'\n",
      "\n",
      "\n",
      "\n",
      " good poor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " state\n",
      "\n",
      "\n",
      "For:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "be\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " voices\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " good traitor::\n",
      "\n",
      " Sir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " yet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " whose\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " or\n",
      " voices:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " un\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "Were\n",
      ":\n",
      "SP where:\n",
      "\n",
      "\n",
      "\n",
      " would\n",
      "\n",
      "\n",
      "\n",
      " where o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "An\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upon\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but\n",
      "Though\n",
      "\n",
      "\n",
      "May\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " one\n",
      "of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- but\n",
      "\n",
      " bid\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":?\n",
      " come\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " worthy--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "With which:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " my\n",
      "\n",
      " name:!\n",
      "of as\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      ",,,, for he he for for he,, are for for, for for for he for he,\n",
      "\n",
      " for he for he for for for he for for he for for\n",
      " he for for for he for he for for he he for for he for he he he for he he he he for he he he he he he he he he he for he he he he then then he he for he so he he as he he what he he he if he he he he you he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he but he he he he he he he he he he he he come he he he he he he he he he he he he he he he he he he he he he he he he for he he he he he you he he he he he appear he he he he he he he voices he he you he he he he further he he he what he heCOM he he he proud he he he he he he he sir here he he he he he he he go he IHe he he he he sure he he he he he he he he he he voices come he he he he he he he: he he he he he he he he he he he he he he he he he he he he he he he we he he he these he he he them he he he he he he he he he he he he he he heThat no he he he he he he he he you he he he he he he he things he he-- he he he he he he he he he he he he he half he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he then he he he he he go peace he he he he he he he he he he traitor he well he world he he you he sir he he he he he he he what he he he sir he he gates he he he he he what let which he he he he he he he masters he mad he he he he he he he he he he he what he he he he so well he he he he he he he he he gods he he gentle he peace he he he he he\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      " you you you you you are you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you youAN you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you beaten you you you you you you you you you you you you you you you you you you you you you you you you you you you youIA you you you you you you you you you you you you you you you you you you you youN prepared you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you gods you you you you you you you you you you you you you youUT you you you you you you you you you you you you you you you you you you you you you you you you you you. you you you you you you you Serving you you you you you you you you you you you you you you you you you you you youN you you you you you you you you you you you you you you you you you you you you you you you you you you your you you you you you you punish you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you Citizen you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you corn you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you gods you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ", and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Will\n",
      "\n",
      " worthy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "M o\n",
      "\n",
      "\n",
      "::\n",
      "\n",
      "\n",
      " how\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "w\n",
      "\n",
      "\n",
      " whom\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " bring would\n",
      "\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " un\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you shall you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " yetBRBRTh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " nameSBRIN\n",
      "\n",
      "\n",
      "\n",
      " yet he\n",
      "\n",
      "\n",
      " when\n",
      "\n",
      "\n",
      "\n",
      "Of\n",
      "\n",
      "\n",
      "More\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "BR you\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      " whose\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of\n",
      "\n",
      "MAR and:In:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " '\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " even\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "would\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "-- but\n",
      "\n",
      "\n",
      "? But and-- by which\n",
      "\n",
      "All if therefore\n",
      "\n",
      "\n",
      " h;\n",
      "\n",
      "\n",
      " till\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "D\n",
      " come\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " where orAll\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " whether\n",
      "\n",
      " themselves\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " whom\n",
      " purpose\n",
      "\n",
      "\n",
      " if----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "And?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Second\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- goodTo\n",
      "What\n",
      " what\n",
      "\n",
      "\n",
      "US:\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he he he he he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " what he he he he he he\n",
      "\n",
      "\n",
      " he he he he he he he he he he he he he he he he he he he he he he he he he he he he what he he he he he he he he he he he he he he he he he if he he he he he he he he he he he he what he he he he he he he he he so he he he he another he he he he he he he he he he he he he he he he he he he he he he he he he he he tell he yet he he he he he he he he general he he he he he he he he he he he he he he he he he he he he he he he he he he one he he he he he he he he he he he he he he he you he he he he he he he he heas he he what he what soCOM heFirst one he't he he he he he he he he he he he he he sword he he he he he he he he he he which he he he what he he he he he he he he he he he he he he he so welcome he power voices he hold he sir he he he he he get he he he welcome he come he he he he he indeed he he he he he he he he sir heI you he so he he he he he he he he he he he he he well he he what he he he he he he\n",
      " he he he he he he he he he he Senator bring he he he wein heas not he heUT he he he he he tell he cons name he he he gods he he he he he he he he he he he he he he tell he he he he he he he he yourUT he he he he tell he he he he he he he he he he he he he it he you he he stand he he all he he he he he he he state he't he he he he he he he people so he heIf he he he ye sir he he so heI he he what well? he fellow he he he he well so they so he name he he so he he he so he you't he he he't him't you he what he he he kill he heUT they so you Rome't't our you you so he so so you he hear keep not't he be he time\n",
      "---------------\n",
      "\n",
      "\n",
      " him him him him him him him him him him him him him himAnd him him him him he him him him him him him him him him him him him him him him him him him him him him him him him him him him him he him him him him him him him him him him him him him him him him him him him him him us him him him him them him him him him him. him him him him him him him him him him him him you him him him him him him him him him him him him him himator him him hearul him him him him himSUT him him him him him him him him himUS thing say speakThough him him them himI him him him think done worth him himUS thing revenge him us him him done you him thee give meet him England him wordUSUS him him him him workIUS SenatorUSUS him him him follow him know be what home send him love him us him deliver do him't what follow him him kill him him him him heard him thee him ratheral answer thing hear us would cannot them heard time heard what meet him say us us love him us done him what know do\n",
      "\n",
      " all them him\n",
      "I love hear she hear hear him pardon him him thee if him remember lose this him blood do you has him has him he him that him then him hear us is them us him all all he time done meet him me done: he him me honour\n",
      " he him him done so him put turn him him him him your time him rather had't him for him answer him him so be time him will he stand him him him be live you him him me your be be him him him all serve thee thee so him do far them him him him said he heard him him must him him all love be be need me me him them right any them it him them him meet ever\n",
      " ever men him them wear them them them all him here me all all me for hear us all all me lives that that did all by me them all on me much it: all\n",
      ", all love me them does me here comfort thee him any did forth them all them, for all she them\n",
      " all me me my done thee, thee so fight\n",
      "\n",
      "\n",
      " all come my her lay nothing: with them from in fall, thou me, that me too of meet too; and them them; to to thou that that me thee that we answer for my this.\n",
      "\n",
      "\n",
      "\n",
      " my to no is\n",
      "---------------\n",
      "\n",
      "\n",
      "::::::\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " yet:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " O\n",
      "\n",
      "I\n",
      "\n",
      " \n",
      "--\n",
      "\n",
      "\n",
      " where\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      " but\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " but you how? or\n",
      "the\n",
      " worthy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IA\n",
      "\n",
      "\n",
      "Will\n",
      "\n",
      "E you\n",
      "\n",
      "\n",
      " when\n",
      "\n",
      "\n",
      "Second\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " would:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " bidb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " being\n",
      "\n",
      " he?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " being\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A under\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Secondple\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":US:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Will\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " would\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " either i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " guilty\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " and i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " markP\n",
      "\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "Cor\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " good?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TheWith\n",
      "Mess\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "inference_karpathy('/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chkpt = torch.load(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming that the model was only trained with a third of the dataset, it should have a much better inference if trained with the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-15_12:28:12__epoch:1'}}\n",
      "[ \u001b[36m2024-02-15 12:41:15,317 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:15,949 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:15,953 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:15,956 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,395 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,397 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,399 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,400 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,404 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,406 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-15_12:28:12__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,528 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,532 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,553 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,660 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,678 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,691 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,699 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,984 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,989 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,992 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:16,997 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:17,000 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:17,336 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:17,347 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-15_12:41:17_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:17,350 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:42,189 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50160\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:41:42,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/3\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:42:04,593 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50118\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:42:04,600 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/3\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:42:26,777 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50243\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:42:26,785 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/3\u001b[0m\n",
      "[ \u001b[36m2024-02-15 12:42:26,788 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-15_12:41:17_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#generated checkpoint has a ridiculously high perplexity (50.000)\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-15_12:28:12__epoch:1\",\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark trained model which has a loss of 0.1 during training\n",
    "#### Perplexity is about 50.000 which is ridiculously high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 64}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 100, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': 'GPT_2024-02-15_12:28:12__epoch:1', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-16 11:44:51,832 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,835 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,838 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,842 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_11:44:51\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,845 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,848 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,861 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,865 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,870 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,879 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,883 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,886 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,890 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 135210 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:51,896 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-15_12:28:12__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,101 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,194 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,210 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,292 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,306 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:52,621 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:53,618 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m\u001b[0m\n",
      "\n",
      "   avg_ppl \n",
      "\n",
      "   50119.7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=GPT_2024-02-15_12:28:12__epoch:1\",\n",
    "    \"run.num_samples=100\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=tiny_shakespeare\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=64\",\n",
    "    \"dataset.sizes.bench=0.4\"\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For some reason, karpathy's model has an even higher perplexity than ours while still generating better text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 64}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 100, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-16 11:44:39,279 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,283 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,287 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,931 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,934 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,935 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,937 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_11:44:39\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,939 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:39,945 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,425 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,433 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,438 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,466 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,471 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,476 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,480 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 135210 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:40,486 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,274 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mNo info specified if checkpoint is quantized. Assuming false.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,291 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=256, vocab_size=50304, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,502 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,520 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,601 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,621 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,696 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,716 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,803 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,909 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,922 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:42,940 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:43,004 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:43,485 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.88M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:44:45,409 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m\u001b[0m\n",
      "\n",
      "   avg_ppl \n",
      "\n",
      "   55567.4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\",\n",
    "    \"run.num_samples=100\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=tiny_shakespeare\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=64\",\n",
    "    \"dataset.sizes.bench=0.4\"\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually load a checkpoint and feed it with some data to see what the actual perplexity is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "#generated config from loading karpathy's shakespeare model for benchmarking\n",
    "from qtransform.run import load_model, forward_pass, InferType\n",
    "from qtransform.run.bench import measure_perplexity\n",
    "from omegaconf import DictConfig\n",
    "import torch.nn.functional as F\n",
    "CHECKPOINT_KARPATHY = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\"\n",
    "CHECKPOINT = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-15_12:28:12__epoch:1\"\n",
    "cfg = DictConfig({'data': {'dtype': 'float32'}, \n",
    " 'device': 'cuda', 'debug': False, \n",
    " 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', \n",
    "             'module': 'huggingface', \n",
    "             'name': 'tiny_shakespeare', \n",
    "             'root_path': '~/.qtransform/datasets', \n",
    "             'dataset_dir': ['${dataset.root_path}','${dataset.module}', '${dataset.name}'], \n",
    "             'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.4}, \n",
    "             'tokenizer': {'dtype': '${data.dtype}', \n",
    "                           'meta_file': 'meta.pkl', \n",
    "                           'wrapper': 'TikTokenizer', \n",
    "                           'encoding': 'gpt2', \n",
    "                           'module': 'tiktoken'}, \n",
    "             'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, \n",
    "             'type': 'huggingface', \n",
    "             'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, \n",
    " 'seed': 1234567890, \n",
    " 'model': {'calc_loss_in_model': False, \n",
    "           'args': {'block_size': 64}}, \n",
    " 'quantization': {'quantize': False}, \n",
    " 'pipe': '/dev/null', \n",
    " 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, \n",
    " 'run': {'command': 'bench', \n",
    "         'el': 2, \n",
    "         'num_samples': 100, \n",
    "         'out_dir': '', \n",
    "         'checkpoint_dir': 'models', \n",
    "         'from_checkpoint': CHECKPOINT, \n",
    "         'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}})\n",
    "from qtransform.dataset import get_data, get_loader, DatasetWrapper\n",
    "data_wrapper: DatasetWrapper = get_data(cfg.dataset)\n",
    "\n",
    "#dataset hydra config expects block size, currently set in command line. TODO: infer from onnx metadata or checkpoint metadata\n",
    "data_wrapper.load_dataset()\n",
    "dataset_bench = data_wrapper.dataset_info.bench\n",
    "dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_bench)\n",
    "device = device=torch.device('cuda')\n",
    "models = load_model(cfg, device=device)\n",
    "model = models[0].model\n",
    "model.eval()\n",
    "tokenizer = models[0].tokenizer\n",
    "inputs,labels = next(iter(dataloader))\n",
    "inputs = inputs.to(device=device)\n",
    "labels = labels.to(device=device)\n",
    "logits = forward_pass(InferType.CHECKPOINT, model, inputs)\n",
    "#print(measure_perplexity(logits, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is quite uncertain the next token as the highest softmaxed prediction for the first word is 0.00008\n",
    "#### That would explain the ludicruous perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0328, 0.0329, 0.0339, 0.0339, 0.0339, 0.0348, 0.0352, 0.0374, 0.0381,\n",
       "        0.0975], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0].sort().values[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0560e-05, 2.0561e-05, 2.0582e-05, 2.0582e-05, 2.0583e-05, 2.0600e-05,\n",
       "        2.0610e-05, 2.0656e-05, 2.0669e-05, 2.1933e-05], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step between the highest prediction (0.09) and second highest (0.03) is not that high when softmaxed\n",
    "softmaxed = F.softmax(logits, dim=-1)\n",
    "softmaxed[0][0].sort().values[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of cross entropy loss has way less perplexity than the one from pytorch\n",
    "### that is due to softmaxing along the wrong dimension (dim=1 instead of dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8245, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmaxed cross entropy\n",
    "block_size = logits.size()[1]\n",
    "loss_softmaxed = torch.zeros(block_size).to(device=device)\n",
    "for word in range(block_size):\n",
    "    softmaxed_word = softmaxed[0][word]\n",
    "    loss_softmaxed[word] = -1 * torch.log(softmaxed_word[labels[0][word]])\n",
    "loss_softmaxed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1674, 4.1772, 4.1843, 4.1687, 4.1681, 4.1566, 4.1753, 4.1671, 4.1730,\n",
       "        4.1766, 4.1443, 4.1577, 4.1743, 4.1747, 4.0866, 4.1656, 4.1658, 4.1785,\n",
       "        4.1569, 4.1370, 4.1582, 4.1536, 4.1606, 4.1764, 4.0813, 4.1711, 4.1604,\n",
       "        4.1514, 4.1851, 4.1588, 4.1673, 4.1773, 4.1634, 4.1400, 4.1794, 4.1625,\n",
       "        4.1651, 4.1787, 4.1669, 4.1546, 4.1550, 4.1693, 4.0746, 4.1546, 4.1659,\n",
       "        4.1807, 4.1696, 4.1445, 4.1617, 4.1490, 4.1536, 4.1835, 4.0742, 4.1566,\n",
       "        4.1614, 4.1836, 4.1829, 4.1541, 4.1517, 4.1672, 4.1561, 4.1467, 4.1519,\n",
       "        4.1597], device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#that is the loss when softmaxing along the block_size dimension (dim=1) instead of the vocab dimension (dim=2)\n",
    "loss_softmaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6963, 0.4673,    nan, 0.1722, 2.2465, 2.5382, 1.1921, 0.8894,    nan,\n",
       "           nan, 1.9743,    nan,    nan,    nan,    nan,    nan, 0.5171,    nan,\n",
       "           nan, 3.8941,    nan,    nan,    nan,    nan,    nan, 1.6858, 0.7186,\n",
       "        0.5291,    nan, 2.5193,    nan, 1.2689,    nan, 1.4735, 0.7614,    nan,\n",
       "           nan, 0.8097, 1.6700, 3.3430, 3.8243,    nan,    nan,    nan, 0.5673,\n",
       "           nan,    nan, 1.8583,    nan,    nan,    nan,    nan,    nan, 1.7914,\n",
       "        0.7474, 0.5505,    nan, 1.8451,    nan,    nan, 1.4996,    nan, 2.8847,\n",
       "           nan], device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-softmaxed cross entropy\n",
    "#logits need to be softmaxed in order to remove negative values\n",
    "block_size = logits.size()[1]\n",
    "loss = torch.zeros(block_size).to(device=device)\n",
    "for word in range(block_size):\n",
    "    logits_word = logits[0][word]\n",
    "    loss[word] = -1 * torch.log(logits_word[labels[0][word]])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(57170.2422, device='cuda:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4150, -0.7571, -0.0677,  1.1603, -1.5602],\n",
      "        [-0.5520,  0.4000,  1.0496,  1.3627,  1.4628],\n",
      "        [ 0.6792, -0.5768,  0.1862,  0.8596,  0.3433]], requires_grad=True)\n",
      "\n",
      "tensor([2, 0, 4])\n",
      "\n",
      "2.1891212463378906\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "print(f'{input}\\n\\n{target}\\n\\n{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8225, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_viewed = logits.view(-1, logits.size(-1))\n",
    "labels_viewed = labels.view(-1)\n",
    "block_size = logits_viewed.size()[0]\n",
    "loss_manual = torch.zeros(block_size).to(device=device)\n",
    "input_softmaxed = F.softmax(logits_viewed, dim=1)\n",
    "for word in range(block_size):\n",
    "    softmaxed_word = input_softmaxed[word]\n",
    "    input_softmaxed[word] = -1 * torch.log(softmaxed_word[labels_viewed[word]])\n",
    "input_softmaxed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 50256])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_viewed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ReLU and GELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both GeLU and GELU reached a loss of around 1.0 after approx. 450 batches\n",
    "## ReLU loss after 1000 batches: 0.10457251816987992, GeLU loss after 1000 batches: 0.0994626946747303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.8, 'eval': 0.1, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 1, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 1000, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-16 11:54:46,481 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,485 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,488 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,493 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_11:54:46\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,497 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,501 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,516 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,520 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,524 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 270421 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,528 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,532 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 263661 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,549 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,637 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,644 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,657 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,691 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,940 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,957 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:46,960 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:50,079 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.13752040863037\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:53,191 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.518485641479492\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:56,250 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.958098125457763\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:59,267 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.429566478729248\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:02,274 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.969121599197388\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:05,301 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.536770963668824\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:08,273 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.207277059555054\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:11,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.901965475082397\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:14,267 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.685288715362549\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:17,219 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.487595891952514\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:20,142 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.335405397415161\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:23,115 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.1784014225006105\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:26,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 6.105133008956909\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:29,221 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 6.025612449645996\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:32,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.908796119689941\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:35,173 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.865119361877442\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:38,116 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.713294553756714\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:41,055 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.634953546524048\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:44,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.52789158821106\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:46,907 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.387083530426025\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:49,930 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.257603693008423\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:52,904 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 5.019824981689453\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:55,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.740166282653808\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:55:58,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.506546545028686\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:01,816 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.206883239746094\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:04,665 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.931207299232483\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:07,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.6531938314437866\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:10,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.406636428833008\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:13,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.2033509969711305\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:16,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.991090726852417\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:19,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.788081479072571\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:22,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.588704466819763\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:25,403 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.415777015686035\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:28,332 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.2835469484329223\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:31,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.0806671142578126\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:34,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.9277254581451415\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:37,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.7929598093032837\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:40,307 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.6409875869750976\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:43,351 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.56497802734375\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:46,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.436117458343506\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:49,666 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.2926336169242858\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:52,772 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.222376811504364\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:55,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.1143710970878602\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:56:58,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 1.0200085401535035\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:01,720 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.9699277997016906\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:04,830 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.8975473642349243\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:07,768 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.8494621694087983\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:10,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.766130518913269\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:13,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.7340376436710357\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:16,769 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.6755145490169525\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:19,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.6251735627651215\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:22,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.5823479950428009\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:25,904 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.5605066001415253\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:28,970 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.5083397060632706\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:32,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.48067703247070315\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:34,999 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.46739715039730073\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:38,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.4124042153358459\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:41,203 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.4017836570739746\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:44,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.3719729006290436\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:47,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.3550196051597595\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:50,360 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.338726794719696\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:53,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.3099909782409668\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:56,590 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.3071246087551117\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:57:59,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.28379838466644286\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:02,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.2656626895070076\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:05,458 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.2596356749534607\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:08,526 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.24033756852149962\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:11,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.2239035725593567\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:14,450 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.2193165585398674\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:17,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.20365315079689025\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:20,286 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.2043795108795166\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:23,361 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.18807172179222106\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:26,343 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.1851666733622551\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:29,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.1788686752319336\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:32,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.17370037138462066\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:35,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.15973416566848755\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:38,366 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.1637635976076126\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:41,367 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.15236044377088548\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:44,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.15008576363325118\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:47,696 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.13968436419963837\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:50,801 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.13948412537574767\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:53,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.14216571897268296\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:56,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.13009420558810234\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:58:59,810 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.13186105191707612\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:02,706 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.13577666580677034\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:05,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.12504916563630103\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:08,628 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.12115023285150528\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:11,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.11927968859672547\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:14,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.12079485058784485\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:17,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.11717905923724174\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:20,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.11587990298867226\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:23,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.11186821311712265\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:26,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.11345880925655365\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:29,492 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.11029989272356033\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:32,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.10646713972091675\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:35,345 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.10815416127443314\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:38,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.10536901876330376\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:41,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.10328841507434845\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:44,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.10559449568390847\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:59:47,494 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.0994626946747303\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:41,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/1: 0.15003205835819244\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:41,847 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.0994626946747303\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,154 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-16_11:54:46__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,295 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,299 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mExporting Model\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,303 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,306 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_12:00:42\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,310 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,313 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-16_11:54:46__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,489 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,587 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,594 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,604 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,610 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,917 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 12:00:42,925 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mexporting... onnx_(1, 64)_GPT_2024-02-16_11:54:46__epoch:1.onnx\u001b[0m\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 1 ERROR ========================\n",
      "ERROR: missing-standard-symbolic-function\n",
      "=========================================\n",
      "Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.\n",
      "None\n",
      "<Set verbose=True to see more details>\n",
      "\n",
      "\n",
      "[ \u001b[36m2024-02-16 12:00:43,073 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[31mERROR\u001b[0m] - \u001b[31mExport via torch.onnx.utils.export failed, reason\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py\", line 133, in run\n",
      "    export(model, shape, model_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 506, in export\n",
      "    _export(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1548, in _export\n",
      "    graph, params_dict, torch_out = _model_to_graph(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1117, in _model_to_graph\n",
      "    graph = _optimize_graph(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 665, in _optimize_graph\n",
      "    graph = _C._jit_pass_onnx(graph, operator_export_type)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1901, in _run_symbolic_function\n",
      "    raise errors.UnsupportedOperatorError(\n",
      "torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py:133\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mIf ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m        All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1117\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:665\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    663\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 665\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1901\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1899\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context\u001b[38;5;241m.\u001b[39mop(op_name, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs, outputs\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39moutputsSize())  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1902\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1903\u001b[0m         opset_version,\n\u001b[1;32m   1904\u001b[0m         symbolic_function_group\u001b[38;5;241m.\u001b[39mget_min_supported()\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1907\u001b[0m     )\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m      2\u001b[0m EVAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[0;32m---> 21\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:131\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    129\u001b[0m         OmegaConf\u001b[38;5;241m.\u001b[39mupdate(cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.export_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model}\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mexport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m#write checkpoint into fifo\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     write_to_pipe(cfg, last_checkpoint)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py:139\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport via \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mERROR_LOGS[cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mexport_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed, reason\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#write path to fifo\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_to_pipe\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "EVAL_SIZE = 0.2\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=64\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"dataset.sizes.train=0.8\",\n",
    "        \"dataset.sizes.eval=0.1\",\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=1\",\n",
    "        \"run.max_iters=1000\",\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=200\",\n",
    "        \"run.gradient_accumulation_steps=1\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loss with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.8, 'eval': 0.1, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 1, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 1000, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:48:08.863247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-16 11:48:10,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,382 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,386 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,390 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_11:48:10\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,394 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,869 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,875 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,879 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 270421 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,883 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,887 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 263661 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:10,925 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,026 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,048 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,100 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,117 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,423 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:11,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:14,708 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.137335300445557\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:17,956 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.521746063232422\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:21,265 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.967225265502929\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:24,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.436475467681884\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:27,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.973872232437134\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:31,092 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.539710426330567\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:34,355 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.209016990661621\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:37,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.901499843597412\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:40,780 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.684050750732422\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:43,954 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.4829404830932615\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:47,111 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.3278344631195065\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:50,316 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.169185495376587\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:53,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 6.097552156448364\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:56,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 6.02147536277771\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:48:59,716 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.905861949920654\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:02,950 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.865905380249023\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:06,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.71396541595459\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:09,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.637429523468017\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:12,724 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.5276123046875\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:15,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.390403175354004\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:19,139 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.268366384506225\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:22,302 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 5.046403598785401\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:25,579 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.783611249923706\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:28,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.566306829452515\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:32,125 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.27223391532898\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:35,195 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.9998301029205323\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:38,323 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.7198654890060423\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:41,497 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.4747033596038817\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:44,599 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.2764868259429933\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:47,801 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 3.0678876399993897\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:50,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.8661365270614625\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:54,145 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.6691585302352907\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:49:57,327 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4965546131134033\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:00,494 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.364779734611511\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:03,693 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.1622668504714966\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:06,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.008177435398102\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:10,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.8776370644569398\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:13,170 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.721597957611084\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:16,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.6439688205718994\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:19,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.512949323654175\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:22,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.3638608574867248\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:26,149 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.2941950917243958\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:29,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.181825852394104\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:32,525 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 1.0771786868572235\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:35,693 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 1.0247830510139466\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:38,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.9525770485401154\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:42,126 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.902318000793457\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:45,397 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.8175292074680328\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:48,625 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.7775961101055145\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:51,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.7160946726799011\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:54,957 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.6739764213562012\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:50:58,201 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.6263226747512818\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:01,471 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.6030510306358338\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:04,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.5476090550422669\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:07,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.5164531588554382\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:10,914 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.5029224455356598\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:13,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.4502458393573761\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:16,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.4401428699493408\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:20,009 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.40618989765644076\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:22,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.3850091636180878\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:25,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.36731249690055845\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:28,923 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.3385529786348343\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:31,919 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.336708065867424\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:34,881 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.311114576458931\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:37,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.29119245558977125\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:40,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.2881474673748016\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:43,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.2652915269136429\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:46,807 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.24677373021841048\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:49,837 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.2415074124932289\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:52,929 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.22403722405433654\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:55,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.22465640604496\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:51:58,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.20709400475025178\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:01,944 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.20285273492336273\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:04,920 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.1967678800225258\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:07,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.19003182649612427\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:10,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.17577871233224868\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:13,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.17879295349121094\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:16,943 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.1663919970393181\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:19,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.16446502655744552\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:22,773 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.15217766761779786\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:25,840 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.1512826845049858\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:28,966 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.15387659519910812\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:32,065 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.14146871268749237\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:35,325 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.1417988732457161\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:38,517 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.14565471410751343\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:41,637 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.13498706370592117\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:44,772 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.13000165596604346\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:47,926 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.12773340344429016\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:51,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.12957096993923187\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:54,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.12380671426653862\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:52:57,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.12304233387112617\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:00,422 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.11917709931731224\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:03,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.11969708949327469\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:06,324 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.11685864627361298\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:09,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.11246477365493775\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:12,485 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.11395602822303771\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:15,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.11097676157951356\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:18,550 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.10800690799951554\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:21,464 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.10955117419362068\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:53:24,359 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.10457251816987992\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:18,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/1: 0.16015559434890747\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:18,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.10457251816987992\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:18,716 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-16_11:48:10__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,328 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,332 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mExporting Model\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,336 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,339 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-16_11:54:19\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,344 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,348 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-16_11:48:10__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,543 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,634 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,642 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,653 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,690 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,922 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,928 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mexporting... onnx_(1, 64)_GPT_2024-02-16_11:48:10__epoch:1.onnx\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,949 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:131: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-02-16 11:54:19,967 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:59: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if c < self.num_features:\n",
      "\u001b[0m\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 1 ERROR ========================\n",
      "ERROR: missing-standard-symbolic-function\n",
      "=========================================\n",
      "Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.\n",
      "None\n",
      "<Set verbose=True to see more details>\n",
      "\n",
      "\n",
      "[ \u001b[36m2024-02-16 11:54:20,597 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[31mERROR\u001b[0m] - \u001b[31mExport via torch.onnx.utils.export failed, reason\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py\", line 133, in run\n",
      "    export(model, shape, model_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 506, in export\n",
      "    _export(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1548, in _export\n",
      "    graph, params_dict, torch_out = _model_to_graph(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1117, in _model_to_graph\n",
      "    graph = _optimize_graph(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 665, in _optimize_graph\n",
      "    graph = _C._jit_pass_onnx(graph, operator_export_type)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py\", line 1901, in _run_symbolic_function\n",
      "    raise errors.UnsupportedOperatorError(\n",
      "torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py:133\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mIf ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m        All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1117\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:665\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    663\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 665\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1901\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1899\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context\u001b[38;5;241m.\u001b[39mop(op_name, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs, outputs\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39moutputsSize())  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1902\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1903\u001b[0m         opset_version,\n\u001b[1;32m   1904\u001b[0m         symbolic_function_group\u001b[38;5;241m.\u001b[39mget_min_supported()\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1907\u001b[0m     )\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::unflatten' to ONNX opset version 16 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m      3\u001b[0m EVAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_ReBN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[0;32m---> 22\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:131\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    129\u001b[0m         OmegaConf\u001b[38;5;241m.\u001b[39mupdate(cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.export_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model}\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mexport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m#write checkpoint into fifo\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     write_to_pipe(cfg, last_checkpoint)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/export.py:139\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport via \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mERROR_LOGS[cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mexport_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed, reason\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#write path to fifo\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_to_pipe\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GELU loss after 1000 batches: 0.10986531674861907\n",
    "TRAIN_SIZE = 0.8\n",
    "EVAL_SIZE = 0.2\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_ReBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=64\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"dataset.sizes.train=0.8\",\n",
    "        \"dataset.sizes.eval=0.1\",\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=1\",\n",
    "        \"run.max_iters=1000\",\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=200\",\n",
    "        \"run.gradient_accumulation_steps=1\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm, GELU with batch size of 64 and no gradient accumulation\n",
    "### Model overfitted after the first epoch already, loss did not change much after the second epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.8, 'eval': 0.2, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': [{'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}], 'milestones': None, 'warmup_iters': 100, 'min_lr': 6e-05}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 10, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-20 11:43:51,972 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:51,975 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:51,977 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 11:43:52.576049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-20 11:43:54,015 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:54,018 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:54,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:54,022 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-20_11:43:54\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:54,024 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:54,031 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,584 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,593 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,597 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 270421 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,599 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,602 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 270422 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,605 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,607 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,639 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,808 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,819 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,888 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,897 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,913 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:57,993 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,002 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,085 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,095 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,176 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,284 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:43:58,717 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:00,581 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:00,585 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:05,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.824406147003174\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:09,021 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.5822361469268795\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:12,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.512042999267578\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:16,385 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 6.454743719100952\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:20,123 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 6.45135760307312\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:23,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 6.33178129196167\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:27,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 6.2114013671875\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:31,463 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.0454943656921385\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:35,317 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.956396245956421\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:39,061 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.840814685821533\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:42,794 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.7970476150512695\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:46,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 5.672671127319336\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:50,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.605784845352173\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:55,119 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.462015914916992\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:44:59,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.394042682647705\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:03,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.298422002792359\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:06,752 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.19879560470581\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:10,559 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.170293378829956\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:14,460 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.124086570739746\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:18,200 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.067184114456177\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:21,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.02269549369812\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:25,751 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.9565112590789795\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:29,502 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.886425638198853\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:33,179 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.853456020355225\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:45:36,999 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.832189893722534\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:34,743 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/10: 4.956689357757568\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:34,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.832189893722534\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:35,481 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:35,487 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:39,188 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.761051940917969\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:42,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.742869234085083\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:46,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.723152685165405\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:50,258 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.725078201293945\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:54,031 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.722769832611084\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:46:58,139 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.7202869892120365\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:02,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.702126693725586\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:05,773 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.730746126174926\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:09,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.703732013702393\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:13,273 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.6819737434387205\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:17,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.596856880187988\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:20,981 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.696135997772217\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:24,680 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.5932684421539305\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:28,367 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.579757118225098\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:32,098 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.646166276931763\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:36,015 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.575014209747314\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:39,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.622266387939453\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:43,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.641992568969727\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:47,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.58831558227539\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:51,201 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.596975564956665\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:55,054 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.533093214035034\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:47:59,102 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.525574159622193\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:48:03,169 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.516430473327636\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:48:06,993 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.524337673187256\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:48:10,785 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.503258848190308\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:07,876 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/10: 4.630878448486328\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:07,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.503258848190308\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:08,465 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:08,468 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:12,126 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.563390493392944\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:15,887 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.484571886062622\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:19,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.533788251876831\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:23,449 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.491271686553955\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:27,181 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.4832470417022705\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:31,099 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.480559539794922\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:35,047 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.551026821136475\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:38,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.479832792282105\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:42,556 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.532398414611817\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:46,386 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.456455183029175\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:50,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.498079586029053\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:54,222 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.493864917755127\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:49:58,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.507513189315796\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:01,728 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.4459466457366945\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:05,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.504240798950195\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:09,394 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.49644775390625\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:13,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.45605354309082\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:16,917 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.453216123580932\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:50:20,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.415751600265503\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     24\u001b[0m beta2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m \u001b[38;5;66;03m# make a bit bigger because number of tokens per iter is small\u001b[39;00m\n\u001b[1;32m     26\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeLN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     ]\n\u001b[0;32m---> 49\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:108\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    105\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    111\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:186\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    184\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:227\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m#break one iteration down into multiple batches to simulate a larger batch size\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m micro_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_accumulation_steps):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m#dataloaders iterate through entire dataset\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m#problematic if datasets are large (openwebtext ~21GB) and testing should be done\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m#token tensor of length block_size (context length)\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:442\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1085\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1083\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1118\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1352\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1352\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:254\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    253\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    255\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    256\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:132\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = \"0.8\"\n",
    "EVAL_SIZE = \"0.2\"\n",
    "eval_epoch_interval = 1 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "max_iters = 250\n",
    "epochs = 10 #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = 1 #one large batch, potentially do gradient_accumulation_steps = 8 and batch_size = 8\n",
    "batch_size = 64\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "# baby GPT model :)\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "\n",
    "#not implemented currently\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "\n",
    "#not used currently\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeLN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.sizes.train=\"+TRAIN_SIZE,\n",
    "        \"dataset.sizes.eval=\"+EVAL_SIZE,\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': [{'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}], 'milestones': None, 'warmup_iters': 100, 'min_lr': 6e-05}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2'}}\n",
      "[ \u001b[36m2024-02-20 11:52:48,167 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,173 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,175 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,176 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,178 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,180 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,183 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,546 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,549 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,556 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,692 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,703 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,719 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,776 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,799 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,880 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,898 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:48,999 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,084 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,102 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,119 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,593 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,598 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,600 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,605 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,607 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,610 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,637 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-20_11:52:49_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:52:49,639 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:53:44,082 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50044\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:53:44,086 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:54:36,480 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50244\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:54:36,488 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:55:26,878 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50022\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:55:26,883 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:56:16,903 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50244\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:56:16,907 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 4/10\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:07,981 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 50244\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:07,985 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 5/10\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#inference\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=infer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[0;32m---> 14\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferonnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inferonnx\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:48\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     46\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     47\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:126\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(cfg, device)\u001b[0m\n\u001b[1;32m    123\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_new_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mhex\u001b[39m(\u001b[38;5;28mord\u001b[39m(x))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstart]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- BEGIN INFERENCE -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_infer, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    127\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(text)\n\u001b[1;32m    128\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:93\u001b[0m, in \u001b[0;36minfer.<locals>.write_inference\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m     91\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning inference from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 93\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m#i assume that sorting will take a long time which is redundant without debugging purposes\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;66;03m#log.debug(f'Uniquely generated tokens, sorted in ascending order: {y.unique().sort().values}')\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:167\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model_type, model, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    164\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# the results should not be softmaxed yet as they will be later within this function\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    169\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:147\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model_type, model, idx_cond)\u001b[0m\n\u001b[1;32m    145\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(odict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m InferType\u001b[38;5;241m.\u001b[39mCHECKPOINT:\n\u001b[0;32m--> 147\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForward pass only supported for ONNX models or checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:151\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    148\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_out(x[:, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], :]) \u001b[38;5;66;03m# note: using list [-1] to preserve the time dim\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;66;03m#squeeze batch and block_size dimension together, retain non-softmaxed word probabilities\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m#logits become a 1d tensor, containing the index of the next word\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#inference generated nonsense\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\",\n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"device=cuda\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-20 11:57:57,445 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 64}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': [{'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}], 'milestones': None, 'warmup_iters': 100, 'min_lr': 6e-05}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 100, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-20 11:57:57,673 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,676 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,678 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,680 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-20_11:57:57\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,682 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,684 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,686 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,691 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,696 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,698 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,701 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,703 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,705 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,709 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,711 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,712 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,715 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,717 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,719 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 101408, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,721 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 101408\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,723 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 118309 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,725 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,727 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.4\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,728 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,730 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 135210 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,733 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:57,735 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,160 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,163 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,169 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,305 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,322 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,338 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,384 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,399 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,476 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,491 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,501 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,593 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,602 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,681 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:58,690 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,103 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,126 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,128 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,132 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,134 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:57:59,137 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-20 11:58:00,641 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m\u001b[0m\n",
      "\n",
      "   avg_ppl \n",
      "\n",
      "   55757.4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-20_11:43:54__epoch:2\",\n",
    "    \"run.num_samples=100\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=tiny_shakespeare\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=64\",\n",
    "    \"dataset.sizes.bench=0.4\"\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make console cmd from args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cmd_from_args(args: list[str]):\n",
    "    return \"python -m qtransform \" + ' '.join(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
