{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train openwebtext and shakespeare GPT2 models with either gelu or relu and layernorm or batchnorm and run inference on them\n",
    "### For openwebtext, 4 heads and 4 transformer blocks and for shakespeare, half are used\n",
    "### Tiktoken gpt2 Tokenization is used, we do not currently have gradient accumulation implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GPT2 with Shakespeare GELU BatchNorm, custom_ln is Identity layer\n",
    "### Params similiar to nanoGPT (https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py) except for gpt model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 5000, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 13:44:11.648799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:44:13,308 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,313 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,316 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-13_13:44:13\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,577 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:13,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,939 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,948 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,954 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,986 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:14,991 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,283 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,402 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,411 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,498 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:15,729 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:17,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.0814786911010743\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:18,962 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.028734016418458\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.417726612091064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.841564083099366\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:19,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.336726760864257\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,242 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.842315435409546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,564 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.42349648475647\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:20,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.046977567672729\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,188 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.75757155418396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.5278137683868405\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:21,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.345893001556396\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,180 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.116252470016479\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.0132341384887695\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:22,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.878851127624512\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,109 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.804667854309082\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.695879507064819\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:23,760 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.600269079208374\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,083 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.538007020950317\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.44656195640564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:24,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.251713800430298\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.200865888595581\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.038680076599121\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:25,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.861286687850952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,037 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.634791707992553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.378215074539185\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:26,690 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.140535163879394\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,017 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.918185019493103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.59971444606781\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.332096314430237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:27,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.1224578857421874\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,306 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.891332507133484\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.6721574783325197\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:28,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4845260620117187\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.2823659896850588\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,583 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.1804298162460327\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:29,909 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.009354305267334\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.8618500351905822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,560 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.6786442637443542\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:30,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.5513007402420045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.4092130661010742\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.327401328086853\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:31,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.2216348052024841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.1183194756507873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.0603752493858338\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:32,828 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.9550049722194671\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,122 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.8526090204715728\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.8235867023468018\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:33,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.7416079103946686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,089 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.6766059696674347\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.623449730873108\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:34,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.5610007286071778\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,076 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.5322279185056686\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,400 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.4816823750734329\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:35,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.4518648833036423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.4204030305147171\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,369 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.37947399616241456\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:36,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.35556192696094513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.3440380185842514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.29768029451370237\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.28025399148464203\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:37,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.26523620039224627\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.2554860457777977\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.23417816907167435\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:38,937 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.21371423751115798\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.1988332137465477\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.19074835479259492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:39,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.18297459036111832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,240 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.17245704233646392\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.17341131418943406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:40,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.1595330536365509\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.15104628428816796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.14839548915624617\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:41,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.14153633862733841\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.1328538939356804\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.12690432518720626\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:42,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.13007508590817451\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.125824736058712\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.12251279503107071\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:43,709 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.1187422126531601\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.11048740148544312\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,317 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.10810237675905228\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,641 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.10765649899840354\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:44,969 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.10224309861660004\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,292 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.10713213160634041\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.09892426878213882\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:45,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.10035960376262665\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.09816799089312553\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.09627232924103737\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:46,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.09459140226244926\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,236 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.09456081539392472\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.09531142190098763\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:47,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.09356032237410546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.0900658719241619\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.09381850361824036\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:48,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.08983872383832932\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.089760223031044\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.08956152871251107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:49,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.08698741123080253\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.08797898441553116\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,475 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.08725197464227677\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:50,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.08584734052419662\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.08436140418052673\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.08557359799742699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:51,756 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.08337687328457832\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.08435980305075645\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.08416210934519767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.08072719871997833\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:52,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.08021606504917145\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,268 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07771769538521767\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.08004260808229446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:53,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.07954467162489891\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.08120676279067993\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.07973136603832245\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:54,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.08074760884046554\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.08167456313967705\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.08013327717781067\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:55,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.08386159390211105\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,168 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.08124819472432136\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,493 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.07815293446183205\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:56,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.08068719729781151\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,140 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.07527995184063911\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,460 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1210 loss: 0.0792896255850792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:57,777 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1220 loss: 0.07909879982471466\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,098 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1230 loss: 0.080222849547863\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,416 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1240 loss: 0.07985053658485412\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:58,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1250 loss: 0.07990247905254363\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1260 loss: 0.07967272847890854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1270 loss: 0.07683225870132446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:44:59,700 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1280 loss: 0.07696817442774773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1290 loss: 0.07847564071416854\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1300 loss: 0.07635548561811448\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1310 loss: 0.07755953297019005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:00,980 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1320 loss: 0.08147314414381981\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,299 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1330 loss: 0.0745748072862625\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1340 loss: 0.07801730483770371\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:01,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1350 loss: 0.07652391642332076\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1360 loss: 0.07694418206810952\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1370 loss: 0.07574513629078865\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:02,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1380 loss: 0.08034205883741379\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1390 loss: 0.07813942767679691\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1400 loss: 0.07715454623103142\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:03,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1410 loss: 0.07432392910122872\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,172 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1420 loss: 0.07728920727968216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,496 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1430 loss: 0.07314848452806473\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:04,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1440 loss: 0.07422819398343564\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1450 loss: 0.07614408656954766\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,481 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1460 loss: 0.07296678051352501\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:05,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1470 loss: 0.07522793412208557\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,124 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1480 loss: 0.07485603466629982\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1490 loss: 0.07659061178565026\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:06,768 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1500 loss: 0.07550743147730828\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1510 loss: 0.07389946877956391\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1520 loss: 0.07652528509497643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:07,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1530 loss: 0.07560347318649292\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,056 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1540 loss: 0.07402549535036088\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,380 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1550 loss: 0.074910619109869\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:08,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1560 loss: 0.07472982481122017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1570 loss: 0.07554230615496635\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:45:09,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1580 loss: 0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/100: 0.133059561252594\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:00,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07694078162312508\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,172 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/100\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,348 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.005953484401106835\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07082101553678513\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:01,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.07401572242379188\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.07121982723474503\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,635 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.07029064297676087\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:02,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.06943750269711017\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.07120902799069881\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.06863513328135014\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:03,903 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.07117352485656739\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.06729949042201042\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,552 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07023796513676643\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:04,871 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07243234217166901\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.0731443539261818\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.0650293543934822\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:05,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.07004907317459583\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07083249166607856\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,487 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.06878587603569031\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:06,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.06674405559897423\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,128 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.06930300667881965\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,433 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07282493859529496\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:07,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.0702348593622446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,074 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.06613621674478054\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.065633849427104\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:08,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07187155038118362\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07209198959171773\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07056076899170875\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:09,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 0.0731478177011013\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 0.06732679381966591\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 0.06573506109416485\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,640 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 0.07029695585370063\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:10,964 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 0.06923590525984764\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 0.06766384989023208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,611 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 0.06799498610198498\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:11,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 0.06996757984161377\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,255 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 0.06888032145798206\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,580 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 0.06892801485955716\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:12,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 0.0686967846006155\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 0.07068811431527137\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,488 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 0.06987420693039895\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:13,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 0.06994570903480053\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 0.06992909088730812\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 0.06451450362801552\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:14,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 0.0671445544809103\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 0.0680693306028843\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,430 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 0.07334598451852799\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:15,752 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 0.06892706342041492\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 0.06757157891988755\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 0.07052591480314732\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:16,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 0.06981403641402721\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 0.06944975815713406\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 0.07077371105551719\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:17,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 510 loss: 0.06996183507144452\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 520 loss: 0.0694230530411005\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,345 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 530 loss: 0.06557997539639474\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 540 loss: 0.06862628795206546\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:18,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 550 loss: 0.0700732484459877\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 560 loss: 0.06813231781125069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,618 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 570 loss: 0.06853835545480251\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:19,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 580 loss: 0.06956844478845596\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 590 loss: 0.06586511470377446\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,582 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 600 loss: 0.07010632418096066\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:20,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 610 loss: 0.06835015043616295\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 620 loss: 0.06821108944714069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,557 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 630 loss: 0.06943667121231556\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:21,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 640 loss: 0.0673607874661684\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 650 loss: 0.06755397357046604\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 660 loss: 0.06880321577191353\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:22,840 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 670 loss: 0.06871512234210968\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 680 loss: 0.07124297022819519\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,482 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 690 loss: 0.06964963674545288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:23,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 700 loss: 0.07146930769085884\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 710 loss: 0.06947363130748271\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 720 loss: 0.06855200305581093\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:24,772 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 730 loss: 0.06658084094524383\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,090 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 740 loss: 0.06878070905804634\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 750 loss: 0.07016171887516975\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:25,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 760 loss: 0.07204182855784894\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,064 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 770 loss: 0.06919909864664078\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,359 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 780 loss: 0.0710775252431631\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,667 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 790 loss: 0.07189789228141308\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:26,986 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 800 loss: 0.07093326337635517\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 810 loss: 0.07051902897655964\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 820 loss: 0.07366400584578514\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:27,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 830 loss: 0.0664932906627655\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,283 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 840 loss: 0.06879346258938313\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,608 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 850 loss: 0.06913681291043758\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:28,932 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 860 loss: 0.069281155616045\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 870 loss: 0.06843326278030873\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 880 loss: 0.06893916204571723\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:29,894 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 890 loss: 0.06852278523147107\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,215 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 900 loss: 0.07043468467891216\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 910 loss: 0.07027224525809288\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:30,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 920 loss: 0.06835661269724369\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,189 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 930 loss: 0.07088794261217117\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,509 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 940 loss: 0.07069383189082146\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:31,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 950 loss: 0.06545544117689132\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,155 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 960 loss: 0.06914999186992646\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 970 loss: 0.06767004244029522\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:32,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 980 loss: 0.07045739553868771\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,120 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 990 loss: 0.0689571488648653\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1000 loss: 0.06914092190563678\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:33,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1010 loss: 0.06643678843975068\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,087 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1020 loss: 0.06644399277865887\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1030 loss: 0.06971023604273796\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:34,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1040 loss: 0.07272332422435283\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1050 loss: 0.06667447723448276\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1060 loss: 0.06759239658713341\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:35,713 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1070 loss: 0.06913544759154319\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1080 loss: 0.07417276315391064\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,363 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1090 loss: 0.06717519052326679\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:36,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1100 loss: 0.06821254156529903\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1110 loss: 0.06578771919012069\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1120 loss: 0.06996179968118668\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1130 loss: 0.066846938803792\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:37,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1140 loss: 0.07002154067158699\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1150 loss: 0.06956944465637208\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,616 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1160 loss: 0.0701913632452488\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:38,935 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1170 loss: 0.07032300047576427\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1180 loss: 0.06583375632762908\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1190 loss: 0.06707629524171352\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:46:39,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 1200 loss: 0.06304956637322903\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrun=train\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdevice=cuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[0;32m---> 16\u001b[0m qtransform\u001b[39m.\u001b[39;49mnotebook_run(args)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[39m=\u001b[39m compose(config_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, overrides\u001b[39m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m main(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m  __main__ \u001b[39mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m mn\u001b[39m.\u001b[39;49mmain(cfg)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m  train\u001b[39m.\u001b[39;49mrun(cfg)\n\u001b[1;32m     45\u001b[0m \u001b[39mcase\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbench\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mqtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[39m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[39m=\u001b[39m train(cfg\u001b[39m=\u001b[39;49mcfg, device\u001b[39m=\u001b[39;49mdevice, model\u001b[39m=\u001b[39;49mmodel, train_data_loader\u001b[39m=\u001b[39;49mtrain_dataloader, eval_data_loader\u001b[39m=\u001b[39;49meval_dataloader, optimizer\u001b[39m=\u001b[39;49moptimizer, scheduler\u001b[39m=\u001b[39;49mscheduler, timestamp\u001b[39m=\u001b[39;49mtimestamp)\n\u001b[1;32m    110\u001b[0m \u001b[39m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mFinished training model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    185\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEPOCH: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     metrics \u001b[39m=\u001b[39m train_one_epoch(cfg, device, model, train_data_loader, optimizer, mini_run)\n\u001b[1;32m    189\u001b[0m     \u001b[39m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39meval_epoch_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m eval_data_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:230\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    228\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    229\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(outputs, labels)\n\u001b[0;32m--> 230\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    231\u001b[0m \u001b[39m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgrad_clip\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mfloat\u001b[39m) \u001b[39mand\u001b[39;00m cfg\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mgrad_clip \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=64\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=5000\",\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=200\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write inference of Shakespeare GELU BatchNorm, custom_ln is Identity layer to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-13 13:52:43,381 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1'}}\n",
      "[ \u001b[36m2024-02-13 13:52:43,567 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,571 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,574 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,577 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,580 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,584 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,587 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,752 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,755 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,765 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,866 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,878 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,895 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:43,912 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,230 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,235 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,238 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,244 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,246 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,250 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,261 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:52:44,263 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   202,   317,   363,   385,   406,   563,   589,   879,   880,\n",
      "          962,  1010,  1152,  1267,  1369,  1408,  1487,  1860,  2143,  2186,\n",
      "         2244,  2266,  2283,  2394,  2566,  2889,  2988,  3209,  3297,  3385,\n",
      "         3599,  3708,  3765,  3770,  3843,  3889,  4252,  4256,  4388,  4393,\n",
      "         4410,  4611,  4692,  4891,  4930,  5398,  5399,  5507,  5675,  5785,\n",
      "         5801,  5933,  6006,  6100,  6111,  6458,  6481,  6596,  6625,  6760,\n",
      "         6783,  6821,  6868,  7134,  7248,  7251,  7491,  7555,  7840,  7845,\n",
      "         9030,  9041,  9067,  9194,  9341,  9352,  9388,  9477,  9839,  9855,\n",
      "         9912, 10180, 10458, 10546, 10605, 10643, 10687, 10827, 10873, 11037,\n",
      "        11325, 11415, 11481, 11507, 11520, 11717, 11748, 11769, 11788, 11793,\n",
      "        11872, 12002, 12476, 12579, 12659, 12694, 12711, 13030, 13072, 13091,\n",
      "        13178, 13201, 13216, 13255, 13346, 13451, 13509, 13562, 13563, 13635,\n",
      "        13675, 13816, 13858, 13903, 13905, 13949, 14001, 14073, 14202, 14327,\n",
      "        14383, 14505, 14515, 14532, 14725, 14840, 14855, 14921, 15073, 15096,\n",
      "        15121, 15222, 15304, 15314, 15433, 15794, 15860, 15861, 15989, 16035,\n",
      "        16067, 16392, 16416, 16437, 16527, 16824, 16862, 16999, 17013, 17108,\n",
      "        17113, 17318, 17556, 17574, 17602, 17667, 17672, 17856, 18135, 18198,\n",
      "        18250, 18301, 18333, 18362, 18371, 18471, 18734, 18926, 18948, 18976,\n",
      "        19104, 19145, 19296, 19306, 19393, 19524, 19762, 19899, 19949, 20126,\n",
      "        20128, 20188, 20326, 20517, 20518, 20627, 20656, 20706, 20856, 20893,\n",
      "        21354, 21369, 21381, 21567, 21694, 21710, 21722, 21910, 22076, 22425,\n",
      "        22530, 22568, 22591, 22634, 22968, 23201, 23230, 23253, 23480, 23530,\n",
      "        23678, 23689, 23738, 23852, 24144, 24208, 24403, 24513, 24758, 25452,\n",
      "        25492, 25580, 25810, 25867, 26008, 26248, 26268, 26476, 26770, 26944,\n",
      "        27156, 27611, 27620, 27800, 27858, 27884, 27923, 28165, 28205, 28563,\n",
      "        28682, 28699, 28705, 28992, 29037, 29094, 29116, 29445, 29483, 29485,\n",
      "        29557, 29746, 29963, 30178, 30365, 30468, 30521, 30872, 30962, 30966,\n",
      "        31103, 31107, 31456, 31510, 31643, 31673, 31736, 31788, 32060, 32336,\n",
      "        32493, 32728, 32780, 32829, 33010, 33161, 33174, 33280, 33324, 33535,\n",
      "        33727, 33769, 33871, 33877, 33883, 34004, 34042, 34227, 34273, 34278,\n",
      "        34311, 34373, 34412, 34571, 34626, 34937, 35030, 35075, 35418, 35814,\n",
      "        35903, 36082, 36114, 36220, 36315, 36492, 36673, 36697, 36873, 37061,\n",
      "        37090, 37206, 37691, 37728, 38118, 38356, 38385, 38432, 38535, 38538,\n",
      "        38687, 38798, 38965, 39075, 39232, 39377, 39384, 39403, 39698, 39700,\n",
      "        39781, 39801, 39805, 39984, 40009, 40062, 40078, 40388, 40427, 40430,\n",
      "        40508, 40599, 40604, 40693, 40701, 40713, 40755, 40850, 40869, 40901,\n",
      "        41318, 41534, 41550, 41736, 41762, 41924, 42139, 42192, 42248, 42259,\n",
      "        42452, 42585, 42598, 42834, 42993, 43034, 43045, 43189, 43218, 43289,\n",
      "        43426, 43503, 43865, 43888, 43901, 43956, 43984, 44153, 44159, 44205,\n",
      "        44360, 44624, 44678, 44679, 44730, 44822, 45186, 45208, 45384, 45479,\n",
      "        45529, 45567, 45589, 45627, 45638, 45674, 45754, 45789, 46087, 46153,\n",
      "        46241, 46253, 46304, 46403, 46414, 46581, 46696, 46807, 46912, 46944,\n",
      "        47316, 47470, 47653, 47746, 47871, 47892, 47894, 47992, 48013, 48112,\n",
      "        48305, 48377, 48479, 48614, 48664, 48752, 48763, 48999, 49026, 49100,\n",
      "        49143, 49153, 49533, 49615, 49792, 49945, 49969, 50158, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:10,207 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,705 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  153,   198,   251,   317,   650,   824,   829,   832,   878,   901,\n",
      "          919,   962,  1195,  1267,  1270,  1451,  1473,  1683,  1710,  1772,\n",
      "         1858,  1937,  2048,  2053,  2143,  2375,  2491,  2537,  2878,  2881,\n",
      "         2898,  2989,  3006,  3134,  3267,  3322,  3357,  3383,  3408,  3538,\n",
      "         3542,  3599,  3770,  3901,  3914,  3921,  4388,  4421,  4688,  4719,\n",
      "         4746,  4831,  5128,  5291,  5420,  5664,  5884,  5942,  6035,  6113,\n",
      "         6295,  6536,  6718,  6760,  6783,  6851,  6969,  6994,  7027,  7056,\n",
      "         7111,  7123,  7134,  7251,  7358,  7449,  7496,  7546,  7575,  7590,\n",
      "         7596,  8361,  8455,  8489,  8590,  8604,  8689,  8773,  8967,  9029,\n",
      "         9194,  9404,  9538,  9717,  9943,  9976, 10282, 10308, 10310, 10403,\n",
      "        10547, 10691, 10715, 10930, 10946, 11089, 11286, 11584, 11682, 11701,\n",
      "        11818, 11862, 11885, 11903, 12259, 12490, 12559, 12587, 12694, 12729,\n",
      "        12990, 13018, 13084, 13324, 13361, 13502, 13792, 13822, 13871, 13929,\n",
      "        13949, 14001, 14073, 14138, 14331, 14379, 14455, 14478, 14560, 14594,\n",
      "        14725, 14866, 14992, 15023, 15073, 15275, 15559, 15582, 15646, 15685,\n",
      "        15828, 16009, 16025, 16156, 16161, 16313, 16465, 16501, 16665, 16670,\n",
      "        16869, 16980, 16983, 17047, 17054, 17218, 17228, 17278, 17312, 17323,\n",
      "        17601, 17681, 17866, 17986, 18150, 18447, 18494, 18511, 18551, 18689,\n",
      "        18760, 18764, 18778, 18948, 18981, 19209, 19285, 19313, 19350, 19500,\n",
      "        19924, 20326, 20333, 20401, 20478, 20514, 20518, 20570, 20627, 20662,\n",
      "        20856, 20881, 20920, 21008, 21122, 21485, 21489, 21604, 21797, 22233,\n",
      "        22358, 22440, 22917, 22926, 22968, 23230, 23312, 23378, 23421, 23689,\n",
      "        23768, 23775, 23848, 24155, 24181, 24471, 24513, 24545, 24626, 24803,\n",
      "        25039, 25087, 25100, 25305, 25312, 25324, 25452, 25467, 25492, 25532,\n",
      "        25535, 25643, 25716, 25810, 25824, 25833, 26071, 26111, 26153, 26239,\n",
      "        26405, 26417, 26424, 26872, 27156, 27310, 27413, 27694, 27775, 27922,\n",
      "        27924, 27951, 28079, 28273, 28444, 28563, 28595, 28747, 28796, 29090,\n",
      "        29094, 29116, 29353, 29405, 29533, 29572, 29663, 29706, 29774, 30000,\n",
      "        30241, 30383, 30558, 30815, 30923, 30982, 31102, 31103, 31156, 31324,\n",
      "        31360, 31618, 31717, 31883, 32045, 32068, 32260, 32275, 32452, 33028,\n",
      "        33099, 33158, 33371, 33418, 33658, 33698, 33831, 33834, 34311, 34575,\n",
      "        34709, 34779, 34835, 34937, 34977, 35030, 35044, 35096, 35273, 35299,\n",
      "        35330, 35335, 35709, 35750, 35818, 35978, 36073, 36113, 36117, 36315,\n",
      "        36394, 36436, 36492, 36529, 36599, 36672, 36772, 36851, 36984, 37342,\n",
      "        37376, 37405, 37515, 37653, 37717, 37914, 37984, 38069, 38141, 38487,\n",
      "        38687, 38754, 38839, 38977, 39027, 39153, 39178, 39204, 39230, 39329,\n",
      "        39426, 39427, 39908, 39984, 39995, 40037, 40172, 40388, 40396, 40426,\n",
      "        40790, 40817, 41105, 41171, 41173, 41181, 41406, 41477, 41484, 41550,\n",
      "        41572, 41598, 41843, 41936, 41989, 41990, 42220, 42228, 42252, 42283,\n",
      "        42487, 42734, 42796, 42798, 42816, 42834, 42993, 43036, 43289, 43389,\n",
      "        43469, 43699, 43826, 43934, 43969, 43984, 44193, 44360, 44678, 44831,\n",
      "        44884, 45336, 45558, 45560, 45638, 45657, 45674, 45760, 45802, 45938,\n",
      "        45975, 46040, 46304, 46601, 46767, 46782, 47102, 47264, 47387, 47447,\n",
      "        47560, 47667, 47733, 47821, 47928, 48013, 48259, 48282, 48312, 48364,\n",
      "        48537, 48752, 48930, 48953, 49355, 49406, 49467, 49655, 49673, 49716,\n",
      "        49798, 50118]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:32,711 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,693 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   369,   784,   788,   821,   847,   869,   901,   962,   998,\n",
      "         1020,  1025,  1147,  1179,  1193,  1201,  1267,  1299,  1370,  1408,\n",
      "         1433,  1508,  1563,  1729,  1783,  1903,  2173,  2288,  2300,  2334,\n",
      "         2566,  2621,  2690,  2878,  2898,  2950,  3134,  3396,  3478,  3631,\n",
      "         3652,  3726,  3757,  3925,  3976,  4096,  4157,  4268,  4367,  4388,\n",
      "         4421,  4423,  4624,  4704,  4898,  5227,  5720,  5831,  5933,  5942,\n",
      "         5994,  6035,  6222,  6229,  6395,  6830,  6921,  6922,  6980,  6994,\n",
      "         7025,  7027,  7071,  7134,  7424,  7568,  7623,  7648,  7705,  8125,\n",
      "         8206,  8442,  8489,  8535,  8967,  9082,  9145,  9194,  9260,  9791,\n",
      "        10043, 10163, 10180, 10328, 10511, 10685, 10801, 10912, 10969, 11286,\n",
      "        11412, 11481, 11626, 11646, 11748, 11769, 11851, 12093, 12374, 13011,\n",
      "        13018, 13128, 13154, 13216, 13415, 13470, 13509, 13518, 13562, 13577,\n",
      "        13581, 13740, 13982, 14001, 14043, 14192, 14280, 14379, 14533, 14730,\n",
      "        14874, 14921, 15222, 15313, 15323, 15350, 15582, 15636, 15670, 15712,\n",
      "        15976, 15982, 16098, 16457, 16665, 16699, 16742, 16754, 16798, 17062,\n",
      "        17108, 17178, 17312, 17381, 17798, 17852, 17871, 17918, 17963, 18074,\n",
      "        18091, 18127, 18133, 18184, 18347, 18364, 18869, 18895, 18981, 18997,\n",
      "        19009, 19021, 19072, 19140, 19165, 19171, 19189, 19215, 19296, 19306,\n",
      "        19386, 19393, 19762, 19916, 19938, 19988, 20048, 20214, 20319, 20468,\n",
      "        20478, 20491, 21086, 21093, 21253, 21287, 21550, 21932, 22193, 22317,\n",
      "        22358, 22540, 22633, 22644, 22879, 23253, 23378, 23405, 23412, 23456,\n",
      "        23627, 23903, 24030, 24065, 24208, 24300, 24306, 24454, 24513, 24718,\n",
      "        24798, 24940, 25039, 25492, 25496, 25564, 25613, 25810, 26034, 26102,\n",
      "        26111, 26248, 26362, 26405, 26670, 26680, 26727, 26872, 26930, 27089,\n",
      "        27162, 27262, 27413, 27765, 27923, 28212, 28337, 28408, 28496, 28563,\n",
      "        28821, 28918, 29094, 29353, 29550, 29559, 29594, 29657, 29869, 30017,\n",
      "        30099, 30139, 30202, 30260, 30483, 30631, 30915, 30946, 31021, 31103,\n",
      "        31107, 31288, 31340, 31360, 31561, 31575, 31851, 31975, 32123, 32168,\n",
      "        32228, 32263, 32443, 32452, 32619, 32860, 33150, 33158, 33562, 33599,\n",
      "        33610, 33687, 33698, 33736, 33740, 34227, 34278, 34591, 34627, 34805,\n",
      "        34811, 34911, 34937, 35030, 35177, 35202, 35290, 35316, 35385, 35466,\n",
      "        35494, 35589, 35867, 35902, 35997, 36222, 36315, 36377, 36509, 36873,\n",
      "        37053, 37117, 37294, 37342, 37344, 37464, 37536, 37608, 37810, 37864,\n",
      "        37994, 38079, 38128, 38248, 38304, 38336, 38356, 38510, 38591, 38985,\n",
      "        39057, 39177, 39427, 39538, 39789, 39797, 39801, 39805, 39907, 39984,\n",
      "        39995, 40009, 40028, 40108, 40198, 40223, 40401, 40549, 40600, 40740,\n",
      "        40755, 40869, 40952, 40976, 41148, 41199, 41274, 41364, 41572, 41632,\n",
      "        41672, 41973, 41990, 42184, 42341, 42470, 42522, 42540, 42598, 42656,\n",
      "        42798, 42851, 43130, 43289, 43648, 43815, 43895, 43988, 44205, 44265,\n",
      "        44333, 44427, 44589, 44598, 44707, 44738, 44978, 45002, 45017, 45041,\n",
      "        45186, 45384, 45479, 45529, 45564, 45627, 45642, 45650, 45674, 45677,\n",
      "        45739, 45752, 45967, 46259, 46408, 46636, 46811, 46969, 47264, 47333,\n",
      "        47392, 47558, 47571, 47746, 47916, 47986, 48013, 48285, 48437, 48622,\n",
      "        48805, 48822, 48877, 48949, 49026, 49153, 49261, 49716, 49720, 49747,\n",
      "        49798, 49923, 49945, 50040, 50243]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:53:54,698 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,603 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   54,    96,   135,   198,   317,   343,   590,   694,   788,   824,\n",
      "          829,   879,   933,   962,  1010,  1020,  1113,  1267,  1416,  1620,\n",
      "         1645,  1656,  1733,  1926,  2334,  2451,  2616,  2872,  2874,  2889,\n",
      "         3063,  3073,  3298,  3322,  3478,  3510,  3534,  3546,  3629,  3726,\n",
      "         3759,  3770,  3929,  4062,  4283,  4294,  4422,  4641,  4671,  4716,\n",
      "         4719,  4795,  4930,  5065,  5359,  5476,  5533,  5578,  5720,  5748,\n",
      "         6013,  6087,  6336,  6760,  6783,  6810,  6932,  6968,  7027,  7134,\n",
      "         7160,  7226,  7552,  7568,  7672,  7943,  8035,  8489,  8978,  9260,\n",
      "         9388,  9439,  9538,  9674,  9717,  9746,  9870,  9912, 10282, 10503,\n",
      "        10573, 10607, 11182, 11265, 11481, 11544, 11584, 11588, 11647, 11748,\n",
      "        11769, 11780, 11793, 11870, 11872, 11885, 11974, 12346, 12407, 12489,\n",
      "        12587, 12703, 12723, 13444, 13451, 13460, 13502, 13656, 13684, 13725,\n",
      "        13807, 13822, 13939, 14073, 14105, 14186, 14268, 14614, 14725, 14836,\n",
      "        14992, 15073, 15222, 15350, 15501, 15609, 15872, 16031, 16067, 16457,\n",
      "        16634, 16665, 16722, 16941, 16992, 16998, 17013, 17098, 17210, 17228,\n",
      "        17233, 17247, 17323, 17452, 17575, 17913, 17945, 18356, 18556, 18577,\n",
      "        18659, 18699, 18758, 18789, 18839, 18948, 19064, 19145, 19402, 19480,\n",
      "        19485, 19601, 19751, 19762, 19949, 19990, 20048, 20060, 20326, 20478,\n",
      "        20627, 20751, 20893, 20920, 21020, 21079, 21117, 21246, 21253, 21404,\n",
      "        21420, 21459, 21544, 21567, 21714, 21723, 21819, 21864, 21910, 22080,\n",
      "        22089, 22317, 22451, 22568, 22633, 22880, 23194, 23316, 23412, 23678,\n",
      "        23689, 23695, 23798, 23821, 23868, 23982, 24065, 24431, 24513, 24550,\n",
      "        24666, 24798, 24872, 24917, 24991, 25089, 25100, 25452, 25492, 25546,\n",
      "        25595, 25734, 25755, 25810, 26119, 26153, 26343, 26476, 26493, 26660,\n",
      "        26920, 27085, 27277, 27319, 27775, 28003, 28252, 28256, 28563, 28583,\n",
      "        28821, 29127, 29405, 29440, 29502, 29565, 29594, 29908, 30000, 30241,\n",
      "        30260, 30315, 30381, 30558, 30571, 30641, 30716, 30812, 30837, 30886,\n",
      "        31103, 31288, 31353, 31618, 31705, 31851, 31974, 32228, 32262, 32443,\n",
      "        32650, 32652, 32674, 32876, 33174, 33216, 33520, 33687, 33769, 33780,\n",
      "        33905, 34004, 34051, 34213, 34373, 34433, 34586, 34604, 34706, 34761,\n",
      "        34766, 34801, 34895, 34899, 35044, 35316, 35335, 35466, 35494, 35559,\n",
      "        35589, 35749, 35997, 36017, 36110, 36418, 36439, 36453, 36486, 36492,\n",
      "        36570, 36673, 36694, 36857, 36998, 37061, 37116, 37163, 37282, 37536,\n",
      "        37637, 37653, 37835, 37887, 37898, 37914, 37988, 37994, 38118, 38125,\n",
      "        38128, 38131, 38139, 38244, 38461, 38518, 38520, 38640, 38687, 38819,\n",
      "        38839, 38889, 39067, 39074, 39109, 39189, 39258, 39311, 39315, 39427,\n",
      "        39775, 39984, 40062, 40300, 40323, 40388, 40401, 40604, 40783, 40817,\n",
      "        40828, 40976, 41265, 41442, 41762, 41906, 41957, 41962, 42120, 42259,\n",
      "        42298, 42413, 42457, 42834, 42845, 43058, 43295, 43298, 43320, 43815,\n",
      "        43826, 43851, 43863, 43895, 43910, 43960, 44024, 44265, 44427, 44477,\n",
      "        44598, 44738, 44796, 44856, 44970, 45186, 45384, 45389, 45477, 45479,\n",
      "        45558, 45599, 45805, 45829, 45831, 46304, 46345, 46355, 46420, 46807,\n",
      "        47130, 47227, 47287, 47335, 47698, 47894, 47990, 48000, 48119, 48212,\n",
      "        48215, 48312, 48479, 48629, 48659, 48789, 49100, 49119, 49214, 49655,\n",
      "        49792, 49798, 49813, 49823, 50087, 50216]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:18,609 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,214 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   590,   824,  1073,  1074,  1096,  1147,  1179,  1594,  1800,\n",
      "         1880,  2139,  2244,  2300,  2451,  2552,  2690,  2898,  3063,  3101,\n",
      "         3322,  3363,  3403,  3564,  3599,  3756,  3770,  3834,  3889,  4206,\n",
      "         4283,  4367,  4628,  4804,  5097,  5209,  5348,  5476,  5528,  5578,\n",
      "         5630,  5675,  5755,  5892,  5923,  5957,  5965,  6011,  6035,  6051,\n",
      "         6060,  6229,  6442,  6549,  6578,  6604,  6684,  6895,  6994,  7027,\n",
      "         7054,  7118,  7251,  7598,  7623,  7962,  8024,  8137,  8149,  8334,\n",
      "         8482,  8532,  8593,  8789,  9067,  9071,  9232,  9369,  9814,  9912,\n",
      "        10193, 10266, 10282, 10360, 10362, 10458, 10668, 10687, 10794, 10839,\n",
      "        10958, 10970, 11244, 11268, 11609, 11723, 11941, 12046, 12203, 12307,\n",
      "        12481, 12694, 12866, 13011, 13012, 13154, 13172, 13361, 13380, 13431,\n",
      "        13527, 13528, 13562, 13699, 13713, 13733, 13939, 14001, 14270, 14532,\n",
      "        14725, 14750, 14842, 14866, 14988, 15323, 15353, 15396, 15540, 15636,\n",
      "        15651, 15712, 15782, 15794, 15796, 15863, 16031, 16099, 16450, 16495,\n",
      "        16541, 16665, 16666, 16989, 17218, 17356, 17464, 17535, 17575, 17759,\n",
      "        17875, 17906, 17986, 18145, 18307, 18316, 18333, 18869, 18895, 18926,\n",
      "        18948, 18997, 19115, 19337, 19485, 19856, 19921, 19988, 20048, 20060,\n",
      "        20223, 20257, 20305, 20468, 20478, 20481, 20483, 20517, 20518, 20575,\n",
      "        20627, 20746, 20881, 20983, 21121, 21246, 21273, 21301, 21308, 21404,\n",
      "        21604, 21753, 21925, 21980, 22025, 22049, 22080, 22233, 22274, 22346,\n",
      "        22372, 22566, 22591, 22633, 22657, 22798, 22858, 23121, 23128, 23162,\n",
      "        23201, 23253, 23449, 23678, 23690, 23852, 23938, 23965, 24216, 24275,\n",
      "        24335, 24345, 24396, 24422, 24479, 24826, 25039, 25168, 25412, 25532,\n",
      "        25622, 25631, 25679, 25716, 25810, 25839, 25939, 25977, 26008, 26119,\n",
      "        26322, 26517, 26657, 26694, 26748, 26843, 27020, 27022, 27413, 27497,\n",
      "        27644, 27775, 28058, 28068, 28131, 28328, 28334, 28335, 28337, 28563,\n",
      "        28617, 28669, 28682, 28740, 29124, 29150, 29305, 29350, 29353, 29533,\n",
      "        29558, 29594, 29783, 29847, 30035, 30241, 30381, 30449, 30458, 30468,\n",
      "        30741, 30947, 31103, 31107, 31145, 31185, 31239, 31264, 31322, 31328,\n",
      "        31360, 31430, 31534, 31822, 31851, 31922, 32147, 32532, 32739, 33028,\n",
      "        33032, 33056, 33061, 33093, 33174, 33222, 33293, 33346, 33914, 34004,\n",
      "        34268, 34311, 34575, 34805, 34937, 35082, 35097, 35566, 35589, 36011,\n",
      "        36073, 36116, 36248, 36401, 36492, 36582, 36599, 36673, 36786, 36865,\n",
      "        36873, 36977, 37059, 37294, 37308, 37612, 38064, 38141, 38150, 38322,\n",
      "        38385, 38436, 39027, 39067, 39107, 39426, 39514, 39653, 39808, 39826,\n",
      "        39856, 39901, 39908, 39984, 40014, 40201, 40219, 40426, 40557, 40599,\n",
      "        40693, 40752, 40755, 40817, 41112, 41318, 41472, 41505, 41550, 41572,\n",
      "        41630, 41632, 41762, 42119, 42120, 42220, 42298, 42452, 42470, 42528,\n",
      "        42554, 42614, 42760, 42790, 42821, 42851, 42880, 42967, 43079, 43136,\n",
      "        43289, 43391, 43499, 43554, 43643, 43766, 43815, 43851, 43930, 43956,\n",
      "        43984, 44123, 44130, 44153, 44214, 44402, 44427, 44459, 44562, 44600,\n",
      "        44678, 44711, 44978, 45041, 45134, 45336, 45529, 45627, 45752, 45760,\n",
      "        45877, 45896, 46046, 46153, 46251, 46960, 47145, 47172, 47277, 47675,\n",
      "        47745, 47803, 47851, 47856, 48013, 48171, 48215, 48230, 48479, 48528,\n",
      "        48529, 48622, 49026, 49100, 49129, 49312, 49355, 49425, 49449, 49453,\n",
      "        49486, 49602, 49615, 49732, 49744, 49765, 49900, 49910, 49923, 50160]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:54:40,293 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 4/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,601 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   259,   285,   317,   406,   521,   653,   664,   829,   901,\n",
      "          962,   972,  1010,  1248,  1267,  1508,  1616,  1774,  1880,  2005,\n",
      "         2173,  2207,  2235,  2300,  2454,  2485,  2653,  2898,  2950,  2956,\n",
      "         3061,  3355,  3383,  3480,  3534,  3569,  3776,  3889,  4157,  4294,\n",
      "         4388,  4400,  4429,  4491,  4624,  4641,  4712,  4725,  4804,  4930,\n",
      "         5195,  5291,  5305,  5371,  5399,  5443,  5507,  5630,  5905,  5960,\n",
      "         5965,  6011,  6035,  6175,  6274,  6442,  6480,  6557,  6596,  6760,\n",
      "         6775,  7027,  7118,  7251,  7324,  7450,  8188,  8248,  8334,  8345,\n",
      "         8347,  8354,  8569,  8705,  8744,  8804,  8854,  8892,  9153,  9209,\n",
      "         9323,  9328,  9433,  9452,  9589,  9906,  9918, 10010, 10076, 10282,\n",
      "        10328, 10379, 10404, 11015, 11087, 11244, 11515, 11557, 11588, 11796,\n",
      "        11872, 11911, 11968, 12282, 12723, 12782, 12941, 13030, 13084, 13154,\n",
      "        13169, 13178, 13415, 13455, 13562, 13586, 13611, 13733, 13851, 13903,\n",
      "        14001, 14043, 14335, 14532, 14563, 14741, 14794, 14850, 14855, 14996,\n",
      "        15203, 15302, 15360, 15382, 15441, 15470, 15582, 15636, 16031, 16098,\n",
      "        16148, 16392, 16454, 16501, 16600, 16624, 16891, 16918, 17005, 17062,\n",
      "        17067, 17189, 17249, 17323, 17647, 17918, 17986, 18054, 18386, 18411,\n",
      "        18795, 18840, 18895, 18926, 18948, 18958, 19047, 19313, 19332, 19402,\n",
      "        19485, 19637, 19716, 19745, 19890, 20060, 20128, 20199, 20468, 20514,\n",
      "        20662, 20777, 20817, 20966, 21089, 21206, 21239, 21273, 21400, 21457,\n",
      "        21714, 21910, 22043, 22358, 22431, 22553, 22591, 22633, 22968, 23014,\n",
      "        23214, 23230, 23316, 23412, 23647, 23689, 23769, 23815, 23962, 24065,\n",
      "        24545, 24623, 24654, 25275, 25412, 25622, 25716, 25800, 25810, 26093,\n",
      "        26248, 26261, 26282, 26523, 26560, 26752, 26814, 26850, 26944, 27122,\n",
      "        27270, 27331, 27444, 27445, 27488, 27924, 28014, 28079, 28153, 28221,\n",
      "        28393, 28561, 28563, 28566, 28682, 28699, 28918, 29006, 29094, 29116,\n",
      "        29141, 29353, 29405, 29744, 29884, 29888, 30465, 30468, 30565, 30757,\n",
      "        30816, 30921, 30946, 31040, 31103, 31107, 31200, 31360, 31427, 31621,\n",
      "        31713, 32228, 32350, 32409, 32443, 32452, 32699, 32889, 33020, 33032,\n",
      "        33099, 33286, 33409, 33479, 33569, 33727, 33889, 33893, 34051, 34130,\n",
      "        34278, 34439, 34581, 34767, 35030, 35096, 35290, 35341, 35410, 35748,\n",
      "        35805, 35814, 35866, 36116, 36117, 36173, 36241, 36315, 36377, 36492,\n",
      "        36582, 36599, 36665, 36814, 37218, 37994, 38060, 38067, 38089, 38118,\n",
      "        38141, 38356, 38487, 38516, 38645, 38687, 38701, 38852, 39073, 39128,\n",
      "        39177, 39291, 39426, 39480, 39495, 39877, 39931, 39984, 40198, 40237,\n",
      "        40249, 40430, 40580, 40676, 40686, 40693, 40772, 40782, 40817, 40976,\n",
      "        41308, 41414, 41632, 41736, 41966, 42184, 42254, 42334, 42418, 42470,\n",
      "        42798, 42845, 43079, 43130, 43142, 43232, 43298, 43377, 43444, 43481,\n",
      "        43489, 43699, 43805, 43814, 43986, 44156, 44290, 44402, 44580, 44678,\n",
      "        44738, 44992, 45024, 45069, 45260, 45291, 45456, 45569, 45772, 45999,\n",
      "        46161, 46420, 46434, 46577, 46610, 46776, 46876, 46885, 46951, 46954,\n",
      "        47113, 47227, 47254, 47335, 47449, 47629, 47667, 47678, 47697, 47698,\n",
      "        47733, 47845, 47847, 47986, 48013, 48134, 48306, 48307, 48350, 48395,\n",
      "        48442, 48489, 48644, 48664, 48929, 48962, 48999, 49100, 49190, 49272,\n",
      "        49355, 49462, 49630, 49674, 49765, 49792, 49875, 49969, 50095, 50116]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:01,605 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 5/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,793 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   31,   153,   198,   211,   227,   317,   371,   433,   481,   486,\n",
      "          527,   589,   633,   857,   879,   880,   962,  1020,  1025,  1074,\n",
      "         1408,  1422,  1428,  1861,  2019,  2138,  2266,  2280,  2300,  2334,\n",
      "         2379,  2451,  2743,  2897,  2950,  3182,  3383,  3599,  3757,  3770,\n",
      "         4012,  4157,  4162,  4388,  4459,  4491,  4514,  4628,  4858,  4930,\n",
      "         4971,  5020,  5109,  5119,  5134,  5209,  5373,  5578,  5675,  5758,\n",
      "         5929,  6458,  6596,  6657,  6687,  6717,  6760,  6783,  6868,  6950,\n",
      "         6968,  6975,  6983,  7038,  7056,  7111,  7118,  7134,  7324,  7546,\n",
      "         8095,  8125,  8188,  8250,  8277,  8334,  8609,  8684,  8906,  9035,\n",
      "         9145,  9323,  9511,  9621,  9672,  9855,  9901,  9959, 10085, 10180,\n",
      "        10404, 10460, 10804, 10993, 11430, 11530, 11588, 11654, 11717, 12025,\n",
      "        12282, 12307, 12539, 12582, 12587, 12680, 12694, 12889, 13012, 13084,\n",
      "        13168, 13172, 13224, 13448, 13509, 13523, 13562, 13611, 13628, 13822,\n",
      "        13927, 14001, 14041, 14043, 14532, 14554, 14603, 14666, 14725, 14866,\n",
      "        14919, 15073, 15135, 15222, 15472, 15501, 15630, 15860, 15894, 15976,\n",
      "        16084, 16180, 16818, 16880, 16937, 17047, 17164, 17278, 17320, 17842,\n",
      "        17906, 18032, 18237, 18283, 18336, 18831, 18869, 18948, 19148, 19171,\n",
      "        19215, 19337, 19369, 19371, 19559, 19716, 19745, 20326, 20478, 20512,\n",
      "        20518, 20627, 20757, 20785, 20792, 20881, 20900, 21105, 21121, 21184,\n",
      "        21266, 21404, 21420, 21457, 21637, 21800, 21910, 21985, 22136, 22319,\n",
      "        22393, 22568, 22657, 22709, 22954, 23128, 23412, 23628, 24065, 24479,\n",
      "        24798, 24935, 24999, 25100, 25256, 25528, 25532, 25562, 25716, 25810,\n",
      "        25833, 25939, 26146, 26153, 26381, 26413, 26537, 26551, 26645, 26670,\n",
      "        26852, 27115, 27123, 27211, 27496, 27647, 27946, 28011, 28056, 28099,\n",
      "        28121, 28165, 28237, 28388, 28561, 28769, 29116, 29130, 29224, 29292,\n",
      "        29445, 29488, 29558, 29565, 29594, 29657, 29717, 29743, 30025, 30026,\n",
      "        30241, 30301, 30327, 30449, 30621, 30658, 30797, 30812, 30852, 31261,\n",
      "        31264, 31328, 31394, 31519, 31682, 31824, 31862, 31887, 32111, 32371,\n",
      "        32635, 32690, 32717, 32754, 32843, 33174, 33187, 33376, 33549, 33594,\n",
      "        33660, 34236, 34304, 34398, 34575, 34662, 34761, 34805, 34929, 35065,\n",
      "        35166, 35290, 35335, 35417, 35418, 35420, 35466, 35515, 35618, 36241,\n",
      "        36250, 36377, 36492, 36529, 36570, 36673, 36795, 37053, 37198, 37294,\n",
      "        37324, 37342, 37345, 37405, 37464, 37468, 37540, 37691, 37776, 37988,\n",
      "        38064, 38089, 38114, 38128, 38209, 38289, 38336, 38346, 38356, 38376,\n",
      "        38416, 38518, 38687, 38739, 38764, 38777, 39047, 39109, 39119, 39250,\n",
      "        39296, 39384, 39495, 39750, 39801, 39805, 39812, 40009, 40037, 40138,\n",
      "        40568, 40693, 40772, 40795, 40952, 40955, 40982, 41062, 41297, 41374,\n",
      "        41463, 41472, 41511, 41550, 41669, 41776, 41892, 42120, 42199, 42295,\n",
      "        42298, 42457, 42776, 42845, 42928, 42993, 43006, 43022, 43045, 43047,\n",
      "        43173, 43243, 43286, 43289, 43469, 43605, 43689, 43702, 43815, 43826,\n",
      "        44065, 44176, 44245, 44310, 44362, 44427, 44540, 44543, 44598, 44678,\n",
      "        44849, 45002, 45260, 45384, 45423, 45441, 45585, 45605, 45627, 45657,\n",
      "        45802, 45837, 45959, 46241, 46314, 46324, 46367, 46407, 46680, 46952,\n",
      "        47243, 47448, 47629, 47706, 47866, 48013, 48364, 48523, 48622, 48624,\n",
      "        48823, 48880, 48929, 49026, 49178, 49235, 49256, 49486, 49792, 49811,\n",
      "        49963, 49969, 50144, 50166, 50236]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:23,797 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 6/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,895 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,    47,   198,   208,   370,   476,   486,   518,   563,   590,\n",
      "          761,   801,   854,   879,   901,   962,  1005,  1077,  1127,  1264,\n",
      "         1267,  1533,  1563,  1772,  1860,  1880,  1887,  1992,  2055,  2127,\n",
      "         2457,  2660,  2690,  2694,  2832,  2874,  2950,  3196,  3322,  3341,\n",
      "         3355,  3363,  3444,  3510,  3593,  3665,  3770,  3843,  3877,  4223,\n",
      "         4256,  4328,  4333,  4367,  4388,  4491,  4620,  4624,  4641,  4692,\n",
      "         4740,  4831,  5072,  5276,  5320,  5373,  5385,  5399,  5456,  5630,\n",
      "         5905,  6229,  6760,  6783,  6830,  6994,  7027,  7111,  7134,  7256,\n",
      "         7330,  7344,  7553,  7648,  8124,  8555,  8590,  8609,  8648,  8689,\n",
      "         8874,  8915,  9190,  9194,  9323,  9352,  9369,  9450,  9694,  9795,\n",
      "         9855,  9901,  9925, 10403, 10435, 10669, 10691, 10719, 10769, 10858,\n",
      "        10946, 11089, 11143, 11286, 11872, 11885, 12207, 12283, 12490, 12673,\n",
      "        12739, 12955, 13027, 13283, 13346, 13548, 13618, 13635, 13695, 13822,\n",
      "        14013, 14064, 14073, 14280, 14464, 14532, 14643, 14725, 14988, 15222,\n",
      "        15244, 15314, 15559, 15636, 15737, 15794, 15860, 16035, 16081, 16410,\n",
      "        16614, 16716, 16998, 17046, 17067, 17101, 17151, 17191, 17218, 17278,\n",
      "        17575, 17654, 17759, 17829, 17852, 17875, 17883, 17913, 17928, 17986,\n",
      "        18032, 18237, 18307, 18331, 18347, 18731, 18849, 18895, 18948, 19072,\n",
      "        19215, 19274, 19313, 19374, 19384, 19409, 19524, 19548, 19593, 19813,\n",
      "        19916, 20017, 20048, 20060, 20303, 20360, 20393, 20402, 20518, 20627,\n",
      "        20706, 20882, 21103, 21117, 21239, 21345, 21395, 21457, 21496, 21824,\n",
      "        21841, 22182, 22233, 22633, 23194, 23230, 23320, 23347, 24275, 24349,\n",
      "        24431, 24872, 24991, 24999, 25306, 25357, 25468, 25535, 25612, 25677,\n",
      "        25716, 25810, 25904, 25934, 25939, 26008, 26111, 26362, 26405, 26471,\n",
      "        26493, 26500, 26670, 26791, 26890, 26944, 27037, 27331, 27479, 27488,\n",
      "        27496, 27498, 27576, 27775, 27898, 28252, 28264, 28332, 28379, 28489,\n",
      "        28491, 28563, 28682, 28844, 28919, 29094, 29116, 29233, 29313, 29353,\n",
      "        29485, 29488, 29509, 29550, 29678, 29774, 29869, 29894, 29984, 29988,\n",
      "        30000, 30041, 30103, 30274, 30808, 30858, 30946, 31040, 31103, 31433,\n",
      "        31547, 31676, 31686, 31815, 32068, 32266, 32324, 32443, 32588, 32591,\n",
      "        32650, 32860, 32900, 33032, 33187, 33199, 33475, 33827, 34064, 34150,\n",
      "        34171, 34278, 34311, 34507, 34575, 34825, 34937, 34951, 35030, 35220,\n",
      "        35335, 35631, 35709, 35723, 35790, 35814, 35929, 35962, 36082, 36116,\n",
      "        36117, 36255, 36259, 36599, 36673, 36779, 37053, 37344, 37405, 37536,\n",
      "        37568, 37588, 37994, 38128, 38172, 38210, 38253, 38438, 38516, 38585,\n",
      "        38586, 38598, 38839, 38883, 38903, 39027, 39081, 39311, 39348, 39426,\n",
      "        39495, 39622, 39696, 39789, 39826, 39984, 40198, 40388, 40422, 40676,\n",
      "        40693, 40817, 40907, 40911, 40960, 40976, 41435, 41572, 41681, 41863,\n",
      "        41962, 42350, 42550, 42635, 42702, 42784, 42821, 42834, 42919, 42991,\n",
      "        43045, 43173, 43513, 43664, 43699, 43788, 43805, 44105, 44214, 44427,\n",
      "        44738, 44953, 44976, 45024, 45327, 45372, 45585, 45752, 45764, 45849,\n",
      "        46241, 46304, 46678, 47110, 47200, 47264, 47821, 47913, 47936, 47951,\n",
      "        48000, 48013, 48108, 48309, 48479, 48523, 48612, 48664, 48935, 49026,\n",
      "        49045, 49085, 49091, 49105, 49715, 49783, 50156, 50160, 50195, 50232]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:55:46,899 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 7/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,707 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  107,   198,   220,   230,   608,   821,   879,   901,   948,   962,\n",
      "         1108,  1248,  1267,  1668,  1714,  1760,  1858,  1880,  1887,  1928,\n",
      "         1964,  2027,  2152,  2173,  2199,  2288,  2483,  2690,  2804,  2877,\n",
      "         2878,  2894,  2966,  2989,  3101,  3582,  3599,  3708,  3741,  3809,\n",
      "         3829,  4096,  4115,  4549,  4584,  4590,  4746,  4804,  5053,  5369,\n",
      "         5496,  5531,  5559,  5578,  5588,  5675,  5922,  6011,  6175,  6229,\n",
      "         6267,  6363,  6512,  6515,  6619,  6717,  6760,  6783,  6886,  6968,\n",
      "         6975,  6994,  7009,  7056,  7118,  7130,  7251,  7588,  7623,  7948,\n",
      "         8057,  8073,  8193,  8206,  8334,  8354,  8482,  8554,  8609,  8853,\n",
      "         9029,  9145,  9153,  9250,  9323,  9328,  9538,  9642,  9700,  9746,\n",
      "         9747, 10017, 10212, 10289, 10824, 11481, 11584, 11667, 11769, 11811,\n",
      "        11818, 11858, 11886, 12207, 12255, 12402, 12582, 12694, 12716, 12729,\n",
      "        12905, 12913, 13028, 13154, 13201, 13223, 13270, 13355, 13444, 13576,\n",
      "        13611, 13635, 13676, 13733, 13939, 14073, 14243, 14383, 14541, 14606,\n",
      "        14685, 14794, 14842, 14988, 14992, 15023, 15275, 15768, 15826, 15889,\n",
      "        15965, 15989, 16009, 16030, 16131, 16180, 16384, 16504, 16746, 16853,\n",
      "        17053, 17100, 17323, 17535, 17566, 17654, 17687, 17852, 18113, 18142,\n",
      "        18221, 18313, 18671, 18706, 18926, 18948, 19296, 19350, 19365, 19385,\n",
      "        19398, 19485, 19587, 19684, 19875, 19916, 19924, 19988, 20222, 20281,\n",
      "        20326, 20424, 20468, 20517, 20518, 20674, 20751, 20785, 20856, 20881,\n",
      "        21020, 21027, 21121, 21185, 21287, 21420, 21461, 21485, 21710, 21723,\n",
      "        21783, 21835, 22078, 22136, 22393, 22568, 22633, 22879, 22968, 23012,\n",
      "        23013, 23107, 23201, 23278, 23344, 23689, 23893, 24065, 24154, 24398,\n",
      "        24479, 24536, 24550, 24798, 25053, 25085, 25268, 25288, 25412, 25447,\n",
      "        25529, 25672, 25716, 25810, 26034, 26143, 26322, 26346, 26657, 26670,\n",
      "        26727, 26906, 27775, 27924, 28235, 28332, 28444, 28496, 28563, 28682,\n",
      "        28858, 28913, 28993, 29103, 29353, 29572, 29594, 29663, 30103, 30468,\n",
      "        30534, 30558, 30621, 30677, 30682, 30698, 30893, 31011, 31103, 31561,\n",
      "        31623, 31686, 31789, 32123, 32161, 32228, 32443, 32455, 32532, 32614,\n",
      "        32666, 32674, 32731, 33040, 33099, 33112, 33187, 33300, 33403, 33460,\n",
      "        33548, 33563, 33631, 33670, 33736, 33871, 34278, 34311, 34356, 34591,\n",
      "        34694, 34766, 34977, 34979, 35030, 35150, 35220, 35335, 35492, 35541,\n",
      "        35559, 35696, 35723, 35805, 35806, 35939, 36222, 36315, 36377, 36492,\n",
      "        36599, 36673, 36772, 37053, 37116, 37151, 37233, 37282, 37294, 37308,\n",
      "        37436, 37612, 37688, 37717, 37914, 38067, 38118, 38128, 38251, 38385,\n",
      "        38522, 38568, 38591, 38687, 39291, 39327, 39427, 39696, 39700, 39801,\n",
      "        39812, 39824, 39881, 39960, 40033, 40062, 40105, 40325, 40355, 40507,\n",
      "        40538, 40693, 40772, 40803, 40828, 41081, 41111, 41151, 41279, 41454,\n",
      "        41681, 41831, 42204, 42220, 42298, 42413, 42546, 42661, 42747, 42784,\n",
      "        42798, 42882, 43130, 43289, 43292, 43463, 43469, 43489, 43814, 43815,\n",
      "        43895, 43907, 43949, 44049, 44214, 44427, 44463, 44477, 44543, 44738,\n",
      "        44932, 45038, 45159, 45186, 45599, 45653, 45985, 46017, 46101, 46153,\n",
      "        46188, 46347, 46417, 46553, 46579, 46602, 46647, 46685, 46696, 46811,\n",
      "        46969, 47070, 47447, 47534, 47542, 47560, 47629, 47667, 47916, 47933,\n",
      "        48023, 48026, 48032, 48112, 48479, 48715, 48763, 48774, 48933, 48985,\n",
      "        49039, 49301, 49355, 49629, 49744, 49923, 49945, 50008, 50086, 50161]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:07,712 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 8/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,803 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   40,   122,   137,   153,   198,   415,   433,   589,   650,   655,\n",
      "          901,   919,  1025,  1030,  1044,  1074,  1187,  1267,  1418,  1599,\n",
      "         1641,  1714,  1903,  2138,  2300,  2334,  2451,  2470,  2566,  2690,\n",
      "         2890,  2905,  3006,  3478,  3652,  3708,  3741,  3757,  3770,  3793,\n",
      "         3925,  4160,  4430,  4491,  4611,  4730,  4804,  4818,  5482,  5507,\n",
      "         5578,  5675,  5708,  5785,  5809,  5957,  6317,  6456,  6619,  6760,\n",
      "         6907,  6968,  7027,  7056,  7160,  7334,  7369,  7568,  7583,  7623,\n",
      "         7712,  7739,  7753,  8082,  8216,  8248,  8334,  8347,  8832,  9145,\n",
      "         9155,  9167,  9194,  9260,  9323,  9538,  9541,  9627,  9683,  9943,\n",
      "        10060, 10076, 10180, 10282, 10404, 10622, 10719, 10738, 10800, 11286,\n",
      "        11788, 11796, 11872, 11885, 11970, 12046, 12307, 12396, 12476, 12514,\n",
      "        12582, 12641, 12729, 13158, 13172, 13173, 13204, 13346, 13364, 13415,\n",
      "        13542, 13650, 13939, 14001, 14073, 14243, 14268, 14725, 14730, 14842,\n",
      "        14855, 14879, 14988, 15169, 15222, 15226, 15275, 15636, 15737, 16040,\n",
      "        16180, 16457, 16504, 16600, 16631, 16877, 16918, 17098, 17256, 17320,\n",
      "        17323, 17416, 17483, 17558, 17842, 17866, 17949, 17951, 18092, 18375,\n",
      "        18681, 18869, 18948, 18979, 18989, 19044, 19277, 19296, 19645, 19687,\n",
      "        19746, 20060, 20202, 20257, 20303, 20457, 20468, 20478, 20517, 20706,\n",
      "        20741, 20881, 20893, 20998, 21155, 21400, 21472, 21481, 21614, 21636,\n",
      "        21800, 21910, 21998, 22136, 22317, 22442, 22591, 22633, 22657, 23024,\n",
      "        23087, 23102, 23201, 23297, 23347, 23428, 23530, 23553, 23769, 23788,\n",
      "        23852, 23901, 24065, 24361, 24506, 24513, 24550, 25039, 25452, 25492,\n",
      "        25532, 25535, 25683, 25716, 25734, 25755, 25782, 25786, 25810, 25904,\n",
      "        25962, 26008, 26176, 26385, 26670, 26791, 26900, 27136, 27455, 27474,\n",
      "        27488, 27516, 27891, 28068, 28271, 28563, 28682, 28705, 29047, 29116,\n",
      "        29458, 29543, 29546, 29565, 29572, 29663, 29777, 29975, 30009, 30182,\n",
      "        30558, 30757, 30851, 30946, 31040, 31043, 31215, 31365, 31398, 31510,\n",
      "        31571, 31575, 31581, 31822, 31845, 31975, 32129, 32443, 32452, 32514,\n",
      "        32532, 32674, 32776, 32829, 33148, 33161, 33687, 33736, 34064, 34222,\n",
      "        34321, 34339, 34373, 34472, 34561, 34592, 34726, 34761, 34937, 35004,\n",
      "        35007, 35030, 35098, 35130, 35166, 35174, 35284, 35290, 35324, 35335,\n",
      "        35418, 35468, 35814, 35929, 36082, 36116, 36134, 36217, 36220, 36236,\n",
      "        36280, 36305, 36315, 36316, 36372, 36394, 36582, 36769, 36821, 36835,\n",
      "        37318, 37319, 37405, 37495, 37539, 37838, 37887, 37895, 37914, 37988,\n",
      "        38009, 38118, 38121, 38128, 38169, 38336, 38356, 38436, 38481, 38516,\n",
      "        38777, 38798, 38839, 38885, 39067, 39176, 39209, 39427, 39801, 39872,\n",
      "        39895, 39960, 39984, 40009, 40014, 40062, 40118, 40125, 40153, 40330,\n",
      "        40401, 40521, 40599, 40828, 41025, 41135, 41171, 41318, 41414, 41721,\n",
      "        41736, 41744, 41951, 41962, 42120, 42155, 42328, 42503, 42575, 42595,\n",
      "        42598, 42798, 42805, 43045, 43130, 43189, 43251, 43287, 43532, 43895,\n",
      "        44017, 44104, 44176, 44190, 44310, 44362, 44378, 44402, 44752, 44778,\n",
      "        45291, 45336, 45347, 45384, 45441, 45564, 45585, 45627, 45676, 45849,\n",
      "        45938, 46331, 46379, 46608, 46701, 46939, 47165, 47388, 47489, 47614,\n",
      "        47651, 47891, 47916, 48013, 48268, 48420, 48479, 48573, 48622, 48739,\n",
      "        48929, 48952, 48953, 49026, 49076, 49156, 49223, 49299, 49339, 49732,\n",
      "        49923]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450]))\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,809 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 9/10\u001b[0m\n",
      "[ \u001b[36m2024-02-13 13:56:27,812 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH=\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\"\n",
    "\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=\"+CHECKPOINT_PATH,\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 08:44:40,317 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 08:44:40,322 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 08:44:40,326 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test = torch.load(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a shakespeare model from nanoGPT to check if inference script is faulty\n",
    "#### Model had a loss of around 0.7 after training and predicted words that resembled shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_NANOGPT_PATH = \"/home/mabot004/nanoGPT/out-shakespeare/ckpt.pt\"\n",
    "checkpoint = torch.load(CHECKPOINT_NANOGPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_cfg instead of model_args, model_state_dict instead of model\n",
    "#no tokenizer config -> specify in hydra config\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'iter_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miter_num\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'iter_num'"
     ]
    }
   ],
   "source": [
    "checkpoint[\"epoch\"] = checkpoint[\"iter_num\"]\n",
    "checkpoint[\"model_state_dict\"] = checkpoint[\"model\"]\n",
    "checkpoint[\"model_cfg\"] = checkpoint[\"model_args\"]\n",
    "del checkpoint[\"iter_num\"]\n",
    "del checkpoint[\"model\"]\n",
    "del checkpoint[\"model_args\"]\n",
    "checkpoint[\"tokenizer_cfg\"] = {'dtype': 'float32', \n",
    "                               'meta_file': 'meta.pkl', \n",
    "                               'wrapper': 'TikTokenizer', \n",
    "                               'encoding': 'gpt2', \n",
    "                               'module': 'tiktoken', \n",
    "                               'meta': {\n",
    "                                   'max_token_value': 50256, \n",
    "                                   'encoding': 'gpt2', \n",
    "                                   'dtype': 'float32', \n",
    "                                   'num_tokens': 338027, \n",
    "                                   'module': 'tiktoken'\n",
    "                                }\n",
    "                              }\n",
    "checkpoint[\"model_cfg\"] = {\n",
    "    \"cls\": \"GPT\",\n",
    "    \"calc_loss_in_model\": True,\n",
    "    \"args\": {\n",
    "      \"n_layer\" : checkpoint[\"model_cfg\"][\"n_layer\"],\n",
    "      \"n_head\" : checkpoint[\"model_cfg\"][\"n_head\"],\n",
    "      \"n_embd\" : checkpoint[\"model_cfg\"][\"n_embd\"],\n",
    "      \"dropout\" : checkpoint[\"model_cfg\"][\"dropout\"],\n",
    "      \"bias\" :  checkpoint[\"model_cfg\"][\"bias\"],\n",
    "      \"block_size\" : checkpoint[\"model_cfg\"][\"block_size\"],\n",
    "      \"vocab_size\" : checkpoint[\"model_cfg\"][\"vocab_size\"],\n",
    "      \"transformer_active_func\": \"GELU\",\n",
    "      \"norm_layer\": \"LayerNorm\",\n",
    "      \"flash\": False \n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(checkpoint, \"karpathy_shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since karpathy used a larger vocabulary than the tokenizer, some tokens could not be encoded\n",
    "#### Even though karpathy's inference generated good sentences, ours does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 09:17:24,190 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare'}}\n",
      "[ \u001b[36m2024-02-14 09:17:24,383 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,388 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,391 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,395 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,399 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,403 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,408 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,711 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mNo info specified if checkpoint is quantized. Assuming false.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,719 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'cls': 'GPT', 'calc_loss_in_model': True, 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': False, 'block_size': 256, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,721 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,760 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=256, vocab_size=50304, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:24,991 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,010 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,096 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,113 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,202 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,213 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,295 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,313 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,390 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,401 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,423 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,501 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,970 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.88M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,977 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': 'float32',\n",
      " 'encoding': 'gpt2',\n",
      " 'meta': {'dtype': 'float32',\n",
      "          'encoding': 'gpt2',\n",
      "          'max_token_value': 50256,\n",
      "          'module': 'tiktoken',\n",
      "          'num_tokens': 338027},\n",
      " 'meta_file': 'meta.pkl',\n",
      " 'module': 'tiktoken',\n",
      " 'wrapper': 'TikTokenizer'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,981 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,990 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': 'float32', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,993 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenizer config is of type dict. Creating DictConfig object.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:25,999 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': 'float32', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,498 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,523 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating infer dir: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,530 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_09:17:26_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:17:26,534 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:18:54,495 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   113,   154,   198,   201,   394,   496,   702,   760,   763,\n",
      "         1004,  1286,  1512,  1634,  1765,  2001,  2326,  2347,  2418,  2676,\n",
      "         2894,  3497,  3736,  3802,  4071,  4133,  4206,  4262,  4337,  4367,\n",
      "         4368,  4459,  4592,  4633,  4712,  4803,  4994,  5073,  5130,  5210,\n",
      "         5511,  5557,  5632,  5929,  6072,  6491,  6597,  6733,  6772,  6924,\n",
      "         6995,  7358,  7485,  7668,  8022,  8085,  8124,  8248,  8266,  8300,\n",
      "         8346,  8374,  8488,  8512,  8675,  8740,  8772,  8852,  8911,  8923,\n",
      "         9047,  9162,  9171,  9172,  9225,  9670,  9680,  9895,  9938,  9980,\n",
      "        10020, 10175, 10319, 10355, 10391, 10556, 10608, 10628, 10802, 10840,\n",
      "        11002, 11134, 11160, 11168, 11236, 11299, 11340, 11414, 11456, 11588,\n",
      "        11762, 11782, 12137, 12469, 12592, 12702, 12858, 12977, 13102, 13134,\n",
      "        13542, 13553, 13678, 13868, 14166, 14345, 14347, 14396, 14469, 14557,\n",
      "        14658, 14659, 14847, 14872, 15278, 15336, 15521, 15586, 15657, 16276,\n",
      "        16400, 16402, 16525, 16720, 16777, 16864, 16919, 17083, 17084, 17224,\n",
      "        17372, 17426, 17540, 17958, 18069, 18233, 18243, 18293, 18526, 18551,\n",
      "        18686, 18701, 18890, 19126, 19157, 19583, 20096, 20681, 21208, 21340,\n",
      "        21379, 21487, 21493, 21682, 21710, 21729, 22057, 22081, 22198, 22245,\n",
      "        22382, 22415, 22437, 22605, 22651, 22822, 23068, 23098, 23166, 23259,\n",
      "        23498, 23586, 24040, 24111, 24148, 24408, 24434, 24695, 24751, 25159,\n",
      "        25171, 25684, 25788, 25817, 25925, 26128, 26217, 26249, 26540, 26603,\n",
      "        26713, 26769, 26869, 26921, 27290, 27305, 27328, 27681, 27682, 27740,\n",
      "        27850, 27934, 28114, 28436, 28588, 28836, 28876, 28913, 29077, 29456,\n",
      "        29544, 29713, 29806, 29851, 30087, 30271, 30424, 30475, 30584, 30712,\n",
      "        30715, 30967, 31121, 31194, 31223, 31647, 31720, 31852, 31862, 31906,\n",
      "        31945, 32072, 32231, 32325, 32455, 32680, 32752, 32837, 32955, 32956,\n",
      "        33055, 33085, 33135, 33176, 33281, 33316, 33449, 33458, 33464, 33627,\n",
      "        33671, 33782, 33926, 34044, 34686, 34807, 35171, 35440, 35445, 35514,\n",
      "        35548, 35639, 35944, 36152, 36295, 36668, 36680, 36738, 36949, 37048,\n",
      "        37143, 37366, 37555, 37744, 37993, 38148, 38322, 38437, 38501, 38725,\n",
      "        38766, 38963, 39497, 39524, 39591, 39631, 39659, 39707, 39928, 40295,\n",
      "        40332, 40376, 40461, 40477, 40808, 40815, 40976, 41027, 41074, 41166,\n",
      "        41179, 41440, 41559, 41562, 41673, 41845, 42200, 42211, 42394, 42734,\n",
      "        42750, 42808, 42835, 42894, 42962, 43449, 43620, 43878, 43916, 44025,\n",
      "        44040, 44058, 44472, 44500, 44691, 44826, 44913, 45707, 45914, 46066,\n",
      "        46158, 46294, 46310, 46312, 46470, 46856, 46951, 46989, 47112, 47198,\n",
      "        47276, 47343, 47753, 47793, 47968, 48052, 48138, 48459, 48532, 48583,\n",
      "        48679, 48834, 49075, 49159, 49209, 49281, 49794, 49841, 50096, 50186]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:18:54,500 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:20:23,594 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   77,   198,   208,   817,  1051,  1653,  1791,  1997,  2169,  2767,\n",
      "         2807,  3032,  3336,  3350,  3393,  3736,  3997,  4007,  4136,  4331,\n",
      "         4343,  4414,  4459,  4468,  4528,  4615,  4803,  5063,  5216,  5252,\n",
      "         5421,  5540,  5569,  5657,  5929,  6586,  6750,  6839,  6844,  7723,\n",
      "         7761,  7983,  8079,  8124,  8225,  8250,  8374,  8512,  8577,  8610,\n",
      "         8768,  9047,  9159,  9172,  9245,  9387,  9591,  9612,  9985,  9989,\n",
      "        10020, 10022, 10260, 10271, 10319, 10398, 10574, 10827, 10907, 11134,\n",
      "        11216, 11224, 11540, 11551, 11601, 11699, 11976, 12160, 12281, 12376,\n",
      "        12930, 12969, 13076, 13417, 13595, 13641, 13893, 13904, 13964, 13966,\n",
      "        14014, 14102, 14129, 14196, 14250, 14277, 14579, 14850, 14857, 15196,\n",
      "        15232, 15354, 15459, 15464, 15666, 15716, 15948, 15978, 16276, 16429,\n",
      "        16470, 17010, 17199, 17282, 17330, 17464, 17976, 18206, 18220, 18302,\n",
      "        18365, 18485, 18526, 18551, 18731, 18890, 19036, 19192, 19230, 19260,\n",
      "        19316, 19537, 19552, 19583, 19807, 20104, 20148, 20280, 20281, 20300,\n",
      "        20382, 20525, 20637, 20681, 20808, 21002, 21017, 21118, 21229, 21409,\n",
      "        21549, 21578, 21666, 21713, 21831, 22092, 22162, 22172, 22532, 22832,\n",
      "        22887, 23058, 23131, 23178, 23257, 23269, 23586, 23933, 24040, 24785,\n",
      "        25148, 25246, 25336, 25554, 25595, 26119, 26190, 26217, 26364, 26389,\n",
      "        26663, 26687, 27394, 27416, 27653, 27798, 28436, 28466, 28641, 28842,\n",
      "        28876, 28967, 29164, 29170, 29544, 29657, 29802, 29805, 29905, 30055,\n",
      "        30216, 30303, 30390, 30584, 30660, 30712, 30715, 30813, 31223, 31480,\n",
      "        31583, 31705, 31761, 31998, 32114, 32497, 32653, 32752, 32759, 33050,\n",
      "        33224, 33316, 33348, 33352, 33421, 33627, 33724, 33812, 34227, 34538,\n",
      "        34818, 35440, 35578, 35666, 35724, 35753, 35942, 35944, 36077, 36161,\n",
      "        36194, 36492, 36668, 36895, 37378, 37539, 37689, 37707, 37744, 38008,\n",
      "        38029, 38129, 38473, 38559, 38600, 38633, 38703, 38871, 39100, 39241,\n",
      "        39248, 39361, 39428, 39496, 39707, 39752, 39888, 39932, 40200, 40332,\n",
      "        40363, 40371, 40376, 40528, 40587, 40709, 40737, 40747, 40980, 41253,\n",
      "        41312, 41470, 41474, 41825, 42094, 42407, 42437, 42456, 42709, 42757,\n",
      "        43194, 43569, 43848, 43878, 44040, 44115, 44500, 44573, 44636, 44817,\n",
      "        44875, 44903, 44908, 44919, 45312, 45714, 45725, 45952, 46060, 46110,\n",
      "        46123, 46815, 46951, 47002, 47114, 47276, 47299, 47819, 47956, 48176,\n",
      "        48329, 48765, 48834, 48838, 48883, 49182, 49197, 49248, 49530, 49962,\n",
      "        50098, 50108, 50173, 50227]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:20:23,599 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:22:36,013 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   90,   172,   198,   325,   496,   826,   860,  1003,  1150,  1335,\n",
      "         1541,  1595,  1689,  1828,  1938,  1957,  1998,  2018,  2342,  2553,\n",
      "         2844,  2894,  3033,  3336,  3440,  3462,  4007,  4012,  4157,  4390,\n",
      "         4406,  4582,  4667,  4803,  5073,  5075,  5130,  5144,  5289,  5340,\n",
      "         5714,  6076,  6370,  6386,  6431,  6488,  6505,  6625,  6668,  6728,\n",
      "         6744,  6810,  7051,  7252,  7405,  7437,  7559,  7668,  7704,  7723,\n",
      "         7935,  7985,  8079,  8240,  8250,  8374,  8419,  8518,  8610,  8663,\n",
      "         8740,  8809,  9025,  9075,  9298,  9334,  9640,  9789,  9820, 10020,\n",
      "        10227, 10370, 10417, 10669, 10827, 10900, 10907, 10958, 10985, 10991,\n",
      "        11107, 11183, 11333, 11382, 11630, 12264, 12445, 13054, 13104, 13486,\n",
      "        13595, 13620, 13678, 13760, 13833, 13904, 14156, 14307, 14460, 14790,\n",
      "        14795, 14842, 14872, 14931, 15104, 15213, 15315, 15396, 15569, 16174,\n",
      "        16276, 16326, 16598, 16864, 17063, 17226, 17248, 17282, 17409, 17528,\n",
      "        17670, 17808, 17909, 18180, 18266, 18426, 18551, 18688, 18878, 19263,\n",
      "        19292, 19422, 19800, 19915, 19937, 20052, 20255, 20281, 20668, 20681,\n",
      "        21033, 21130, 21156, 21229, 21289, 21487, 21573, 21710, 21986, 21996,\n",
      "        22063, 22198, 22296, 22300, 22370, 22415, 22824, 22990, 23056, 23068,\n",
      "        23184, 23265, 23269, 23529, 23662, 23796, 24111, 24202, 24281, 24527,\n",
      "        24747, 24797, 24829, 24951, 25084, 25273, 25562, 25638, 25712, 25762,\n",
      "        25820, 25983, 26030, 26190, 26217, 26296, 26355, 26384, 26394, 26585,\n",
      "        26712, 26800, 27002, 27249, 27265, 27330, 27407, 27826, 27899, 28012,\n",
      "        28323, 28466, 28641, 28718, 29013, 29015, 29513, 30238, 30316, 30353,\n",
      "        30374, 30474, 30618, 30712, 31223, 31554, 31583, 31669, 31758, 31852,\n",
      "        32236, 32653, 32713, 32752, 33095, 33181, 33264, 33415, 33864, 34044,\n",
      "        34133, 34212, 34459, 35099, 35156, 35305, 35440, 35445, 35666, 35870,\n",
      "        35944, 36152, 36510, 36676, 36680, 36787, 36895, 37048, 37106, 37144,\n",
      "        37201, 37205, 37241, 37490, 37744, 37765, 37948, 38332, 38405, 38533,\n",
      "        38579, 38600, 38630, 38703, 38733, 38772, 38872, 39015, 39059, 39065,\n",
      "        39149, 39241, 39307, 39611, 39707, 39711, 39723, 39734, 39754, 39836,\n",
      "        39978, 40141, 40371, 40668, 40787, 41090, 41463, 41557, 41819, 41845,\n",
      "        42016, 42211, 42479, 42646, 43299, 43302, 43659, 43878, 43920, 43956,\n",
      "        43993, 44025, 44040, 44058, 44590, 44647, 44685, 44857, 44913, 45236,\n",
      "        45480, 45495, 45714, 45971, 45972, 46150, 46167, 46278, 46539, 46588,\n",
      "        46685, 46770, 46814, 46951, 46954, 47169, 47276, 47679, 47777, 47819,\n",
      "        48054, 48075, 48249, 48312, 48432, 49000, 49166, 49181, 49746, 49776,\n",
      "        49787, 49794, 49843, 50098, 50141, 50224]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:22:36,017 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:24:35,904 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   65,   198,   456,   558,   689,   858,   937,  1221,  1485,  1512,\n",
      "         1757,  1938,  1950,  1952,  2406,  2418,  2585,  2767,  3280,  3477,\n",
      "         3707,  3769,  4071,  4331,  4509,  4649,  4769,  4803,  5063,  5073,\n",
      "         5160,  5200,  5235,  5328,  5829,  6214,  6399,  6625,  6694,  7440,\n",
      "         7723,  7890,  7983,  8300,  8456,  8585,  8591,  8931,  9140,  9184,\n",
      "         9492,  9578,  9612,  9652,  9680,  9699,  9799,  9867, 10022, 10238,\n",
      "        10319, 10373, 10398, 10462, 10508, 10574, 11002, 11156, 11224, 11331,\n",
      "        11382, 11540, 11630, 11844, 12257, 12469, 12730, 12964, 13018, 13323,\n",
      "        13333, 13417, 13462, 13654, 13706, 14056, 14307, 14336, 14384, 14404,\n",
      "        14893, 14931, 14995, 15117, 15196, 15509, 15555, 15994, 16065, 16326,\n",
      "        16429, 16470, 16537, 16787, 17010, 17064, 17282, 17311, 17393, 17666,\n",
      "        18069, 18320, 18356, 18451, 18525, 18832, 18890, 19120, 19554, 19670,\n",
      "        19729, 19800, 19905, 20084, 20382, 20495, 20525, 20681, 20842, 21156,\n",
      "        21310, 21548, 21549, 21713, 21729, 21888, 21957, 21970, 21986, 22109,\n",
      "        22242, 22245, 22269, 22296, 22807, 22900, 22960, 22972, 23295, 23395,\n",
      "        23792, 23794, 24065, 24111, 24685, 24694, 24695, 24829, 24850, 24872,\n",
      "        24908, 25076, 25084, 25182, 25231, 25348, 25435, 25595, 25878, 26044,\n",
      "        26047, 26120, 26217, 26384, 26673, 26798, 27233, 27798, 27807, 28124,\n",
      "        28273, 28436, 28466, 28487, 28538, 28647, 28718, 28876, 28967, 29015,\n",
      "        29017, 29209, 29216, 29390, 29669, 29880, 29922, 30250, 30424, 30450,\n",
      "        30712, 30715, 30967, 31198, 31206, 31420, 31480, 31626, 31693, 31705,\n",
      "        31847, 31852, 32353, 32497, 32832, 32925, 33050, 33204, 33224, 33421,\n",
      "        33627, 33728, 33924, 33949, 33950, 34049, 34374, 34545, 34828, 34965,\n",
      "        35196, 35262, 35461, 35667, 35825, 35856, 35905, 36443, 36529, 36540,\n",
      "        36676, 37102, 37479, 37718, 37744, 37792, 37908, 37959, 38012, 38029,\n",
      "        38134, 38388, 38490, 38703, 38733, 38810, 38939, 39058, 39707, 39708,\n",
      "        39711, 39718, 39752, 39767, 39888, 39932, 40011, 40029, 40073, 40281,\n",
      "        40587, 40729, 40840, 40974, 41000, 41074, 41116, 41117, 41162, 41253,\n",
      "        41269, 41303, 41652, 41845, 42094, 42200, 42437, 42528, 43159, 43235,\n",
      "        43302, 43596, 43620, 43669, 43804, 44043, 44353, 44573, 44875, 44908,\n",
      "        45312, 45714, 45736, 45952, 46023, 46222, 46310, 46317, 46466, 46794,\n",
      "        46875, 46931, 46951, 47002, 47006, 47276, 47317, 47733, 47819, 47935,\n",
      "        48120, 48249, 48294, 48472, 48765, 49020, 49183, 49209, 49299, 49315,\n",
      "        49408, 49530, 49811, 49997, 50113, 50179, 50186, 50228]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:24:35,914 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-14 09:26:31,805 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([  198,   201,   208,   304,   394,   554,   558,   630,   689,   991,\n",
      "         1329,  1375,  1503,  1705,  1757,  1788,  1791,  1957,  1962,  1977,\n",
      "         1997,  2079,  2560,  2568,  2742,  2767,  3012,  3118,  3280,  3349,\n",
      "         3350,  3704,  3846,  3997,  4117,  4274,  4421,  4468,  4655,  5063,\n",
      "         5073,  5235,  5340,  5375,  5413,  5479,  5714,  5821,  5929,  6399,\n",
      "         6418,  6582,  6597,  7130,  7301,  7597,  7935,  7983,  8040,  8079,\n",
      "         8188,  8225,  8242,  8374,  8488,  8610,  8675,  8809,  8931,  9047,\n",
      "         9100,  9225,  9325,  9347,  9459,  9569, 10020, 10022, 10054, 10278,\n",
      "        10373, 10398, 10827, 10964, 10991, 11002, 11224, 11700, 11940, 12160,\n",
      "        12257, 12296, 12420, 12534, 12581, 12785, 12900, 12930, 13018, 13036,\n",
      "        13104, 13125, 13221, 13264, 13282, 13417, 13654, 13709, 13904, 13948,\n",
      "        14019, 14062, 14307, 14743, 14829, 14872, 15045, 15078, 15117, 15196,\n",
      "        15341, 15701, 15865, 16288, 16484, 16864, 17171, 17282, 17323, 17444,\n",
      "        17464, 17811, 18266, 18526, 18890, 18946, 19230, 19260, 19352, 19357,\n",
      "        19579, 19607, 19729, 19862, 19983, 20084, 20201, 20396, 20405, 20681,\n",
      "        20797, 20808, 20842, 20972, 20999, 21002, 21033, 21038, 21229, 21825,\n",
      "        21967, 22109, 22269, 22296, 22532, 22627, 22957, 23392, 24031, 24111,\n",
      "        24195, 24451, 24695, 24756, 25244, 25274, 25336, 25389, 25449, 25638,\n",
      "        26030, 26217, 26364, 26384, 26537, 26673, 26713, 27057, 27223, 27225,\n",
      "        27269, 27416, 27807, 27837, 27869, 28085, 28439, 28452, 28641, 28752,\n",
      "        29337, 29496, 29501, 29827, 29841, 29848, 30250, 30320, 30374, 30509,\n",
      "        30584, 30660, 30712, 30774, 30849, 31205, 31480, 31496, 31626, 31907,\n",
      "        32049, 32270, 32357, 32448, 32499, 32653, 32752, 33204, 33263, 33316,\n",
      "        33415, 33458, 33627, 33787, 33860, 33971, 34049, 34109, 34165, 34502,\n",
      "        34519, 34536, 34630, 34785, 34828, 34903, 35077, 35216, 35440, 35445,\n",
      "        35461, 35944, 36194, 36317, 36387, 36540, 36676, 36738, 36787, 36929,\n",
      "        36946, 37042, 37099, 37102, 37191, 37539, 37718, 37792, 38004, 38129,\n",
      "        38362, 38490, 38565, 38796, 38835, 38963, 39052, 39248, 39428, 39675,\n",
      "        39718, 39784, 39928, 40073, 40329, 40339, 40371, 40587, 40877, 40958,\n",
      "        41067, 41166, 41174, 41591, 41683, 41747, 41765, 41870, 42089, 42315,\n",
      "        42479, 42550, 42750, 43123, 43146, 43576, 43582, 43738, 43795, 43834,\n",
      "        44025, 44040, 44550, 44573, 44898, 45114, 45312, 45359, 45714, 45898,\n",
      "        45952, 45972, 46023, 46310, 47002, 47062, 47169, 47276, 47299, 47317,\n",
      "        47365, 47377, 47679, 47968, 48120, 48604, 48765, 48834, 48966, 49009,\n",
      "        49277, 49594, 49752, 49787, 50287]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344]))\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at src/lib.rs:201:64:\n",
      "no entry found for key\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "no entry found for key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=infer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferonnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inferonnx\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:52\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     51\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:230\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(cfg, device)\u001b[0m\n\u001b[1;32m    227\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_new_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mhex\u001b[39m(\u001b[38;5;28mord\u001b[39m(x))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstart]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    229\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- BEGIN INFERENCE -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_infer):\n\u001b[1;32m    231\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:203\u001b[0m, in \u001b[0;36minfer.<locals>.write_inference\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m    200\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniquely generated tokens, sorted in ascending order: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39msort()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#TODO: model could have larger vocabulary size than the tokenizer's max_token_value\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#      for character tokenization, a sequence of <UNKNOWN> chars will be printed. for tiktoken, inference crashes\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/dataset/tokenizer/tiktoken.py:35\u001b[0m, in \u001b[0;36mTikTokenizer.decode\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tiktoken/core.py:254\u001b[0m, in \u001b[0;36mEncoding.decode\u001b[0;34m(self, tokens, errors)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;31mPanicException\u001b[0m: no entry found for key"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/karpathy_shakespeare\",\n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=10\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"device=cuda\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make our checkpoint compatible with karpathy's inference script and see if inference is bettereki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAKESPEARE_QTRANSFORM_PATH = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1\"\n",
    "checkpoint_qtransform = torch.load(SHAKESPEARE_QTRANSFORM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_qtransform[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_qtransform[\"model_args\"] = dict(checkpoint_qtransform[\"model_cfg\"][\"args\"])\n",
    "del checkpoint_qtransform[\"model_args\"][\"transformer_active_func\"]\n",
    "del checkpoint_qtransform[\"model_args\"][\"norm_layer\"]\n",
    "del checkpoint_qtransform[\"model_args\"][\"flash\"]\n",
    "checkpoint_qtransform[\"iter_num\"] = checkpoint_qtransform[\"epoch\"]\n",
    "checkpoint_qtransform[\"optimizer\"] = checkpoint_qtransform[\"optimizer_state_dict\"]\n",
    "checkpoint_qtransform[\"model\"] = checkpoint_qtransform[\"model_state_dict\"]\n",
    "checkpoint_qtransform[\"best_val_loss\"] = checkpoint_qtransform[\"metrics\"]\n",
    "checkpoint_qtransform[\"config\"] = {\n",
    "    'out_dir': 'out-shakespeare',\n",
    "    'eval_interval': 250,\n",
    "     'log_interval': 10,\n",
    "     'eval_iters': 200,\n",
    "     'eval_only': False,\n",
    "     'always_save_checkpoint': False,\n",
    "     'init_from': 'scratch',\n",
    "     'wandb_log': False,\n",
    "     'wandb_project': 'shakespeare',\n",
    "     'wandb_run_name': 'mini-gpt',\n",
    "     'dataset': 'shakespeare',\n",
    "     'gradient_accumulation_steps': 1,\n",
    "     'batch_size': 64,\n",
    "     'block_size': 256,\n",
    "     'n_layer': 2,\n",
    "     'n_head': 2,\n",
    "     'n_embd': 256,\n",
    "     'dropout': 0.0,\n",
    "     'bias': True,\n",
    "     'learning_rate': 0.001,\n",
    "     'max_iters': 5000,\n",
    "     'weight_decay': 0.1,\n",
    "     'beta1': 0.9,\n",
    "     'beta2': 0.99,\n",
    "     'grad_clip': 1.0,\n",
    "     'decay_lr': True,\n",
    "     'warmup_iters': 100,\n",
    "     'lr_decay_iters': 5000,\n",
    "     'min_lr': 0.0001,\n",
    "     'backend': 'nccl',\n",
    "     'device': 'cuda',\n",
    "     'dtype': 'bfloat16',\n",
    "     'compile': True}\n",
    "del checkpoint_qtransform[\"model_state_dict\"]\n",
    "del checkpoint_qtransform[\"optimizer_state_dict\"]\n",
    "del checkpoint_qtransform[\"epoch\"]\n",
    "del checkpoint_qtransform[\"model_cfg\"]\n",
    "del checkpoint_qtransform[\"tokenizer_cfg\"]\n",
    "del checkpoint_qtransform[\"metrics\"]\n",
    "del checkpoint_qtransform[\"quant_cfg\"]\n",
    "del checkpoint_qtransform[\"quantized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_args', 'iter_num', 'optimizer', 'model', 'best_val_loss', 'config'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_qtransform.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(checkpoint_qtransform, \"/home/mabot004/nanoGPT/out-shakespeare/qtransform_shakespeare_karpathy.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despite using karpathy's inference script, our model still generates garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy karpathy's params one by one and see if that fixes the issue, it probably won't though\n",
    "##### Update: it did not. Hopefully that is due to using BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:20:19,566 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 200, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-14 11:20:19,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,795 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,796 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,798 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-14_11:20:19\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,800 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,802 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,803 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,809 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,814 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,816 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,819 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,821 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,823 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,826 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,828 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,829 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,831 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,833 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,835 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 77744, start is 0.23, end is 0.28\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,836 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 77744\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,838 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,841 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,843 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,849 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,851 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,857 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:19,992 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,007 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,023 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,094 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,112 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,121 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,206 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,215 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,302 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,312 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,327 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,400 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,823 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,849 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,852 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'weight_decay', 'betas', 'lr'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:20,859 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:21,082 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.098453426361084\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:21,373 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.431071472167968\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:21,658 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.451612043380737\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:21,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.204511833190918\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:22,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 6.091616296768189\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:22,457 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 6.066249370574951\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:22,725 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 6.006870222091675\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:22,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 5.95970344543457\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:23,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 5.920333957672119\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:23,478 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.814046096801758\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:23,762 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.71551022529602\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:24,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.592127799987793\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:24,331 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 5.392239999771118\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:24,609 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.254548168182373\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:24,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.18837080001831\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:25,183 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.043006324768067\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:25,470 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.015917539596558\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:25,758 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.792355966567993\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:26,042 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.5253876686096195\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:26,309 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.087005972862244\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:26,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.5460363388061524\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:26,844 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.1062373161315917\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:27,131 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.6856496572494506\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:27,423 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.3114134550094603\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:27,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 1.9779144525527954\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:20:27,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 1.7432068705558776\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:29,346 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.1661949038505554\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:29,627 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.569076132774353\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:29,910 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.5487379908561707\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:30,190 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.4718637585639953\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:30,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.4926554679870605\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:30,760 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.456683349609375\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:31,052 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.4017399668693542\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:31,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.3604660391807557\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:31,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.365690493583679\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:31,907 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.3608040690422059\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:32,195 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.3095009326934814\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:32,482 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.2451961278915404\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:32,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.2445067524909974\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:33,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.1863411664962769\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:33,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.1614354610443116\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:33,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.135852038860321\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:33,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.136926805973053\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:34,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.086854875087738\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:34,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.0708426117897034\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:34,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.0205255627632142\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:34,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.0045159876346588\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:35,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.9732412934303284\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:35,462 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.9637054800987244\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:35,707 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.9300277054309845\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:35,952 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.885767149925232\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:21:36,211 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.8896452724933624\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:37,071 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/200: 0.8345377445220947\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:37,074 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.8896452724933624\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:37,570 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:37,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:37,770 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.08778286576271058\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:38,026 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.8620264589786529\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:38,273 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.8699794232845306\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:38,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.8668958425521851\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:38,767 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.8512939155101776\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:39,014 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.8459382116794586\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:39,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.861956924200058\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:39,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.8585825800895691\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:39,835 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.8452194809913636\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:40,118 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.823996651172638\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:40,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.8269696891307831\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:40,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.8148660123348236\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:40,932 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.824990963935852\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:41,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.8145950555801391\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:41,475 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.8357496798038483\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:41,767 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.8314695715904236\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:42,056 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.8223270654678345\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:42,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.8237864315509796\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:42,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.8163373708724976\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:42,909 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.8215369939804077\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:43,184 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.80654057264328\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:43,469 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.8052502274513245\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:43,751 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.7931360721588134\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:44,013 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.7952046990394592\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:44,287 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.7926875293254853\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:22:44,567 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.8018427610397338\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:44,865 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/200: 0.7425030469894409\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:44,868 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.8018427610397338\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:45,444 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:45,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:45,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.07884328365325928\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:45,950 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.7855452001094818\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:46,240 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.7943936109542846\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:46,499 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.7866355538368225\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:46,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.7995205879211426\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:47,063 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.79046750664711\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:47,352 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7968900024890899\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:47,635 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.7937025368213654\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:47,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.7539595544338227\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:48,162 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.7929368913173676\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:48,445 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7689240097999572\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:48,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.8027533650398254\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:49,014 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.7964541375637054\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:49,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.793471896648407\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:49,579 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.79183109998703\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:49,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7963714361190796\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:50,147 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7755733251571655\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:50,429 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.8017605602741241\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:50,719 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.8053416967391968\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:51,001 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7727034687995911\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:51,252 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.8002297520637512\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:51,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.8090357840061188\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:51,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.7936515927314758\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:51,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.7820655703544617\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:52,268 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.7785438179969788\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:23:52,546 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.7874517560005188\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:51,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/200: 0.7352635264396667\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:51,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7874517560005188\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:52,362 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:52,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:52,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.08482912182807922\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:52,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.7837585210800171\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:53,134 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.8106225907802582\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:53,414 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.7936228036880493\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:53,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.792316871881485\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:53,950 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.7707329392433167\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:54,197 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7730776607990265\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:54,479 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.7998674154281616\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:54,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.8203842520713807\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:55,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.788605272769928\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:55,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7797812938690185\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:55,517 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7643006265163421\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:55,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.767253452539444\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:56,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.7745423018932343\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:56,361 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.7801180243492126\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:56,642 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7798244059085846\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:56,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7853838264942169\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:57,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.7658005714416504\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:57,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.7759013175964355\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:57,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7877790272235871\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:58,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.8027339518070221\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:58,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.8112851083278656\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:58,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.7710364043712616\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:58,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.7938832581043244\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:59,090 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.7921382308006286\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:24:59,367 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.7770238518714905\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:01,209 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/200: 0.739122211933136\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:01,212 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7770238518714905\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:01,737 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:01,739 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:01,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.07944759130477905\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:02,223 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.7941636145114899\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:02,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.7613398551940918\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:02,788 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.8000483572483063\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:03,070 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.7798070669174194\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:03,357 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.8137636601924896\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:03,641 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7835375010967255\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:03,938 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.7923454821109772\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:04,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.8048782646656036\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:04,513 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.782671046257019\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:04,796 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7830665469169616\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:05,081 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7780338764190674\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:05,369 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.7762489080429077\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:05,652 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.8057691991329193\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:05,912 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.7894391059875489\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:06,174 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.8027321457862854\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:06,465 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7679335415363312\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:06,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.7861329555511475\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:07,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.7850863873958588\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:07,325 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.802662992477417\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:07,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.765050607919693\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:07,904 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.7691540122032166\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:08,158 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.7935426592826843\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:08,416 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.8155919492244721\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:08,701 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.8045679926872253\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:26:08,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.7890620052814483\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:10,913 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 6/200: 0.7352938652038574\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:10,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7890620052814483\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:11,512 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:11,514 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 7/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:11,724 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.08433324098587036\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:12,015 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.7678649008274079\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:12,301 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.7640969812870025\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:12,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.8080681681632995\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:12,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.7851514518260956\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:13,153 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.7867571234703064\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:13,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7991528272628784\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:13,725 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.7699081361293793\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:13,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.7807905554771424\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:14,229 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.7888868272304534\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:14,474 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7747383415699005\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:14,721 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7515770077705384\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:14,966 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.7669803142547608\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:15,211 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.7908602714538574\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:15,455 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.7636851489543914\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:15,710 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7661406219005584\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:15,991 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7707150638103485\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:16,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.7960928678512573\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:16,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.8060112476348877\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:16,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7875767171382904\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:17,123 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.7675362825393677\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:17,407 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.8036145210266114\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:17,691 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.7975046038627625\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:17,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.7757655501365661\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:18,256 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.7764021217823028\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:27:18,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.7774224162101746\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f5f126b1750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 49086) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 47\u001b[0m\n\u001b[1;32m     24\u001b[0m beta2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m \u001b[38;5;66;03m# make a bit bigger because number of tokens per iter is small\u001b[39;00m\n\u001b[1;32m     26\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeBN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:191\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     losses, mean \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAVERAGE EVAL LOSS FOR EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    193\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;28mstr\u001b[39m(metrics))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:266\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(cfg, device, model, evaldata)\u001b[0m\n\u001b[1;32m    264\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_iters:\n\u001b[0;32m--> 266\u001b[0m     vdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevaldata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     vinputs, vlabels \u001b[38;5;241m=\u001b[39m vdata\n\u001b[1;32m    268\u001b[0m     vinputs \u001b[38;5;241m=\u001b[39m vinputs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice_singleton\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 49086) exited unexpectedly"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Socket.__del__ at 0x7f5f12bf0ee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 178, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py\n",
    "#karpathy evaluates after 250 iterations, we implemented eval to do so after every epoch -> max_iters = 5000 / 200\n",
    "eval_epoch_interval = 1 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "max_iters = 250\n",
    "epochs = 200 #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = 1 #one large batch, potentially do gradient_accumulation_steps = 8 and batch_size = 8\n",
    "batch_size = 64\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "# baby GPT model :)\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "\n",
    "#not implemented currently\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "\n",
    "#not used currently\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if custom_ln layers had their params back propagated\n",
    "import re\n",
    "ckpt_shakespeare = torch.load(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\")\n",
    "#custom_ln are identity layers in this case\n",
    "list(filter(lambda x: re.search(r'custom_ln[1-2]', x), ckpt_shakespeare[\"model_state_dict\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:29:31,267 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6'}}\n",
      "[ \u001b[36m2024-02-14 11:29:31,458 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,461 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,462 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,463 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,465 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,466 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,469 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,804 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,807 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,815 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,947 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,962 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:31,993 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,008 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,090 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,110 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,209 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,223 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,311 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,323 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,390 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,410 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,906 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,909 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,911 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,916 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,917 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,920 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,938 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_11:29:32_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:29:32,940 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:30:31,301 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   16,   198,   218,   221,   261,   317,   546,   699,   844,   868,\n",
      "         1148,  1179,  1217,  1291,  1477,  1746,  1821,  1912,  2049,  2163,\n",
      "         2168,  2227,  2293,  2409,  2694,  2770,  2774,  2869,  2914,  2959,\n",
      "         3006,  3060,  3072,  3109,  3236,  3315,  3448,  3596,  3645,  3653,\n",
      "         3723,  3978,  4053,  4057,  4206,  4263,  4323,  4453,  4947,  5050,\n",
      "         5095,  5123,  5161,  5309,  5571,  5596,  5672,  5788,  5813,  5908,\n",
      "         6109,  6256,  6291,  6298,  6491,  6640,  6641,  6708,  6718,  6824,\n",
      "         6844,  7008,  7108,  7181,  7232,  7275,  7495,  7530,  7660,  7681,\n",
      "         7842,  7943,  7989,  8335,  8510,  8705,  8730,  8738,  8804,  8887,\n",
      "         8896,  8936,  9122,  9230,  9235,  9531,  9562,  9627,  9640,  9728,\n",
      "         9928, 10030, 10087, 10112, 10202, 10441, 10547, 10661, 10718, 10914,\n",
      "        11065, 11163, 11487, 11694, 11745, 11761, 11778, 11866, 11869, 11916,\n",
      "        12172, 12245, 12433, 12448, 12451, 12611, 12772, 12798, 12878, 12879,\n",
      "        12901, 13022, 13379, 13490, 13519, 13680, 13829, 13861, 13891, 14054,\n",
      "        14696, 14752, 14823, 15034, 15151, 15218, 15401, 15426, 15619, 15947,\n",
      "        15975, 16221, 16242, 16600, 16748, 17060, 17083, 17097, 17280, 17337,\n",
      "        17346, 17443, 17553, 17886, 18126, 18186, 18268, 18311, 18418, 18606,\n",
      "        19076, 19254, 19341, 19414, 19551, 19554, 19598, 19693, 19697, 19755,\n",
      "        19952, 19954, 19989, 20138, 20184, 20372, 20397, 20476, 20807, 20860,\n",
      "        20903, 20914, 20971, 21004, 21193, 21213, 21230, 21537, 21546, 21587,\n",
      "        21593, 21653, 21895, 21897, 21914, 21944, 22359, 22363, 22473, 22520,\n",
      "        22649, 22660, 22907, 22978, 23014, 23105, 23252, 23274, 23691, 23768,\n",
      "        23807, 23855, 24009, 24570, 24646, 24738, 24773, 24931, 25407, 25835,\n",
      "        25901, 25942, 25956, 26009, 26052, 26519, 26664, 26740, 26847, 26981,\n",
      "        27017, 27034, 27269, 27614, 27994, 28121, 28207, 28217, 28295, 28308,\n",
      "        28409, 28414, 28449, 28456, 28502, 29015, 29208, 29333, 29585, 29733,\n",
      "        29790, 29953, 30055, 30091, 30201, 30223, 30307, 30522, 30648, 30680,\n",
      "        30746, 31084, 31118, 31324, 31484, 31485, 31536, 31675, 31813, 32052,\n",
      "        32090, 32289, 32510, 32543, 32897, 32931, 32953, 32967, 33044, 33648,\n",
      "        33755, 33764, 33809, 33944, 34020, 34152, 34335, 34529, 34624, 34685,\n",
      "        34762, 34798, 34863, 34916, 35044, 35053, 35148, 35157, 35338, 35627,\n",
      "        35736, 35966, 36135, 36163, 36307, 36562, 36741, 36786, 37109, 37187,\n",
      "        37195, 37208, 37383, 37414, 37451, 37542, 37740, 37756, 37866, 37882,\n",
      "        37941, 37965, 38062, 38090, 38110, 38266, 38434, 38513, 38557, 38585,\n",
      "        38697, 38781, 38840, 39061, 39190, 39239, 39334, 39346, 39608, 39626,\n",
      "        39708, 39971, 39986, 40068, 40339, 40372, 40493, 40587, 40620, 40725,\n",
      "        40924, 41010, 41099, 41190, 41207, 41226, 41394, 41395, 41767, 41864,\n",
      "        42508, 42551, 42687, 42725, 42743, 42785, 43042, 43095, 43410, 43556,\n",
      "        43580, 43589, 43642, 43657, 43788, 43859, 43876, 44233, 44309, 44489,\n",
      "        44610, 44644, 44675, 44716, 44795, 44831, 44850, 44861, 44980, 45117,\n",
      "        45247, 45303, 45364, 45395, 45412, 45415, 45440, 45526, 45593, 45656,\n",
      "        45743, 45776, 45826, 46036, 46418, 46425, 46439, 46552, 46567, 46651,\n",
      "        46799, 46924, 46951, 47264, 47322, 47349, 47419, 47549, 47654, 47713,\n",
      "        47859, 47892, 48105, 48134, 48420, 48447, 48483, 48640, 48657, 48693,\n",
      "        48850, 48922, 49034, 49070, 49126, 49149, 49238, 49260, 49345, 49483,\n",
      "        49688, 49693, 49734, 49742, 49802, 49815]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:30:31,306 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:31:30,300 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   55,   116,   143,   152,   198,   221,   245,   317,   386,   435,\n",
      "          501,   562,   679,   724,   764,  1108,  1119,  1148,  1157,  1337,\n",
      "         1394,  1436,  1477,  1527,  1592,  1668,  1673,  2065,  2317,  2349,\n",
      "         2355,  2357,  2422,  2640,  2646,  2955,  3009,  3089,  3448,  3975,\n",
      "         3988,  4053,  4218,  4412,  4438,  4475,  4503,  4685,  4686,  5123,\n",
      "         5126,  5145,  5161,  5409,  5795,  5840,  6173,  6291,  6335,  6564,\n",
      "         6640,  6724,  6728,  6825,  6866,  6946,  7120,  7227,  7232,  7289,\n",
      "         7675,  7772,  7953,  8114,  8172,  8799,  9122,  9135,  9485,  9532,\n",
      "         9849, 10087, 10100, 10119, 10127, 10165, 10209, 10389, 10407, 10570,\n",
      "        10718, 10896, 10990, 11058, 11303, 11486, 11488, 11504, 11745, 12066,\n",
      "        12075, 12172, 12191, 12207, 12233, 12237, 12382, 12390, 12629, 12680,\n",
      "        12750, 12879, 12984, 13144, 13223, 13552, 13809, 13844, 13906, 14019,\n",
      "        14142, 14506, 14556, 14605, 14688, 14740, 14752, 14757, 14873, 15061,\n",
      "        15063, 15083, 15151, 15229, 15236, 15260, 15458, 15688, 15803, 15812,\n",
      "        15841, 15947, 15974, 16176, 16292, 16308, 16335, 16388, 16436, 16526,\n",
      "        16711, 16900, 17105, 17236, 17292, 17443, 17594, 17726, 17730, 17882,\n",
      "        17886, 17985, 17997, 18070, 18239, 18300, 18311, 18351, 18584, 18850,\n",
      "        19096, 19170, 19178, 19245, 19406, 19508, 19570, 19693, 19697, 19715,\n",
      "        19734, 19755, 19794, 20105, 20115, 20276, 20382, 20483, 20903, 20914,\n",
      "        20957, 21004, 21124, 21213, 21245, 21357, 21373, 21459, 21593, 21652,\n",
      "        21706, 21798, 21927, 22246, 22534, 22681, 22907, 23019, 23105, 23234,\n",
      "        23274, 23943, 24026, 24148, 24273, 24463, 24493, 24611, 24617, 24931,\n",
      "        25199, 25288, 25311, 25371, 25511, 25901, 25920, 26130, 26264, 26376,\n",
      "        26494, 26814, 26816, 26878, 27004, 27080, 27246, 27281, 27329, 27593,\n",
      "        27643, 27743, 27927, 28042, 28049, 28112, 28193, 28217, 28229, 28240,\n",
      "        28265, 28281, 28292, 28303, 28311, 28414, 28456, 28524, 28601, 28878,\n",
      "        29012, 29237, 29291, 29304, 29305, 29400, 29615, 29617, 29718, 29731,\n",
      "        30131, 30156, 30246, 30275, 30333, 30502, 30518, 30526, 30560, 30565,\n",
      "        30637, 30729, 30919, 31167, 31360, 31382, 31484, 31485, 31542, 31558,\n",
      "        31572, 31675, 31769, 31773, 31848, 31993, 32090, 32162, 32198, 32302,\n",
      "        32419, 32814, 32939, 33035, 33308, 33617, 33648, 33881, 33939, 34032,\n",
      "        34078, 34094, 34159, 34198, 34462, 34605, 34668, 34725, 34727, 34788,\n",
      "        34804, 34872, 34898, 35018, 35163, 35215, 35511, 35685, 35693, 35717,\n",
      "        35865, 35873, 36354, 36562, 36786, 36828, 37093, 37168, 37383, 37411,\n",
      "        37504, 37509, 37778, 37866, 37978, 38023, 38132, 38355, 38378, 38390,\n",
      "        38434, 38589, 38689, 38695, 38781, 38947, 39204, 39300, 39334, 39456,\n",
      "        39564, 39814, 39858, 39968, 39993, 40326, 40582, 40587, 40604, 40726,\n",
      "        40769, 40803, 40953, 40956, 40973, 40995, 41059, 41310, 41389, 41395,\n",
      "        41413, 41450, 41782, 42224, 42257, 42321, 42350, 42618, 42718, 42789,\n",
      "        43099, 43142, 43171, 43426, 43456, 43478, 43595, 43691, 43702, 43856,\n",
      "        43945, 44138, 44184, 44357, 44488, 44506, 44543, 44930, 44964, 45127,\n",
      "        45197, 45236, 45684, 45859, 45869, 45874, 46079, 46138, 46206, 46222,\n",
      "        46438, 46865, 46929, 47091, 47176, 47322, 47328, 47493, 47827, 47859,\n",
      "        47893, 47903, 47974, 48000, 48068, 48507, 48614, 48640, 48734, 48788,\n",
      "        48922, 48989, 49070, 49131, 49152, 49238, 49260, 49351, 49378, 49436,\n",
      "        49655, 49662, 49731, 49734, 50183, 50190]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:31:30,303 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:32:24,998 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   42,   116,   198,   221,   386,   457,   685,   699,   772,   787,\n",
      "          844,   851,  1051,  1148,  1158,  1266,  1494,  1769,  2129,  2163,\n",
      "         2209,  2211,  2235,  2357,  2409,  2411,  2580,  2675,  2807,  2962,\n",
      "         2990,  3165,  3286,  3294,  3423,  3510,  3856,  3947,  4018,  4026,\n",
      "         4053,  4169,  4440,  4669,  4693,  4744,  4829,  4869,  5049,  5145,\n",
      "         5161,  5203,  5300,  5409,  5418,  5444,  5549,  5870,  5911,  5987,\n",
      "         6245,  6279,  6291,  6640,  6713,  6900,  6942,  7067,  7154,  7213,\n",
      "         7232,  7275,  7456,  7495,  7660,  7681,  8195,  8204,  8317,  8428,\n",
      "         8487,  8676,  8755,  9122,  9295,  9422,  9433,  9618,  9699,  9742,\n",
      "        10000, 10100, 10554, 10640, 10759, 11064, 11486, 11488, 11625, 11648,\n",
      "        11712, 11745, 11761, 11803, 12036, 12075, 12172, 12390, 12395, 12734,\n",
      "        12753, 12818, 12891, 13400, 13565, 13730, 13861, 14038, 14092, 14109,\n",
      "        14252, 14297, 14556, 14602, 14712, 14747, 14752, 14785, 14826, 14862,\n",
      "        15006, 15151, 15426, 15455, 15630, 15947, 16174, 16392, 16489, 16657,\n",
      "        16685, 16689, 16825, 16889, 16900, 16908, 17010, 17047, 17193, 17245,\n",
      "        17474, 17501, 17860, 18107, 18113, 18256, 18275, 18300, 18351, 18584,\n",
      "        18606, 18685, 18827, 18985, 19067, 19100, 19341, 19343, 19414, 19425,\n",
      "        19472, 19598, 19693, 19751, 19903, 20014, 20058, 20184, 20377, 20412,\n",
      "        20458, 20494, 20539, 20655, 20729, 21004, 21045, 21180, 21213, 21373,\n",
      "        21478, 21537, 21653, 21720, 21744, 21770, 22163, 22176, 22191, 22311,\n",
      "        22473, 22546, 22569, 22916, 22978, 23105, 23110, 23132, 23274, 23383,\n",
      "        23423, 23539, 23897, 23929, 23952, 23955, 24034, 24185, 24245, 24322,\n",
      "        24342, 24390, 24646, 24759, 24931, 25031, 25136, 25444, 25497, 25511,\n",
      "        25558, 25890, 25956, 26264, 26360, 26866, 26871, 26982, 27254, 27269,\n",
      "        27329, 27345, 27667, 27736, 27915, 27959, 28049, 28072, 28112, 28121,\n",
      "        28126, 28193, 28292, 28293, 28308, 28341, 28502, 28624, 28878, 29218,\n",
      "        29332, 29418, 29531, 29624, 29658, 29713, 29733, 29992, 30031, 30159,\n",
      "        30197, 30314, 30319, 30486, 30488, 30585, 30606, 30640, 30816, 31167,\n",
      "        31456, 31635, 31675, 31774, 31848, 32112, 32216, 32245, 32419, 32423,\n",
      "        32601, 32604, 32657, 32677, 32695, 32931, 32933, 32939, 33044, 33102,\n",
      "        33296, 33881, 33977, 34008, 34078, 34189, 34234, 34272, 34303, 34316,\n",
      "        34748, 34799, 34932, 35338, 35437, 35471, 35533, 35683, 35700, 35757,\n",
      "        35863, 35865, 36181, 36218, 36387, 36562, 36701, 36891, 37109, 37201,\n",
      "        37329, 37385, 37520, 37778, 38132, 38190, 38216, 38242, 38458, 38557,\n",
      "        38685, 38689, 38750, 38787, 39057, 39297, 39970, 39986, 40007, 40332,\n",
      "        40339, 40720, 40769, 41017, 41052, 41062, 41090, 41129, 41267, 41294,\n",
      "        41395, 41425, 41532, 41535, 41850, 41988, 42077, 42083, 42350, 42417,\n",
      "        42566, 42633, 42650, 42687, 42952, 43021, 43251, 43357, 43410, 43459,\n",
      "        43478, 43556, 43652, 44041, 44233, 44274, 44275, 44386, 44657, 44675,\n",
      "        44708, 44850, 44964, 45021, 45062, 45380, 45440, 45494, 45610, 45883,\n",
      "        46055, 46066, 46370, 46415, 46455, 46505, 46550, 46688, 46799, 46881,\n",
      "        46929, 46939, 46951, 47010, 47011, 47097, 47207, 47225, 47407, 47417,\n",
      "        47549, 47562, 47824, 47892, 47941, 47969, 48111, 48250, 48360, 48436,\n",
      "        48507, 48563, 48606, 48640, 48733, 48734, 48856, 48887, 48922, 49069,\n",
      "        49105, 49260, 49278, 49393, 49397, 49410, 49441, 49479, 49574, 49610,\n",
      "        49662, 49792, 50132, 50183, 50190]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:32:25,002 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:32:25,003 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_11:29:32_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#run inference again, this time with karpathy's params\n",
    "#generated file: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/out_infer/INFER_2024-02-14_11:29:32_CHECKPOINT.out\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LayerNorm and check if inference is going better\n",
    "#### The loss after the first epoch is way higher than with batchnorm (4.5 with layernorm compared to 1.7 with batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:45:55,161 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 200, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-14 11:45:55,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,377 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,379 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-14_11:45:55\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,380 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,381 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,382 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,387 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,393 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,394 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,397 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,398 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,400 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,402 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,404 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,405 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,407 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,409 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,410 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 77744, start is 0.23, end is 0.28\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,411 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 77744\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,412 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 94647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,415 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,417 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 64, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,423 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,424 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,429 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,570 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,592 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,611 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,620 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,714 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,794 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,817 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,890 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,918 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:55,991 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,015 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,097 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,538 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,559 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,562 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,565 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'weight_decay', 'betas', 'lr'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,567 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,571 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:56,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.098379898071289\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.4451171875\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.493041849136352\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.343642950057983\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:57,955 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 6.271537733078003\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 6.260971879959106\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 6.223812294006348\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:58,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 6.129550313949585\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,083 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 5.998541307449341\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.842705154418946\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.726846313476562\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:45:59,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.6491515159606935\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,206 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 5.490401029586792\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,479 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 5.358382272720337\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:00,757 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 5.272852039337158\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,035 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.118131446838379\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,314 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.096799993515015\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.945697975158692\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:01,867 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.849489974975586\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,149 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.819471025466919\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.774138307571411\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.759201622009277\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:02,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.679498815536499\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,259 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.632328081130981\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,539 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.531281566619873\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:46:03,811 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.531898069381714\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:02,782 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/200: 4.566741943359375\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:02,784 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.531898069381714\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,356 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,568 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.45096392631530763\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:03,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.449897623062133\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.47121376991272\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.357682180404663\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.440573263168335\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:04,966 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.40941891670227\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.347307109832764\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.344051218032837\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:05,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.370062971115113\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,077 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.4027611255645756\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.366365909576416\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,632 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.328843879699707\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:06,915 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.340453290939331\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,193 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.270555067062378\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,470 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.294016790390015\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:07,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.271436929702759\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.313546085357666\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.2667933940887455\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,581 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.268800735473633\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:08,866 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.229130458831787\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,147 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.261798858642578\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,427 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.209460020065308\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,703 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.269817972183228\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:09,981 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.221245336532593\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:10,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.170434999465942\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:47:10,539 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.249877405166626\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:12,575 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/200: 4.283267974853516\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:12,578 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.249877405166626\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,164 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,166 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.41936407089233396\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.1783356189727785\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:13,959 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.227576971054077\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,238 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.184141683578491\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.19386739730835\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:14,790 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.15916690826416\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,067 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.233234643936157\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,360 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.212756824493408\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,638 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.195131778717041\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:15,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.193507337570191\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.17202582359314\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,477 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.129647874832154\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:16,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.204125595092774\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,034 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.137046575546265\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.209381437301635\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.1829833984375\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:17,873 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.190350818634033\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.191827774047852\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.201117897033692\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.1918213844299315\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:18,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.1793681383132935\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,160 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.164639210700988\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.1492725849151615\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,718 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.193277215957641\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:19,993 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.156240749359131\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:48:20,269 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.184376573562622\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/200: 4.248761177062988\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.184376573562622\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,939 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:18,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,146 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.41558589935302737\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,435 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.14961838722229\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.181100654602051\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:19,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.149146556854248\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.20280351638794\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.186082553863526\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:20,825 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.152220773696899\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.156947660446167\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,383 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.098524475097657\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,659 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.16286678314209\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:21,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.129656267166138\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,210 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.194824528694153\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.150462031364441\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:22,753 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.187245082855225\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,027 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.134180188179016\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,307 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.180058479309082\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,587 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.152106857299804\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:23,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.163632011413574\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,142 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.188905572891235\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.136766052246093\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.1829733610153195\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:24,971 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.183438682556153\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.165262365341187\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,531 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.162627553939819\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:25,806 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 4.150086116790772\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:49:26,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 4.154836225509643\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5dc88ad3f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5dc88ad3f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:50:25,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/200: 4.248138904571533\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.154836225509643\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,648 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/200\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:25,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 0.42351655960083007\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.157746601104736\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 4.187510585784912\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.150379395484924\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:26,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.159930467605591\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.140870571136475\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,562 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.117836356163025\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:27,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.203375673294067\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.2219672203063965\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,381 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.129686570167541\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:50:28,662 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.113000202178955\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_GeLN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[0;32m---> 22\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:109\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    106\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    112\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs_to_run:\n\u001b[1;32m    185\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:226\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    224\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss_in_model:\n\u001b[0;32m--> 226\u001b[0m     outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:141\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:264\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:192\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!HELP ME\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# this if block is needed for toprch <2.21 where flash attention onnx export does not work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m#QuantMultiheadAttention does not have is_causal in constructor -> use attention mask instead\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5187\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5188\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[1;32m   4763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[1;32m   4764\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[0;32m-> 4765\u001b[0m         proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4766\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4767\u001b[0m         proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeLN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"run.export=True\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 11:51:24,747 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'test': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'out_dir': 'None', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4'}}\n",
      "[ \u001b[36m2024-02-14 11:51:24,939 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,941 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,943 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,945 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,946 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cpu. Using device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,948 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cpu\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:24,950 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,251 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,253 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,260 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,402 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,498 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,535 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,606 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,623 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,706 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,723 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,802 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,820 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,890 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,916 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:25,925 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,429 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,433 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,434 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,440 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,442 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,444 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,463 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating infer dir: /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,469 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None/INFER_2024-02-14_11:51:26_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:51:26,471 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:52:25,807 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   45,    62,    98,   198,   220,   252,   309,   595,   960,  1016,\n",
      "         1132,  1247,  1438,  1786,  1879,  2118,  2122,  2260,  2346,  2497,\n",
      "         2630,  2990,  2994,  3109,  3373,  3536,  3579,  3582,  3588,  3833,\n",
      "         3862,  3906,  4021,  4120,  4357,  4696,  4873,  5210,  5274,  5558,\n",
      "         5773,  5899,  5949,  6047,  6129,  6231,  6270,  6329,  6337,  6375,\n",
      "         6383,  6441,  6475,  6557,  6576,  6743,  6822,  7145,  7169,  7180,\n",
      "         7260,  7391,  7434,  7502,  8214,  8284,  8767,  8830,  8860,  9087,\n",
      "         9216,  9243,  9299,  9569,  9586,  9700,  9904, 10086, 10329, 10363,\n",
      "        10494, 10565, 10627, 10840, 10880, 10900, 10949, 10969, 11164, 11247,\n",
      "        11263, 11290, 11719, 11778, 11962, 12406, 12459, 12725, 12772, 12869,\n",
      "        12994, 13004, 13215, 13544, 13682, 13813, 13884, 13934, 14038, 14194,\n",
      "        14200, 14302, 14311, 14452, 14787, 14898, 15035, 15097, 15195, 15869,\n",
      "        15902, 16112, 16287, 16375, 16520, 16705, 16916, 16985, 17054, 17090,\n",
      "        17291, 17518, 17531, 17653, 17703, 17768, 17793, 17876, 18401, 18516,\n",
      "        18574, 18601, 18674, 18771, 18808, 18871, 18971, 19111, 19237, 19682,\n",
      "        19686, 19722, 19802, 19820, 20133, 20142, 20283, 20560, 20683, 20690,\n",
      "        20762, 21464, 21610, 21753, 21901, 22092, 22130, 22146, 22277, 22278,\n",
      "        22308, 22338, 22566, 22640, 23459, 23479, 23493, 23755, 23835, 24299,\n",
      "        24334, 24423, 24582, 24634, 24699, 24819, 25056, 25173, 25292, 25528,\n",
      "        25605, 25695, 25759, 25791, 25915, 25997, 26150, 26162, 26204, 26298,\n",
      "        26414, 26473, 26531, 26575, 26801, 26836, 26838, 26842, 26922, 27065,\n",
      "        27286, 27403, 27460, 27519, 27574, 27678, 28040, 28136, 28367, 28414,\n",
      "        28471, 28550, 28627, 28739, 28751, 28788, 29063, 29304, 29384, 29561,\n",
      "        29814, 29861, 29992, 30064, 30562, 30602, 30686, 30718, 30730, 30930,\n",
      "        31274, 31381, 31419, 31496, 31513, 31552, 31794, 31816, 31997, 32006,\n",
      "        32043, 32248, 32286, 32328, 32457, 32732, 32903, 32927, 33114, 33471,\n",
      "        33501, 33970, 33992, 34286, 34289, 34350, 34702, 34819, 34844, 34881,\n",
      "        34950, 35114, 35121, 35125, 35182, 35297, 35487, 35592, 35624, 35654,\n",
      "        35870, 35878, 35911, 36004, 36184, 36297, 36318, 36364, 36382, 36784,\n",
      "        37038, 37045, 37356, 37456, 37502, 37533, 37615, 37674, 37748, 38137,\n",
      "        38419, 38436, 38485, 38536, 38556, 38623, 38683, 38983, 39061, 39141,\n",
      "        39183, 39462, 39524, 40006, 40015, 40076, 40120, 40249, 40519, 40847,\n",
      "        41087, 41093, 41179, 41286, 41328, 41563, 41783, 41914, 41939, 41972,\n",
      "        42012, 42524, 42791, 42880, 42944, 42955, 43168, 43173, 43245, 43302,\n",
      "        43386, 43520, 43586, 43840, 43883, 44221, 44402, 44454, 44484, 44561,\n",
      "        44653, 44829, 44939, 44945, 45076, 45131, 45463, 45849, 45863, 45945,\n",
      "        45999, 46042, 46045, 46149, 47054, 47169, 47239, 47309, 47411, 47421,\n",
      "        47703, 47735, 48198, 48275, 48362, 48395, 48431, 48533, 48690, 48742,\n",
      "        48797, 49050, 49113, 49153, 49218, 49325, 49739, 49939, 50002, 50044]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:52:25,812 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 0/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:53:26,197 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   141,   198,   220,   486,   500,   684,   757,   780,   795,\n",
      "          967,  1016,  1043,  1438,  1523,  1528,  1601,  1670,  1709,  1819,\n",
      "         2030,  2032,  2124,  2630,  2843,  2848,  2914,  3032,  3072,  3109,\n",
      "         3337,  3359,  3536,  3588,  3694,  3708,  3906,  4107,  4183,  4357,\n",
      "         4477,  4490,  4532,  4769,  4952,  5059,  5161,  5232,  5250,  5391,\n",
      "         5439,  5586,  5614,  5748,  5765,  5782,  5790,  6006,  6158,  6181,\n",
      "         6231,  6329,  6333,  6375,  6397,  6441,  6518,  6549,  6551,  6754,\n",
      "         6807,  6905,  6907,  7139,  7169,  7709,  7893,  8020,  8767,  8815,\n",
      "         8931,  9114,  9431, 10086, 10128, 10436, 10501, 10565, 10644, 10694,\n",
      "        10709, 10868, 10926, 10969, 11006, 11577, 11719, 11812, 11913, 11962,\n",
      "        12157, 12367, 12680, 12869, 12963, 13098, 13132, 13682, 13811, 13813,\n",
      "        13934, 14351, 14415, 14604, 14759, 14857, 15035, 15122, 15131, 15275,\n",
      "        15376, 15902, 16287, 16700, 16705, 16723, 16927, 17054, 17090, 17245,\n",
      "        17336, 17365, 17392, 17496, 17586, 17696, 17738, 17943, 18081, 18269,\n",
      "        18411, 18564, 18682, 18706, 18771, 19131, 19544, 19573, 19616, 19794,\n",
      "        19802, 19878, 20283, 20459, 20714, 20758, 20873, 21163, 21468, 22000,\n",
      "        22092, 22231, 22385, 22490, 22566, 22804, 22839, 23023, 23158, 23203,\n",
      "        23235, 23493, 23596, 23748, 23926, 23997, 24129, 24169, 24299, 24311,\n",
      "        24334, 24350, 24423, 24582, 24634, 24675, 24849, 24955, 25528, 25617,\n",
      "        25714, 25745, 25759, 26150, 26204, 26298, 26443, 26473, 26480, 26836,\n",
      "        26838, 26842, 27065, 27281, 27485, 27574, 27599, 27672, 27809, 27926,\n",
      "        28025, 28147, 28152, 28414, 28451, 28471, 28831, 29141, 29167, 29304,\n",
      "        29357, 29705, 29992, 30431, 30518, 30686, 30718, 30730, 31017, 31064,\n",
      "        31086, 31274, 31379, 31387, 31388, 31496, 31513, 31550, 32043, 32162,\n",
      "        32283, 32285, 32357, 32600, 32770, 32826, 32997, 33075, 33114, 33379,\n",
      "        33432, 33536, 33787, 33848, 33992, 34101, 34219, 34289, 34330, 34350,\n",
      "        34474, 34611, 34673, 34983, 35059, 35297, 36147, 36382, 36537, 36784,\n",
      "        36881, 37130, 37356, 37414, 37448, 37502, 37834, 38058, 38242, 38519,\n",
      "        38619, 38683, 38823, 38832, 38983, 39021, 39086, 39141, 39236, 39237,\n",
      "        39270, 39462, 39610, 39690, 39847, 39898, 40249, 40349, 40613, 40751,\n",
      "        40761, 40847, 41151, 41183, 41563, 42502, 42524, 42853, 42908, 42984,\n",
      "        43173, 43390, 43449, 43478, 43672, 43840, 43856, 44136, 44295, 44394,\n",
      "        44404, 44619, 44653, 44939, 44977, 45418, 45464, 45580, 45731, 45818,\n",
      "        46045, 46098, 46149, 46524, 46917, 47018, 47026, 47054, 47105, 47168,\n",
      "        47169, 47238, 47290, 47320, 47553, 47634, 47741, 47835, 47928, 48122,\n",
      "        48275, 48341, 48376, 48403, 48481, 48619, 48676, 48690, 48723, 49141,\n",
      "        49218, 49579, 49688, 50244]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:53:26,202 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 1/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,494 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUniquely generated tokens, sorted in ascending order: torch.return_types.sort(\n",
      "values=tensor([   62,   136,   191,   198,   472,   473,   486,   549,   638,  1016,\n",
      "         1117,  1132,  1190,  1449,  1523,  1622,  1786,  1813,  2144,  2424,\n",
      "         2751,  2848,  3359,  3373,  3492,  3536,  3582,  3588,  3716,  3842,\n",
      "         3939,  3983,  4053,  4089,  4357,  4383,  4491,  4612,  4906,  5232,\n",
      "         5468,  5708,  5748,  5790,  6047,  6162,  6181,  6231,  6270,  6274,\n",
      "         6281,  6375,  6402,  6421,  6475,  6549,  6551,  6655,  6761,  6897,\n",
      "         6905,  6934,  7034,  7139,  7169,  7249,  7260,  7280,  7353,  7873,\n",
      "         8005,  8020,  8203,  8279,  8346,  8367,  8665,  8734,  8741,  8931,\n",
      "         9087,  9443,  9619,  9664,  9830,  9982, 10015, 10086, 10267, 10562,\n",
      "        10752, 10766, 10785, 10844, 10851, 10868, 10880, 10969, 10997, 11014,\n",
      "        11167, 11299, 11577, 11719, 11907, 11913, 11962, 12115, 12139, 12157,\n",
      "        12260, 12313, 12589, 12680, 12844, 12869, 13003, 13132, 13218, 13252,\n",
      "        13362, 13421, 13682, 13813, 13933, 13934, 14095, 14183, 14415, 14726,\n",
      "        15035, 15097, 15195, 15215, 15337, 15376, 15412, 15483, 15503, 15876,\n",
      "        16158, 16217, 16227, 16323, 16891, 16916, 17054, 17518, 18161, 18260,\n",
      "        18268, 18296, 18390, 18411, 18516, 18682, 18894, 19000, 19130, 19264,\n",
      "        19277, 19651, 19780, 19794, 19797, 19802, 19963, 20487, 20503, 20558,\n",
      "        20560, 20683, 20721, 20839, 20864, 20873, 21163, 21883, 22278, 22572,\n",
      "        22621, 22839, 22993, 23235, 23408, 23448, 23721, 23835, 23926, 23948,\n",
      "        23997, 24325, 24334, 24423, 25089, 25240, 25528, 25621, 25714, 25759,\n",
      "        25851, 25908, 25982, 25989, 26107, 26150, 26178, 26204, 26298, 26567,\n",
      "        26587, 26698, 27388, 27523, 27926, 27935, 28292, 28344, 28350, 28365,\n",
      "        28378, 28414, 28451, 28471, 28850, 28913, 29000, 29107, 29167, 29474,\n",
      "        29518, 29577, 29673, 29724, 29851, 29961, 29992, 30379, 30439, 30442,\n",
      "        30551, 30569, 30686, 30718, 30750, 30989, 30995, 31017, 31085, 31304,\n",
      "        31381, 31387, 31548, 31552, 31846, 32162, 32164, 32248, 32412, 32986,\n",
      "        32997, 33075, 33114, 33118, 33459, 33632, 33706, 33764, 34286, 34289,\n",
      "        34330, 34350, 34493, 34568, 35059, 35114, 35297, 35405, 35492, 35597,\n",
      "        35620, 35870, 35909, 36294, 36297, 36316, 36382, 36516, 36537, 36717,\n",
      "        36784, 37130, 37264, 37358, 37533, 37615, 37676, 37758, 37918, 38058,\n",
      "        38159, 38172, 38283, 39004, 39141, 39237, 39253, 39462, 39690, 39756,\n",
      "        39898, 40053, 40061, 40097, 40131, 40155, 40273, 40683, 40686, 40761,\n",
      "        40829, 40914, 41088, 41231, 41328, 41380, 41532, 41563, 41891, 41893,\n",
      "        41914, 42021, 42502, 42557, 42853, 42955, 43217, 43393, 43432, 43449,\n",
      "        43478, 43542, 43587, 43672, 43771, 43840, 43911, 44008, 44455, 44488,\n",
      "        44561, 44829, 45076, 45403, 45464, 45628, 45818, 45938, 46045, 46421,\n",
      "        47018, 47101, 47168, 47386, 47703, 48122, 48359, 48376, 48481, 48611,\n",
      "        48619, 49593, 49667, 49819, 49967, 50022]),\n",
      "indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375]))\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,498 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting sample: 2/3\u001b[0m\n",
      "[ \u001b[36m2024-02-14 11:54:16,499 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mFinished writing into file \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/None/INFER_2024-02-14_11:51:26_CHECKPOINT.out\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#layernorm did not help either\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:45:55__epoch:4\",\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-14 10:05:50,939 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-14 10:05:51,034 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 10:05:51,049 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 10:05:51,090 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 10:05:51,095 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-14 10:05:51,331 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.98M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ENB\n",
      "ENENENENENENENENENENENENENENENENENENENENEN\n",
      "DENENENENENENENENENENENENEN prosper are are Here HereENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENEN nasalENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENEN\n",
      "---------------\n",
      "\n",
      "ANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANAN\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 95\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:289\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    287\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m idx_next \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# append sampled index to the running sequence and continue\u001b[39;00m\n\u001b[1;32m    291\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sample.py from nanoGPT, adjusted for our models\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from qtransform.model.gpt import GPTConfig, GPT\n",
    "import omegaconf\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "#device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/home/mabot004/nanoGPT/out-shakespeare/ckpt.pt\"\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(MODEL_PATH)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make console cmd from args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cmd_from_args(args: list[str]):\n",
    "    return \"python -m qtransform \" + ' '.join(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
