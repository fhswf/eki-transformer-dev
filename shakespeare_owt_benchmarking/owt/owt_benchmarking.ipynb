{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0029805-edb7-443d-8b8b-c040f37fa7c6",
   "metadata": {},
   "source": [
    "# Train openwebtext GPT2 models with either gelu or relu and layernorm or batchnorm and run inference on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2923a9a7-27c7-44a1-bc8f-c0c1a5836688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn\n",
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ff7760-d5c3-4c29-a7ac-ad26f7c4d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from qtransform.model import gpt as qtransform_gpt\n",
    "\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "def sample(ckpt_path, start: str = \"\\n\", max_new_tokens: int = 500):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "    out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "    num_samples = 10 # number of samples to draw\n",
    "    temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "    top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "    dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "    compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "    #exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "    # -----------------------------------------------------------------------------\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "    \n",
    "    # model\n",
    "    if init_from == 'resume':\n",
    "        # init from a model saved in a specific directory\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        gptconf = qtransform_gpt.GPTConfig(**checkpoint['model_cfg'][\"args\"])\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        model = qtransform_gpt.GPT(gptconf)\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "    elif init_from.startswith('gpt2'):\n",
    "        # init from a given GPT-2 model\n",
    "        model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "    # look for the meta pickle in case it is available in the dataset folder\n",
    "    load_meta = False\n",
    "    if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "        meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "        load_meta = os.path.exists(meta_path)\n",
    "    if load_meta:\n",
    "        print(f\"Loading meta from {meta_path}...\")\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    else:\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc371860-fb87-427c-9922-1c1cd3ffabd8",
   "metadata": {},
   "source": [
    "####################\n",
    "# TODO: it seems that shuffling with large datasets causes the entire dataset to be loaded into memory\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfd8dd-865b-4177-a955-c79e66ce483c",
   "metadata": {},
   "source": [
    "## Train ReLU and BatchNorm with karpathy's parameters\n",
    "### Our tokeniization of Openwebtext does not add padding after each sample, that could be implemented to keep context within samples\n",
    "### eval is nan due to an error in the currently installed torch version on the cluster, eval is computed with torch version 2.2, however that disables gpu training due to old nvidia drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c037759-b665-40c3-82dd-c0ac7ac826da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py and https://github.com/karpathy/nanoGPT/blob/master/config/train_gpt2.py\n",
    "#karpathy used a larger transformer model for openwebtext alongside more epochs\n",
    "eval_epoch_interval = str(1) # every epoch, meaning after max_iters\n",
    "eval_iters = str(200)\n",
    "max_iters = str(500)\n",
    "epochs = \"20\" #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = \"2\"\n",
    "batch_size = \"32\"\n",
    "block_size = \"256\"\n",
    "\n",
    "grad_clip=\"1.0\"\n",
    "\n",
    "n_layer = \"6\"\n",
    "n_head = \"6\"\n",
    "n_embd = \"384\"\n",
    "dropout = \"0.2\"\n",
    "\n",
    "learning_rate = str(1e-3) # with baby networks can afford to go a bit higher\n",
    "seed = \"1337\"\n",
    "\n",
    "step_size = epochs\n",
    "\n",
    "model = \"gpt_2_h2l2e256b64_ReBN\" #RELU BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d3e007-c770-421f-a8d4-f6d2d2a97b25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 12:04:26,290 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 500, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-28 12:04:26,469 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,472 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,474 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,476 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-28_12:04:26\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,478 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,482 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,486 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,491 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,493 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,496 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,498 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,500 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,503 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,505 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,506 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,509 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,511 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,512 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 2709600996, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,514 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 2709600996\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,516 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3161201164 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,518 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,519 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,521 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,523 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,526 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,529 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,535 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,537 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,542 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,543 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,785 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,878 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,914 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,997 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,017 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,097 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,118 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,184 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,208 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,277 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,306 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,317 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,812 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,846 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,849 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'lr', 'betas', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,854 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,855 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f9a38def1f0>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,859 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:36,407 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.327135443687439. time: 8415.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:44,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 3.7978119611740113. time: 8285.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:52,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.95255970954895. time: 8292.91ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:01,291 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 3.926949453353882. time: 8292.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:09,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 3.609880208969116. time: 8294.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:17,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 3.6377697944641114. time: 8300.65ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:26,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 3.71623432636261. time: 8288.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:34,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 3.8099358320236205. time: 8292.95ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:42,774 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 3.7783620357513428. time: 8287.16ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:51,066 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 3.5389353752136232. time: 8287.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:59,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 3.32785279750824. time: 8294.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:07,667 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 3.78099946975708. time: 8298.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:15,956 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 3.639142322540283. time: 8285.29ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:24,246 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 3.6023594617843626. time: 8287.09ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:32,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 3.7320327758789062. time: 8290.37ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:40,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 3.7642956733703614. time: 8293.73ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:49,132 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.6174857139587404. time: 8290.79ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:57,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.6943739891052245. time: 8298.12ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:05,730 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.202271723747254. time: 8292.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:14,030 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.958717703819275. time: 8297.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:22,331 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.535782241821289. time: 8296.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:30,630 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.5407889604568483. time: 8295.44ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:38,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 3.562412714958191. time: 8290.52ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:47,219 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 3.6171810626983643. time: 8290.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:55,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 3.5712056159973145. time: 8287.58ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:03,798 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.483549618721008. time: 8285.52ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:12,108 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.6996013164520263. time: 8307.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:20,402 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.686589241027832. time: 8290.82ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:28,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.7768951892852782. time: 8297.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:37,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 3.556287384033203. time: 8299.08ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:45,296 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 3.4522799253463745. time: 8287.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:53,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 3.6408530950546263. time: 8284.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:01,874 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 3.6766660690307615. time: 8287.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:10,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 3.4410290718078613. time: 8280.10ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:18,450 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 3.418845009803772. time: 8290.32ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:26,750 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 3.3109395503997803. time: 8296.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:35,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 3.3370896577835083. time: 8290.19ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:43,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 3.3152429342269896. time: 8293.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:51,639 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 3.3816690921783445. time: 8295.38ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:59,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 3.5775759696960447. time: 8297.56ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:08,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 3.6935659885406493. time: 8288.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:16,526 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 3.618050694465637. time: 8292.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:24,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 3.62907497882843. time: 8291.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:33,117 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.8581554174423216. time: 8294.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:41,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 3.3921287059783936. time: 8292.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:49,707 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 3.5641777753829955. time: 8290.78ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:57,991 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 3.652488946914673. time: 8281.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:06,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 3.671545958518982. time: 8270.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:14,548 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 3.468773436546326. time: 8280.30ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:22,823 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 3.3844356536865234. time: 8272.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:23,066 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.3844356536865234\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,834 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:59,233 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 3.1929492712020875. time: 8283.62ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:07,522 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.9940556049346925. time: 8285.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:15,802 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.0043869733810427. time: 8277.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:24,079 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.942632794380188. time: 8273.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:32,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.8636070013046266. time: 8272.31ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:40,640 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.870943522453308. time: 8283.31ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:48,919 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.941968536376953. time: 8276.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:57,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.980938959121704. time: 8280.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:05,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.954316759109497. time: 8285.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:13,775 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.8078702449798585. time: 8281.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:22,049 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.6682265520095827. time: 8270.26ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:30,351 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.967451286315918. time: 8299.19ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:38,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.913192701339722. time: 8293.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:46,930 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.9574296951293944. time: 8280.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:55,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.829618287086487. time: 8258.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:03,463 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.9233726263046265. time: 8267.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:11,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.9565386056900023. time: 8281.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:20,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.037624979019165. time: 8270.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:28,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 3.143631649017334. time: 8269.44ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:36,578 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.018936347961426. time: 8282.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:44,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.0869062900543214. time: 8276.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:53,135 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.1456120014190674. time: 8275.28ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:01,422 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.896369791030884. time: 8283.42ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:09,710 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.962426018714905. time: 8285.23ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:17,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.925130295753479. time: 8281.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:26,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.859836053848267. time: 8282.97ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:34,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.0628945589065553. time: 8288.62ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:42,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.055732846260071. time: 8279.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:51,134 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.0108960390090944. time: 8275.54ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:59,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.839269995689392. time: 8275.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:07,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.897292137145996. time: 8267.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:15,957 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.9678061485290526. time: 8271.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:24,230 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.8944671154022217. time: 8270.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:32,500 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.858667325973511. time: 8266.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:40,790 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.961489272117615. time: 8284.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:49,070 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.960615944862366. time: 8276.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:57,357 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.9382026195526123. time: 8284.32ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:05,642 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.8870409965515136. time: 8282.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:13,928 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.9044100999832154. time: 8282.92ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:22,201 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.975853204727173. time: 8269.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:30,472 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.98272819519043. time: 8268.02ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:38,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.9435110092163086. time: 8278.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:47,034 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.99189772605896. time: 8277.34ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:55,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.0420265913009645. time: 8277.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:03,598 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.7617907524108887. time: 8280.39ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:11,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.955462408065796. time: 8285.13ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:20,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.9882348775863647. time: 8274.92ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:28,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.906810474395752. time: 8276.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:36,724 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.808030533790588. time: 8276.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:45,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.8852347135543823. time: 8278.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:12,397 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:12,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.8852347135543823\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,033 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,038 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:21,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9917736291885375. time: 8277.77ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:29,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.8065508365631104. time: 8268.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:37,967 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.8441134691238403. time: 8271.36ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:46,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.788425326347351. time: 8274.39ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:54,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.7369828701019285. time: 8264.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:02,789 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.7084540843963625. time: 8273.57ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:11,053 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.757987904548645. time: 8262.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:19,322 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.862232518196106. time: 8265.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:27,597 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.7414689302444457. time: 8272.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:35,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.7070297241210937. time: 8281.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:44,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.5827844381332397. time: 8279.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:52,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.8864506244659425. time: 8274.48ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:00,722 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.7899402379989624. time: 8276.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:08,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.8162834882736205. time: 8264.79ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:17,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.7024369716644285. time: 8270.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:25,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.775118827819824. time: 8274.77ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:33,815 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.8138137578964235. time: 8269.95ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:42,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.841794228553772. time: 8275.23ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:50,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.9459991693496703. time: 8278.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:58,661 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.9409534215927122. time: 8283.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:06,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.01933069229126. time: 8269.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:15,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.077498507499695. time: 8260.33ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:23,461 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.729004645347595. time: 8258.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:31,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.751947808265686. time: 8269.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:40,007 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.694729471206665. time: 8270.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:48,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.6955225467681885. time: 8271.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:56,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.9306941747665407. time: 8269.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:04,826 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.8254024028778075. time: 8268.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:13,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.7571573734283445. time: 8275.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:21,385 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.7187278509140014. time: 8276.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:29,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.7404979467391968. time: 8275.61ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:37,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.7975340127944945. time: 8281.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:46,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.741619086265564. time: 8290.66ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:54,518 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.713063931465149. time: 8274.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:02,789 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.807782459259033. time: 8267.82ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:11,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.8191112518310546. time: 8265.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:19,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.839112710952759. time: 8273.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:27,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.7884613037109376. time: 8279.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:35,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.76355881690979. time: 8276.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:44,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.785447883605957. time: 8274.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:52,458 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.8150417804718018. time: 8279.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:00,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.763792538642883. time: 8282.97ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:09,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.731421160697937. time: 8272.80ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:17,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.8265005350112915. time: 8279.58ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:25,590 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.6528093814849854. time: 8284.17ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:33,862 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.7490429401397707. time: 8269.49ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:42,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.837496542930603. time: 8270.98ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:50,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.75510458946228. time: 8271.10ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:58,691 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.6838581562042236. time: 8276.15ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:06,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.762780284881592. time: 8273.98ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,349 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,353 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.762780284881592\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,902 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,907 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:43,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.851698565483093. time: 8284.64ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:51,576 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.6997380018234254. time: 8275.36ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:59,866 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.7737808227539062. time: 8286.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:08,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.650785517692566. time: 8274.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:16,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.6235146522521973. time: 8274.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:24,702 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.61886990070343. time: 8277.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:32,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.7179662466049193. time: 8272.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:41,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.7501431941986083. time: 8279.33ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:49,542 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.693390464782715. time: 8277.93ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:57,831 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.6571080446243287. time: 8285.91ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:06,115 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.5941498994827272. time: 8280.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:14,403 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.7372282981872558. time: 8285.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:22,684 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.724133825302124. time: 8277.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:30,965 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.7724583625793455. time: 8277.30ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:39,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.6392184257507325. time: 8273.13ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:47,513 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.7117538690567016. time: 8269.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:55,794 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.7169060468673707. time: 8277.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:04,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.8080204725265503. time: 8275.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:12,343 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.955156135559082. time: 8267.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:20,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.9142407178878784. time: 8266.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:28,889 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.0484140634536745. time: 8271.65ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:37,160 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.0302297353744505. time: 8268.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:45,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.718441915512085. time: 8268.05ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:53,706 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.687196898460388. time: 8269.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:01,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.6516018152236938. time: 8283.35ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:10,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.6681648969650267. time: 8274.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:18,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.8916533470153807. time: 8269.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:26,823 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.7350722312927247. time: 8276.55ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:35,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.648666739463806. time: 8277.50ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:43,379 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.6002538919448854. time: 8271.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:51,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.631929612159729. time: 8281.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:59,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.6977185249328612. time: 8274.64ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:08,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.6883314847946167. time: 8275.05ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:16,507 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.648334503173828. time: 8284.50ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:24,795 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.734477400779724. time: 8285.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:33,077 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.735826015472412. time: 8279.35ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:41,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.7530935764312745. time: 8274.08ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:49,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.7007143259048463. time: 8266.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:57,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.6625132322311402. time: 8269.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:06,179 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.7425496339797975. time: 8278.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:14,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.7114676952362062. time: 8290.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:22,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.6848283290863035. time: 8278.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:31,051 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.7488788843154905. time: 8293.14ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:39,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.8026312351226808. time: 8283.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:47,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.6324623346328737. time: 8281.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:55,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.7296985387802124. time: 8272.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:04,167 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.745193696022034. time: 8265.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:12,450 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.7007166862487795. time: 8279.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:20,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.7331544160842896. time: 8270.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:29,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.7636995792388914. time: 8268.04ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,373 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.7636995792388914\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,937 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:05,351 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9943984746932983. time: 8297.54ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:13,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.7048344135284426. time: 8276.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:21,910 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.7297203540802. time: 8274.66ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:30,187 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.6494465112686156. time: 8273.26ms\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_2_h2l2e256b64_ReBN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#RELU BatchNorm\u001b[39;00m\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mseed,\n\u001b[1;32m     25\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m ]\n\u001b[0;32m---> 50\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:110\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    107\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    113\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:191\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    187\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#dataloader always returns the same tensors after each epoch because it is casted inside of function call\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#therefore, cast it before training\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#TODO: find a more elegant solution, maybe by manipulating its seed with a torch.Generator?\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:245\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    243\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss_in_model:\n\u001b[0;32m--> 245\u001b[0m     outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:206\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m     N, C, L \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m#due to inference, input features can be lower than specified max context length which causes problem during attention calculation\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    207\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y))\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1193\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m-> 1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m,\n\u001b[1;32m   1194\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args = [\n",
    "    'seed='+seed,\n",
    "     'run=train',\n",
    "     'run.export=False',\n",
    "     'run.epochs='+epochs,\n",
    "     'run.max_iters='+max_iters,\n",
    "     'run.eval_epoch_interval='+eval_epoch_interval,\n",
    "     'run.eval_iters='+eval_iters,\n",
    "     'run.grad_clip='+grad_clip,\n",
    "    'run.gradient_accumulation_steps='+gradient_accumulation_steps,\n",
    "     'model='+model,\n",
    "     'model.args.dropout='+dropout,\n",
    "    'model.args.n_layer='+n_layer,\n",
    "    'model.args.n_head='+n_head,\n",
    "    'model.args.n_embd='+n_embd,\n",
    "    'model.args.block_size='+n_embd,\n",
    "     'dataset=huggingface',\n",
    "     'dataset/tokenizer=tiktoken',\n",
    "     'dataset.tokenizer.encoding=gpt2',\n",
    "     'dataset.name=openwebtext',\n",
    "     'dataset.dataloader.shuffle=False',\n",
    "     'dataset.dataloader.batch_size='+batch_size,\n",
    "     'optim.args.learning_rate='+learning_rate,\n",
    "     'optim.scheduler.schedulers.1.args.step_size='+step_size,\n",
    "     'device=cuda',\n",
    "     'debug=True',\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa312e5-fbd4-4699-b0af-187e5c64ed96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4', 'epochs': 16, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 500, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-03-01 11:22:50,913 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:51,562 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:51,565 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:51,568 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 11:22:52.172185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-01 11:22:52,928 \u001b[0m][\u001b[2;37mtensorflow\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFalling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,036 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,038 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,040 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,041 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,725 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,730 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-01_11:22:53\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,732 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,738 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,827 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,832 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,834 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,837 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,839 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:53,845 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mStarting new HTTPS connection (1): openaipublic.blob.core.windows.net:443\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:54,454 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://openaipublic.blob.core.windows.net:443 \"GET /gpt-2/encodings/main/vocab.bpe HTTP/1.1\" 200 456318\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:55,197 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mStarting new HTTPS connection (1): openaipublic.blob.core.windows.net:443\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:55,774 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://openaipublic.blob.core.windows.net:443 \"GET /gpt-2/encodings/main/encoder.json HTTP/1.1\" 200 1042301\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,931 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,937 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,940 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,942 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,945 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,948 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,950 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 2709600996, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,951 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 2709600996\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,953 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3161201164 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,955 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,957 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,958 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,960 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,962 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,965 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,969 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,971 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,972 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,982 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:56,984 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,169 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,212 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,304 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,317 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,478 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,504 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,602 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,614 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,702 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,714 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,804 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:57,816 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:22:58,329 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,308 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,312 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,318 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'betas', 'lr', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,324 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,326 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,328 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f25ccfc5360>\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,331 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mResuming training from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,333 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEpoch is 4, running for 16\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:00,756 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:04,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9934693813323974. time: 3601.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:07,092 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.708436131477356. time: 2487.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:09,592 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.7324522018432615. time: 2495.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:12,089 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.6515779972076414. time: 2491.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:14,568 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.6180811166763305. time: 2474.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:17,045 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.5800665378570558. time: 2473.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:19,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.6461410999298094. time: 2472.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:22,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.698442506790161. time: 2478.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:24,504 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.7013391256332397. time: 2494.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:26,975 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.5633448123931886. time: 2467.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:29,469 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.5273911237716673. time: 2491.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:31,960 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.7862908840179443. time: 2487.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:34,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.6065518379211428. time: 2481.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:36,923 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.750535726547241. time: 2472.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:39,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.64811487197876. time: 2471.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:41,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.63667094707489. time: 2492.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:44,387 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.64972505569458. time: 2487.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:46,883 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.736677861213684. time: 2492.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:49,373 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.7848241567611693. time: 2485.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:51,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.888981509208679. time: 2493.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:54,361 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.984301972389221. time: 2482.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:56,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.9658021688461305. time: 2482.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:23:59,350 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.665901279449463. time: 2498.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:01,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.680038070678711. time: 2498.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:04,343 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.6483017683029173. time: 2487.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:06,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.655659890174866. time: 2487.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:09,349 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.761703276634216. time: 2511.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:11,841 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.653615713119507. time: 2487.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:14,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.6066794633865356. time: 2500.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:16,840 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.546913242340088. time: 2491.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:19,328 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.542153072357178. time: 2484.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:21,816 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.6646825313568114. time: 2485.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:24,302 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.593114995956421. time: 2481.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:26,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.6222180843353273. time: 2480.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:29,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.7158302545547484. time: 2479.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:31,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.68402955532074. time: 2472.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:34,233 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.6955020904541014. time: 2481.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:36,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.594304156303406. time: 2480.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:39,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.5963368654251098. time: 2482.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:41,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.634357523918152. time: 2488.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:44,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.652396535873413. time: 2486.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:46,673 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.6045878171920775. time: 2483.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:49,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.647083044052124. time: 2481.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:51,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.620129632949829. time: 2486.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:54,144 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.5430412530899047. time: 2493.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:56,639 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.5974558115005495. time: 2490.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:24:59,141 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.676216411590576. time: 2497.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:01,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.5852993965148925. time: 2499.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:04,138 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.57610547542572. time: 2491.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:06,629 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.645438385009766. time: 2486.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:06,856 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:15,563 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:15,568 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.645438385009766\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:16,169 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:16,172 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:16,173 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:18,796 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9025732278823853. time: 2508.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:21,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.6629587173461915. time: 2487.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:23,785 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.716835641860962. time: 2491.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:26,282 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.625142455101013. time: 2493.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:28,770 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.5241902589797975. time: 2483.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:31,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.5198663473129272. time: 2486.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:33,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.6136037349700927. time: 2490.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:36,244 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.71072371006012. time: 2484.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:38,741 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.586889410018921. time: 2495.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:41,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.5480175495147703. time: 2482.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:43,714 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.459803247451782. time: 2482.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:46,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.6539686918258667. time: 2490.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:48,689 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.550350308418274. time: 2476.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:51,197 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.641469955444336. time: 2503.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:53,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.5150314807891845. time: 2492.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:56,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.586227560043335. time: 2493.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:25:58,680 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.5931232929229737. time: 2482.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:01,156 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.6331212520599365. time: 2471.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:03,638 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.8300948619842528. time: 2477.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:06,122 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.8355610847473143. time: 2481.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:08,608 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.9949601173400877. time: 2483.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:11,101 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.9238924026489257. time: 2491.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:13,596 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.615396809577942. time: 2490.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:16,091 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.625481390953064. time: 2491.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:18,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.5904253482818604. time: 2491.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:21,081 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.6050488233566282. time: 2490.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:23,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.6983373880386354. time: 2477.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:26,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.6449198961257934. time: 2475.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:28,514 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.653797459602356. time: 2472.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:30,999 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.5159168004989625. time: 2481.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:33,475 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.4365363121032715. time: 2471.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:35,955 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.5857004642486574. time: 2476.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:38,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.593095898628235. time: 2487.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:40,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.5160682678222654. time: 2489.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:43,427 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.6248534679412843. time: 2484.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:45,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.6063204526901247. time: 2477.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:48,392 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.634655499458313. time: 2480.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:50,877 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.517927122116089. time: 2481.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:53,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.5010813236236573. time: 2485.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:55,856 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.5805050134658813. time: 2484.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:26:58,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.6060673236846923. time: 2483.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:00,833 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.525932717323303. time: 2486.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:03,314 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.6037238597869874. time: 2478.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:05,794 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.5741249084472657. time: 2477.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:08,285 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.4938628911972045. time: 2486.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:10,774 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.550629758834839. time: 2485.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:13,269 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.677928638458252. time: 2490.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:15,759 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.5899777889251707. time: 2486.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:18,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.4746497869491577. time: 2489.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:20,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.5788002729415895. time: 2482.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:29,646 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 6/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:29,650 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.5788002729415895\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:30,222 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:30,224 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:30,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 7/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:32,841 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.863487458229065. time: 2509.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:35,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.7163811206817625. time: 2497.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:37,839 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.6487847805023192. time: 2492.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:40,324 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.546040654182434. time: 2481.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:42,803 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.5150441408157347. time: 2474.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:45,291 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.534139847755432. time: 2484.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:47,780 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.555379629135132. time: 2485.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:50,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.6600309371948243. time: 2486.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:52,756 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.531611752510071. time: 2483.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:55,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.445481467247009. time: 2487.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:27:57,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.3778913974761964. time: 2489.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:00,248 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.611758065223694. time: 2503.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:02,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.5389822006225584. time: 2488.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:05,230 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.5913039207458497. time: 2485.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:07,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.495480513572693. time: 2478.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:10,198 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.511265778541565. time: 2481.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:12,691 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.5263844966888427. time: 2490.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:15,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.622844123840332. time: 2489.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:17,676 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.7407567262649537. time: 2483.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:20,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.809213709831238. time: 2482.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:22,649 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.916380214691162. time: 2482.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:25,133 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.837195086479187. time: 2480.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:27,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.555907392501831. time: 2486.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:30,108 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.6087907552719116. time: 2480.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:32,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.5672083377838133. time: 2481.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:35,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.5383008241653444. time: 2474.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:37,549 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.6387139797210692. time: 2473.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:40,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.5917619943618773. time: 2483.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:42,517 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.5183428049087526. time: 2477.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:45,001 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.5002103090286254. time: 2480.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:47,494 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.426165795326233. time: 2489.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:49,991 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.529010033607483. time: 2492.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:52,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.5266091346740724. time: 2484.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:54,964 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.482553482055664. time: 2477.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:57,455 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.5606327295303344. time: 2486.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:28:59,943 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.545033264160156. time: 2485.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:02,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.5470143795013427. time: 2486.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:04,921 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.5016406774520874. time: 2486.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:07,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.4196012020111084. time: 2492.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:09,911 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.470384192466736. time: 2493.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:12,402 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.5185837745666504. time: 2487.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:14,890 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.4322983980178834. time: 2484.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:17,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.507758450508118. time: 2504.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:19,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.5565236806869507. time: 2496.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:22,392 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.4178282499313353. time: 2489.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:24,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.553628611564636. time: 2498.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:27,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.573845052719116. time: 2491.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:29,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.487893056869507. time: 2484.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:32,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.4143525838851927. time: 2480.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:34,850 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.5114755153656008. time: 2482.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:43,758 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 7/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:43,761 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.5114755153656008\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:44,334 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:7\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:44,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:44,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 8/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:46,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.8089391946792603. time: 2504.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:49,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.588251161575317. time: 2493.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:51,943 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.54507429599762. time: 2497.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:54,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.5152520895004273. time: 2486.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:56,925 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.436222052574158. time: 2486.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:29:59,414 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.462653303146362. time: 2486.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:01,903 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.5571403026580812. time: 2485.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:04,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.609345817565918. time: 2482.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:06,880 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.483292078971863. time: 2486.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:09,367 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.449528169631958. time: 2483.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:11,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.346349000930786. time: 2478.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:14,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.688144159317017. time: 2491.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:16,831 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.5064567804336546. time: 2483.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:19,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.5909971237182616. time: 2478.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:21,796 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.454014539718628. time: 2480.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:24,277 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.4907342195510864. time: 2479.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:26,777 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.469472312927246. time: 2495.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:29,266 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.5585091590881346. time: 2487.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:31,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.7645164012908934. time: 2494.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:34,251 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.774902820587158. time: 2482.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:36,738 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.8977585315704344. time: 2484.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:39,227 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.760022592544556. time: 2485.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:41,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.5826029062271116. time: 2481.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:44,191 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.4937057733535766. time: 2475.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:46,689 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.518957281112671. time: 2493.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:49,167 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.5398380041122435. time: 2473.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:51,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.561298894882202. time: 2480.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:54,140 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.512474012374878. time: 2485.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:56,628 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.5038166284561156. time: 2485.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:30:59,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.411815643310547. time: 2473.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:01,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.3834137678146363. time: 2478.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:04,085 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.52125289440155. time: 2494.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:06,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4717275142669677. time: 2497.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:09,081 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.453177285194397. time: 2488.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:11,565 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.4761767625808715. time: 2479.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:14,046 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.470547389984131. time: 2477.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:16,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.442259979248047. time: 2489.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:19,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.413880157470703. time: 2480.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:21,511 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.4307044744491577. time: 2483.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:23,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.455372142791748. time: 2482.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:26,490 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.4943092823028565. time: 2489.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:28,979 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.447306275367737. time: 2484.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:31,470 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.462212324142456. time: 2485.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:33,953 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.516350746154785. time: 2480.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:36,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.4410062789916993. time: 2487.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:38,931 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.4633485794067385. time: 2483.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:41,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.5272454738616945. time: 2486.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:43,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.4629008054733275. time: 2480.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:46,390 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.37309889793396. time: 2480.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:48,887 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.490056347846985. time: 2490.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:57,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 8/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:57,809 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.490056347846985\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:58,409 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:8\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:58,411 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:31:58,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 9/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:01,021 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.6648025274276734. time: 2490.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:03,506 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.5713696479797363. time: 2481.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:05,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.550571346282959. time: 2482.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:08,485 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.4903703927993774. time: 2488.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:10,975 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.3763662815093993. time: 2486.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:13,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.4149523258209227. time: 2473.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:15,929 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.484982395172119. time: 2472.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:18,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.5436416625976563. time: 2475.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:20,889 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.44303982257843. time: 2476.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:23,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.373604583740234. time: 2472.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:25,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.306834101676941. time: 2474.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:28,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.5673577785491943. time: 2482.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:30,801 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.443823528289795. time: 2467.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:33,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.4660123586654663. time: 2476.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:35,757 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.3862500190734863. time: 2472.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:38,239 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.4505444288253786. time: 2477.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:40,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.3757460832595827. time: 2472.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:43,187 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.469579553604126. time: 2468.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:45,671 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.6459622144699098. time: 2480.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:48,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.7154571771621705. time: 2481.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:50,652 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.8756465911865234. time: 2490.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:53,144 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.722680425643921. time: 2488.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:55,639 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.456604075431824. time: 2491.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:32:58,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.517553949356079. time: 2486.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:00,614 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.434351396560669. time: 2480.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:03,087 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.441172790527344. time: 2469.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:05,565 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.5544371128082277. time: 2471.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:08,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.465410828590393. time: 2479.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:10,543 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.4278159856796266. time: 2490.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:13,032 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.3766775131225586. time: 2485.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:15,515 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.3212982654571532. time: 2478.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:18,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.512212896347046. time: 2481.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:20,492 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4651904106140137. time: 2488.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:22,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.397382664680481. time: 2479.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:25,458 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.423622965812683. time: 2476.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:27,945 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.397356915473938. time: 2483.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:30,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.4607401371002195. time: 2487.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:32,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.382703495025635. time: 2481.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:35,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.37772421836853. time: 2479.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:37,890 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.443753170967102. time: 2481.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:40,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.4934436559677122. time: 2494.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:42,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.4398608446121215. time: 2486.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:45,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.5553961515426638. time: 2479.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:47,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.5547138452529907. time: 2480.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:50,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.3652838230133058. time: 2482.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:52,824 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.443796467781067. time: 2484.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:55,308 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.5393243074417113. time: 2480.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:33:57,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.551707148551941. time: 2480.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:00,273 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.3066534042358398. time: 2475.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:02,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.4606423377990723. time: 2476.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:11,646 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 9/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:11,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.4606423377990723\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:12,243 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:9\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:12,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:12,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 10/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:14,863 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.6601829290390016. time: 2509.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:17,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.523061156272888. time: 2496.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:19,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.5367312669754027. time: 2492.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:22,346 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.483071780204773. time: 2482.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:24,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.3599183797836303. time: 2493.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:27,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.372483229637146. time: 2493.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:29,832 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.4485975980758665. time: 2488.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:32,329 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.4938173055648805. time: 2493.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:34,815 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.435270094871521. time: 2483.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:37,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.3046585083007813. time: 2484.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:39,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.242942476272583. time: 2484.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:42,298 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.546262264251709. time: 2501.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:44,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.3898453235626222. time: 2491.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:47,296 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.4028033018112183. time: 2498.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:49,797 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.3444316148757935. time: 2496.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:52,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.3673826456069946. time: 2487.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:54,775 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.3799124479293825. time: 2482.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:57,258 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.486031436920166. time: 2478.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:34:59,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.7010812997817992. time: 2481.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:02,238 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.724205803871155. time: 2490.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:04,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.7704270839691163. time: 2487.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:07,215 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.608740210533142. time: 2482.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:09,701 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.532200574874878. time: 2481.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:12,194 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.425721454620361. time: 2487.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:14,681 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.3632808208465574. time: 2482.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:17,161 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.3956563234329225. time: 2475.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:19,645 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.4757558345794677. time: 2480.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:22,131 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.424370217323303. time: 2482.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:24,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.3598797082901. time: 2478.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:27,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.3468623638153074. time: 2476.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:29,579 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.253277826309204. time: 2481.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:32,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.445455813407898. time: 2477.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:34,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4547138929367067. time: 2481.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:37,037 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.3765565156936646. time: 2488.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:39,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.4397753715515136. time: 2487.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:42,021 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.403938364982605. time: 2488.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:44,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.433406352996826. time: 2485.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:47,000 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.3468968868255615. time: 2485.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:49,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.344216060638428. time: 2487.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:51,983 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.4141425132751464. time: 2487.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:54,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.4335970640182496. time: 2491.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:56,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.384430193901062. time: 2484.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:35:59,467 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.4582756757736206. time: 2491.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:01,965 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.417076015472412. time: 2493.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:04,451 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.3923604249954225. time: 2482.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:06,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.4828510761260985. time: 2487.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:09,440 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.441442275047302. time: 2494.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:11,923 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.486289715766907. time: 2479.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:14,403 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.3466394662857057. time: 2476.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:16,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.361510133743286. time: 2486.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:25,828 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 10/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:25,832 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.361510133743286\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:26,395 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:10\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:26,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:26,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 11/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:28,996 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.656497073173523. time: 2491.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:31,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.4457457304000854. time: 2490.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:33,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.4499945878982543. time: 2478.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:36,461 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.3730074405670165. time: 2483.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:38,945 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.3305156230926514. time: 2480.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:41,443 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.348020839691162. time: 2493.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:43,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3740607023239138. time: 2492.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:46,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.5043574810028075. time: 2502.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:48,946 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.393189263343811. time: 2496.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:51,440 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.3314981698989867. time: 2489.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:53,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.2949472427368165. time: 2489.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:56,428 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.6012194871902468. time: 2491.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:36:58,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.347068023681641. time: 2495.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:01,414 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.3471922397613527. time: 2484.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:03,906 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.2753872871398926. time: 2488.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:06,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.364343357086182. time: 2510.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:08,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.3756126403808593. time: 2484.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:11,398 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.378401780128479. time: 2485.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:13,891 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.63565456867218. time: 2489.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:16,384 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.6125695943832397. time: 2488.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:18,876 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.697499942779541. time: 2483.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:21,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.578673315048218. time: 2490.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:23,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.4030452013015746. time: 2487.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:26,353 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.377201867103577. time: 2488.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:28,844 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.345544528961182. time: 2487.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:31,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.36507728099823. time: 2489.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:33,833 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.430130195617676. time: 2492.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:36,318 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.4058491230010985. time: 2480.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:38,822 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.33901960849762. time: 2499.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:41,318 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.284010124206543. time: 2493.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:43,813 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.2364887952804566. time: 2490.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:46,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.404119300842285. time: 2482.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:48,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4128623723983766. time: 2487.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:51,276 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.3492340564727785. time: 2478.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:53,759 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.397304081916809. time: 2479.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:56,246 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.390965962409973. time: 2482.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:37:58,735 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.3540326595306396. time: 2485.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:01,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.309080982208252. time: 2482.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:03,703 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.3521884202957155. time: 2479.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:06,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.434109926223755. time: 2485.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:08,679 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.400218677520752. time: 2482.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:11,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.340192985534668. time: 2477.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:13,642 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.4228251218795775. time: 2474.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:16,119 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.464126205444336. time: 2472.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:18,602 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.3485016107559202. time: 2478.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:21,096 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.421163320541382. time: 2490.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:23,590 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.433000636100769. time: 2488.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:26,084 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.3979729890823362. time: 2489.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:28,587 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.3598182439804076. time: 2497.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:31,074 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.35925772190094. time: 2482.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:39,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 11/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:39,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.35925772190094\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:40,603 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:11\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:40,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:40,606 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 12/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:43,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.615362548828125. time: 2492.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:45,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.4539801359176634. time: 2484.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:48,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.4179327964782713. time: 2490.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:50,676 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.249099016189575. time: 2485.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:53,169 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.2714062213897703. time: 2489.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:55,671 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.333839249610901. time: 2498.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:38:58,159 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3930017948150635. time: 2484.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:00,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.4320934057235717. time: 2483.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:03,137 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.3272416591644287. time: 2486.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:05,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.2298907995224. time: 2484.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:08,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.17004189491272. time: 2477.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:10,595 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.5026923418045044. time: 2485.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:13,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.342556667327881. time: 2488.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:15,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.287104630470276. time: 2496.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:18,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.2341590404510496. time: 2491.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:20,582 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2897944927215574. time: 2490.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:23,071 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.309362292289734. time: 2481.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:25,553 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.343880820274353. time: 2479.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:28,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.6544893980026245. time: 2484.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:30,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.6031598806381226. time: 2495.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:33,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.655275821685791. time: 2492.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:35,525 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.5233084678649904. time: 2484.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:38,011 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.3022992610931396. time: 2481.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:40,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.3525062799453735. time: 2483.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:42,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.2820671081542967. time: 2487.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:45,490 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.2910521745681764. time: 2495.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:47,994 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.413488507270813. time: 2499.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:50,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.4151920557022093. time: 2493.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:52,982 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.298188257217407. time: 2485.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:55,464 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.2160698890686037. time: 2478.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:39:57,944 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.200125479698181. time: 2476.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:00,437 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4386436700820924. time: 2488.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:02,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4131803274154664. time: 2481.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:05,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.3424890518188475. time: 2479.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:07,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.3462937116622924. time: 2495.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:10,392 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.376004719734192. time: 2487.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:12,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.3839555978775024. time: 2483.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:15,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.290082049369812. time: 2478.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:17,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.368886637687683. time: 2483.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:20,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.4037259817123413. time: 2483.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:22,818 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.381848359107971. time: 2478.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:25,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.285732126235962. time: 2478.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:27,791 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.3872442483901977. time: 2487.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:30,290 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.3717432498931883. time: 2494.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:32,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.312719941139221. time: 2497.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:35,284 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.350616431236267. time: 2487.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:37,770 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.4080052852630613. time: 2481.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:40,257 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.290298867225647. time: 2482.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:42,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.2281903266906737. time: 2486.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:45,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.308733892440796. time: 2490.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:54,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 12/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:54,109 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.308733892440796\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:54,720 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:12\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:54,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:54,725 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 13/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:57,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.592057299613953. time: 2491.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:40:59,816 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.3986782312393187. time: 2490.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:02,299 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.423133301734924. time: 2479.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:04,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.32716588973999. time: 2495.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:07,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.2283318758010866. time: 2491.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:09,789 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.2821316003799437. time: 2490.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:12,269 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.345854663848877. time: 2476.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:14,753 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.344971704483032. time: 2479.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:17,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.2951098680496216. time: 2484.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:19,717 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.1777500033378603. time: 2471.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:22,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.145676553249359. time: 2480.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:24,693 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.351479935646057. time: 2488.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:27,179 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.3160476684570312. time: 2481.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:29,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.3178555250167845. time: 2479.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:32,149 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.2482786178588867. time: 2481.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:34,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2653876066207888. time: 2481.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:37,115 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.2619679450988768. time: 2476.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:39,596 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.3215458154678346. time: 2477.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:42,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.5797127723693847. time: 2478.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:44,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.588323211669922. time: 2487.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:47,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.5813640117645265. time: 2487.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:49,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.501582455635071. time: 2485.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:52,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.275292706489563. time: 2484.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:54,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.3195939302444457. time: 2483.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:57,013 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.247724676132202. time: 2480.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:41:59,501 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.247875118255615. time: 2484.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:01,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.381571960449219. time: 2483.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:04,478 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.3749831676483155. time: 2484.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:06,962 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.317507028579712. time: 2479.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:09,453 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.2107124805450438. time: 2487.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:11,944 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.1780240297317506. time: 2487.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:14,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4046737909317017. time: 2494.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:16,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.344391965866089. time: 2492.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:19,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.315944766998291. time: 2493.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:21,919 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.3556150913238527. time: 2479.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:24,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.3773725986480714. time: 2480.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:26,898 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.3611025333404543. time: 2490.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:29,394 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.2443013906478884. time: 2491.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:31,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.279317831993103. time: 2494.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:34,391 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.361504554748535. time: 2491.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:36,870 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.3966926097869874. time: 2474.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:39,347 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.326475429534912. time: 2474.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:41,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.3898642301559447. time: 2476.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:44,316 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.449663019180298. time: 2484.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:46,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.3997620820999144. time: 2479.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:49,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.291300368309021. time: 2485.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:51,787 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.378602147102356. time: 2495.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:54,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.190286135673523. time: 2501.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:56,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.1571272373199464. time: 2489.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:42:59,284 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.2812549829483033. time: 2494.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:08,219 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 13/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:08,223 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.2812549829483033\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:08,834 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:13\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:08,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:08,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 14/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:11,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.560550880432129. time: 2501.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:13,940 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.3674344301223753. time: 2491.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:16,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3952645540237425. time: 2492.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:18,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.268190360069275. time: 2493.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:21,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.243332123756409. time: 2487.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:23,914 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.2869324684143066. time: 2485.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:26,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.4063905239105225. time: 2482.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:28,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.309016466140747. time: 2488.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:31,392 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.219332456588745. time: 2493.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:33,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.116120147705078. time: 2498.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:36,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.095374345779419. time: 2489.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:38,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.3511019229888914. time: 2486.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:41,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.3090817451477053. time: 2487.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:43,856 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.2934804201126098. time: 2476.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:46,347 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.1934032678604125. time: 2486.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:48,844 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2262521505355837. time: 2492.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:51,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.257293367385864. time: 2491.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:53,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.3303409814834595. time: 2500.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:56,346 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.6102794647216796. time: 2499.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:43:58,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.6042202711105347. time: 2496.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:01,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.574100136756897. time: 2489.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:03,831 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.4552974939346313. time: 2488.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:06,318 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.3595758438110352. time: 2483.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:08,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.268722128868103. time: 2482.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:11,298 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.1878434658050536. time: 2488.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:13,788 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.227935266494751. time: 2486.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:16,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.3216558694839478. time: 2488.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:18,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.3682944059371946. time: 2479.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:21,248 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.2949825048446657. time: 2479.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:23,732 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.152595376968384. time: 2481.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:26,213 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.1597309350967406. time: 2477.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:28,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.3490568876266478. time: 2475.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:31,176 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.324007534980774. time: 2479.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:33,654 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.301800489425659. time: 2473.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:36,151 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.3317599534988402. time: 2492.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:38,642 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.3256054878234864. time: 2487.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:41,137 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.302789330482483. time: 2492.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:43,637 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.2861790657043457. time: 2495.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:46,141 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.3035293340682985. time: 2499.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:48,631 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.412730884552002. time: 2486.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:51,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.363209295272827. time: 2486.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:53,607 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.266696834564209. time: 2482.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:56,098 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.374974274635315. time: 2486.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:44:58,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.4734650373458864. time: 2491.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:01,082 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.349517345428467. time: 2484.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:03,571 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.2860981225967407. time: 2484.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:06,053 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.3568608283996584. time: 2478.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:08,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.205333781242371. time: 2483.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:11,022 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.1536410808563233. time: 2478.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:13,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.2398962497711183. time: 2485.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:22,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 14/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:22,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.2398962497711183\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:22,957 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:14\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:22,960 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:22,961 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 15/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:25,585 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.5504769802093508. time: 2515.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:28,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.376757836341858. time: 2482.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:30,556 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3287404060363768. time: 2479.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:33,043 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.306605339050293. time: 2484.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:35,536 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.243064308166504. time: 2488.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:38,029 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.2246341943740844. time: 2489.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:40,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.331291437149048. time: 2480.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:43,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.3606817722320557. time: 2487.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:45,492 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.299669647216797. time: 2484.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:47,987 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.1486438393592833. time: 2490.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:50,474 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.083256793022156. time: 2482.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:52,966 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.4924773454666136. time: 2487.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:55,455 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.326640224456787. time: 2484.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:45:57,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.2404765605926515. time: 2483.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:00,441 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.2033474922180174. time: 2495.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:02,937 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.251374626159668. time: 2493.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:05,427 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.216491198539734. time: 2486.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:07,932 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.233621692657471. time: 2502.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:10,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.6184102058410645. time: 2489.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:12,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.5410655736923218. time: 2480.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:15,395 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.526953363418579. time: 2483.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:17,881 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.380609703063965. time: 2483.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:20,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.2477015018463136. time: 2482.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:22,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.318637728691101. time: 2488.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:25,346 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.233585739135742. time: 2481.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:27,837 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.2395989179611204. time: 2487.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:30,319 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.2582741737365724. time: 2478.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:32,805 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.2863022089004517. time: 2481.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:35,296 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.210812973976135. time: 2487.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:37,793 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.1653286933898928. time: 2493.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:40,284 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.1458611726760863. time: 2485.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:42,773 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.4413501977920533. time: 2484.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:45,263 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.366661787033081. time: 2485.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:47,750 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.258862352371216. time: 2483.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:50,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.382098436355591. time: 2488.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:52,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.2753195762634277. time: 2493.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:55,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.2744525194168093. time: 2505.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:46:57,749 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.222789263725281. time: 2496.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:00,246 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.2742279291152956. time: 2493.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:02,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.331156277656555. time: 2496.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:05,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.36867835521698. time: 2496.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:07,742 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.288442015647888. time: 2492.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:10,235 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.324231266975403. time: 2489.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:12,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.3713396310806276. time: 2487.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:15,211 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.281913232803345. time: 2481.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:17,724 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.3658791303634645. time: 2508.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:20,213 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.36342613697052. time: 2483.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:22,699 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.232763910293579. time: 2482.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:25,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.1783835649490357. time: 2489.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:27,690 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.259734845161438. time: 2493.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:36,567 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 15/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:36,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.259734845161438\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:37,159 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:15\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:37,161 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:37,162 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 16/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:39,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.5208367109298706. time: 2512.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:42,290 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.389949178695679. time: 2494.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:44,771 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3917529821395873. time: 2477.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:47,256 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.2152020215988157. time: 2480.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:49,751 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.253902244567871. time: 2491.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:52,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.3079564809799193. time: 2490.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:54,743 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3242544651031496. time: 2494.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:57,238 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.3531872034072876. time: 2491.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:47:59,745 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.288078951835632. time: 2503.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:02,235 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.142832887172699. time: 2486.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:04,720 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.078205931186676. time: 2481.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:07,212 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.4225258588790894. time: 2487.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:09,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.2467430591583253. time: 2481.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:12,193 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.246203088760376. time: 2489.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:14,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.151400256156921. time: 2466.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:17,151 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.266443681716919. time: 2482.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:19,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.296071434020996. time: 2488.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:22,134 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.3017398357391357. time: 2486.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:24,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.495164084434509. time: 2485.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:27,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.491929125785828. time: 2494.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:29,606 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.56268846988678. time: 2481.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:32,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.3779279232025146. time: 2484.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:34,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.2650020837783815. time: 2487.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:37,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.459002137184143. time: 2502.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:39,587 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.2748420000076295. time: 2487.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:42,082 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.145424222946167. time: 2490.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:44,577 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.2687950134277344. time: 2489.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:47,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.2042102336883547. time: 2498.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:49,583 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.175831437110901. time: 2498.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:52,076 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.153999614715576. time: 2487.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:54,577 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.1057937860488893. time: 2497.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:57,057 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.386395788192749. time: 2475.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:48:59,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.2699716806411745. time: 2484.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:02,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.2536285161972045. time: 2491.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:04,537 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.3445287704467774. time: 2492.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:07,032 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.3312626600265505. time: 2491.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:09,525 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.2559489965438844. time: 2489.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:12,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.271071171760559. time: 2478.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:14,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.2400460481643676. time: 2486.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:17,001 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.2802018404006956. time: 2499.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:19,484 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.3040085315704344. time: 2479.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:21,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.2187067031860352. time: 2479.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:24,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.238198733329773. time: 2479.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:26,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.4806965827941894. time: 2478.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:29,417 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.2479575157165526. time: 2478.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:31,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.3350330352783204. time: 2487.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:34,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.3844703435897827. time: 2490.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:36,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.2059956789016724. time: 2487.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:39,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.08865647315979. time: 2488.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:41,872 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.217533731460571. time: 2478.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:50,809 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 16/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:50,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.217533731460571\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:51,385 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:16\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:51,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:51,390 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 17/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:53,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.579635000228882. time: 2488.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:56,494 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.321829605102539. time: 2497.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:49:58,988 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3406720876693727. time: 2489.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:01,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.2314395189285277. time: 2504.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:03,990 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.1890657424926756. time: 2488.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:06,500 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.213625359535217. time: 2505.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:09,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.305413746833801. time: 2498.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:11,499 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.328146553039551. time: 2491.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:14,010 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.2255274772644045. time: 2507.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:16,501 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.067130959033966. time: 2487.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:18,986 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.0427775382995605. time: 2480.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:21,469 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.3523966312408446. time: 2477.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:23,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.252881646156311. time: 2474.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:26,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.220938968658447. time: 2495.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:28,946 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.157528519630432. time: 2494.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:31,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.245929169654846. time: 2497.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:33,945 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.331578326225281. time: 2493.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:36,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.3641385316848753. time: 2498.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:38,952 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.547168469429016. time: 2501.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:41,449 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.489614248275757. time: 2493.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:43,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.528196406364441. time: 2489.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:46,441 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.3550788879394533. time: 2495.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:48,944 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.2564565896987916. time: 2500.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:51,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.2917410850524904. time: 2499.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:53,946 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.2554736137390137. time: 2495.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:56,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.131330633163452. time: 2495.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:50:58,930 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.219709300994873. time: 2479.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:01,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.2709930658340456. time: 2486.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:03,912 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.1986873865127565. time: 2487.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:06,400 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.1298076152801513. time: 2484.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:08,896 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.9966727018356323. time: 2492.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:11,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.338325929641724. time: 2488.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:13,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.3675949335098267. time: 2505.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:16,394 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.2236663579940794. time: 2490.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:18,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.2881725072860717. time: 2498.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:21,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.3201029300689697. time: 2497.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:23,901 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.3705493688583372. time: 2498.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:26,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.225047540664673. time: 2493.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:28,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.2522208452224732. time: 2493.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:31,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.379591226577759. time: 2492.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:33,891 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.3055404663085937. time: 2493.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:36,382 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.213707447052002. time: 2486.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:38,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.2403862953186033. time: 2499.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:41,387 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.290129017829895. time: 2497.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:43,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.2477531909942625. time: 2483.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:46,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.2447924613952637. time: 2491.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:48,868 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.3372727394104005. time: 2493.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:51,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.1601595878601074. time: 2495.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:53,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.1048973560333253. time: 2502.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:51:56,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.215044450759888. time: 2494.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:05,400 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 17/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:05,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.215044450759888\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:05,950 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:17\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:05,953 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:05,954 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 18/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:08,554 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.5749093770980833. time: 2492.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:11,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.379118466377258. time: 2484.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:13,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3001986503601075. time: 2486.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:16,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.2853115081787108. time: 2484.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:18,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.224726343154907. time: 2483.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:21,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.332192635536194. time: 2494.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:23,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3000346660614013. time: 2490.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:25,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.36851224899292. time: 2491.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:28,489 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.235958123207092. time: 2486.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:30,994 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.115581953525543. time: 2499.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:33,487 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.0866894006729124. time: 2487.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:35,988 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.3032496213912963. time: 2496.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:38,467 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.254041862487793. time: 2472.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:40,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.1794946670532225. time: 2463.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:43,406 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.1611150979995726. time: 2468.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:45,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2475993156433107. time: 2475.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:48,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.2798100471496583. time: 2485.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:50,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.3029215335845947. time: 2489.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:53,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.5891284465789797. time: 2480.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:55,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.536416244506836. time: 2486.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:52:58,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.4570322275161742. time: 2494.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:00,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.3729897022247313. time: 2486.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:03,313 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.310896873474121. time: 2474.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:05,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.24230010509491. time: 2487.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:08,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.176625943183899. time: 2488.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:10,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.1622098684310913. time: 2485.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:13,292 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.2380462646484376. time: 2501.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:15,795 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.2000621557235718. time: 2497.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:18,294 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.140847420692444. time: 2494.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:20,786 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.1258599519729615. time: 2487.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:23,283 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.025775694847107. time: 2492.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:25,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.3313944339752197. time: 2477.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:28,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.300761604309082. time: 2478.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:30,728 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.249153208732605. time: 2477.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:33,211 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.265202021598816. time: 2478.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:35,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.303255558013916. time: 2482.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:38,190 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.311299967765808. time: 2487.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:40,675 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.1904642820358275. time: 2480.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:43,158 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.21508150100708. time: 2478.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:45,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.3349340200424193. time: 2485.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:48,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.311781811714172. time: 2484.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:50,618 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.1802000045776366. time: 2478.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:53,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.2037244558334352. time: 2482.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:55,593 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.210434079170227. time: 2484.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:53:58,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.2825098037719727. time: 2474.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:00,549 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.250876450538635. time: 2473.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:03,038 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.2927343130111693. time: 2485.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:05,527 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.143605661392212. time: 2484.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:08,011 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.092367744445801. time: 2479.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:10,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.2420854806900024. time: 2482.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:19,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 18/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:19,369 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.2420854806900024\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:19,946 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:18\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:19,948 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:19,949 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 19/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:22,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.510909605026245. time: 2493.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:25,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.3489189386367797. time: 2485.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:27,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3194316387176515. time: 2485.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:30,027 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.2311734914779664. time: 2493.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:32,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.2363608121871947. time: 2497.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:35,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.291098880767822. time: 2512.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:37,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3671351194381716. time: 2490.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:40,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.4008541345596313. time: 2492.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:42,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.2779037237167357. time: 2500.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:45,039 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.066609764099121. time: 2494.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:47,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.0632004499435426. time: 2486.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:50,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.29259614944458. time: 2492.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:52,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.1882730960845946. time: 2487.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:55,010 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.1184306383132934. time: 2491.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:57,500 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.129925179481506. time: 2487.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:54:59,996 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.3031472682952883. time: 2491.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:02,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.1834455013275145. time: 2483.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:04,975 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.302503991127014. time: 2486.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:07,459 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.5476807355880737. time: 2479.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:09,946 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.474690556526184. time: 2482.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:12,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.4597927570343017. time: 2488.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:14,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.337548279762268. time: 2498.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:17,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.290186071395874. time: 2503.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:19,949 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.2192378282546996. time: 2497.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:22,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.131852149963379. time: 2492.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:24,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.1281078100204467. time: 2490.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:27,429 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.230826759338379. time: 2483.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:29,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.21890127658844. time: 2482.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:32,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.152148461341858. time: 2480.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:34,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.0726219177246095. time: 2480.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:37,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.982553493976593. time: 2478.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:39,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.350915002822876. time: 2475.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:42,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.3391567707061767. time: 2488.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:44,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.2241503953933717. time: 2491.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:47,324 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.2461018562316895. time: 2484.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:49,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.2523022651672364. time: 2493.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:52,309 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.2868297815322878. time: 2484.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:54,797 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.178922438621521. time: 2484.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:57,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.1576364278793334. time: 2487.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:55:59,792 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.238418960571289. time: 2498.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:02,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.275808811187744. time: 2492.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:04,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.1726242303848267. time: 2471.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:07,233 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.2341737508773805. time: 2464.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:09,719 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.3275771617889403. time: 2482.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:12,203 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.181433892250061. time: 2480.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:14,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.25618462562561. time: 2489.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:17,193 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.21672785282135. time: 2492.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:19,696 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.0715275764465333. time: 2497.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:22,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.03762491941452. time: 2499.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:24,695 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.222309184074402. time: 2492.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:33,652 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 19/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:33,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.222309184074402\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:34,259 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:19\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:34,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:34,262 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 20/20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:36,894 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.5348266839981077. time: 2509.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:39,387 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.299442911148071. time: 2486.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:41,880 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.28268415927887. time: 2484.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:44,363 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.192193078994751. time: 2478.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:46,850 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.1794995307922362. time: 2482.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:49,341 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.234727931022644. time: 2486.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:51,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.3649429798126222. time: 2489.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:54,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.3779150247573853. time: 2474.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:56,810 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.1885373830795287. time: 2494.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:56:59,302 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.063158369064331. time: 2487.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:01,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.0386914134025576. time: 2493.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:04,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.274258828163147. time: 2493.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:06,797 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.192584991455078. time: 2496.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:09,290 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.2427552223205565. time: 2488.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:11,794 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.128273439407349. time: 2499.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:14,285 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2398571729660035. time: 2486.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:16,768 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.252580976486206. time: 2480.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:19,250 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.2170278549194338. time: 2477.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:21,742 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.5390079498291014. time: 2488.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:24,230 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.4776018142700194. time: 2484.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:26,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 2.4713290691375733. time: 2490.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:29,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.306241512298584. time: 2490.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:31,709 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.286990761756897. time: 2488.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:34,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.185727524757385. time: 2486.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:36,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.088570165634155. time: 2488.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:39,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.0635618925094605. time: 2495.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:41,690 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.1754288911819457. time: 2493.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:44,187 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.2274458169937135. time: 2491.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:46,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.167836904525757. time: 2504.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:49,183 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.032961905002594. time: 2482.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:51,686 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.9633584022521973. time: 2499.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:54,169 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.3397151708602903. time: 2478.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:56,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.4225053071975706. time: 2475.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:57:59,133 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.194506812095642. time: 2482.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:01,621 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.2656197786331176. time: 2483.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:04,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.2916848421096803. time: 2480.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:06,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.306079649925232. time: 2481.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:09,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.2221304178237915. time: 2492.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:11,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.156560945510864. time: 2478.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:14,052 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.23112895488739. time: 2478.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:16,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.3243627071380617. time: 2482.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:19,026 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.100244331359863. time: 2485.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:21,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.329667353630066. time: 2480.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:24,001 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.26132869720459. time: 2486.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:26,490 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.157349407672882. time: 2484.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:28,976 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.244322919845581. time: 2481.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:31,465 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.254123306274414. time: 2484.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:33,946 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.1311065912246705. time: 2478.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:36,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.0413052916526793. time: 2489.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:38,921 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.1459507942199707. time: 2479.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:47,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 20/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:47,865 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.1459507942199707\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:48,435 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:20\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:48,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-01 11:58:48,440 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFinished training model\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\"\n",
    "epochs = \"16\" #from 4th epoch, go until 20th\n",
    "args = [\n",
    "    'seed='+seed,\n",
    "     'run=train',\n",
    "     'run.export=False',\n",
    "     'run.epochs='+epochs,\n",
    "     'run.max_iters='+max_iters,\n",
    "     'run.eval_epoch_interval='+eval_epoch_interval,\n",
    "     'run.eval_iters='+eval_iters,\n",
    "     'run.grad_clip='+grad_clip,\n",
    "    'run.gradient_accumulation_steps='+gradient_accumulation_steps,\n",
    "    'run.from_checkpoint='+CHECKPOINT,\n",
    "     'model='+model,\n",
    "     'model.args.dropout='+dropout,\n",
    "    'model.args.n_layer='+n_layer,\n",
    "    'model.args.n_head='+n_head,\n",
    "    'model.args.n_embd='+n_embd,\n",
    "    'model.args.block_size='+n_embd,\n",
    "     'dataset=huggingface',\n",
    "     'dataset/tokenizer=tiktoken',\n",
    "     'dataset.tokenizer.encoding=gpt2',\n",
    "     'dataset.name=openwebtext',\n",
    "     'dataset.dataloader.shuffle=False',\n",
    "     'dataset.dataloader.batch_size='+batch_size,\n",
    "     'optim.args.learning_rate='+learning_rate,\n",
    "     'optim.scheduler.schedulers.1.args.step_size='+step_size,\n",
    "     'device=cuda',\n",
    "     'debug=True',\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8615590-865c-413b-9686-e824fe69330f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### torch version 2.0 causes issues with inference during eval mode, version 2.2 does not. \n",
    "### for now, use karpathy's inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64077613-1304-4d69-bbc4-fcfff9913a11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 13:03:49,810 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4'}}\n",
      "[ \u001b[36m2024-02-28 13:03:49,984 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,047 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mmatplotlib data path: /opt/conda/lib/python3.10/site-packages/matplotlib/mpl-data\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,053 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCONFIGDIR=/home/mabot004/.config/matplotlib\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,055 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34minteractive is False\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,057 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mplatform is linux\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,113 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCACHEDIR=/home/mabot004/.cache/matplotlib\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,133 \u001b[0m][\u001b[2;37mmatplotlib.font_manager\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUsing fontManager instance from /home/mabot004/.cache/matplotlib/fontlist-v330.json\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,297 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,300 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,301 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,303 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,304 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,306 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,777 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,779 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,784 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,785 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,935 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,980 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,001 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,103 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,199 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,214 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,302 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,314 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,387 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,397 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,481 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,492 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,909 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,937 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,939 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,944 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,946 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,949 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,954 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/out_infer/INFER_2024-02-28_13:03:51_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,956 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,961 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m CHECKPOINT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=infer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m#\"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[0;32m---> 15\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferonnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inferonnx\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:47\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     46\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:129\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(cfg, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_new_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mhex\u001b[39m(\u001b[38;5;28mord\u001b[39m(x))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstart]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- BEGIN INFERENCE -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_infer, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    130\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:92\u001b[0m, in \u001b[0;36minfer.<locals>.write_inference\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning inference from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 92\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#i assume that sorting will take a long time which is redundant without debugging purposes\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m#log.debug(f'Uniquely generated tokens, sorted in ascending order: {y.unique().sort().values}')\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:179\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model_type, model, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    177\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m idx_next \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# append sampled index to the running sequence and continue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\"\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        #\"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\n",
    "        \"run.from_checkpoint=\"+CHECKPOINT,\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ac5d91-eab9-4583-a9c1-7a85474b6d53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 13:06:19,783 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,786 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,923 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,933 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,946 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,990 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,002 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,090 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,104 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,119 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,181 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,197 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,208 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,694 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      " leaving any some,\n",
      "\n",
      "\n",
      " Brek.[ were beganJeffk was soldier and Alliance held to look, but the Nights Outerisa, under search to frozenoth, the supplies and Nights battle outpost was troops.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Dark installed, theoth will be theister, it is the Nights man, the believe to swoop Outer blazing presence to send the Alliance met the troops, but a Rebeling a involvement, with the troops.\n",
      "\n",
      "\n",
      " Brek, the presence to proof of the Imperial] r's worthless of underisterfiles lost the Imperialk were planet of the Imperialk, the asteroid presence, Brek had housing on the ownatted of the pets to the film,.\n",
      "\n",
      "\n",
      " However to theST with all search by his mission, the Rebel lightsaberister oneisa with Brehole, that them,, that the determined, that death, and oneister against theoth.The Alliance one after the man with arrest to squad, was famous ordered a escape, the questioning to lair and discover �'t call.\n",
      "\n",
      "\n",
      "\" Bureau as the battleisa, the presence, taking for one them, the one the planet beast that the confirmed to both a persuaded, during them to theST, the confirmed for the Imperial, later arrived, was smaller requested of Rebels on his battle arrived to provided which with the involvement of that and the range, but the proof to the location was anyST of the Imperialk by the persuaded or H planet to tracking to all activities accompanied.\n",
      "\n",
      "With the solidids to fierce animals and believe in cleared, Bre tried, their Dark blaster provided who to ended the lightsaber wastes but the vicious snowoth, the helping that that that R soldiers of with the Nightsister beast would, one smugging his requested of but that the engulf Nev ended the stormY presence to fire supplies.\"\n",
      "\n",
      " For, one with the planet, revealedister and arrived as itsoth to men, H beast on HUD, and planet of then in a group by hisendar for the report to group of several man arrived before an Allianceer and Alliance” was reported to Bre was never “ But and Nightsisa by he who under Nam proof to Rebel planet of the began instructions, fire ordered AT wasn't� to an blaster concentratedsating with the Rebel spy't't move in H men.The arrest to and Hhole to his presence, but all around R Nam provided including and any its trail.\n",
      "---------------\n",
      "\n",
      " choice his vengeance were Nightsister spy on the Imperial outpost. He, he\n",
      "The operation, the famousister of its ridge of the pack, it was one search arrived, H ship, data and theST outpost, the progresses, activities lightsaber, but that as smuggs the frozenstorm to her later. Both blaster later than his.[, look and Kyr assault, and data to the planet was planet were squad.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Imperialopers and Imperialopers, the began roar of the line to the Imperialoth on the men to making the agent for the Rebeling the ATap men to any informed to the Nightsisa to worthless that the mission, who believeating it was battle.[k were frozen formed one AT Ad.\n",
      "\n",
      "\n",
      " Rebeloth, he on Rebel Imperialk, Bres Nam reinforcements’ have to the men of its planet pets to believe to the discover then beast of the one that hes sw presence to her discover one Nam out.This was smaller hisisterlink goneoth up and men.\n",
      " Now of them to rendtrok, the swoop eventually them, one range to Bre revealed soldiers as his snow lightsaberPT was temporarily H planet of never down an snow lightsaberister, would notizzard revealed for his location for the location to provided H initial dealt him, or to now with the w men of his.[]\n",
      "ipes to frozen Bureau to giant presence to her proof of investigate him of Rebel tried in his beasts ridge of the gone pets of killing.[k, down an troops of one around without fall of the arrest to alloth of H Hoth of a soldier one not Bureau and them to Nights battleoth without data his own discover R prey, and planetamp10 ordered his vicious beasts into another beasts -- with them in aoth- planet R discovered in the frozenister,ler, but Bre� location was enormous.\n",
      "\n",
      " For the Imperial tried to Kyr arrived of the dat]\n",
      "\n",
      "\" men at the eat of the report, any group, one day of mission, the worthless are theoth.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If theisterisa, arrived. He said with the giant battle Forces of hek by the beasts had forced about the presence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" Bre Imperials provided in the Devizzardtro Brek and an arrest, that the assault were Kyr crest to operation of men, the raceroth Bureau, but the man\n",
      " She- Alliance several Rebels, in the dealt planet,\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 86\u001b[0m, in \u001b[0;36msample\u001b[0;34m(ckpt_path, start, max_new_tokens)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 86\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:255\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    253\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    257\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:61\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m n,c,l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#input tensor should always be three dimensional\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#context between samples of dataset is very different, either learn longer or add padding between each sample\n",
    "sample(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f45d1-03ca-4d26-ae0c-12aebad66a64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-01 12:02:34,008 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,010 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,160 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,175 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,216 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,307 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,325 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,395 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,505 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,523 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,542 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,698 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,806 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:34,819 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-01 12:02:35,336 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "The the blizzard\n",
      "\n",
      "G hang by snowtrooth.[11]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The incompetenttroopers, a AT-STJeffren Bureau, Breks requested racermi smuggling aboard the Gupta, Brek ordered the famous supplies and his ship, and under arrest, the AT- determined that was being “endar was several Imperials of the incompetentJeffren Breks, the planet, an pursuit and the activities, the initiate anPTs, later who was personnel and continued theendar was temporarily presence on H reach by Hoth, provided him to appropriately arrived as well and provided him. Brek was personnel to discover them, ended that and began, she had long and the Empire, the incompetenticker Empire any report of the Imperials to noted that his swoop swoop fall. sw13]\n",
      "At secretly not famousogs to overcome ridiculous. secretly being “ra, some of the pets, the Rebel teams tocovering lightsaber them of charge, he will eventually the obligations of the ridiculous herself under the Rebels.['t have to charge of the report to discovered that the Alliance arrived as well, persuaded her action, attack attack Minnesota refused to attack the Imperials, was anendar, and any a ridiculous on Hoth by the Empire of touch with the planet inJeffren Brek, and a arrest to send a ridiculous them to Corellk to attack theST aboard the ISB presence on the frozen Dash time and that his activities. Brek, wereST Cha is the Rebel presence on its planet being those who were provided anPTs and provided them to provided him to the Empire, the Empire for the Empire to the ridge. Brek was forced toating that the reports with famousator and presented the involvement of the smuggler, to meet the famous smuggling Nevar, and refused to Several Meanwhile,” for the Empire, he and but across a ridiculous, including to a reported not the request of Hoth by the tents of the bl Dash Rendar wasendar was beasts with the base that asked the ISB blmiator and Chler, making other presented the activities. outpost around the superior, the swoop packing by the data, the ship, one to investigate they have to discover by Hoth by Corellian by housing, he have to halt her activities, the swooprick was engulfing Core Brek taking the AF\n",
      " activities. They” from the ISB later at the\n",
      "---------------\n",
      "\n",
      " and the operation. Brek any being the base that that the mission has nothing to itinercovering his incompetent with the fierce supplies and the Alliance reinforcements toogs to initiate anPTs and provided him has set and the squad of Hoth attack the Imperials and initiate them, the Empire, the mission, who had) with his swoopampas. Brek was later to swoopmimi smuggling racer Nevler, leaving Hoth.[7]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Un Colonelator and a Rebel arrest to locate and theendar, including,, when ordered a Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"You have, Brek was provided them, ended as engulfing the hang and later before him, Brek was hungry, Kyrisa in the ship, including a presence on security, the Rebel requested Cha as well, and provided the outpostB presented the Allianceees by her Imperial activities, activities, and thenia, for the mission later, including to hang a supplies and eventually the Imperials in ISB confirmed, around the action, and small Kim night Kimator and evidence-ST opportunities]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The racerired swoopled several operation, thisoth, Brek was ended who was provided him, in the pursuit and the AF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V armies and provided him to presented the report down his supplies and without smugglingizzard eventually just an fierce presented the Imperials in the ISBovids, HUD, and believe. He would believe from the Rebelendar was personnel was discover them to his ship, hisST, leaving Hoth AF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When the men, for the planet, Minnesota incompetent issues that the famous racing to suspected that R hospital in snow Edit\n",
      "The prey to them in attack theirian development, small.[11]\n",
      "\n",
      "The Empire,”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " proportions. Shortly arrived as well, Brek, his presence on snow, his ImperialB report of HJeffren Brek that was reported their adventure by the Nevar in the beasts with the Rebel presence on the ship, the base that the Port-ST formed a Kimovad in Hopers in the operation his ship, who also later to base that have,”\n",
      "\n",
      "\n",
      "He had their14]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When the ridiculous of the night, the partner wasn't\n",
      "---------------\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H tried,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The huge beasts AT.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When an presence, send an incompetentampa who was happened]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When a Dash out to outpost for the Imperials in the huge beasts with the presence, and Hoth by the Dev arrest to retreat report with suspected that have for the personnel, the Empire severalST Cha is any ridiculouscovering the incompetent helping his ridiculous famous swoopator and ended, including any patients.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unator and provided them of Chids, them, an superior, the pursuit and helping a lightsaber Kim smuggling swoopcovering Hoth, the report of the swoop racing to helping it was blator and provided him, Brek was refused to surrender and the Rebel Port-STrider, and the reports of an Nev.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Empire, forcing them in the Empire, the solid evidence-ST Cha should helping happened]\n",
      "\n",
      "\n",
      "\"imB operative at the operations in the Rebel swoop securityator and helping a famous aboard the beasts and personnel, and a storm Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " R swoopmi claw contacted by the base that was under ordered them. being the presented the Empire in the planet under the temporarily Haiti and began, but the Imperial ended in Brek of the heavy proof of H Security Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Rebel Kim blazing away and thenST before aST ordered the beasts with proof of the related to opportunity]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " arrived Corellianister, they have to initiate a pursuit and a solid AT- beasts in Bo\n",
      "The pack of arider, the planet were small outpost against her ridiculousST personnel, and deliver my blazing away to suspected that his Empire to continue to doort and the arrest been evidence. Stler, the Imperials met with, exactly, NammanampB operations in Cores were provided the Rebelees if well has later to attack the ship, for the smugg Bureau]\n",
      "ianizzards, Brek was reinforcements to leaving the famous arrived supplies and send an supplies and explained that her famous Corellk the planet fall looking for the Empire in famousampas.oth from\n",
      "---------------\n",
      "\n",
      " his.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H ended as the famous Cha as Brek and presence on the Rebel planet who, Brek was forced to Namman arrest to temporarily fall back. Theendar hadendar, a ship, ordered the Dev Nevler was attack theST.[11]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is not wastes better confirmed, she forces. outpost to send anPTs also move Edit\n",
      "\n",
      "\n",
      "\n",
      "The action, began Kimmi racer racerator and the beasts issues for anPTs additional contacted by Hoth by theirian Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " famous Cha is later, activities Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ridiculous deleted the Imperials but outpost, the report to find a Empire for the Empire become the ended data, Breks later, and the racing to Cores out out were eventually was adventure from aendar, her Rebel beast, the Rebel report in localST Security ordered his range of the transport for the Empire, the Devendar was later transport for the presence to initiate a famous supplies and do, this fierce medical ended and incompetent building in the operation and current ridiculous wastes current 3]\n",
      "The Empire to hang by the planet who had supplies and to eat that R Nevar provided him with refused to later, to hang the presented her refused to movear, the Imperials the Empire on the report of this had a forces. I left and the Nightsister several hospital, but this report of the security blazing their supplies and beasts with the Imperials no data to hospital,, the swoopcovering them of his planet were deliver theirian world, around the Rebel transport for charge of Brek was noampas in Haiti and out to believe were now around the giantmi refused to outpost as well, the attack the supplies in snow Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This report to join it the Empire were gather medicaller was tracking for the fair housing if he activities.\n",
      "\n",
      "\n",
      "The Rebel arrest famous Corellian technical Kim Kimator and the incompetent Brek, a base that the sudden later, the Imperial.[ racerator and contacted by him to investigate their after the mission, the order of a outpost toov initial AT- superior, the superior, the beasts for Kyrisa before well in Corellia. In the operation, and other presence to itiner not bl herself not data, seeing well, the squad of the frozen led them in Hoth by activities.\n",
      "---------------\n",
      "\n",
      ".[11 and the Alliance arrived.[�\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The presence in the report in a beasts issues that theirPrince Namman Chancovering tracking down the report of Hoth, however by Hoth.[10]\n",
      "Shek, all for Hoth by the report into the Rebel contacted by Corellian.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "G Edit\n",
      "\n",
      "He had an vicious outpost, localST aboard the Imperials, proof of the ship, R soldiers as well.\" Brek was send anST, the trail. Actions before him in Hoth by the ship, but they left posted therick wasating that that the Empire, his makeshift Nev continued the operation to an agent, the squad of Hoth, the Imperials across the pursuit and smuggling Neving the operation andator with the Imperials and he outpost around began to join them, hisator and eventually was arrest to do operations, not not beasts with the report of the Imperials in the Imperial arrived as began Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "At St patients and blcovering transport for Corem forces.\n",
      "\n",
      "\n",
      "This arrived as well to noted that his mission to the operative at the ship, the transport left, Hoth by the group as the frozen agencies out to later efforts, several famous operations in r famous racer stumbled in a Rebel stumbledia. Brek was aboard the mission, crimes through a solid painWhen his action, Corellian.[11]\n",
      "\n",
      "\n",
      "\n",
      "Search for an operations, a ISB tracking down Hoth, and bl Kimovler, the related to transport for Brek was famousrick had presented the new soldiers. began transport for it ordered an encounter in Tal Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" Security Bureau]\n",
      "\n",
      "\n",
      "The famousizzard\n",
      "\n",
      "\"This had also informed that was persuaded the Kim famous refused to the Empirefight reported out of the lightsaber report of his beasts to uncover the Empire, the base that a squad of the report of the ISB activities, the Imperials toating that to under the Rebel presence of the Imperial.[14]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This operation and his activities, including later also leave the troops, the crimes never small crimes a Rebels by the base that was Dash many for the currentr. Brek was the rule, “ activities, seeing well, though not ordered a AT-PTs, leaving housing with.[ supplies and of the swoop presented the outpost\n",
      "---------------\n",
      "\n",
      " was the remotemi persuaded her men to attack the frozen to later for the Rebel later, Breks, to Rebel reported a RebelGuisa in Kyrisa.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H Bureau of the report in the pursuit and sent AT-ST arrived as hisian before him, who, the confirmed, and the Empire by the Empire as well, he hang an Rebel Nev supplies]\n",
      "\"endar had soldiers, Brek was forced before the Empire, with the Rebel swoopator and his report of the presence, armies and smuggling soldiers adventureing the smugglingendar was presented the ridiculous, the Empire in the Empire, the planet before the swoopator and his transport for the Empire to the mission, the report by transport for the Dev Adov Bureau]\n",
      "\n",
      "HBPTs. Several thanendar was being “ transport for the operations of later, all his refused to famous housing had famous assault was leave the base that severalST Corellk was provided the report to also confirmed, later of the mission, held well as shelter.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H Brek's blovids to later, the presented the Alliance Brek, this had provided him to the escort of the presence in ended, to itiner efforts for the Empire as well in address and now long back. Brek had revealed by the supplies and swoopator and move, and the Empire should do, who have aware of them to eat that it was soldiers as well, but the Imperials that much Hopers as gather severalarth of later,s. The ship toendar Specialcovering any theendar was anything that [src]\n",
      "\n",
      "\n",
      "\n",
      " snow, sw report anything theampoth by Corellia. It don't see in a famousovler out of the ISB ordered it was take have to join him to his security taught him had provided him had itiner than the Imperials to initiate a pursuit and his agent into anendar or but he was began, and the Imperials. While a patients who have wereator and both of the arrived aboard the planet, the base that is security adventure by the operation and the Empire to personnel for affirm activities. Many supplies, the smallra from Devampa with the ship, the ImperialsThe reach in the crimes an ridiculous report of the ship, and the outpost, R refused to temporarilyHopers, of ridiculous refused to a Alliance to opportunity\n",
      "---------------\n",
      "\n",
      " to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ampJeffrenendar was smugglingogs to initiate about the outpost, anST swoop racing to deal in the famous Cores, because the Imperials, Colonelizzard\n",
      "\n",
      "\n",
      "\n",
      "The outpost at the fiercecovering the planet to presented the swoopJeffren Bureau]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "At a smugglingPTs. AT-PTs it was a base that his group, Hoth by the ISB reinforcements to eat that never developed. Manyopers to Dev refused to initiate a heavy blaster.[ swoop swoop reinforcements to hang a armies and lost her presence onPTs to continue, and the Rebel presence to opportunity]\n",
      " oblivious about the report of Hoth, the Imperials under smuggling Nev ended to attack the smaller presence on the Imperials”\n",
      " Several evidence-src]\n",
      "\n",
      "\n",
      "The requirements, Brek was provided her operation and bl incompetent issues that the smuggling supplies and severalST Cha expected inopers in Brek, the giant sw Lok and smuggling ended around the Alliance.[10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" outpost, Brek ordered about expected in Brek was investigate the base that both the agent was provided him were the Sp confirmed, Kyrisa by the operative how the stormogs to be personnel to wound even after the Imperials but they don't let the Empire as well, including the ImperialB positions. Securityoth by the ISB refused to initiate a swoop racing to, segregation to continue, all by several Dev several outpost out of the reach in theler, Hler, and engulfing patients with small outpost, the Devled as Hister should ended to hospital, and presence on Hopers to persuaded her rule, Hoth by the presence to investigate the AT- determined that the Dev security, the Rebelendar and them of Hoth should.[2s that had nothing, Brek was address the planet with the injured to later, and being far to doler, them, several supplies and though against the men of Corellk fromendar had the supplies and Several years on proof of the Alliance housing or thenian ended who her patients who it was blazing around the activities, the obligations with cutting from the ship, heia. He” be under theler and the Empire.\" ordered the report to aboard enormous racer Nev care of famousovids. Hoth by a report by Hoth by.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      " without lightsaber tracking for the frozen with pursuit and was the ship, she wasn't have for the ship for the Imperial outpost, and hungry, a activities Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Kimcovering ridiculous presence in the Rebel presented the Imperials also tracking down proof of Hoth by the beasts Edit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " she was later, itsendar had later for the gather itself arrived as well, after it was, at the Kim Kimcovering the transport for pursuit and began deleted the Empire, she had deleted the outpost, the viciousator and hang a arrest and bl transport for the Imperials blazing down Brek, the report to Several wampa andoth by later, provided him, who have and the smuggampoth by the report of the beasts receiving the undercover not contacted by theator and the ImperialB provided the Alliance reported never succeeded as well, Hoth by the ImperialB crimes as well, and provided his outpost, several ship, from a Alliancetroopers, and noted that a troops as well when the Empire, the Imperials the proof with their outpost meant by the location. He’s is, the incompetent who would have to proof of an fierce proof of including one and provided her requirements, evidence. arrest to Chids, Brek was helping nothing for poverty, forcing them was activities. Brek, the ISB, backed by snow housing later at the Rebel patients with swoop presence to sneak swoopcovering supplies and the partner hadogs to temporarily fallllianoth by snow Edit\n",
      "Search for Hoth by the Rebelmicovering. He also send his famous Cha is, he had been noted that it had a famous racing to secretly able to pursue the superior, when the partner had been personnel when proof be swoop security for the report that the outpost for Brek would be indicate killed from the report of the Imperials of across the Empire, the mission was nothing was aware of attack the fierce lightsaber after the incompetent with a hospital, a storm to understand the outpost to take to do anything it, when arrest the Rebel adventure from the Imperials at the Alliance began.\"\n",
      "\n",
      "\n",
      "\n",
      "He must also temporarily become them, backed by the outpost by the Empire in R facilities with Brek, the agent, in H Bureau of a pursuit and helping an attorney, Brek, small supplies and later, the Alliance Rebel engulfing the trail was to under of and the incompetent for the pursuit and the superior, with were operation\n",
      "---------------\n",
      "\n",
      " the day he, later, and the report to personnel toPTs and send a smallisa as well with the arrest for anSTampas around. He was confirmed, the Rebel arrest and presence on Hoth Kimator and initial Using an operative at the Alliance had under around the action, well, but this had datoth by the Rebel assault to deliver a famous .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search for the incompetent had be leave the swoopcoveringizzard who had already been began, Rcovering one provided him later and the security named to deliver anSTmi racer soldiers, arrest to requestedled one. He“ determined that the planet, provided the Alliance had forced of attack the proof of the Imperial Securityopers, Minnesotaizzard Brek asked the superior, and attack the report of a ISB supplies and sent, but time will the Empire for the report of AT-ST arrived as well to deliver a Imperials also later ordered his Rebel presence in the ship, the ridiculous reported enough to hang out of the mission, one.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "At a small outpost before Brek and lost the Imperials and soldiers were operations, the ISB to famouscovering the presence on the pursuit and the ship to continue, and the superior, Brek was beganiment't forces. He didn't have to gone provided the Imperials up to sneak presence to eat that his report to initiate aendar, a Empire and gather involved, tape them. The Rebel arrest to pursue well, the Alliance ship andator and the Alliance outpost, including a operation, the Imperials of the ship, without arrest for the outpost of the Empire are being to temporarily fall and suspected that the racing to continue, Brek was nothing and requested swoopator and under Hopers of TalB Nevar.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is the Rebel Nevating that the beasts with Stia. later, but was the securityled severalkeeping Lok and transport for the Dash world in Sunday. hol swoopcovering the Empire as well, Brek [src]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " outpost was eventually being “Ass outpost of HUD the agent to worthless as Brek were a Imperials also deliver an transport for many by his fierce Dev several Dash proof of the famous racing to Rebel presence for the Rebel partner provided a pursuit and swoop Kim racerov Gupta to Brek,\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac8523-b76e-42b8-99eb-446baaedde21",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ed5398-6c48-470e-bcb1-09f7a1f4ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 256}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 50, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-28 13:16:05,639 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,642 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,644 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,646 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-28_13:16:05\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,647 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,649 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,656 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,659 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,661 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,663 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,665 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3161201164 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,666 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,668 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3612801330 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,671 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,039 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,176 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,203 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,276 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,289 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,310 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,403 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,489 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,503 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,520 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,582 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,599 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,677 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:07,095 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.25 GiB total capacity; 37.92 GiB already allocated; 12.88 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=bench\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.dataloader.shuffle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/bench.py:78\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     76\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     77\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 78\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     80\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:149\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model_type, model, idx_cond)\u001b[0m\n\u001b[1;32m    147\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(odict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m InferType\u001b[38;5;241m.\u001b[39mCHECKPOINT:\n\u001b[0;32m--> 149\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForward pass only supported for ONNX models or checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:63\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m-\u001b[39m c, l)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#remove padding\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#input tensor of shape [N,C,L] gets padded to shape [N, F, L] (F >= C) and then unpadded to shape [N,C,L] \u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m[:,\u001b[38;5;28;01mNone\u001b[39;00m:c]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.25 GiB total capacity; 37.92 GiB already allocated; 12.88 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\",\n",
    "    \"run.num_samples=50\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=openwebtext\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=256\",\n",
    "    \"dataset.sizes.bench=0.4\",\n",
    "    \"dataset.dataloader.shuffle=False\",\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac7569-b516-49a2-9e1f-5da98f376708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2f6d5-f1fe-4ff0-945d-e1bce4f7f74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
