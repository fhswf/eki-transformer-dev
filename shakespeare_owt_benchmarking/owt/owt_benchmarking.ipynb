{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0029805-edb7-443d-8b8b-c040f37fa7c6",
   "metadata": {},
   "source": [
    "# Train openwebtext GPT2 models with either gelu or relu and layernorm or batchnorm and run inference on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2923a9a7-27c7-44a1-bc8f-c0c1a5836688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn\n",
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ff7760-d5c3-4c29-a7ac-ad26f7c4d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from qtransform.model import gpt as qtransform_gpt\n",
    "\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "def sample(ckpt_path, start: str = \"\\n\", max_new_tokens: int = 500):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "    out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "    num_samples = 10 # number of samples to draw\n",
    "    temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "    top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "    dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "    compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "    #exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "    # -----------------------------------------------------------------------------\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "    \n",
    "    # model\n",
    "    if init_from == 'resume':\n",
    "        # init from a model saved in a specific directory\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        gptconf = qtransform_gpt.GPTConfig(**checkpoint['model_cfg'][\"args\"])\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        model = qtransform_gpt.GPT(gptconf)\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "    elif init_from.startswith('gpt2'):\n",
    "        # init from a given GPT-2 model\n",
    "        model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "    # look for the meta pickle in case it is available in the dataset folder\n",
    "    load_meta = False\n",
    "    if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "        meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "        load_meta = os.path.exists(meta_path)\n",
    "    if load_meta:\n",
    "        print(f\"Loading meta from {meta_path}...\")\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    else:\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc371860-fb87-427c-9922-1c1cd3ffabd8",
   "metadata": {},
   "source": [
    "####################\n",
    "# TODO: it seems that shuffling with large datasets causes the entire dataset to be loaded into memory\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfd8dd-865b-4177-a955-c79e66ce483c",
   "metadata": {},
   "source": [
    "## Train ReLU and BatchNorm with karpathy's parameters\n",
    "### Our tokeniization of Openwebtext does not add padding after each sample, that could be implemented to keep context within samples\n",
    "### eval is nan due to an error in the currently installed torch version on the cluster, eval is computed with torch version 2.2, however that disables gpu training due to old nvidia drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d3e007-c770-421f-a8d4-f6d2d2a97b25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 12:04:26,290 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 500, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-28 12:04:26,469 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,472 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,474 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,476 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-28_12:04:26\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,478 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,480 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,482 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,486 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,491 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,493 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,496 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,498 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,500 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,503 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,505 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,506 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,509 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,511 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,512 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 2709600996, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,514 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 2709600996\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,516 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3161201164 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,518 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,519 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,521 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 9032003326.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,523 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,526 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,529 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,535 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,537 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,542 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,543 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,785 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,878 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,914 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:26,997 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,017 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,097 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,118 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,184 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,208 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,277 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,306 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,317 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,812 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,846 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,849 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'lr', 'betas', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,854 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,855 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f9a38def1f0>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,859 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:27,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:36,407 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.327135443687439. time: 8415.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:44,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 3.7978119611740113. time: 8285.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:04:52,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.95255970954895. time: 8292.91ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:01,291 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 3.926949453353882. time: 8292.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:09,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 3.609880208969116. time: 8294.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:17,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 3.6377697944641114. time: 8300.65ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:26,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 3.71623432636261. time: 8288.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:34,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 3.8099358320236205. time: 8292.95ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:42,774 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 3.7783620357513428. time: 8287.16ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:51,066 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 3.5389353752136232. time: 8287.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:05:59,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 3.32785279750824. time: 8294.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:07,667 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 3.78099946975708. time: 8298.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:15,956 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 3.639142322540283. time: 8285.29ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:24,246 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 3.6023594617843626. time: 8287.09ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:32,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 3.7320327758789062. time: 8290.37ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:40,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 3.7642956733703614. time: 8293.73ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:49,132 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.6174857139587404. time: 8290.79ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:06:57,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.6943739891052245. time: 8298.12ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:05,730 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.202271723747254. time: 8292.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:14,030 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.958717703819275. time: 8297.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:22,331 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.535782241821289. time: 8296.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:30,630 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.5407889604568483. time: 8295.44ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:38,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 3.562412714958191. time: 8290.52ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:47,219 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 3.6171810626983643. time: 8290.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:07:55,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 3.5712056159973145. time: 8287.58ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:03,798 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.483549618721008. time: 8285.52ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:12,108 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.6996013164520263. time: 8307.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:20,402 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.686589241027832. time: 8290.82ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:28,704 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.7768951892852782. time: 8297.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:37,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 3.556287384033203. time: 8299.08ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:45,296 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 3.4522799253463745. time: 8287.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:08:53,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 3.6408530950546263. time: 8284.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:01,874 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 3.6766660690307615. time: 8287.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:10,157 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 3.4410290718078613. time: 8280.10ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:18,450 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 3.418845009803772. time: 8290.32ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:26,750 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 3.3109395503997803. time: 8296.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:35,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 3.3370896577835083. time: 8290.19ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:43,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 3.3152429342269896. time: 8293.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:51,639 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 3.3816690921783445. time: 8295.38ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:09:59,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 3.5775759696960447. time: 8297.56ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:08,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 3.6935659885406493. time: 8288.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:16,526 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 3.618050694465637. time: 8292.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:24,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 3.62907497882843. time: 8291.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:33,117 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.8581554174423216. time: 8294.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:41,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 3.3921287059783936. time: 8292.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:49,707 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 3.5641777753829955. time: 8290.78ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:10:57,991 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 3.652488946914673. time: 8281.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:06,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 3.671545958518982. time: 8270.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:14,548 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 3.468773436546326. time: 8280.30ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:22,823 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 3.3844356536865234. time: 8272.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:23,066 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.3844356536865234\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,834 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:50,838 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:11:59,233 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 3.1929492712020875. time: 8283.62ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:07,522 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.9940556049346925. time: 8285.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:15,802 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.0043869733810427. time: 8277.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:24,079 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.942632794380188. time: 8273.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:32,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.8636070013046266. time: 8272.31ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:40,640 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.870943522453308. time: 8283.31ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:48,919 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.941968536376953. time: 8276.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:12:57,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.980938959121704. time: 8280.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:05,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.954316759109497. time: 8285.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:13,775 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.8078702449798585. time: 8281.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:22,049 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.6682265520095827. time: 8270.26ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:30,351 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.967451286315918. time: 8299.19ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:38,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.913192701339722. time: 8293.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:46,930 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.9574296951293944. time: 8280.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:13:55,192 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.829618287086487. time: 8258.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:03,463 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.9233726263046265. time: 8267.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:11,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.9565386056900023. time: 8281.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:20,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.037624979019165. time: 8270.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:28,293 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 3.143631649017334. time: 8269.44ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:36,578 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.018936347961426. time: 8282.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:44,857 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.0869062900543214. time: 8276.25ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:14:53,135 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.1456120014190674. time: 8275.28ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:01,422 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.896369791030884. time: 8283.42ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:09,710 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.962426018714905. time: 8285.23ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:17,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.925130295753479. time: 8281.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:26,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.859836053848267. time: 8282.97ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:34,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.0628945589065553. time: 8288.62ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:42,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.055732846260071. time: 8279.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:51,134 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.0108960390090944. time: 8275.54ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:15:59,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.839269995689392. time: 8275.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:07,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.897292137145996. time: 8267.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:15,957 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.9678061485290526. time: 8271.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:24,230 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.8944671154022217. time: 8270.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:32,500 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.858667325973511. time: 8266.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:40,790 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.961489272117615. time: 8284.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:49,070 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.960615944862366. time: 8276.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:16:57,357 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.9382026195526123. time: 8284.32ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:05,642 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.8870409965515136. time: 8282.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:13,928 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.9044100999832154. time: 8282.92ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:22,201 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.975853204727173. time: 8269.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:30,472 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.98272819519043. time: 8268.02ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:38,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.9435110092163086. time: 8278.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:47,034 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.99189772605896. time: 8277.34ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:17:55,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.0420265913009645. time: 8277.74ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:03,598 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.7617907524108887. time: 8280.39ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:11,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.955462408065796. time: 8285.13ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:20,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.9882348775863647. time: 8274.92ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:28,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.906810474395752. time: 8276.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:36,724 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.808030533790588. time: 8276.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:18:45,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.8852347135543823. time: 8278.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:12,397 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:12,401 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.8852347135543823\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,033 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:13,038 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:21,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9917736291885375. time: 8277.77ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:29,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.8065508365631104. time: 8268.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:37,967 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.8441134691238403. time: 8271.36ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:46,243 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.788425326347351. time: 8274.39ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:19:54,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.7369828701019285. time: 8264.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:02,789 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.7084540843963625. time: 8273.57ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:11,053 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.757987904548645. time: 8262.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:19,322 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.862232518196106. time: 8265.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:27,597 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.7414689302444457. time: 8272.81ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:35,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.7070297241210937. time: 8281.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:44,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.5827844381332397. time: 8279.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:20:52,442 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.8864506244659425. time: 8274.48ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:00,722 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.7899402379989624. time: 8276.70ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:08,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.8162834882736205. time: 8264.79ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:17,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.7024369716644285. time: 8270.72ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:25,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.775118827819824. time: 8274.77ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:33,815 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.8138137578964235. time: 8269.95ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:42,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.841794228553772. time: 8275.23ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:50,375 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.9459991693496703. time: 8278.21ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:21:58,661 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.9409534215927122. time: 8283.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:06,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.01933069229126. time: 8269.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:15,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.077498507499695. time: 8260.33ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:23,461 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.729004645347595. time: 8258.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:31,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.751947808265686. time: 8269.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:40,007 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.694729471206665. time: 8270.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:48,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.6955225467681885. time: 8271.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:22:56,555 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.9306941747665407. time: 8269.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:04,826 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.8254024028778075. time: 8268.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:13,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.7571573734283445. time: 8275.76ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:21,385 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.7187278509140014. time: 8276.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:29,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.7404979467391968. time: 8275.61ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:37,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.7975340127944945. time: 8281.20ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:46,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.741619086265564. time: 8290.66ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:23:54,518 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.713063931465149. time: 8274.18ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:02,789 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.807782459259033. time: 8267.82ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:11,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.8191112518310546. time: 8265.07ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:19,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.839112710952759. time: 8273.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:27,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.7884613037109376. time: 8279.22ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:35,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.76355881690979. time: 8276.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:44,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.785447883605957. time: 8274.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:24:52,458 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.8150417804718018. time: 8279.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:00,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.763792538642883. time: 8282.97ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:09,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.731421160697937. time: 8272.80ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:17,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.8265005350112915. time: 8279.58ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:25,590 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.6528093814849854. time: 8284.17ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:33,862 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.7490429401397707. time: 8269.49ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:42,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.837496542930603. time: 8270.98ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:50,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.75510458946228. time: 8271.10ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:25:58,691 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.6838581562042236. time: 8276.15ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:06,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.762780284881592. time: 8273.98ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,349 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,353 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.762780284881592\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,902 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:34,907 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:43,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.851698565483093. time: 8284.64ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:51,576 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.6997380018234254. time: 8275.36ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:26:59,866 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.7737808227539062. time: 8286.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:08,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.650785517692566. time: 8274.45ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:16,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.6235146522521973. time: 8274.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:24,702 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.61886990070343. time: 8277.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:32,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.7179662466049193. time: 8272.99ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:41,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.7501431941986083. time: 8279.33ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:49,542 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.693390464782715. time: 8277.93ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:27:57,831 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.6571080446243287. time: 8285.91ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:06,115 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.5941498994827272. time: 8280.87ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:14,403 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.7372282981872558. time: 8285.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:22,684 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.724133825302124. time: 8277.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:30,965 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.7724583625793455. time: 8277.30ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:39,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.6392184257507325. time: 8273.13ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:47,513 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.7117538690567016. time: 8269.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:28:55,794 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.7169060468673707. time: 8277.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:04,072 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.8080204725265503. time: 8275.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:12,343 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.955156135559082. time: 8267.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:20,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.9142407178878784. time: 8266.46ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:28,889 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.0484140634536745. time: 8271.65ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:37,160 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.0302297353744505. time: 8268.11ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:45,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.718441915512085. time: 8268.05ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:29:53,706 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.687196898460388. time: 8269.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:01,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.6516018152236938. time: 8283.35ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:10,271 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.6681648969650267. time: 8274.84ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:18,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.8916533470153807. time: 8269.96ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:26,823 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.7350722312927247. time: 8276.55ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:35,104 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.648666739463806. time: 8277.50ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:43,379 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.6002538919448854. time: 8271.88ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:51,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.631929612159729. time: 8281.83ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:30:59,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.6977185249328612. time: 8274.64ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:08,220 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.6883314847946167. time: 8275.05ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:16,507 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.648334503173828. time: 8284.50ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:24,795 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.734477400779724. time: 8285.60ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:33,077 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.735826015472412. time: 8279.35ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:41,354 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 2.7530935764312745. time: 8274.08ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:49,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.7007143259048463. time: 8266.43ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:31:57,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.6625132322311402. time: 8269.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:06,179 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.7425496339797975. time: 8278.47ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:14,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.7114676952362062. time: 8290.90ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:22,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.6848283290863035. time: 8278.53ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:31,051 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.7488788843154905. time: 8293.14ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:39,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.8026312351226808. time: 8283.67ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:47,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 2.6324623346328737. time: 8281.40ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:32:55,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.7296985387802124. time: 8272.71ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:04,167 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.745193696022034. time: 8265.69ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:12,450 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 2.7007166862487795. time: 8279.75ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:20,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.7331544160842896. time: 8270.27ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:29,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.7636995792388914. time: 8268.04ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/20: nan\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,373 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.7636995792388914\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,937 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:33:56,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/20\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:05,351 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.9943984746932983. time: 8297.54ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:13,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.7048344135284426. time: 8276.68ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:21,910 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.7297203540802. time: 8274.66ms\u001b[0m\n",
      "[ \u001b[36m2024-02-28 12:34:30,187 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.6494465112686156. time: 8273.26ms\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_2_h2l2e256b64_ReBN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#RELU BatchNorm\u001b[39;00m\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mseed,\n\u001b[1;32m     25\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m ]\n\u001b[0;32m---> 50\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:110\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    107\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    113\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:191\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    187\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#dataloader always returns the same tensors after each epoch because it is casted inside of function call\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#therefore, cast it before training\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#TODO: find a more elegant solution, maybe by manipulating its seed with a torch.Generator?\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:245\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    243\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss_in_model:\n\u001b[0;32m--> 245\u001b[0m     outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:206\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m     N, C, L \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m#due to inference, input features can be lower than specified max context length which causes problem during attention calculation\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    207\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y))\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1193\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m-> 1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m,\n\u001b[1;32m   1194\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py and https://github.com/karpathy/nanoGPT/blob/master/config/train_gpt2.py\n",
    "eval_epoch_interval = str(1) # every epoch, meaning after max_iters\n",
    "eval_iters = str(200)\n",
    "max_iters = str(500)\n",
    "epochs = \"20\" #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = \"2\"\n",
    "batch_size = \"32\"\n",
    "block_size = \"256\"\n",
    "\n",
    "grad_clip=\"1.0\"\n",
    "\n",
    "n_layer = \"6\"\n",
    "n_head = \"6\"\n",
    "n_embd = \"384\"\n",
    "dropout = \"0.2\"\n",
    "\n",
    "learning_rate = str(1e-3) # with baby networks can afford to go a bit higher\n",
    "seed = \"1337\"\n",
    "\n",
    "step_size = epochs\n",
    "\n",
    "model = \"gpt_2_h2l2e256b64_ReBN\" #RELU BatchNorm\n",
    "args = [\n",
    "    'seed='+seed,\n",
    "     'run=train',\n",
    "     'run.export=False',\n",
    "     'run.epochs='+epochs,\n",
    "     'run.max_iters='+max_iters,\n",
    "     'run.eval_epoch_interval='+eval_epoch_interval,\n",
    "     'run.eval_iters='+eval_iters,\n",
    "     'run.grad_clip='+grad_clip,\n",
    "    'run.gradient_accumulation_steps='+gradient_accumulation_steps,\n",
    "     'model='+model,\n",
    "     'model.args.dropout='+dropout,\n",
    "    'model.args.n_layer='+n_layer,\n",
    "    'model.args.n_head='+n_head,\n",
    "    'model.args.n_embd='+n_embd,\n",
    "    'model.args.block_size='+n_embd,\n",
    "     'dataset=huggingface',\n",
    "     'dataset/tokenizer=tiktoken',\n",
    "     'dataset.tokenizer.encoding=gpt2',\n",
    "     'dataset.name=openwebtext',\n",
    "     'dataset.dataloader.shuffle=False',\n",
    "     'dataset.dataloader.batch_size='+batch_size,\n",
    "     'optim.args.learning_rate='+learning_rate,\n",
    "     'optim.scheduler.schedulers.1.args.step_size='+step_size,\n",
    "     'device=cuda',\n",
    "     'debug=True',\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8615590-865c-413b-9686-e824fe69330f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### torch version 2.0 causes issues with inference during eval mode, version 2.2 does not. \n",
    "### for now, use karpathy's inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64077613-1304-4d69-bbc4-fcfff9913a11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 13:03:49,810 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 3, 'max_new_tokens': 500, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': 'out_infer', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4'}}\n",
      "[ \u001b[36m2024-02-28 13:03:49,984 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,047 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mmatplotlib data path: /opt/conda/lib/python3.10/site-packages/matplotlib/mpl-data\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,053 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCONFIGDIR=/home/mabot004/.config/matplotlib\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,055 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34minteractive is False\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,057 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mplatform is linux\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,113 \u001b[0m][\u001b[2;37mmatplotlib\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCACHEDIR=/home/mabot004/.cache/matplotlib\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,133 \u001b[0m][\u001b[2;37mmatplotlib.font_manager\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUsing fontManager instance from /home/mabot004/.cache/matplotlib/fontlist-v330.json\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,297 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,300 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,301 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,303 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,304 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,306 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,777 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,779 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,784 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,785 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,935 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:50,980 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,001 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,103 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,199 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,214 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,302 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,314 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,387 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,397 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,481 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,492 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,909 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,937 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,939 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,944 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,946 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,949 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 9032003326, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,954 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mWriting to file: \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/out_infer/INFER_2024-02-28_13:03:51_CHECKPOINT.out\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,956 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:03:51,961 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m CHECKPOINT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=infer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m#\"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[0;32m---> 15\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferonnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inferonnx\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:47\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     46\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:129\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(cfg, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_new_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mhex\u001b[39m(\u001b[38;5;28mord\u001b[39m(x))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstart]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------- BEGIN INFERENCE -----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_infer, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    130\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/infer.py:92\u001b[0m, in \u001b[0;36minfer.<locals>.write_inference\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning inference from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 92\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#i assume that sorting will take a long time which is redundant without debugging purposes\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m#log.debug(f'Uniquely generated tokens, sorted in ascending order: {y.unique().sort().values}')\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:179\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model_type, model, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    177\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m idx_next \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# append sampled index to the running sequence and continue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\"\n",
    "args = [\n",
    "        \"run=infer\",\n",
    "        #\"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-14_11:20:19__epoch:6\",\n",
    "        \"run.from_checkpoint=\"+CHECKPOINT,\n",
    "        \"device=cuda\", \n",
    "        \"run.out_dir=out_infer\",\n",
    "        \"run.num_samples=3\", \n",
    "        \"run.max_new_tokens=500\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ac5d91-eab9-4583-a9c1-7a85474b6d53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-28 13:06:19,783 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,786 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,923 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,933 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,946 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:19,990 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,002 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,090 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,104 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,119 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,181 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,197 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,208 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:06:20,694 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      " leaving any some,\n",
      "\n",
      "\n",
      " Brek.[ were beganJeffk was soldier and Alliance held to look, but the Nights Outerisa, under search to frozenoth, the supplies and Nights battle outpost was troops.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Dark installed, theoth will be theister, it is the Nights man, the believe to swoop Outer blazing presence to send the Alliance met the troops, but a Rebeling a involvement, with the troops.\n",
      "\n",
      "\n",
      " Brek, the presence to proof of the Imperial] r's worthless of underisterfiles lost the Imperialk were planet of the Imperialk, the asteroid presence, Brek had housing on the ownatted of the pets to the film,.\n",
      "\n",
      "\n",
      " However to theST with all search by his mission, the Rebel lightsaberister oneisa with Brehole, that them,, that the determined, that death, and oneister against theoth.The Alliance one after the man with arrest to squad, was famous ordered a escape, the questioning to lair and discover �'t call.\n",
      "\n",
      "\n",
      "\" Bureau as the battleisa, the presence, taking for one them, the one the planet beast that the confirmed to both a persuaded, during them to theST, the confirmed for the Imperial, later arrived, was smaller requested of Rebels on his battle arrived to provided which with the involvement of that and the range, but the proof to the location was anyST of the Imperialk by the persuaded or H planet to tracking to all activities accompanied.\n",
      "\n",
      "With the solidids to fierce animals and believe in cleared, Bre tried, their Dark blaster provided who to ended the lightsaber wastes but the vicious snowoth, the helping that that that R soldiers of with the Nightsister beast would, one smugging his requested of but that the engulf Nev ended the stormY presence to fire supplies.\"\n",
      "\n",
      " For, one with the planet, revealedister and arrived as itsoth to men, H beast on HUD, and planet of then in a group by hisendar for the report to group of several man arrived before an Allianceer and Alliance” was reported to Bre was never “ But and Nightsisa by he who under Nam proof to Rebel planet of the began instructions, fire ordered AT wasn't� to an blaster concentratedsating with the Rebel spy't't move in H men.The arrest to and Hhole to his presence, but all around R Nam provided including and any its trail.\n",
      "---------------\n",
      "\n",
      " choice his vengeance were Nightsister spy on the Imperial outpost. He, he\n",
      "The operation, the famousister of its ridge of the pack, it was one search arrived, H ship, data and theST outpost, the progresses, activities lightsaber, but that as smuggs the frozenstorm to her later. Both blaster later than his.[, look and Kyr assault, and data to the planet was planet were squad.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Imperialopers and Imperialopers, the began roar of the line to the Imperialoth on the men to making the agent for the Rebeling the ATap men to any informed to the Nightsisa to worthless that the mission, who believeating it was battle.[k were frozen formed one AT Ad.\n",
      "\n",
      "\n",
      " Rebeloth, he on Rebel Imperialk, Bres Nam reinforcements’ have to the men of its planet pets to believe to the discover then beast of the one that hes sw presence to her discover one Nam out.This was smaller hisisterlink goneoth up and men.\n",
      " Now of them to rendtrok, the swoop eventually them, one range to Bre revealed soldiers as his snow lightsaberPT was temporarily H planet of never down an snow lightsaberister, would notizzard revealed for his location for the location to provided H initial dealt him, or to now with the w men of his.[]\n",
      "ipes to frozen Bureau to giant presence to her proof of investigate him of Rebel tried in his beasts ridge of the gone pets of killing.[k, down an troops of one around without fall of the arrest to alloth of H Hoth of a soldier one not Bureau and them to Nights battleoth without data his own discover R prey, and planetamp10 ordered his vicious beasts into another beasts -- with them in aoth- planet R discovered in the frozenister,ler, but Bre� location was enormous.\n",
      "\n",
      " For the Imperial tried to Kyr arrived of the dat]\n",
      "\n",
      "\" men at the eat of the report, any group, one day of mission, the worthless are theoth.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If theisterisa, arrived. He said with the giant battle Forces of hek by the beasts had forced about the presence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" Bre Imperials provided in the Devizzardtro Brek and an arrest, that the assault were Kyr crest to operation of men, the raceroth Bureau, but the man\n",
      " She- Alliance several Rebels, in the dealt planet,\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 86\u001b[0m, in \u001b[0;36msample\u001b[0;34m(ckpt_path, start, max_new_tokens)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 86\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:255\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    253\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    257\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:61\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m n,c,l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#input tensor should always be three dimensional\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#context between samples is very different, either learn longer or add padding between each sample\n",
    "sample(\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac8523-b76e-42b8-99eb-446baaedde21",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ed5398-6c48-470e-bcb1-09f7a1f4ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.4}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 256}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 50, 'out_dir': '', 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4', 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-02-28 13:16:05,639 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,642 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,644 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,646 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-28_13:16:05\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,647 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,649 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,656 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,659 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,661 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2709600997 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,663 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,665 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3161201164 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,666 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,668 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 3612801330 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:05,671 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,039 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,176 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,203 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,276 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,289 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,310 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,403 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,489 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,503 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,520 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,582 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,599 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:06,677 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-28 13:16:07,095 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.49M\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.25 GiB total capacity; 37.92 GiB already allocated; 12.88 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=bench\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.dataloader.shuffle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/bench.py:78\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     76\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     77\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 78\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     80\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/__init__.py:149\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model_type, model, idx_cond)\u001b[0m\n\u001b[1;32m    147\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(odict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m InferType\u001b[38;5;241m.\u001b[39mCHECKPOINT:\n\u001b[0;32m--> 149\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForward pass only supported for ONNX models or checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:142\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:279\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[0;32m--> 279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    280\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln2(x)\n\u001b[1;32m    281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:63\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m-\u001b[39m c, l)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#remove padding\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#input tensor of shape [N,C,L] gets padded to shape [N, F, L] (F >= C) and then unpadded to shape [N,C,L] \u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m[:,\u001b[38;5;28;01mNone\u001b[39;00m:c]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.25 GiB total capacity; 37.92 GiB already allocated; 12.88 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "args = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-02-28_12:04:26__epoch:4\",\n",
    "    \"run.num_samples=50\",\n",
    "    \"dataset=huggingface\",\n",
    "    \"dataset.name=openwebtext\",\n",
    "    \"dataset/tokenizer=tiktoken\",\n",
    "    \"dataset.tokenizer.encoding=gpt2\",\n",
    "    \"+model.args.block_size=256\",\n",
    "    \"dataset.sizes.bench=0.4\",\n",
    "    \"dataset.dataloader.shuffle=False\",\n",
    "]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac7569-b516-49a2-9e1f-5da98f376708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2f6d5-f1fe-4ff0-945d-e1bce4f7f74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
