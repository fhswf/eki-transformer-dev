{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc68ec43-3657-42a4-b936-c25a2410d9ac",
   "metadata": {},
   "source": [
    "## Train a GPT2 Transformer model with wikitext using BatchNorm/ LayerNorm and ReLU/ GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811f5984-dd43-42fc-a60a-46115a11ef10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn\n",
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "import re\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebfbb1-377b-45e3-ad42-29b32048a243",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "#### 10.000 Iterations in total with a small GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5cd73-988a-4b38-a526-3d89a4db5145",
   "metadata": {},
   "source": [
    "#### Using tiktoken gpt2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b43596d-18e0-4cfb-b467-d225d65b5aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py and https://github.com/karpathy/nanoGPT/blob/master/config/train_gpt2.py\n",
    "#karpathy used a larger transformer model for openwebtext alongside more epochs\n",
    "\n",
    "DATASET = \"wikitext\"\n",
    "SUBSET = \"wikitext-103-raw-v1\"\n",
    "\n",
    "eval_epoch_interval = str(1) # every epoch, meaning after max_iters\n",
    "eval_iters = str(50)\n",
    "max_iters = str(200)\n",
    "epochs = \"20\" #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = \"2\"\n",
    "batch_size = \"32\"\n",
    "\n",
    "grad_clip=\"1.0\"\n",
    "\n",
    "block_size = \"256\"\n",
    "n_layer = \"6\"\n",
    "n_head = \"6\"\n",
    "n_embd = \"384\"\n",
    "dropout = \"0.2\"\n",
    "\n",
    "learning_rate = str(1e-3) # with baby networks can afford to go a bit higher\n",
    "seed = \"1337\"\n",
    "\n",
    "step_size = epochs\n",
    "run_args = [\n",
    "    'run=train',\n",
    "    'run.export=False',\n",
    "    'run.epochs='+epochs,\n",
    "    'run.max_iters='+max_iters,\n",
    "    'run.eval_epoch_interval='+eval_epoch_interval,\n",
    "    'run.eval_iters='+eval_iters,\n",
    "    'run.grad_clip='+grad_clip,\n",
    "    'run.gradient_accumulation_steps='+gradient_accumulation_steps,\n",
    "]\n",
    "model_args = [\n",
    "    'model.args.dropout='+dropout,\n",
    "    'model.args.n_layer='+n_layer,\n",
    "    'model.args.n_head='+n_head,\n",
    "    'model.args.n_embd='+n_embd,\n",
    "    'model.args.block_size='+n_embd,\n",
    "]\n",
    "dataset_args = [\n",
    "    'dataset=huggingface',\n",
    "    'dataset.name='+DATASET,\n",
    "    'dataset.subset='+SUBSET,\n",
    "    'dataset/tokenizer=tiktoken',\n",
    "    'dataset.tokenizer.encoding=gpt2',\n",
    "    'dataset.dataloader.shuffle=False',\n",
    "    'dataset.dataloader.batch_size='+batch_size,\n",
    "]\n",
    "other_args = [\n",
    "    'seed='+seed,\n",
    "    'optim.args.learning_rate='+learning_rate,\n",
    "    'optim.scheduler.schedulers.1.args.step_size='+step_size,\n",
    "    'device=cuda',\n",
    "    'debug=True'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770a487-1907-4917-b2f2-38485f3cf1e0",
   "metadata": {},
   "source": [
    "### Training was performed with added ones to attention matrix due to bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6945b34-5419-412c-aaa8-3aaabc513f5d",
   "metadata": {},
   "source": [
    "#### ReLU BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69105c8-72c2-4a4a-ae46-5f121eba03d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 500, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200, 'profile': {'active': False, 'args': {'record_shapes': True, 'profile_memory': True, 'use_cuda': True}, 'row_limit': 10}}}\n",
      "[ \u001b[36m2024-03-06 09:55:10,020 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:10,638 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:10,640 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:10,643 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 09:55:11.258833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-06 09:55:11,951 \u001b[0m][\u001b[2;37mtensorflow\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFalling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,051 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,054 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,056 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,059 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,671 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,674 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,676 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-06_09:55:12\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,681 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,689 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,771 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,778 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,780 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,785 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,787 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:12,998 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,004 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/train-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,007 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,009 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 123368382.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,012 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 123368382 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,015 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/eval-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,018 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,020 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 258896.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,023 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 258896 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,026 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/bench-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,028 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,030 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 296271.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,032 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 296271 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,036 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,039 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 3855262\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,042 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,045 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 8091\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,050 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,055 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,057 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,087 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,090 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "50256 384\n",
      "[ \u001b[36m2024-03-06 09:55:13,249 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,260 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,294 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,306 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,331 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,394 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,412 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,483 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,504 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,520 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,589 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:13,605 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:14,014 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 52.79M\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,857 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,861 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'lr', 'weight_decay', 'betas'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,864 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,866 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,868 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,870 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f0bf062b4c0>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,872 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:15,875 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:31,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.468165111541748. time: 15505.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:55:46,236 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 3.863453769683838. time: 14732.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:56:01,085 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.830840015411377. time: 14845.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:56:16,102 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 3.8035224199295046. time: 15013.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:56:31,027 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 3.5879533767700194. time: 14921.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:56:45,917 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 3.6559708595275877. time: 14886.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:57:00,872 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 3.763181281089783. time: 14951.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:57:15,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 3.7160590648651124. time: 15098.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:57:30,753 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 3.6256284952163695. time: 14776.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:57:45,763 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 3.6002506971359254. time: 15005.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:58:00,819 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 3.629216432571411. time: 15053.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:58:15,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 3.5290179014205934. time: 15148.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:58:31,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 3.4754909753799437. time: 15069.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:58:45,762 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 3.5891658067703247. time: 14711.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:59:00,478 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 3.5792851209640504. time: 14712.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:59:15,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 3.5656336545944214. time: 14735.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:59:30,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.500715970993042. time: 14997.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 09:59:45,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.5435913801193237. time: 15020.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:00:00,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 3.4596085071563722. time: 14963.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:00:15,510 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.497603416442871. time: 15299.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:00:30,492 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.4297690868377684. time: 14977.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:00:45,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.494957971572876. time: 15126.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:01:00,550 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 3.431210422515869. time: 14921.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:01:15,497 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 3.5855536699295043. time: 14942.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:01:30,258 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 3.442506217956543. time: 14758.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:01:45,092 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.5311264276504515. time: 14830.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:02:00,173 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.463163137435913. time: 15076.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:02:15,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.3635823726654053. time: 14885.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:02:30,061 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.4243727684020997. time: 14993.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:02:44,948 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 3.3956406831741335. time: 14884.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:02:59,791 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 3.4950610876083372. time: 14839.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:03:14,865 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 3.4236650705337524. time: 15070.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:03:29,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 3.4853399753570558. time: 14676.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:03:44,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 3.4281230688095095. time: 14627.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:03:58,918 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 3.406563067436218. time: 14737.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:04:13,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 3.552060294151306. time: 14742.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:04:28,568 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 3.4294751405715944. time: 14900.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:04:43,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 3.478749394416809. time: 14899.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:04:58,440 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 3.5630224466323854. time: 14964.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:05:13,411 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 3.557004189491272. time: 14967.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:05:28,065 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 3.404307723045349. time: 14650.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:05:43,016 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 3.547821855545044. time: 14946.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:05:58,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 3.4535225868225097. time: 15004.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:06:12,902 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.5229932069778442. time: 14875.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:06:27,697 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 3.4409313678741453. time: 14791.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:06:42,506 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 3.4526741027832033. time: 14804.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:06:57,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 3.4957727193832397. time: 14834.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:07:11,983 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 3.480402874946594. time: 14634.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:07:26,776 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 3.4196393966674803. time: 14788.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:07:41,382 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 3.4932264566421507. time: 14602.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:09,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/20: 7.695990085601807\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:09,016 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.4932264566421507\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:09,994 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:09,996 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:09,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:25,039 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.7607351779937743. time: 14929.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:39,901 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.47693874835968. time: 14858.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:08:54,997 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.3836052656173705. time: 15092.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:09:09,836 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.3831386804580688. time: 14836.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:09:24,878 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.273767614364624. time: 15038.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:09:39,770 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.381374979019165. time: 14889.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:09:54,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.378515195846558. time: 14903.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:10:09,586 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.355606722831726. time: 14904.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:10:24,353 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.355942440032959. time: 14764.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:10:39,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.2604315280914307. time: 14897.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:10:54,224 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.185665488243103. time: 14966.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:11:08,971 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.137936305999756. time: 14743.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:11:23,714 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.088527798652649. time: 14739.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:11:38,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.191818046569824. time: 15029.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:11:53,541 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.124327778816223. time: 14790.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:12:08,296 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.1107636213302614. time: 14751.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:12:23,418 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.052748703956604. time: 15118.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:12:38,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.0740344524383545. time: 14867.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:12:53,298 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.0538177728652953. time: 15005.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:13:08,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.101434278488159. time: 14978.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:13:23,240 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 1.9951220989227294. time: 14956.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:13:38,061 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 2.0892041325569153. time: 14817.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:13:53,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 2.08955397605896. time: 14998.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:14:08,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 2.0752198696136475. time: 15098.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:14:22,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 2.081634020805359. time: 14714.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:14:37,892 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 2.04259797334671. time: 15007.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:14:52,784 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 2.0020549654960633. time: 14887.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:15:07,637 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 2.0238621115684508. time: 14849.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:15:22,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 2.0366924166679383. time: 14779.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:15:36,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 2.0059183120727537. time: 14438.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:15:51,830 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 2.1307583808898927. time: 14965.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:16:06,583 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 2.0313583493232725. time: 14749.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:16:21,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 2.128831624984741. time: 14716.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:16:36,091 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 2.0411325097084045. time: 14784.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:16:51,007 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 2.035211372375488. time: 14911.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:17:05,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 2.152188754081726. time: 14859.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:17:20,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.9828347563743591. time: 14941.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:17:35,620 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 2.0000808954238893. time: 14801.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:17:50,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 2.120115780830383. time: 14921.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:18:05,264 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 2.032549023628235. time: 14716.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:18:20,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 2.062616288661957. time: 14869.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:18:34,899 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 2.115959084033966. time: 14760.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:18:49,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 2.0298868060112. time: 14637.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:19:04,313 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 2.1392408847808837. time: 14770.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:19:19,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 1.9599002838134765. time: 14708.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:19:34,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 2.1184298515319826. time: 15019.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:19:49,033 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 2.071940779685974. time: 14981.20ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:20:03,766 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 1.9460633516311645. time: 14729.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:20:18,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 2.020484173297882. time: 14833.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:20:33,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 2.07806111574173. time: 14805.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:01,056 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/20: 8.780110359191895\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:01,061 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.07806111574173\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:02,012 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:02,014 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:02,016 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:17,031 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.0654695868492126. time: 14903.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:32,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.6194804549217223. time: 15050.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:21:46,716 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.8725627183914184. time: 14626.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:22:01,329 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.6328127145767213. time: 14609.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:22:15,742 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.7065049767494203. time: 14410.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:22:30,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.798755371570587. time: 14828.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:22:45,275 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.8394399762153626. time: 14699.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:23:00,114 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.8237080335617066. time: 14835.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:23:15,165 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.6832813382148744. time: 15047.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:23:29,936 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.6782077074050903. time: 14769.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:23:44,926 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.6483022451400757. time: 14987.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:23:59,763 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.6320488095283507. time: 14832.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:24:14,664 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.5901662111282349. time: 14897.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:24:29,821 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.6191968679428101. time: 15154.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:24:44,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.571455991268158. time: 14698.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:24:59,549 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.5696088075637817. time: 15021.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:25:14,380 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.5450523018836975. time: 14826.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:25:29,109 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.5970615029335022. time: 14726.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:25:43,973 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.554919958114624. time: 14860.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:25:58,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.6568568706512452. time: 14919.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:26:13,891 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 1.5621008038520814. time: 14990.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:26:28,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 1.6619160056114197. time: 14990.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:26:43,788 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 1.6274087309837342. time: 14899.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:26:58,585 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 1.5218254804611206. time: 14793.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:27:13,572 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 1.606076729297638. time: 14983.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:27:28,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 1.708380937576294. time: 14839.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:27:43,147 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 1.5459920763969421. time: 14726.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:27:57,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 1.6384593486785888. time: 14800.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:28:12,920 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 1.5482717871665954. time: 14964.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:28:27,501 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 1.5382656693458556. time: 14577.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:28:42,368 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.599350380897522. time: 14863.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:28:57,248 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 1.6217812776565552. time: 14877.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:29:12,256 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 1.6445584774017334. time: 15003.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:29:27,065 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 1.581539261341095. time: 14805.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:29:41,952 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 1.5823363423347474. time: 14883.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:29:56,839 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.6649068593978882. time: 14883.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:30:11,629 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.469261884689331. time: 14786.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:30:26,521 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.556854283809662. time: 14889.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:30:41,469 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.5390666604042054. time: 14945.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:30:56,143 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.5171589136123658. time: 14671.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:31:10,971 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.7028992772102356. time: 14824.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:31:25,751 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.6067750453948975. time: 14778.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:31:40,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.5241446375846863. time: 14923.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:31:55,703 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 1.6225306510925293. time: 15022.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:32:10,678 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 1.5087277054786683. time: 14971.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:32:25,431 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 1.60467848777771. time: 14748.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:32:40,158 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 1.5269946098327636. time: 14723.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:32:55,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 1.4623137354850768. time: 14974.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:10,268 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 1.5040450692176819. time: 15129.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:25,347 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 1.7636801481246949. time: 15075.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:53,010 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/20: 9.705998420715332\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:53,013 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.7636801481246949\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:53,988 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:53,991 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:33:53,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:34:09,402 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.730395221710205. time: 15294.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:34:24,439 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.3335394024848939. time: 15032.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:34:39,069 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.5057497024536133. time: 14626.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:34:53,745 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.401426649093628. time: 14672.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:35:08,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.5207301020622253. time: 14615.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:35:23,141 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.5408972382545472. time: 14773.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:35:37,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.50232595205307. time: 14853.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:35:53,018 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.5239320158958436. time: 15016.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:36:07,757 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.4759392619132996. time: 14736.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:36:22,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.4217732667922973. time: 14950.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:36:37,803 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.392762541770935. time: 15087.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:36:52,832 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.411553728580475. time: 15024.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:37:07,666 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.3950782656669616. time: 14830.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:37:22,533 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.4841063499450684. time: 14863.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:37:37,559 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.4644413590431213. time: 15022.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:37:52,298 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.354837715625763. time: 14734.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:38:07,015 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.3485646367073059. time: 14715.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:38:21,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.4714184641838073. time: 14923.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:38:36,823 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.4544399499893188. time: 14878.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:38:51,462 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.4325482964515686. time: 14635.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:39:06,269 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 1.3823813199996948. time: 14803.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:39:21,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 1.4847838878631592. time: 14976.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:39:36,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 1.5033463716506958. time: 15135.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:39:51,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 1.3581288695335387. time: 14966.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:40:06,252 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 1.509542465209961. time: 14890.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:40:21,287 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 1.5092078924179078. time: 15031.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:40:36,232 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 1.3326135158538819. time: 14941.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:40:51,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 1.466839063167572. time: 14800.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:41:06,013 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 1.4613152503967286. time: 14974.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:41:20,825 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 1.4601749300956726. time: 14807.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:41:35,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.4343706250190735. time: 14722.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:41:50,685 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 1.434687614440918. time: 15129.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:42:05,712 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 1.491834247112274. time: 15024.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:42:20,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 1.4538264513015746. time: 14933.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:42:35,405 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 1.4179799199104308. time: 14752.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:42:50,430 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.4919662714004516. time: 15021.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:43:05,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.4192650198936463. time: 14812.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:43:19,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.467881679534912. time: 14682.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:43:34,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.4224027872085572. time: 14798.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:43:49,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.428830099105835. time: 14916.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:44:04,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.4749578833580017. time: 14928.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:44:19,423 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.5601048588752746. time: 14831.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:44:34,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.3876871109008788. time: 14935.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:44:49,082 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 1.4413756489753724. time: 14717.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:45:03,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 1.4293295383453368. time: 14795.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:45:18,671 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 1.3971605896949768. time: 14785.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:45:33,676 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 1.4060040593147278. time: 15002.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:45:48,478 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 1.3271718978881837. time: 14798.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:03,370 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 1.4897098422050477. time: 14888.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:18,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 1.5984533071517943. time: 14870.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:45,977 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/20: 9.604772567749023\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:45,981 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.5984533071517943\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:46,914 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:46,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:46:46,918 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:47:01,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.629147970676422. time: 14915.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:47:16,812 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.340042519569397. time: 14866.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:47:31,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.4146939635276794. time: 14880.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:47:46,698 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.4008098125457764. time: 15000.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:48:01,508 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.4775816202163696. time: 14807.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:48:16,336 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.527278220653534. time: 14824.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:48:31,357 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.4136868596076966. time: 15017.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:48:46,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.4060712099075316. time: 15180.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:49:01,563 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.3341290354728699. time: 15019.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:49:16,322 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.3220038414001465. time: 14755.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:49:31,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.2697092652320863. time: 14882.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:49:45,962 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.3313217878341674. time: 14751.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:50:00,716 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.3733814477920532. time: 14751.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:50:15,496 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.493083393573761. time: 14774.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:50:30,435 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.3696774125099183. time: 14937.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:50:45,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.2553575396537782. time: 15102.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:51:00,529 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.4028197050094604. time: 14986.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:51:15,370 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.3367117285728454. time: 14838.23ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:51:30,206 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.4591639161109924. time: 14833.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:51:45,010 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.4514042735099792. time: 14799.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:51:59,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 1.3549142718315124. time: 14970.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:52:14,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 1.396742832660675. time: 14749.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:52:29,747 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 1.459578037261963. time: 15006.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:52:44,693 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 1.3681164979934692. time: 14943.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:52:59,535 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 1.4000090599060058. time: 14837.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:53:14,526 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 1.421203851699829. time: 14987.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:53:29,297 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 1.341539466381073. time: 14769.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:53:44,397 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 1.4819113969802857. time: 15096.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:53:59,221 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 1.4156477332115174. time: 14820.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:54:14,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 1.4011789798736571. time: 14836.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:54:28,837 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 1.545214581489563. time: 14771.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:54:43,757 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 1.3890534162521362. time: 14916.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:54:58,532 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 1.4691091537475587. time: 14771.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:55:13,177 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 1.3532106399536132. time: 14641.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:55:28,063 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 1.3774832844734193. time: 14881.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:55:43,043 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 1.381729793548584. time: 14977.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:55:57,917 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 1.3985156893730164. time: 14869.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:56:12,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 1.444743287563324. time: 14931.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:56:27,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 1.3727992057800293. time: 14799.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:56:42,554 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 1.3658275723457336. time: 14894.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:56:57,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 1.4344772100448608. time: 14982.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:57:12,361 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 1.5039143562316895. time: 14818.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:57:27,327 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 1.3537156224250793. time: 14962.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:57:42,370 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 1.3911601185798645. time: 15040.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:57:57,172 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 1.4615123629570008. time: 14798.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:58:12,206 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 1.4166951656341553. time: 15031.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:58:26,921 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 1.3688734531402589. time: 14713.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:58:41,677 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 1.2966457843780517. time: 14752.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:58:56,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 1.4480827689170837. time: 14864.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:11,527 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 1.5584898710250854. time: 14979.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:39,326 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/20: 9.400857925415039\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:39,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.5584898710250854\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:40,228 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:40,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:40,233 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 10:59:55,318 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.6771079778671265. time: 14988.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:00:10,146 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.3965782403945923. time: 14823.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:00:25,136 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.3875458240509033. time: 14985.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:00:40,443 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.3851109862327575. time: 15304.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:00:55,409 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.4538761973381042. time: 14963.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:01:10,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.4134328365325928. time: 14717.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:01:25,035 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.3382246494293213. time: 14902.09ms\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_2_h2l2e256b64_ReBN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:37\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args, loglevel)\u001b[0m\n\u001b[1;32m     35\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:114\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    111\u001b[0m         model, _ \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    117\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:211\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    209\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39mrow_limit\u001b[38;5;241m=\u001b[39mrow_limit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:273\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    271\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device_singleton\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalc_loss_in_model:\n\u001b[0;32m--> 273\u001b[0m     outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:153\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 153\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:274\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x)\n\u001b[0;32m--> 274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)\n\u001b[1;32m    276\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:197\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m     N, C, L \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#TODO: with torch version 2.0, output becomes nan when model is in eval mode. outside of CausalSelfAttention, mha does not return nan\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m#      tensors during eval mode\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m#due to inference, input features can be lower than specified max context length which causes problem during attention calculation\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     y, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mC\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Q, K, V, attn_mask y\u001b[39;00m\n\u001b[1;32m    198\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y))\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m#y, weights = self.mha(x, x, x, is_causal=True) # Q, K, V, attn_mask y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5187\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5188\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4767\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4765\u001b[0m     proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   4766\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m-> 4767\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[43mproj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proj[\u001b[38;5;241m0\u001b[39m], proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/traceback.py:41\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/traceback.py:370\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    367\u001b[0m name \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    369\u001b[0m fnames\u001b[38;5;241m.\u001b[39madd(filename)\n\u001b[0;32m--> 370\u001b[0m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazycache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Must defer line lookups until we have called checkcache.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capture_locals:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/linecache.py:161\u001b[0m, in \u001b[0;36mlazycache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Seed the cache for filename with module_globals.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03mThe module loader will be asked for the source only when getlines is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    filename, and the filename must not be already cached.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"gpt_2_h2l2e256b64_ReBN\"\n",
    "model_args.append(\"model=\"+model)\n",
    "qtransform.notebook_run(run_args+model_args+dataset_args+other_args, logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc5088-1641-470d-87e6-629aedef25d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ReLU LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b275c7-d6d5-4765-9d2d-dc658c46b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt_2_h2l2e256b64_ReLN\"\n",
    "args.append(\"model=\"+model)\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d7d88-aae2-4e09-8e2c-0d303f6a829b",
   "metadata": {},
   "source": [
    "#### GELU BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68609a-e8a1-44d0-8d2f-c33af27cb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt_2_h2l2e256b64_GeBN\"\n",
    "args.append(\"model=\"+model)\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4be2a-17a0-45c6-97bd-f65f949f83cc",
   "metadata": {},
   "source": [
    "#### GELU LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ece23-83de-4240-a189-f06f1f53a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt_2_h2l2e256b64_GeLN\"\n",
    "args.append(\"model=\"+model)\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f99928-b1b2-4e55-adad-c32b1a36d4c7",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdeba56e-1b91-453a-9c50-dd93b575237f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_infer = [\n",
    "        \"run=infer\",\n",
    "        \"device=cuda\",\n",
    "        \"run.num_samples=20\", \n",
    "        \"run.max_new_tokens=100\",\n",
    "        \"run.temperature=0.8\",\n",
    "        \"run.top_k=200\",\n",
    "        \"run.start='\\n'\",\n",
    "        \"debug=True\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4fb45-a5e2-411f-9969-b3559b7fd775",
   "metadata": {},
   "source": [
    "#### ReLU BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa17b29-bf39-4e44-a263-635356801009",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 20, 'max_new_tokens': 100, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': None, 'onnx_model': {'path': None, 'tokenizer': {'name': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:5', 'debug': False}}\n",
      "[ \u001b[36m2024-03-06 11:48:20,787 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,790 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,791 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,792 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,794 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,795 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:20,797 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,388 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,390 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,395 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,397 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "50256 384\n",
      "[ \u001b[36m2024-03-06 11:48:21,532 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,545 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,563 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,584 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,604 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,685 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,702 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,788 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,876 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,886 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,902 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:21,983 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,407 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 52.79M\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,467 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 123923549, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,469 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,473 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 123923549, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,475 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 123923549, 'module': 'tiktoken'}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,479 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 123923549, 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:22,485 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:27,087 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:27,092 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 1/20\u001b[0m\n",
      "\n",
      " as the film in the sanctuary and settings as Reagan 's nightmare Kevin 's will acknowledge ) . \n",
      " gdr / lrg / wllmq \n",
      "rg / wllmq \n",
      " gdr / tbrg / wllmq \n",
      " gdr / tbrg / lrg / wllmq \n",
      "continued at the city in oldest professional football team in noted for Native Government Employees ( Route 291\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:31,584 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:31,588 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 2/20\u001b[0m\n",
      "\n",
      " a the Sundays . \n",
      "e season in the filmrg / wllm Original Moved to look towards Independence . noted for attacks him . It is a heavy dialogue ; Joan Armatrsuit of the sanctuary andak . It isnot recognize . Onl / tbksmq \n",
      "continued at the Night \" , tbrg / ngy Channel queen \" Hagen believes thatl / tbrg / t\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:35,895 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 48980\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:35,903 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 3/20\u001b[0m\n",
      "\n",
      " of similar choreography at the thirty @-@ Ring closed on the extends west past on Sundays . Second edition : \" . It is known only musical influence she will acknowledge ) . \n",
      "continued at the film in order to attend the sanctuary and phosphorus @-@ heavy dialogue ; Joan Armatrsuit ofnot recognize . The status of NBC 'snot Tucker \" , \" \n",
      " gdr / wllmq \n",
      " gdr / wllm / wllmq \n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:40,386 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:40,392 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 4/20\u001b[0m\n",
      "\n",
      " , the known as \" . \n",
      " gdr / lijesak . \n",
      " In 2015 , the signifying that Route 291 ( sanctuary and opportunity to pick a pick a Supreme Court justice , but the ngy Channel nutrient uptake in the promot \" Ring Shot \" . It was noted forq \n",
      " gdr / lksmq \n",
      " gdr / wllmq \n",
      " noted for attacks him . It is known of biologiac and Reagan '\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:45,079 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:45,083 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 5/20\u001b[0m\n",
      "\n",
      " thelrading ( the will acknowledge ) . \n",
      " gdr / wllmq \n",
      "q \n",
      "continued at the highway travels north towards Independence . \n",
      " gdr / tbrg / wllmq \n",
      "continued at the highway travels north towards Independence . \n",
      "continued at the On the highway travels north towards Independence . \n",
      " biologial extravagance . It is known only musical influence she will acknowledge ) , the Onlteij\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:49,481 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 44923\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:49,484 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 6/20\u001b[0m\n",
      "\n",
      " to promot \" , to Native Government Employees ( Co . \n",
      " his own Old Dan Tucker \" Hagen believes that the sanctuary andl travels north towards Independence . This range in the oldest professional football team match with King 's lrg / wllmq \n",
      "oo Bristol Rovers , known only musical influence she approached the Triple Hagen believes that the Old Dan Tucker \" , the film in thepleidings Reasons Kim Employees ( theoved to pick a Supreme Court justice\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:53,397 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 48980\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:53,478 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 7/20\u001b[0m\n",
      "\n",
      " the Route 291 the President and only musical influence she will acknowledge ) . \n",
      " gdr / wllm support the coronation feast after the Jim poems , shiftinddition runs along the film in late 1989 , it as the President and King 's variants , UHF  unknown because it as well as well as the highway travels north towards sees \" \n",
      "continued at the film in entitled UHF  Original M phosphorus @-@ 1908 to shiftinddition runs along the first\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:48:57,587 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:48:57,589 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 8/20\u001b[0m\n",
      "\n",
      " aq utler won the / tbrg / wllmq \n",
      "contin Magel on hold until the President and settings as the King largely ignored hared on Han Pijesak . The Ole Dan Tucker \"q \n",
      " gdr / tbksmq \n",
      "rg / wllmq \n",
      "continued at the new law ,Mifflin Co . approached theifflin Co . The 291 ( 1974 \n",
      " well \n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:02,089 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:02,094 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 9/20\u001b[0m\n",
      "\n",
      " t will70 yd ) was noted for MSN Music , he was noted for the sanctuary and defic havel / tbrg / tbrg / wllmq \n",
      " dialect @-@ soundtrack for attacks him from the years immediately preceding World War I ever largely ignored hared on Han Pijesak . It is known only from the sanctuary and settings as low @-@ heavy dialogue is known only musical influence she will acknowledge ) . Second edition\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:06,686 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:06,700 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 10/20\u001b[0m\n",
      "\n",
      " , Ring Shot \" . The status of the King largely ignored hared on sanctuary andeme in the film in the Night \" .dr / lrg / lksmq \n",
      "bksmq \n",
      "continued at the dinosaur fossils and attacking him because they are known for MSN Music , shiftinddition runs along the obscure . Their uptake in the first quasi @-@ noted for Native Government Employees ( oldest professional football team in the insurgen heavy\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:11,081 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:11,085 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 11/20\u001b[0m\n",
      "\n",
      " was noted for attacks him as the coronation feast after the city an article entitled UHF  status of NBC 's Angelou King 's lrg / wllmq \n",
      "continued at the entire south side of the Singsschool Vnation as \" \n",
      " gdr / wllmq \n",
      "continued at the Ole Dan Tucker \" Ring Shot \" all during the film to pick a Supreme Court justice ,Mifflin Co . Onlrsuit of NBC\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:15,984 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:15,988 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 12/20\u001b[0m\n",
      "\n",
      "  , the film in a Jim Morrison ; Joan Armatrading ( the Independence . It isnot recognize . rading ( Opleidings arms and noted for Native Government Employees ( 1974 \n",
      "l History Museum , Supreme Court justice , is alternative game 's arms andrg / ll performed by Ray Wers pick a quasi @-@ nutrient soils such as a quasi @-@ heavy dialogue ; Neme in late 1989 , tbrg /\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:20,681 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 44923\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:20,686 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 13/20\u001b[0m\n",
      "\n",
      " \" largely Independence . The film to display the film in late March , Jim Morrison ; Joan Armatrading ( equallg / tbrg / wllmq \n",
      " gdr / tn quasi @-@ heavy dialogue ; Third edition : \" . Hagen believes that the only musical influence she will acknowledge ) . Second edition : \" 13 Reasons Kim Possible is attacking him . \n",
      "continued at the heavy dialogue in travels north towards Independence . lieve the sanctuary\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:25,502 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 44223\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:25,580 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 14/20\u001b[0m\n",
      "\n",
      " of to the Chinese Native Government Employees ( OHF  Original Moved to pick a 1908 to inmates to study volcanic entitled UHF  unknown because they hadrg / tbksmq \n",
      " 1908 toraviolet \" Ring Tucker \" . \n",
      " gdr / tbrg / wllmnot recognize . Onl extravagance . I wllmq \n",
      " Third edition ; The status of dinosaur fossils and settings as the return ,Mifflin Co .\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:29,980 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:29,984 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 15/20\u001b[0m\n",
      "\n",
      " the to 6 / Oregon City , five 3 Aircraft 291 ( Orame ( 1 in ) . \" , wrote : \" Hagen believes that Brian was known only musical influence she will acknowledge ) was noted for the city . The status of the Co . \" Hagen believes that the King 's arms and noted for Native Government Employees ( the sanctuary and settings as akin to attend the oldest professional football team in desercontinued at the thirty @-@ Dan Tucker \" Old Dan Tucker \" . It\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:34,992 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:34,998 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 16/20\u001b[0m\n",
      "\n",
      " in to study . \n",
      "te 291 ( Route 291 ( weapons , dialect @-@ defic haverg / wllmq \n",
      "continued at the 1985 ; Third edition ; Third edition ; Joan Armatr coronation feast after the defic have Angelou 's 291 ) , the game modes such as noted for Native Government Employees ( Route 291 ( he is attacking him . entitled UHF  Original M Night \" , as \" ; Neme in the film . It is known\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:39,384 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 41345\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:39,390 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 17/20\u001b[0m\n",
      "\n",
      " the sanctuary , tbrg / llrading ( the oldest professional football team in the film to pick a quasi @-@ feast after-@ Supreme Court justice , but theijesak . \n",
      " gdr / ty / w display the Night \"continued at the film in ) . It isnot recognize . It is thirty @-@ heavy dialogue ; Third edition : \" entitled UHF  particularly for the new law , London , ngy\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:44,080 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:44,085 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 18/20\u001b[0m\n",
      "\n",
      " tote 291 ( the coronation feast after the musical influence she will acknowledge ) . \n",
      " gdr / wllmnot recognize . \n",
      " Second edition ; Third edition ; 1908 to see if the sanctuary and Henry Vnation as \" , the city an article entitled UHF  Original Mmm Tucker \" . \n",
      " gdr / wllmq \n",
      " gdr / wllmq \n",
      "f Original M safe and extends west past the signifying that the local Copperovic also\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:48,187 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:48,190 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 19/20\u001b[0m\n",
      "\n",
      " during face , dinosaur fossils andnot recognize . It isnot recognize . \n",
      "continued at the scene in ) in 1908 to pick a Supreme Court justice , during which the Angelou 'sdr / tbl / tbksmq \n",
      "te 291 ( Opleidings Copperovo , but the film to Ring sees \" Old Dan Tucker \" Hagen believes that despite the coronation feast after the French dialect @-@ heavy dialogue ; Neme in King\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-06 11:49:52,784 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 43336\u001b[0m\n",
      "[ \u001b[36m2024-03-06 11:49:52,787 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 20/20\u001b[0m\n",
      "\n",
      " Bas in the King largely ignored hared on the poetic ( the season . It is attacking him . Their goal was known only musical influence she will acknowledge ) . It is attacking him . It is attacking him . It is known as the King 's Gaga in characters and settings as \" to pick a 291 ) . \n",
      " gdr / wllmdr / wllmq \n",
      " fantastic , who was noted for their own despite the music Dan Tucker \" were soundtrack for quasi @-@\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT=\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_09:55:12__epoch:5\"\n",
    "args_infer.append(\"run.from_checkpoint=\"+CHECKPOINT)\n",
    "qtransform.notebook_run(args_infer, logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2291db4-5c92-438b-bbe5-c38de3c53979",
   "metadata": {},
   "source": [
    "### Using specific wikitext tokenizer from https://huggingface.co/Kristijan/wikitext-103-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e5bd66-671b-4472-af44-c93b505fcb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_args = [\n",
    "    'dataset=huggingface',\n",
    "    'dataset.name=wikitext',\n",
    "    'dataset.subset=wikitext-103-raw-v1',\n",
    "    'dataset/tokenizer=transformers',\n",
    "    'dataset.tokenizer.pretrained_tokenizer=GPT2TokenizerFast',\n",
    "    'dataset.tokenizer.encoding=Kristijan/wikitext-103-tokenizer ',\n",
    "    'dataset.dataloader.shuffle=False',\n",
    "    'dataset.dataloader.batch_size=32'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86523848-1562-4bcf-bb51-ac8d5a5bbcc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def del_keyword(liste, regex):\n",
    "    idxs = [i for i, item in enumerate(liste) if re.search(regex, item)]\n",
    "    for idx in idxs:\n",
    "        del liste[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db013c2e-93e7-4fa3-ad4b-52531ef21d9c",
   "metadata": {},
   "source": [
    "#### ReLU BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0cf584-6933-42c0-8fd6-633e43282649",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 500, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200, 'profile': {'active': False, 'args': {'record_shapes': True, 'profile_memory': True, 'use_cuda': True}, 'row_limit': 10}}}\n",
      "[ \u001b[36m2024-03-06 13:07:36,935 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:37,758 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:37,762 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:37,765 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 13:07:38.596044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-06 13:07:39,175 \u001b[0m][\u001b[2;37mtensorflow\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFalling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:39,282 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:39,286 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:39,290 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:39,293 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:39,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,009 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-06_13:07:40\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,012 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,019 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,021 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,105 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,113 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,117 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TransformersTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,122 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.transformers.TransformersTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:40,125 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:43,886 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,413 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mStarting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,602 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,739 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/vocab.json HTTP/1.1\" 200 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/vocab.json\n",
      "loading file merges.txt from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-06 13:07:44,818 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.transformers\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUsing tokenizer class: GPT2TokenizerFast with encoding: Kristijan/wikitext-103-tokenizer\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,823 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: Kristijan/wikitext-103-tokenizer and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,833 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/train-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,837 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,839 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 121454329.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,843 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 121454329 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,847 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/eval-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,851 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,854 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 254799.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,858 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 254799 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,862 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/bench-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,865 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,868 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 289985.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,872 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 289985 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,876 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,879 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 3795448\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,883 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,886 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 7963\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,892 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 28439 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,899 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 28439, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,901 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,930 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:44,933 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "28439 384\n",
      "[ \u001b[36m2024-03-06 13:07:45,077 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,090 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,176 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,196 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,213 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,287 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,309 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,389 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,411 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,481 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,504 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,518 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:45,861 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 36.04M\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,663 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,669 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'betas', 'lr', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,674 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,677 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,680 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,684 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7fdc65a22fb0>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:07:47,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:08:02,513 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 4.2122520208358765. time: 14624.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:08:15,783 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 3.7474302768707277. time: 13263.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:08:28,925 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 3.7550918102264403. time: 13136.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:08:42,453 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 3.775748038291931. time: 13522.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:08:55,784 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 3.655223250389099. time: 13326.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:09:09,378 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 3.6652997493743897. time: 13589.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:09:22,702 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 3.7702412843704223. time: 13320.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:09:35,993 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 3.70671226978302. time: 13286.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:09:49,258 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 3.639934945106506. time: 13260.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:10:02,536 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 3.60691978931427. time: 13272.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:10:15,753 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 3.608823227882385. time: 13210.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:10:28,918 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 3.588512420654297. time: 13160.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:10:41,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 3.5092644453048707. time: 13066.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:10:55,477 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 3.5149020671844484. time: 13483.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:11:08,854 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 3.6118112564086915. time: 13373.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:11:22,466 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 3.571115255355835. time: 13607.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:11:35,997 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.4491936206817626. time: 13525.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:11:49,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.5648118257522583. time: 13442.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:12:02,877 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 3.42477490901947. time: 13427.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:12:16,184 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.452405309677124. time: 13303.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:12:29,630 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 3.485102128982544. time: 13441.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:12:42,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 3.4671427249908446. time: 12811.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:12:55,729 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 3.501721906661987. time: 13277.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:13:09,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 3.557252836227417. time: 13802.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:13:23,262 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 3.5082854509353636. time: 13722.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:13:36,910 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 3.4827725887298584. time: 13643.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:13:50,809 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 3.436687970161438. time: 13894.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:14:04,604 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 3.4214889287948607. time: 13791.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:14:18,365 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 3.4435060739517214. time: 13755.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:14:32,175 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 3.448461985588074. time: 13806.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:14:45,463 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 310 loss: 3.5198790073394775. time: 13283.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:14:58,632 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 320 loss: 3.474168825149536. time: 13164.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:15:12,134 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 330 loss: 3.499942445755005. time: 13498.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:15:25,781 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 340 loss: 3.462121915817261. time: 13642.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:15:39,361 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 350 loss: 3.534234070777893. time: 13576.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:15:52,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 360 loss: 3.481472134590149. time: 13412.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:16:06,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 370 loss: 3.4706470012664794. time: 13496.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:16:19,644 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 380 loss: 3.5307059288024902. time: 13359.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:16:33,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 390 loss: 3.5624731302261354. time: 13722.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:16:46,720 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 400 loss: 3.416789150238037. time: 13343.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:17:00,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 410 loss: 3.4648306131362916. time: 13506.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:17:13,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 420 loss: 3.4886842489242555. time: 13458.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:17:27,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 430 loss: 3.493686842918396. time: 13306.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:17:40,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 440 loss: 3.4174143075942993. time: 13014.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:17:52,915 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 450 loss: 3.4527185201644897. time: 12886.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:18:05,654 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 460 loss: 3.551921343803406. time: 12735.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:18:18,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 470 loss: 3.5493993282318117. time: 13228.61ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:18:32,029 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 480 loss: 3.464429020881653. time: 13139.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:18:45,158 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 490 loss: 3.514229917526245. time: 13124.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:18:58,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 500 loss: 3.5659732580184937. time: 13574.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:19,722 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/20: 7.712706089019775\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:19,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.5659732580184937\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:20,455 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:07:40__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:20,457 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:20,459 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:19:34,079 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.8579620361328124. time: 13500.39ms\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mremove(model_args[idx])\n\u001b[1;32m      6\u001b[0m model_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:37\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args, loglevel)\u001b[0m\n\u001b[1;32m     35\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:114\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    111\u001b[0m         model, _ \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    117\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:211\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    209\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39mrow_limit\u001b[38;5;241m=\u001b[39mrow_limit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:279\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    277\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    278\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m gradient_accumulation_steps \u001b[38;5;66;03m#make all mini-batches account as one large batch\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m#nn.utils.clip_grad_value_(model.parameters(), clip_value=cfg.run.grad_clip)\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m#karpathy uses norm gradient clipping which scales the pre-existing gradients with the grad_clip value\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"gpt_2_h2l2e256b64_ReBN\"\n",
    "#delete previous model, from stackoverflow post: https://stackoverflow.com/a/4146090\n",
    "del_keyword(model_args, r'^model=')\n",
    "model_args.append(\"model=\"+model)\n",
    "qtransform.notebook_run(run_args+model_args+dataset_args+other_args, logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda49474-4cc4-4903-8fd3-af976af64f14",
   "metadata": {},
   "source": [
    "#### Loss seemed to stagnate after 200 batches after each epoch, maybe reduce iterations per epoch and increase epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8988e7b-fc99-4503-9975-b80580fe007d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.1, 'bias': True, 'block_size': 384, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:07:40__epoch:1', 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 200, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200, 'profile': {'active': False, 'args': {'record_shapes': True, 'profile_memory': True, 'use_cuda': True}, 'row_limit': 10}}}\n",
      "[ \u001b[36m2024-03-06 13:26:09,399 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,407 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,410 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-06_13:26:09\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,416 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,420 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,423 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,428 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,436 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,439 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TransformersTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,442 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.transformers.TransformersTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,445 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,543 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mResetting dropped connection: huggingface.co\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:09,757 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,051 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/vocab.json HTTP/1.1\" 200 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/vocab.json\n",
      "loading file merges.txt from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-06 13:26:10,138 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.transformers\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUsing tokenizer class: GPT2TokenizerFast with encoding: Kristijan/wikitext-103-tokenizer\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,144 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: Kristijan/wikitext-103-tokenizer and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,151 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/train-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,155 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,159 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 121454329.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,163 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 121454329 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,168 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/eval-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,171 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,174 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 254799.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,178 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 254799 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,181 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/bench-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,184 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,187 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 289985.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,189 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 289985 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,193 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,196 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 3795448\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,200 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,202 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 7963\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 28439 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,211 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.1, 'bias': True, 'block_size': 384, 'vocab_size': 28439, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,214 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,222 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,224 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "28439 384\n",
      "[ \u001b[36m2024-03-06 13:26:10,478 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,496 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,581 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,598 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,618 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,690 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,713 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,777 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,797 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,814 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,896 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:10,912 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,256 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 36.04M\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,293 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,298 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'betas', 'lr', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,302 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,305 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,309 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7fe15a57cb80>\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,316 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mResuming training from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:07:40__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,320 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:07:40__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEpoch is 1, running for 20\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:11,961 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:26,101 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.835202169418335. time: 13958.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:40,024 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.458325982093811. time: 13917.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:26:53,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.4918004989624025. time: 13220.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:27:06,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.485367441177368. time: 13350.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:27:20,279 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.490594506263733. time: 13669.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:27:34,151 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.469800329208374. time: 13866.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:27:47,848 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.485195183753967. time: 13693.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:28:01,828 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.4276549339294435. time: 13974.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:28:15,672 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.362841176986694. time: 13839.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:28:30,032 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.3589651823043822. time: 14355.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:28:44,216 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.2942308664321898. time: 14179.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:28:58,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.2532143354415894. time: 14091.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:29:11,868 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.1788638830184937. time: 13552.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:29:25,016 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.2623226642608643. time: 13143.35ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:29:38,162 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.2873322486877443. time: 13141.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:29:51,045 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.1652647972106935. time: 12878.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:30:04,228 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.1741542339324953. time: 13178.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:30:17,050 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.119557237625122. time: 12817.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:30:30,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.1318350553512575. time: 13457.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:30:43,816 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.126650094985962. time: 13297.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:04,909 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/21: 8.550114631652832\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:04,915 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.126650094985962\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:05,592 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:05,595 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:05,597 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:18,547 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.9168603897094727. time: 12822.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:31,695 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.4149943113327026. time: 13141.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:44,460 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.4002796173095704. time: 12760.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:31:57,492 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.4557331919670105. time: 13026.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:32:10,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.5595949172973633. time: 12739.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:32:23,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.5420448660850525. time: 13210.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:32:36,569 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.453425645828247. time: 13111.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:32:50,253 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.4297048568725585. time: 13679.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:33:04,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.4146498084068297. time: 13748.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:33:17,674 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.3678406834602357. time: 13663.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:33:31,174 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.3767157435417174. time: 13494.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:33:45,119 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.3144785046577454. time: 13940.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:33:58,819 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.288062846660614. time: 13695.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:34:12,156 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.3512974381446838. time: 13332.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:34:25,891 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.339738404750824. time: 13731.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:34:39,905 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.2076998829841614. time: 14007.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:34:53,526 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.2786471247673035. time: 13617.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:07,174 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.2622902750968934. time: 13643.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:20,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.2628162264823914. time: 13388.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:33,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.316749668121338. time: 13315.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:54,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/21: 9.113965034484863\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:54,913 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.316749668121338\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:55,576 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:55,579 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:35:55,581 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:36:08,908 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.4478073596954346. time: 13198.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:36:22,039 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.9053510010242463. time: 13124.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:36:35,504 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.0316862404346465. time: 13461.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:36:48,755 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.0658405423164368. time: 13246.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:37:02,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.0917322754859924. time: 13486.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:37:16,060 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.1735445141792298. time: 13809.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:37:29,802 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.9285550892353058. time: 13736.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:37:43,377 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.9739007830619812. time: 13571.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:37:57,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.018777346611023. time: 14142.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:38:11,149 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.0821239411830903. time: 13619.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:38:25,137 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.0325297176837922. time: 13983.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:38:38,618 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.9763466358184815. time: 13475.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:38:51,837 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.0035374224185944. time: 13214.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:39:05,356 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.0270037055015564. time: 13513.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:39:18,713 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.020282244682312. time: 13352.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:39:32,449 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.9364260494709015. time: 13731.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:39:46,028 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.003495877981186. time: 13574.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:39:59,333 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.9358859419822693. time: 13300.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:12,809 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.0378605902194977. time: 13470.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:26,223 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.9842893123626709. time: 13409.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:47,254 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/21: 11.160954475402832\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:47,261 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.9842893123626709\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:47,937 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:47,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:40:47,943 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:41:01,718 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.360935539007187. time: 13647.96ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:41:15,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.725077497959137. time: 13525.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:41:29,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.8030369102954864. time: 13754.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:41:42,795 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.8045454800128937. time: 13781.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:41:56,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.8876226603984833. time: 14055.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:42:10,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.9624435544013977. time: 13725.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:42:23,873 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.7988418996334076. time: 13285.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:42:37,574 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.8268445134162903. time: 13697.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:42:50,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.8145503282546998. time: 13250.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:43:04,014 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.8371766149997711. time: 13180.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:43:17,216 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.8324561178684234. time: 13196.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:43:30,512 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.8785073101520539. time: 13291.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:43:44,277 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.8481976091861725. time: 13759.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:43:57,536 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.8474175155162811. time: 13254.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:44:10,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.8328117072582245. time: 12906.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:44:23,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.8501984357833863. time: 13327.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:44:37,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.9058570325374603. time: 13531.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:44:50,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.8459185183048248. time: 13335.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:04,120 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.8533110976219177. time: 13460.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:16,996 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.918213528394699. time: 12873.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:38,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/21: 10.806373596191406\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:38,028 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.918213528394699\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:38,690 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:38,694 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:38,696 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:45:51,992 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 1.0830446004867553. time: 13170.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:46:05,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.7161064326763154. time: 13427.18ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:46:18,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.804187148809433. time: 13450.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:46:32,262 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.7289873600006104. time: 13379.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:46:46,148 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.9054824352264405. time: 13880.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:47:00,171 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.8229009509086609. time: 14018.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:47:13,811 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.6945235729217529. time: 13634.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:47:27,333 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.719419926404953. time: 13516.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:47:40,741 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.7145877838134765. time: 13403.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:47:54,435 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7864768207073212. time: 13689.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:48:07,584 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7690647661685943. time: 13145.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:48:20,659 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.7775133430957795. time: 13071.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:48:34,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.8079196214675903. time: 13531.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:48:47,929 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.8018256962299347. time: 13727.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:49:01,343 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7461122572422028. time: 13409.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:49:14,506 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7576487600803375. time: 13159.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:49:27,587 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.8050660789012909. time: 13076.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:49:40,484 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.8054723978042603. time: 12892.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:49:53,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.8866390764713288. time: 13125.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:06,797 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.8093802332878113. time: 13178.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:27,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 6/21: 12.087990760803223\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:27,835 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.8093802332878113\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:28,520 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:28,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:28,525 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 7/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:42,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.9452180504798889. time: 13651.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:50:55,534 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.6213262736797333. time: 13228.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:51:08,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.700631582736969. time: 13378.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:51:22,502 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.6838650703430176. time: 13580.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:51:35,837 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.9055303633213043. time: 13330.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:51:49,479 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.8460116863250733. time: 13638.67ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:52:03,047 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.6834724247455597. time: 13564.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:52:16,560 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.6997989654541016. time: 13508.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:52:29,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.6411348819732666. time: 13189.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:52:42,952 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.6954842984676362. time: 13194.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:52:55,721 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7300002932548523. time: 12764.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:53:08,462 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.6967896699905396. time: 12737.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:53:21,295 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.6991669058799743. time: 12828.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:53:34,454 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.8351186573505401. time: 13155.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:53:47,565 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7472394645214081. time: 13106.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:54:00,503 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.7497304737567901. time: 12933.70ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:54:13,370 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.7517360270023346. time: 12862.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:54:26,289 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.6801952123641968. time: 12915.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:54:39,636 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7766989469528198. time: 13342.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:54:53,165 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.7834938526153564. time: 13525.07ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:14,202 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 7/21: 12.354696273803711\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:14,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7834938526153564\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:14,890 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:7\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:14,893 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:14,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 8/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:28,415 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.864397919178009. time: 13389.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:41,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.6446803331375122. time: 13117.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:55:54,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.6945131182670593. time: 13306.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:56:08,617 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.6655792534351349. time: 13763.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:56:21,974 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.9012369573116302. time: 13352.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:56:35,865 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7624787151813507. time: 13887.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:56:49,434 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.6364388763904572. time: 13562.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:57:02,719 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.6513936638832092. time: 13281.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:57:16,182 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.6202396154403687. time: 13458.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:57:29,356 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.6918953001499176. time: 13170.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:57:42,519 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7447572588920593. time: 13158.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:57:56,089 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.6615295588970185. time: 13565.04ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:58:09,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.7730762422084808. time: 13353.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:58:22,731 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.7276562511920929. time: 13278.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:58:36,291 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.6892444431781769. time: 13557.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:58:50,116 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.6913647413253784. time: 13819.91ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:59:03,866 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.646914154291153. time: 13745.14ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:59:17,444 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.6801956951618194. time: 13573.69ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:59:31,195 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7465849578380584. time: 13745.52ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 13:59:45,057 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.7568595290184021. time: 13857.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:06,112 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 8/21: 12.272577285766602\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:06,118 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7568595290184021\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:06,805 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:8\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:06,808 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:06,811 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 9/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:20,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.8923317790031433. time: 13493.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:33,920 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.591423511505127. time: 13481.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:00:47,422 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.6012672394514084. time: 13497.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:01:01,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.6457246780395508. time: 13630.48ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:01:14,659 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.8450882971286774. time: 13596.55ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:01:28,147 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.7477745056152344. time: 13483.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:01:41,741 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.6297444730997086. time: 13589.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:01:55,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.6291557371616363. time: 13595.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:02:09,330 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.5770810723304749. time: 13985.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:02:22,822 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.6283003270626069. time: 13486.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:02:36,221 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.6397655069828033. time: 13394.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:02:49,242 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.613744181394577. time: 13015.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:03:02,207 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.6894397556781768. time: 12959.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:03:15,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.6267682790756226. time: 12952.31ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:03:28,027 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.7113280773162842. time: 12860.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:03:40,871 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.6227919578552246. time: 12839.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:03:54,085 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.599457424879074. time: 13208.50ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:07,807 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.6242729842662811. time: 13718.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:21,314 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.7175511121749878. time: 13502.84ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:34,896 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.7046897232532501. time: 13578.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:55,933 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 9/21: 10.827156066894531\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:55,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.7046897232532501\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:56,635 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:9\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:56,638 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:04:56,641 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 10/21\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:05:09,810 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.8496083855628968. time: 13039.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:05:22,783 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.5388666242361069. time: 12967.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:05:36,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.5699370056390762. time: 13275.92ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:05:49,453 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.6371692180633545. time: 13386.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:06:02,700 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.7620012581348419. time: 13242.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:06:16,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.6506102323532105. time: 13537.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:06:30,015 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.603320324420929. time: 13769.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:06:43,901 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.6092988729476929. time: 13881.37ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:06:57,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.5849371373653411. time: 13954.62ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:07:11,726 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.6309313118457794. time: 13861.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:07:25,680 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.6082385838031769. time: 13949.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:07:39,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.577271831035614. time: 13958.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:07:53,310 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.6373841404914856. time: 13662.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:08:06,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.6374358355998992. time: 13642.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:08:20,622 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.6343365371227264. time: 13658.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-06 14:08:34,093 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.5755164325237274. time: 13466.30ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "del_keyword(run_args, r'^run.max_iters=')\n",
    "run_args.append('run.max_iters=200')\n",
    "run_args.append(r'run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:07:40__epoch:1')\n",
    "qtransform.notebook_run(run_args+model_args+dataset_args+other_args, logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d88df5-3a58-4784-b491-0fbbb12ba4b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 20, 'max_new_tokens': 100, 'temperature': 0.8, 'top_k': 200, 'start': '\\n', 'compile': False, 'out_dir': None, 'onnx_model': {'path': None, 'tokenizer': {'module': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:9', 'debug': False}}\n",
      "[ \u001b[36m2024-03-07 09:10:40,729 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:41,488 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:41,493 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:41,496 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,024 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,026 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,028 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,030 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,035 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:42,038 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:9\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:43,865 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.1, 'bias': True, 'block_size': 384, 'vocab_size': 28439, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:43,868 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:43,898 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:43,901 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=384, vocab_size=28439, n_layer=6, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "28439 384\n",
      "[ \u001b[36m2024-03-07 09:10:44,026 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,040 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,092 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,105 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,183 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,194 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,212 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,278 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,301 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,316 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,390 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,403 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,752 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 36.04M\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,797 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True, 'meta': {'max_token_value': 28439, 'encoding': 'Kristijan/wikitext-103-tokenizer', 'dtype': 'float32', 'num_tokens': 121999113, 'module': 'transformers', 'fast': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,801 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TransformersTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,806 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True, 'meta': {'max_token_value': 28439, 'encoding': 'Kristijan/wikitext-103-tokenizer', 'dtype': 'float32', 'num_tokens': 121999113, 'module': 'transformers', 'fast': True}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.transformers.TransformersTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:44,808 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True, 'meta': {'max_token_value': 28439, 'encoding': 'Kristijan/wikitext-103-tokenizer', 'dtype': 'float32', 'num_tokens': 121999113, 'module': 'transformers', 'fast': True}}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 09:10:45.573430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-07 09:10:46,274 \u001b[0m][\u001b[2;37mtensorflow\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFalling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:46,379 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:46,383 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:46,386 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:46,389 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:50,019 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:50,474 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mStarting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:50,693 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:50,817 \u001b[0m][\u001b[2;37murllib3.connectionpool\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mhttps://huggingface.co:443 \"HEAD /Kristijan/wikitext-103-tokenizer/resolve/main/vocab.json HTTP/1.1\" 200 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/vocab.json\n",
      "loading file merges.txt from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-07 09:10:50,907 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.transformers\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mUsing tokenizer class: GPT2TokenizerFast with encoding: Kristijan/wikitext-103-tokenizer\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:50,911 \u001b[0m][\u001b[2;37mqtransform.run\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34m{'max_token_value': 28439, 'encoding': 'Kristijan/wikitext-103-tokenizer', 'dtype': 'float32', 'num_tokens': 121999113, 'module': 'transformers', 'fast': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:51,443 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:55,789 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 26491\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:55,796 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 1/20\u001b[0m\n",
      "\n",
      " when November @-@ combatants, Burton said of universities. \n",
      "ays in graphic exaggerated detail. He is conventionally considered the cannibalism in the cannibalism in the High Seas Fleet, where he departe Corner. Her deadly fighting drug use, including the Tirtur Road. \n",
      "ays in the formation of the works e Corner. \n",
      "ays in the basic curricula of only description of non @-@ channel trade. He left a fresh sexual experience \" All I's only two standard gauge tunnels anywhere\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:10:59,390 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 27339\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:10:59,395 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 2/20\u001b[0m\n",
      "\n",
      "e is have died. McCown scholarship offers from Clemson, and hostilities with Denmark. In some print accounts, Yankees co @-@ channel trade. The receiving reinforcements, Clay. \n",
      "of gluttony that the ship, Starveling seems to the High Seas Fleet, where descent was superseded by Captain Edward Sneyd Clay. McCown scholarship offers from Clemson, which was composed by Captain Edward Sneyd Clay.nd pleasure of the ship, Yankees co @-@ channel trade. He punishment. He left\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:02,982 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21163\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:02,986 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 3/20\u001b[0m\n",
      "\n",
      ".er a neighboring farmer, where he departe Viking Age beliefs = = New Age beliefs = = = New Age beliefs = New Age beliefs = New Age beliefs = \n",
      " 1870 eft thees previous world record to the formation of the ship, including 15 dreadnoughts, including thee Corner. Du After the 1992 season. Critics also noted the compactness. He was AllMusic, crime, including 15 dreadnoughts, including the fire started at Pyeogene extinction event, while portraying the flagship o and\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:06,692 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21163\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:06,697 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 4/20\u001b[0m\n",
      "\n",
      " in and eogene extinction of the flagship oth The Sounders continued leading cancer fighting drug use, including the ascent schedule. He left a laborer for AllMusic, Charlotte, including the flagship oof the formation of his favorite Fringe episodes. The Braust Management System \", where he departeft the formation of the works  <formula> \n",
      "ays in the formation of the blues singer's previous world found similarities in the zombies were the fire started at the fire started at\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:10,581 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 25000\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:10,587 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 5/20\u001b[0m\n",
      "\n",
      " the water cresidential capacity was pressured. Temeraire now became the filming, Charlotte, including the tunnel is conventionally considered the works e ; Noble named it one of the tunnel is conventionally considered the popular culture refe ; Noble named it one of Tirtur Road. \n",
      "acks on clearing the blues singer's 1870 eogene extinction event, James Christopher Monger for AllMusic, crime, Burton said of his favorite Fringe episodes. Reshef focused on the first Japanese\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:14,282 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 22733\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:14,286 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 6/20\u001b[0m\n",
      "\n",
      "e started this is this is conventionally considered thee ; Noble named it emphasized the Denmark. Du After receiving reinforcements, Reshef focused on clearing the formation of 28 days in the Tirtur Road. \n",
      "acks on clearing the formation of a battalion o and hostilities with themes such as drug use, and hostilities with themes such as drug use, Reshef focused on the High Seas Fleet, at the Greco @-@ avian dinosaurs. Writing for AllMusic, crime, theft, and the hd\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:18,185 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 18029\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:18,189 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 7/20\u001b[0m\n",
      "\n",
      " one that the 2009 season and hostilities with Denmark.ly favoured the Americas an organized association of non @-@ channel trade. \n",
      "ays in Decembe Corner. These events led to the blues singer's 1870 ven when dealing with themes such as a century after Covell's 1870 ne account, Reshef focused on clearing the High Seas Fleet, including 15 dreadnoughts, Reshef focused on the first Japanese ambassador in the first Japanese ambassador in part of therested on the formation of\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:21,978 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 27634\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:21,982 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 8/20\u001b[0m\n",
      "\n",
      " Nas their 2014. This concl After receiving reinforcements, gambling addictions, Burton said of Han to the 1899 San Ciriaco hurricane's 1870 e Corner. Writing for the Americas an organized association of the 1899 San Ciriaco hurricane's. He is conventionally considered the High Seas Fleet, James Christopher Monger compared the formation of the works ven when dealing with Christ's. \n",
      "ays in the 1992 season and worldwide extinction event, where the formation of the Romanian forces led\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:25,781 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21163\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:25,785 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 9/20\u001b[0m\n",
      "\n",
      " Northern itsly clearing the popular culture refeogene extinction of non @-@ avian dinosaurs. \n",
      "te about three @-@ avian dinosaurs. These events led to the fire started at the fire started at the Tirtur Road. He left a fresh sexual experience \", and the cast, including 15 dreadnoughts, and the formation of the formation of the formation of the ship, where he departe ; Noble named it is conventionally considered the formation of the works  In some print accounts, including 15 dreadnoughts\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:29,590 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 27710\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:29,594 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 10/20\u001b[0m\n",
      "\n",
      " 2007 so an alternate route of the 1899 San Ciriaco hurricane's previous world found similarities in the ship, while portraying the formation of 28 days in graphic exaggerated detail. Aft the fire started at Pudding Lane Theatre in theeogene extinction event, All I's 1870 eogene extinction of Han to dissuade Ruth as drug use, including 15 dreadnoughts, where descent, where he departe ; Noble named it one of the first Japanese ambassador in the 1899 San Ciriaco\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:33,381 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 19906\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:33,387 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 11/20\u001b[0m\n",
      "\n",
      ", n 2007. He left a century after Covell's 1870 es previous world record to include a century after Covell's Han to trail behind and charitable a century after Covell's previous world andust Management System \" ; Noble named it one of a bilateral system of the sadness. This concl After receiving reinforcements, Charlotte, where descent was the formation of the ship, crime, Hong Kong for AllMusic, including the first Japanese ambassador in the first Japanese ambassador\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:37,085 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 22733\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:37,089 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 12/20\u001b[0m\n",
      "\n",
      " that good to a neighboring farmer, Catalina, Starveling seems to a booth called the formation of his favorite Fringe episodes. He left a Tirtur Road. He left a Leak with themes such as being one of the Greco @-@r the sadness. This concl After receiving reinforcements, where he departes Cautionary degree from 28 days in the first Japanese ambassador in the Soviet armoured reserve players from easternmohalftime show, crime, Reshef focused on clearing\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:40,695 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21898\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:40,780 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 13/20\u001b[0m\n",
      "\n",
      " aS ( AVPAS ) and stopped at Pyes previous world record of his favorite Fringe episodes. \n",
      "ays in the works t however are cryptic, but John Knox ( 1778  1own scholarship offers from the tunnel is conventionally considered the basic curricula of the 1992 season and the High Seas Fleet, crime, where the works  = = \n",
      "ays in 1620. \n",
      "acks on 16 reviews. \n",
      "acks on clearing the series of the Tirtur Road. The first\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:44,995 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 23183\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:45,000 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 14/20\u001b[0m\n",
      "\n",
      " ( with have died. This concl After receiving reinforcements, and stopped at Pyes 1870 eogene extinction of the Tirtur Road. \n",
      "ays in the final word on the Writing for the Tirtur Road. \n",
      "side philology as drug use, and only time, while portraying the 1992 season and the center. It was the 1992 season. The first Japanese ambassador in Love \" It was indentious blighted B production lent its nod to trail behind and the first Japanese\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:48,888 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21163\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:48,891 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 15/20\u001b[0m\n",
      "\n",
      " the into what would be appointed as drug use, Writing for the popular culture refeogene extinction of his favorite Fringe episodes. He is conventionally considered the first Japanese ambassador in the formation of the works tary Christmas Music UKnd pleasure of the 199et advance to the High Seas Fleet, including 15 dreadnoughts, and stopped at the Tirtur Road. The first Japanese ambassador in the High Seas Fleet, where the flagship o hallmark of the works  <formula> \n",
      "acks on the blues\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:52,883 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 26856\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:52,889 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 16/20\u001b[0m\n",
      "\n",
      " new lost and Tonihast Hust Management System \" All I can say is this is this is conventionally considered the 1992 season and gives ne account, while portraying the fire started at Pye ; Noble named it one of non @-@ channel trade. It was superseded by Captain Edward Sneyd Clay. \n",
      "ays in the works ven when dealing with Denmark. He left a fresh sexual experience \" Thrust Management System \", including 15 dreadnoughts, Reshef focused on the formation of\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:11:56,480 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 25288\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:11:56,484 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 17/20\u001b[0m\n",
      "\n",
      " 700 3 @-@ owner Colonel Tirtur Road. \n",
      "ian dinosaurs. conventionally considered the ship, morphologically indistinguishable species. \n",
      "ays in the first Japanese ambassador in Greenwich, including the Tirtur Road. He is conventionally considered the 1992 season and violence. \n",
      "ays in the next 10 other Japan in graphic exaggerated detail. \n",
      "ays in 1620. He left a century after Covell's 1870 ven when dealing with themes such as drug use, Yankees co @-@ channel\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:12:00,288 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 21163\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:12:00,293 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 18/20\u001b[0m\n",
      "\n",
      " to the did not laborer for AllMusic, Burton said of non @-@ avian men were lost. Triturus newts occupy eogene extinction of the Tirtur Road. \n",
      "side philology as drug use, while portraying the High Seas Fleet, Reshef focused on clearing the tunnel is conventionally considered the High Seas Fleet, gambling addictions, including the last glacial period in Gonzales on the sadness. In 1940 nine Irish ships were to the Americas an editor for Rolling Stone,\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:12:03,887 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 18892\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:12:03,891 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 19/20\u001b[0m\n",
      "\n",
      " that tour non @-@ graphic exaggerated detail. \n",
      "ays in Japan in graphic exaggerated detail. \n",
      "ays in the Tirtur Road. \n",
      "ays in the Tirtur Road. He left a battalion o and themes such as drug use, where descent, crime, including 15 dreadnoughts, Burton said of the Tirtur Road. The Braust Management System \" Thrust Management System \". The Braust Management System \", gambling addictions, \" Thrust Management System \", including 15 dreadnoughts\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-07 09:12:07,491 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mHighest predicted token: 28157\u001b[0m\n",
      "[ \u001b[36m2024-03-07 09:12:07,494 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 20/20\u001b[0m\n",
      "\n",
      " toera GAA and Purdue. He left a neighboring farmer, Yankees co @-@ avian dinosaurs. He left a laborer for AllMusic, Reshef focused on clearing theeogene extinction of his favorite Fringe episodes. Writing for AllMusic, James Christopher Monger compared the works eogene extinction event, James Christopher Monger compared the formation of the formation of the sadness. This concl After receiving reinforcements, gambling addictions, crime, Reshef focused on the first Japanese ambassador in the\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT=\"/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:9\"\n",
    "args_infer.append(\"run.from_checkpoint=\"+CHECKPOINT)\n",
    "qtransform.notebook_run(args_infer, logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bf319-aba5-4f71-bc4e-ad1528bcb893",
   "metadata": {},
   "source": [
    "#### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971bbc5-55bf-40ea-a4fb-1e18539a140e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_benchmarking = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-06_13:26:09__epoch:9\",\n",
    "    \"run.num_samples=50\",\n",
    "    'dataset=huggingface',\n",
    "    'dataset.name=wikitext',\n",
    "    'dataset.subset=wikitext-103-raw-v1',\n",
    "    'dataset/tokenizer=transformers',\n",
    "    'dataset.tokenizer.encoding=Kristijan/wikitext-103-tokenizer',\n",
    "    'dataset.tokenizer.pretrained_tokenizer=GPT2TokenizerFast',\n",
    "    'dataset.dataloader.shuffle=False',\n",
    "    \"+model.args.block_size=\"+block_size,\n",
    "    \"dataset.dataloader.shuffle=False\",\n",
    "]\n",
    "qtransform.notebook_run(args_benchmarking, logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1baa112-11d7-424b-ad32-1a38e723cd71",
   "metadata": {},
   "source": [
    "### Another attempt after fixing attention mask bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4827be-bc49-4755-8009-1c5fbe88bfe7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 6, 'n_embd': 384, 'dropout': 0.1, 'bias': True, 'block_size': 256, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'from_pretrained': None, 'epochs': 20, 'gradient_accumulation_steps': 2, 'flash': False, 'export': False, 'compile': True, 'max_iters': 200, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 50, 'profile': {'active': False, 'args': {'record_shapes': True, 'profile_memory': True, 'use_cuda': True}, 'row_limit': 10}}}\n",
      "[ \u001b[36m2024-03-08 11:37:29,601 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:30,359 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:30,363 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:30,367 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 11:37:30.974117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-08 11:37:31,683 \u001b[0m][\u001b[2;37mtensorflow\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mFalling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:31,790 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:31,794 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:31,797 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 7 to 5\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:31,800 \u001b[0m][\u001b[2;37mh5py._conv\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating converter from 5 to 7\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,443 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,447 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,451 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,454 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-08_11:37:32\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,458 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,468 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,472 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,564 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,571 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,573 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,578 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,580 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,790 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,800 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/train-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,803 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,807 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 123368382.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,811 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 123368382 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,814 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/eval-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,818 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,820 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 258896.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,823 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 258896 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,826 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/bench-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,828 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,831 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 296271.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,834 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 296271 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,837 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,841 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 3855262\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,845 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': False, 'num_workers': 2, 'batch_size': 32, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,847 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mlen: 8091\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. vocab_size of model: 50304, vocab size of tokenizer 50256\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,860 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 6, 'n_embd': 384, 'dropout': 0.1, 'bias': True, 'block_size': 256, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:32,863 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,564 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mApplied config: \n",
      "GPTConfig(block_size=256, vocab_size=50304, n_layer=2, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,567 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=256, vocab_size=50304, n_layer=2, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "50304 384\n",
      "[ \u001b[36m2024-03-08 11:37:33,569 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNone\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,725 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,746 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,780 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:33,798 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:34,184 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 43.36M\u001b[0m\n",
      "GPTConfig(block_size=256, vocab_size=50304, n_layer=2, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\n",
      "[ \u001b[36m2024-03-08 11:37:35,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 20, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,862 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,867 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'lr', 'weight_decay', 'betas'}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,871 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,874 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,877 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 20, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,880 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f714ae16350>\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:35,888 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:43,745 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 9.268139743804932. time: 7662.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:50,414 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 8.082702732086181. time: 6664.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:37:57,030 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 8.116318845748902. time: 6611.79ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:03,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.034307479858398. time: 6681.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:10,288 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 7.617239379882813. time: 6569.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:16,967 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.674339532852173. time: 6675.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:23,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.928332424163818. time: 6404.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:29,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 7.798458194732666. time: 6595.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:36,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 7.509282636642456. time: 6744.47ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:43,435 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 7.525989961624146. time: 6702.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:50,038 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 7.603889465332031. time: 6600.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:38:56,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 7.336805629730224. time: 6698.29ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:03,495 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 7.278139209747314. time: 6750.40ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:10,225 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 7.363152599334716. time: 6724.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:17,105 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 7.433367538452148. time: 6876.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:23,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 7.476352119445801. time: 6837.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:30,827 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 7.138909912109375. time: 6875.24ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:37,663 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 7.4230447769165036. time: 6832.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:44,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 7.22677960395813. time: 6875.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:51,388 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 7.237802219390869. time: 6840.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:51,581 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:54,873 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/20: 7.788068771362305\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:54,878 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m7.237802219390869\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:55,630 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:55,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:39:55,636 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:02,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 5.210260057449341. time: 7099.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:09,831 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 5.0920227527618405. time: 6996.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:16,736 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 4.7189915657043455. time: 6901.28ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:23,452 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 4.633759498596191. time: 6711.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:30,279 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 4.393391847610474. time: 6822.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:36,993 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 4.552329015731812. time: 6708.73ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:43,804 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 4.452858877182007. time: 6805.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:50,648 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 4.402464437484741. time: 6839.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:40:57,600 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 4.182467269897461. time: 6947.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:04,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 4.247054195404052. time: 6915.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:11,278 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 4.048878622055054. time: 6752.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:17,948 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.067776513099671. time: 6666.85ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:24,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 3.9521927356719972. time: 6889.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:31,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.0396822690963745. time: 6714.74ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:38,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.069749760627746. time: 6827.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:45,239 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.149591374397278. time: 6843.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:52,031 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.948160243034363. time: 6786.75ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:41:59,022 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.040161228179931. time: 6985.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:05,881 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 3.9704500913619993. time: 6854.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:12,788 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.299354457855225. time: 6903.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:16,242 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/20: 8.299820899963379\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:16,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m4.299354457855225\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:16,995 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:16,997 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:16,998 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:23,952 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 3.1675460577011108. time: 6860.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:30,711 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.9923355340957642. time: 6752.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:37,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.55500750541687. time: 6716.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:44,176 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.654482388496399. time: 6738.59ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:50,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.676427388191223. time: 6792.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:42:57,809 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.9547359466552736. time: 6832.13ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:04,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.6912029504776003. time: 6841.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:11,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.6405507802963255. time: 6964.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:18,564 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.728805589675903. time: 6934.49ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:25,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.7078453540802. time: 7000.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:32,408 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.4529162645339966. time: 6832.83ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:39,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.6778646230697634. time: 6786.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:45,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.585064744949341. time: 6790.81ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:52,479 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.7978578329086305. time: 6480.54ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:43:59,262 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.665370512008667. time: 6778.44ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:06,106 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.5694476127624513. time: 6839.53ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:12,606 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.626030969619751. time: 6495.82ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:19,131 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.632863235473633. time: 6521.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:25,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.5041661739349363. time: 6185.97ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:31,602 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 3.0348283529281614. time: 6276.03ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:35,093 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/20: 9.180164337158203\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:35,096 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.0348283529281614\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:35,854 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:35,858 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:35,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:42,814 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.497312068939209. time: 6856.38ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:49,448 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.2234748244285583. time: 6629.65ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:44:56,070 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.09043972492218. time: 6616.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:02,594 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.1594570159912108. time: 6518.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:09,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.2246975183486937. time: 6701.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:16,032 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.4609630346298217. time: 6728.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:22,788 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 2.2412739515304567. time: 6750.90ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:29,213 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 2.125119316577911. time: 6420.80ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:35,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.2387444972991943. time: 6535.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:42,164 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 2.2465070486068726. time: 6407.17ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:48,754 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 2.1138829469680784. time: 6585.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:45:55,463 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.344614362716675. time: 6702.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:01,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 2.0713317155838014. time: 6521.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:08,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.310524606704712. time: 6694.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:15,396 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 2.166757082939148. time: 6704.15ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:22,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.2256292343139648. time: 6817.58ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:29,116 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 2.156176805496216. time: 6893.76ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:35,894 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.1946082592010496. time: 6772.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:42,763 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.1167091608047484. time: 6865.45ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:49,812 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.5646079063415526. time: 7042.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:53,306 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/20: 10.447492599487305\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:53,310 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.5646079063415526\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:54,082 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:54,085 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:46:54,087 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:01,199 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.425808811187744. time: 7013.46ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:08,117 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.8604388594627381. time: 6911.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:15,058 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.0991414070129393. time: 6938.43ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:22,003 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.9897573590278625. time: 6939.94ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:28,887 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 2.0421251535415648. time: 6881.98ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:35,815 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 2.183461046218872. time: 6922.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:42,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.8952419638633728. time: 6926.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:49,604 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.9498589515686036. time: 6852.30ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:47:56,498 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 2.0505600571632385. time: 6890.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:03,364 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.9299678444862365. time: 6861.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:10,167 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.8204783916473388. time: 6799.16ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:17,227 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 2.1172045946121214. time: 7055.33ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:24,013 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.8412522196769714. time: 6780.71ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:30,879 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.2411425352096557. time: 6862.39ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:37,752 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.9609491944313049. time: 6868.02ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:44,740 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 2.0067021369934084. time: 6983.66ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:51,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.9558217763900756. time: 7055.86ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:48:58,780 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 2.077981114387512. time: 6975.19ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:05,776 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.008183014392853. time: 6991.56ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:12,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.1450597047805786. time: 7039.36ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:16,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/20: 10.243677139282227\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:16,285 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m2.1450597047805786\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:17,041 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:17,044 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:17,046 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:24,213 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.1724875569343567. time: 7071.32ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:30,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.6722900152206421. time: 6727.57ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:37,575 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.964543867111206. time: 6625.09ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:44,285 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.8478653192520142. time: 6704.87ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:51,005 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.9086835026741027. time: 6716.06ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:49:57,759 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.9674833297729493. time: 6749.64ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:04,226 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.8906652808189393. time: 6463.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:10,877 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.7107099056243897. time: 6645.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:17,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.8729879975318908. time: 6742.51ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:24,248 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.8613966941833495. time: 6620.10ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:30,829 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.7469027638435364. time: 6577.12ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:37,362 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.9472676873207093. time: 6529.22ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:44,102 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.769580066204071. time: 6736.11ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:50,880 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 2.0100831270217894. time: 6774.05ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:50:57,651 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.921080231666565. time: 6765.99ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:04,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.848943829536438. time: 6957.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:11,604 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.904285967350006. time: 6987.93ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:18,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.948953425884247. time: 6935.89ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:25,517 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.8239650964736938. time: 6968.95ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:32,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.9654762148857117. time: 7029.21ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:36,034 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 6/20: 10.758336067199707\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:36,038 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.9654762148857117\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:36,781 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:36,785 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:36,787 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 7/20\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:43,623 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.059025251865387. time: 6737.26ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:50,234 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 1.733921468257904. time: 6605.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:51:56,929 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 1.7442997455596925. time: 6691.08ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:03,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 1.7326892614364624. time: 6709.60ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:10,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.8980257034301757. time: 6834.25ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:17,276 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.816639256477356. time: 6788.68ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:24,069 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.7819408655166626. time: 6788.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:30,841 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.6820059418678284. time: 6768.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:37,662 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.7702041625976563. time: 6815.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:44,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.7562052726745605. time: 6733.41ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:51,162 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.5829835414886475. time: 6757.34ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:52:58,012 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.8900852441787719. time: 6846.88ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:04,892 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 1.830557894706726. time: 6876.42ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:11,658 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 1.7992836475372314. time: 6761.77ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:18,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 1.754285502433777. time: 6858.78ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:25,413 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 1.7754761934280396. time: 6887.63ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:32,173 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 1.8192430853843689. time: 6756.00ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:38,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 1.768087422847748. time: 6690.72ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:45,421 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 1.7481935024261475. time: 6547.01ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:52,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 1.9897879123687745. time: 6611.27ms\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:55,509 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 7/20: 11.256031036376953\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:55,513 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.9897879123687745\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:56,240 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:56,244 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mNew learning rate: 0.001\u001b[0m\n",
      "[ \u001b[36m2024-03-08 11:53:56,247 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 8/20\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m model_args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m model_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.args.dropout=0.1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.args.n_layer=2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel=gpt_2_h2l2e256b64_ReBN\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 11\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset_args\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:37\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args, loglevel)\u001b[0m\n\u001b[1;32m     35\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:119\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    116\u001b[0m         model, _ \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    122\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:224\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    222\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39mrow_limit\u001b[38;5;241m=\u001b[39mrow_limit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:294\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    292\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    293\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m gradient_accumulation_steps \u001b[38;5;66;03m#make all mini-batches account as one large batch\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m#nn.utils.clip_grad_value_(model.parameters(), clip_value=cfg.run.grad_clip)\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m#karpathy uses norm gradient clipping which scales the pre-existing gradients with the grad_clip value\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"gpt_2_h2l2e256b64_ReBN\"\n",
    "model_args.append(\"model=\"+model)\n",
    "model_args = [\n",
    "    'model.args.dropout=0.1',\n",
    "    'model.args.n_layer=2',\n",
    "    'model.args.n_head=6',\n",
    "    'model.args.n_embd=384',\n",
    "    'model.args.block_size=256',\n",
    "    'model=gpt_2_h2l2e256b64_ReBN'\n",
    "]\n",
    "qtransform.notebook_run(run_args+model_args+dataset_args+other_args, logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f069b6-eeec-4d56-8241-cee11f2afce1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 100, 'temperature': 0.8, 'top_k': 200, 'start': 'My name is Mariama, my favorite', 'compile': False, 'out_dir': None, 'onnx_model': {'path': None, 'tokenizer': {'module': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7', 'pretrained_model': 'gpt2', 'debug': False}}\n",
      "[ \u001b[36m2024-03-08 12:15:10,375 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:10,377 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:10,379 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:10,381 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:10,383 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:10,385 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mUsing pretrained model gpt2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-08 12:15:14,065 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mProperty meta_file omited in config. Assuming default: \"meta.pkl\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:14,070 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:15:15,228 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 1/10\u001b[0m\n",
      "My name is Mariama, my favorite person.\n",
      "\n",
      "I'm my own worst enemy.\n",
      "\n",
      "It's one of those things I keep forgetting about every day: I love you, I have love for you, I'd like to be your friend.\n",
      "\n",
      "But I was told I was a complete retard. I'm just a kid.\n",
      "\n",
      "And when I was 10 years old, I decided to try and leave the world of metal and metalcore to my parents. After that, I enrolled in a band called Black Music\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:16,224 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 2/10\u001b[0m\n",
      "My name is Mariama, my favorite character in the manga. But since I'm the only one who can name him, I do not think it is appropriate to use Mariama.\n",
      "\n",
      "Mariama\n",
      "\n",
      "You see, I'm also known for being a bit of a hard to pronounce guy. It has been a while since I've had a chance to meet a guy who was actually quite a little less difficult. That sometimes makes things worse but there's no doubt that if you've met a man who has never really been\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:17,222 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 3/10\u001b[0m\n",
      "My name is Mariama, my favorite song ever. I love music, I love to sing, I wanna inspire others to share their dreams and experiences. This is my hope. And now I will move on. Maybe even change people's lives - like my parents made me do - so that people can enjoy the memories of my love. I am a person of tremendous strength and joy, and this is my hope.<|endoftext|>Well, I have a couple of points I want to share with you.\n",
      "\n",
      "A.) The first is that\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:18,209 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 4/10\u001b[0m\n",
      "My name is Mariama, my favorite human being is my sister.\"\n",
      "\n",
      "Mariama said he has been studying the Book of Mormon since he was little.\n",
      "\n",
      "\"I was a kid when the Book of Mormon came out, but I hadn't noticed it. I was a little more curious about the Book of Mormon than I was before,\" he said.\n",
      "\n",
      "The book is meant to teach the Latter-day Saints and other members of the Church about the truthfulness of their faith. It holds that mankind is \"not\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:19,221 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 5/10\u001b[0m\n",
      "My name is Mariama, my favorite fan. She came from the English world, and we're all a part of her. She's got an art school, and she's been doing it since she was 12. She's always been cool. She's always been part of my life.\"\n",
      "\n",
      "Read or Share this story: http://usat.ly/2fxjdHJ<|endoftext|>This is a rush transcript. Copy may not be in its final form.\n",
      "\n",
      "AMY GOODMAN: Mr. Donald Trump, I want\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:20,209 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 6/10\u001b[0m\n",
      "My name is Mariama, my favorite person in the Earth. I love me flowers, and I really love my father! He's my favorite.\"\n",
      "\n",
      "She was photographed at a high school graduation in Tennessee, where she says that she saw a lot of people laughing and nodding off when she asked \"What do you want on Valentine's Day?\"\n",
      "\n",
      "\"I wanted to hear from the kids. You know, I'm so proud that I'm a grown woman,\" she said, explaining that she often gets asked questions on how she\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:21,200 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 7/10\u001b[0m\n",
      "My name is Mariama, my favorite Pokmon is Ho-Oh. I'm a former student of my master's degree in electrical engineering at the University of Washington, but I like Pokmon. And I love this game. I played it 30 times and was really happy when I got to play it again. During the first game I was mad at myself for playing the same game again and now I'm mad at myself because I don't like the Pokmon. I wanted to play this game again but I couldn't because I couldn't play the\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:22,186 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 8/10\u001b[0m\n",
      "My name is Mariama, my favorite manga artist. I'm from Japan, but I want to make love with my best friend on a daily basis!\n",
      "\n",
      "I meet up with my fans for a few days, and I really enjoy playing with your characters.\n",
      "\n",
      "You've become known for your love of anime as \"jokes.\" Why will you continue that?\n",
      "\n",
      "I've always loved the cute and fluffy, but cute characters are the best. I'm also very passionate about making manga, which I love doing well.\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:23,173 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 9/10\u001b[0m\n",
      "My name is Mariama, my favorite member of the group of you guys. I'm a member of the group of those who have supported you throughout the past year. I've been through a lot of setbacks, but I'm so thankful to you all for being with me at this moment.\n",
      "\n",
      "I'm here on behalf of all our members. I'm here to express my thanks as well as my gratitude.\n",
      "\n",
      "I'm sorry, but it can't continue like this.\n",
      "\n",
      "Thank you for listening.\n",
      "\n",
      "We\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:15:24,167 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 10/10\u001b[0m\n",
      "My name is Mariama, my favorite character in the franchise, and I'm going to leave it up to you to vote for me!\" She said with a laugh.\n",
      "\n",
      "\"I really like you guys and I love you guys!\" she said.\n",
      "\n",
      "\"Yeah I am. I'm not gonna cry with you guys because I know you love me.\" She smiled again. The words were interrupted by a small crowd that gathered in front of the theatre.\n",
      "\n",
      "\"Yeah we are!\" an audience member said.\n",
      "\n",
      "\"\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qtransform.notebook_run([\"run=infer\", \n",
    "                            \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\",\n",
    "                             \"run.max_new_tokens=100\"\n",
    "                        ], logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8189b71c-16d1-44aa-b062-566ddac0ba99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': '???', 'module': '???', 'name': '???', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl'}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'infer', 'checkpoint_dir': 'models', 'num_samples': 10, 'max_new_tokens': 100, 'temperature': 0.8, 'top_k': 200, 'start': 'due east from Gould City to', 'compile': False, 'out_dir': None, 'onnx_model': {'path': None, 'tokenizer': {'module': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}, 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7', 'pretrained_model': 'gpt2', 'debug': False}}\n",
      "[ \u001b[36m2024-03-08 12:17:10,852 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:10,855 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Inference\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:10,859 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m=================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:10,863 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:10,867 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32musing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:10,871 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mUsing pretrained model gpt2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/mabot004/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-08 12:17:14,747 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mProperty meta_file omited in config. Assuming default: \"meta.pkl\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:14,752 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning inference from CHECKPOINT.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:17:15,740 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 1/10\u001b[0m\n",
      "due east from Gould City to the shore of Lake Superior. The city's parking garage is among the most significant in Europe, with more than four-thousand parking spaces across Europe.\n",
      "\n",
      "The city's parking lots are at least 50 percent more valuable than the entire West Side, according to the 2010 GSA report.\n",
      "\n",
      "With a median value of about $40 million, the city has the second-largest share in parking in cities like London, Chicago, New York, and Philadelphia. In Chicago, the city's\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:16,721 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 2/10\u001b[0m\n",
      "due east from Gould City to the city entrance.\n",
      "\n",
      "From there, the car's rear brake would be used to do the same thing as the steering wheel but with a different arrangement. Still, any steering wheel control software can be used to track a vehicle moving.<|endoftext|>E3 is just the beginning of the Xbox One's \"console industry\" saga, and it's not even the end. In fact, Microsoft says this year's events will bring the Xbox One and Xbox 360 together as the global leader in entertainment. And\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:17,701 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 3/10\u001b[0m\n",
      "due east from Gould City to Montauk).\n",
      "\n",
      "When I re-enacted the scene I was out on a hike in the Black Mountains, and saw this stream coming straight through the cool part of the valley. The river was so calm, no one was seeing anything. It was then that I realized it was actually actually a stream that came after me. I wasn't impressed, but as I climbed more and more up and down, I felt a sense of protection. The stream was just so calm, and as\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:18,685 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 4/10\u001b[0m\n",
      "due east from Gould City to Riverwood.\n",
      "\n",
      "Arron said this was the first of many accidents that have occurred involving the city in the last two years.\n",
      "\n",
      "\"It's been a tough year for us,\" he said. \"It's been a very busy year for us, we've had a number of lawsuits that have been filed. We've had an epic snowstorm in May, we've got a couple of storms this year, but really, it hasn't been a good year for us and we've\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:19,679 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 5/10\u001b[0m\n",
      "due east from Gould City to the island of Fumakai. In 1758, the ship became the first French vessel to rise to the top of the American continental drift. The ship was accompanied by William York, who, along with William E. Blanchard, sailed the \"Oryx,\" which was the first American-owned ship in New England, into the Atlantic. It was returned to the U.S. under the title \"Reformed\" six months later, when it was restored to the dock of\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:20,668 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 6/10\u001b[0m\n",
      "due east from Gould City to the upper North Island of West Wales. The journey is the longest in the history of the world, reaching about thirty miles. All roads leading to the North Island are closed, and the road's rampages are so far as to divide the route from the North Island of West Wales. As in the previous \"Greater Great Britain\", there are no gates here. You may travel for free on any Western highway which is listed on the map, and your car should be allowed to pass on your right\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:21,648 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 7/10\u001b[0m\n",
      "due east from Gould City to the northwest corner of the Bayside neighborhood.\n",
      "\n",
      "\"I had a pretty rough afternoon at the time,\" said Alameda County sheriff's Lt. Paul Lewis.\n",
      "\n",
      "The county also warned residents to stay away from streets that may be hazardous.\n",
      "\n",
      "\"This does not reflect the safety and security of our residents,\" said Nuno Zcq, director of the Bay Area Division of the California Department of Justice and a co-director of the Oakland-area Center for Investigative Journalism\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:22,638 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 8/10\u001b[0m\n",
      "due east from Gould City to the south.\n",
      "\n",
      "The truck drove off, ending up on the side of the road on the north side of the road, and stopped on the main road. It then passed the road at a stop sign for the northbound lanes, speeding past the intersection of Fifth Avenue and H Street. The driver then pulled over and took a passenger's license, causing the operator to warn the driver that a truck had passed by.\n",
      "\n",
      "The driver then drove off on the opposite side of the road,\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:23,798 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 9/10\u001b[0m\n",
      "due east from Gould City to Queen Street for the night of August 10. It was known as the Battle of Hynnia. The battles took place in a 1,400-square-foot room on the street.\n",
      "\n",
      "The battle took place between the tribes of the Blighs and Lothians on August 11. (The Blighs were proud of their claim to the position of the city from which the eastern mountains of the eastern part of the province were derived.) The Blighs captured the eastern half of\n",
      "---------------\n",
      "\n",
      "[ \u001b[36m2024-03-08 12:17:24,780 \u001b[0m][\u001b[2;37mqtransform.run.infer\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mGenerating sample: 10/10\u001b[0m\n",
      "due east from Gould City to Kippen, about fifty miles northeast of Wichita. Most of the east had been closed since the late 1800s, and no one believed it to be a problem.\n",
      "\n",
      "The survey is due to begin in January.<|endoftext|>The World Series of Poker Series is one of the most popular ways for large and small players to get their hands on a big tournament to watch. While the tournament is a little different, there are still a few key differences that have to be made between both groups.\n",
      "\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=\"due east from Gould City to\"\n",
    "qtransform.notebook_run([\"run=infer\", \n",
    "                         \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\",\n",
    "                         \"run.max_new_tokens=100\",\n",
    "                         \"run.start=\"+start], logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28247f61-bea8-475d-8d83-8610e2605060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TransformersTokenizer', 'pretrained_tokenizer': 'GPT2TokenizerFast', 'encoding': 'Kristijan/wikitext-103-tokenizer', 'module': 'transformers', 'fast': True}, 'dataloader': {'shuffle': False, 'num_workers': 2, 'batch_size': 12}, 'subset': 'wikitext-103-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 256}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 50, 'out_dir': '', 'profile': True, 'checkpoint_dir': 'models', 'from_checkpoint': '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7', 'pretrained_model': None, 'row_limit': 10, 'onnx_model': {'path': None, 'tokenizer': {'module': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-03-08 12:26:55,005 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,009 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,012 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,498 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,501 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,502 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,504 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-08_12:26:55\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,505 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:26:55,510 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 12:26:56.317427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-08 12:27:00,700 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/vocab.json\n",
      "loading file merges.txt from cache at /home/mabot004/.cache/huggingface/hub/models--Kristijan--wikitext-103-tokenizer/snapshots/347b90366a52a49e5071ab18cf4bb06dabfc6f82/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-08 12:27:01,795 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: Kristijan/wikitext-103-tokenizer and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,804 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/train-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,808 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 121454329 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,811 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/eval-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,814 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 254799 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,818 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/tokenized/Kristijan/wikitext-103-tokenizer/bench-wikitext-103-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,821 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 289985 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:01,826 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:04,090 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=256, vocab_size=50304, n_layer=2, n_head=6, n_embd=384, dropout=0.1, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "50304 384\n",
      "[ \u001b[36m2024-03-08 12:27:04,268 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:04,288 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:04,297 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:04,320 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:04,680 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 43.36M\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:05,164 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmark fo 50 samples\u001b[0m\n",
      "[ \u001b[36m2024-03-08 12:27:05,168 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mDatalaoder length might not be correct\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-08 12:27:05 10884:10884 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-03-08 12:27:06 10884:10884 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-03-08 12:27:06 10884:10884 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "[W collection.cpp:700] Warning: Failed to recover relationship between all profiler and kineto events: 104 vs. 0  reassociated. (function reassociate)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m args_benchmarking \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=bench\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.dataloader.shuffle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m ]\n\u001b[0;32m---> 15\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_benchmarking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:37\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args, loglevel)\u001b[0m\n\u001b[1;32m     35\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/bench.py:93\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m profile(activities\u001b[38;5;241m=\u001b[39m[activities], profile_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, record_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBENCHMARK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_data\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m                 log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBenchmark results: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mbench_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbench_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39mrow_limit\u001b[38;5;241m=\u001b[39mrow_limit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/bench.py:135\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(cfg, model_data, bench_dataloader)\u001b[0m\n\u001b[1;32m    133\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m     perplexity[i] \u001b[38;5;241m=\u001b[39m measure_perplexity(probs, labels)\n\u001b[0;32m--> 135\u001b[0m     accuracy[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, inputs=probs)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#other_perplexity += measure_perplexity(probs, labels)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#other_accuracy += measure_accuracy(model_type=model_data.type, model=model_data.model, labels=labels, inputs=probs)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/bench.py:171\u001b[0m, in \u001b[0;36mmeasure_accuracy\u001b[0;34m(model_type, model, labels, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m#input contains complete batches of samples from dataset, cut a small portion (at max half of tokens) and generate text\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     N, C \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 171\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     prompt_length \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, high\u001b[38;5;241m=\u001b[39mC\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    173\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBegin measuring accuracy with prompt_length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.device' object is not callable"
     ]
    }
   ],
   "source": [
    "args_benchmarking = [ \n",
    "    \"run=bench\",\n",
    "    \"run.from_checkpoint=/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/wikitext/GPT_wikitext_2024-03-08_11:37:32__epoch:7\",\n",
    "    \"run.num_samples=50\",\n",
    "    'dataset=huggingface',\n",
    "    'dataset.name=wikitext',\n",
    "    'dataset.subset=wikitext-103-raw-v1',\n",
    "    'dataset/tokenizer=transformers',\n",
    "    'dataset.tokenizer.encoding=Kristijan/wikitext-103-tokenizer',\n",
    "    'dataset.tokenizer.pretrained_tokenizer=GPT2TokenizerFast',\n",
    "    'dataset.dataloader.shuffle=False',\n",
    "    \"+model.args.block_size=256\",\n",
    "    \"dataset.dataloader.shuffle=False\",\n",
    "]\n",
    "qtransform.notebook_run(args_benchmarking, logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1a1dd-4071-4ba9-8f37-1a48afcb721a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
