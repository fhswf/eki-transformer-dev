{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac70f3b-3c47-455f-823d-d3819030a46a",
   "metadata": {},
   "source": [
    "### Compare karpathy qtransform\n",
    "This notebook aims to debug our qtransform application, as currently our models reach a low loss during training but generate poor results during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481dc8f9-368c-4a13-b000-e43108d83de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform\n",
    "import torch\n",
    "from brevitas import nn as qnn\n",
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e07b7-7255-43ed-a63c-4d42a39b13dc",
   "metadata": {},
   "source": [
    "seed is different (1337 instead of 123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72727ea6-1c96-4b85-8db1-ad6d369c457f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from qtransform.model import gpt as qtransform_gpt #import GPTConfig, GPT\n",
    "import model as karpathy_model\n",
    "\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "def sample(ckpt_path, karpathy: bool, start: str = \"\\n\"):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "    out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "    num_samples = 10 # number of samples to draw\n",
    "    max_new_tokens = 500 # number of tokens generated in each sample\n",
    "    temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "    top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "    device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "    dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "    compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "    #exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "    # -----------------------------------------------------------------------------\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "    \n",
    "    # model\n",
    "    if init_from == 'resume':\n",
    "        # init from a model saved in a specific directory\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        if karpathy:\n",
    "            gptconf = karpathy_model.GPTConfig(**checkpoint['model_args'])\n",
    "            state_dict = checkpoint['model']\n",
    "            model = karpathy_model.GPT(gptconf)\n",
    "        else:\n",
    "            gptconf = qtransform_gpt.GPTConfig(**checkpoint['model_cfg'][\"args\"])\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            model = qtransform_gpt.GPT(gptconf)\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "    elif init_from.startswith('gpt2'):\n",
    "        # init from a given GPT-2 model\n",
    "        model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "    # look for the meta pickle in case it is available in the dataset folder\n",
    "    load_meta = False\n",
    "    if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "        meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "        load_meta = os.path.exists(meta_path)\n",
    "    if load_meta:\n",
    "        print(f\"Loading meta from {meta_path}...\")\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    else:\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614b999c-6b93-4dd7-94e5-c05431306c00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.94M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "We are consul:\n",
      "\n",
      "You we shall have so his power to bear and\n",
      "To give away the whole bastard, or else he shall have been,\n",
      "And tell them and they are not speak.\n",
      "\n",
      "First Citizen:\n",
      "Not here we be admitted to see him:\n",
      "The gods of such power in the tribunes, and the rock to give us,\n",
      "We'll have show him, and we march upon the Roman.\n",
      "First Citizen:\n",
      "Second Citizen:\n",
      "Say, we are given to our general\n",
      "\n",
      "The Volscian:\n",
      "SICINIUS: 'IUS:\n",
      "\n",
      "Not at Senator:\n",
      "Wherein Marcius.\n",
      "First Senator:\n",
      "Since we're people, you have done\n",
      "AUFIDIUS:\n",
      "Unseparable, the people.\n",
      "\n",
      "You are in this\n",
      "\n",
      "BRUTUS:\n",
      "Your voices?\n",
      "SICINIUS:\n",
      "\n",
      "Let me all the people,\n",
      "\n",
      "Second Senator:\n",
      "MENENIUS: I will not us us hear'd with tribunes can of thisICINIUS:\n",
      "MENENIUS: for you say,\n",
      "\n",
      "They are thus, what's their voices?\n",
      "BRUTUS:\n",
      "\n",
      "SICINIUS:\n",
      "What we'll be so very consulch you, he's the people,\n",
      "Threely Marcius?\n",
      "Are you were a gods are worthy Marcius,\n",
      "Here comes.\n",
      "You are thus!\n",
      "MENENIUS:\n",
      "Scing the people,\n",
      "More than Marcius, they are so, I had made to him, go, to do not have here.\n",
      "\n",
      "When we have done,\n",
      "CORIUS:\n",
      "\n",
      "\n",
      "COMINIOLANUS:\n",
      "As now.\n",
      "\n",
      "COMINIUS: when you tell him.\n",
      "My tribunes to,,--\n",
      "AUFIDIUS:\n",
      "Meseech you, you?\n",
      "SICINIUS:\n",
      "\n",
      "\n",
      "AUFIDIUS:\n",
      "BRUTUS:\n",
      "Are an gods, and the Vols; but to yourUS:\n",
      "So will and would not meet,\n",
      "Let's a Capitol:' that we are, what flits, so look on the senate-in Corioli,\n",
      "COMINIUS:\n",
      "My true.\n",
      "Thou?\n",
      "SICINIUS:\n",
      "Which is a Vols,\n",
      "MENENI\n",
      "---------------\n",
      "\n",
      "RICHARD III:\n",
      "We are the king, at once more can dissolve and full of false\n",
      "The crown, thus look on?\n",
      "\n",
      "RATCLARET:\n",
      "CLIFFORD:\n",
      "Then let me not not what news, thou hast as they have won a subject didst thou.\n",
      "\n",
      "Hath he is, look yourself to me, and thy knee, or my soul,\n",
      "\n",
      "\n",
      "\n",
      "KING RICHARD III:\n",
      "KING RICHARD III:\n",
      "\n",
      "THUMBERLAND:\n",
      "The king? what's sake, good for him with me hear that blood.\n",
      "KING RICHARD III:\n",
      "KING RICHARD III:\n",
      "QUEEN MARGARET:\n",
      "KING RICHARD III:\n",
      "Ah, we may not my lord, thou art.\n",
      "Why, my lord! what news is dead?\n",
      "KING RICHARD III:\n",
      "\n",
      "PRINCE EDWARD IV:\n",
      "What may speak our brother.\n",
      "KING EDWARD: if Richard? Sir, take it is my lord'st me from thy wife,\n",
      "\n",
      "KING RICHARD III:\n",
      "GLOUCESTER:\n",
      "QUEEN MARGARENCE:\n",
      "'Till not speak my lord!\n",
      "My lord. What is the king I did thy head to their hours the crown, when thou, as my sovereign! do it, Duke of York?\n",
      "We send him there, and my king,\n",
      "And God,\n",
      "\n",
      "Where are in graves?\n",
      "\n",
      "\n",
      "\n",
      "KING RICHARD III:\n",
      "Out of Wales, the prince, I keep me not thy consent, and thou,\n",
      "Second Murderer own arms for thy name is it ere thou fly:\n",
      "And, against thy news?--\n",
      "\n",
      "DORSET:\n",
      "RICHARD III:\n",
      "For Edward's death?\n",
      "KING HENRY VI:\n",
      "THUMBERLAND:\n",
      "KING HENRY VI:\n",
      "And so,\n",
      "And any Richard,\n",
      "TYRICHARDENCE:\n",
      "I am.\n",
      "\n",
      "KING HEN ELIZELO,\n",
      "And here hath slain to Edward is the loving liege, for thy sovereign'st thou woe, I keep the crown upon that thou, I would the king,\n",
      "QUEEN MARGARET:\n",
      "With horse, thou wilt thou kill'd at thy fortune, for my,\n",
      "KING HENRY VI:\n",
      "And not thy looks\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/nanoGPT/out-shakespeare/ckpt.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkarpathy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m, in \u001b[0;36msample\u001b[0;34m(ckpt_path, karpathy)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 94\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanoGPT/model.py:316\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    314\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    318\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nanoGPT/model.py:181\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nanoGPT/model.py:105\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nanoGPT/model.py:89\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[0;32m---> 89\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m(x)\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n\u001b[1;32m     91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#karpathy's model yields good results\n",
    "sample(\"/home/mabot004/nanoGPT/out-shakespeare/ckpt.pt\", karpathy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7abe3a-2b27-4d81-8e87-79bdcf7f692c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-21 12:09:11,346 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:11,349 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:11,352 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:11,949 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,100 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,180 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,198 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,276 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,293 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,376 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,394 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,410 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,576 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,594 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:12,676 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 12:09:13,086 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " II\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Have\n",
      "\n",
      "\n",
      " how\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " you:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " as\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BR\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "II i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Both\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/nanoGPT/GPT_2024-02-21_11:08:06__epoch:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkarpathy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 94\u001b[0m, in \u001b[0;36msample\u001b[0;34m(ckpt_path, karpathy)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m---> 94\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:281\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    279\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    283\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/gpt.py:141\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/model/modules/__init__.py:262\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ln1(x)\n\u001b[1;32m    261\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x)))\n\u001b[0;32m--> 262\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_ln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual2(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)))\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m#x = x + self.attn(self.ln_1(x))\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m#x = x + self.mlp(self.ln_2(x))\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1495\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1495\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#meanwhile ours generates nonsense\n",
    "sample(\"/home/mabot004/nanoGPT/GPT_2024-02-21_11:08:06__epoch:1\", karpathy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64ecc330-b617-4b20-b8d2-3527ff244550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/home/mabot004/nanoGPT/GPT_2024-02-21_11:08:06__epoch:1\")\n",
    "checkpoint[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a78d71-d319-4a30-b893-7bb8815eaaf5",
   "metadata": {},
   "source": [
    "our model lowers its loss much faster than karpathy because the learning rate is divided by 10 after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1ad918-12f4-4589-9aa3-2bb69696e199",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Overriding config with config/train_shakespeare.py:',\n",
       " '# train a miniature character-level shakespeare model',\n",
       " '# good for debugging and playing on macbooks and such',\n",
       " '',\n",
       " \"out_dir = 'out-shakespeare'\",\n",
       " \"eval_interval = 250 # keep frequent because we'll overfit\",\n",
       " 'eval_iters = 200',\n",
       " \"log_interval = 10 # don't print too too often\",\n",
       " '',\n",
       " '# we expect to overfit on this small dataset, so only save when val improves',\n",
       " 'always_save_checkpoint = False',\n",
       " '',\n",
       " 'wandb_log = False # override via command line if you like',\n",
       " \"wandb_project = 'shakespeare'\",\n",
       " \"wandb_run_name = 'mini-gpt'\",\n",
       " '',\n",
       " \"dataset = 'shakespeare'\",\n",
       " 'gradient_accumulation_steps = 1',\n",
       " 'batch_size = 64',\n",
       " 'block_size = 256 # context of up to 256 previous characters',\n",
       " '',\n",
       " '# baby GPT model :)',\n",
       " 'n_layer = 6',\n",
       " 'n_head = 6',\n",
       " 'n_embd = 384',\n",
       " 'dropout = 0.2',\n",
       " '',\n",
       " 'learning_rate = 1e-3 # with baby networks can afford to go a bit higher',\n",
       " 'max_iters = 5000',\n",
       " 'lr_decay_iters = 5000 # make equal to max_iters usually',\n",
       " 'min_lr = 1e-4 # learning_rate / 10 usually',\n",
       " 'beta2 = 0.99 # make a bit bigger because number of tokens per iter is small',\n",
       " '',\n",
       " 'warmup_iters = 100 # not super necessary potentially',\n",
       " '',\n",
       " '# on macbook also add',\n",
       " \"# device = 'cpu'  # run on cpu only\",\n",
       " '# compile = False # do not torch compile the model',\n",
       " '',\n",
       " 'tokens per iteration will be: 16,384',\n",
       " 'Initializing a new model from scratch',\n",
       " 'defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)',\n",
       " 'number of parameters: 29.94M',\n",
       " 'num decayed parameter tensors: 26, with 30,031,872 parameters',\n",
       " 'num non-decayed parameter tensors: 13, with 4,992 parameters',\n",
       " 'using fused AdamW: True',\n",
       " 'compiling the model... (takes a ~minute)',\n",
       " 'step 0: train loss 10.8998, val loss 10.8949',\n",
       " 'iter 0: loss 10.9011, time 23012.68ms, mfu -100.00%',\n",
       " 'iter 10: loss 9.7456, time 92.54ms, mfu 10.60%',\n",
       " 'iter 20: loss 8.9365, time 93.28ms, mfu 10.59%',\n",
       " 'iter 30: loss 7.6917, time 92.73ms, mfu 10.59%',\n",
       " 'iter 40: loss 6.5745, time 92.36ms, mfu 10.59%',\n",
       " 'iter 50: loss 6.0476, time 92.37ms, mfu 10.59%',\n",
       " 'iter 60: loss 5.8015, time 92.04ms, mfu 10.60%',\n",
       " 'iter 70: loss 5.5993, time 92.66ms, mfu 10.60%',\n",
       " 'iter 80: loss 5.3834, time 91.90ms, mfu 10.60%',\n",
       " 'iter 90: loss 5.1898, time 92.03ms, mfu 10.61%',\n",
       " 'iter 100: loss 5.0143, time 92.71ms, mfu 10.60%',\n",
       " 'iter 110: loss 4.8193, time 92.15ms, mfu 10.61%',\n",
       " 'iter 120: loss 4.6688, time 92.65ms, mfu 10.61%',\n",
       " 'iter 130: loss 4.5585, time 92.35ms, mfu 10.61%',\n",
       " 'iter 140: loss 4.5856, time 92.72ms, mfu 10.60%',\n",
       " 'iter 150: loss 4.5026, time 93.02ms, mfu 10.60%',\n",
       " 'iter 160: loss 4.4827, time 92.36ms, mfu 10.60%',\n",
       " 'iter 170: loss 4.3229, time 92.35ms, mfu 10.60%',\n",
       " 'iter 180: loss 4.2855, time 93.09ms, mfu 10.59%',\n",
       " 'iter 190: loss 4.1876, time 92.32ms, mfu 10.60%',\n",
       " 'iter 200: loss 4.1916, time 92.32ms, mfu 10.60%',\n",
       " 'iter 210: loss 4.0886, time 92.31ms, mfu 10.60%',\n",
       " 'iter 220: loss 4.0119, time 92.31ms, mfu 10.60%',\n",
       " 'iter 230: loss 4.0055, time 92.32ms, mfu 10.60%',\n",
       " 'iter 240: loss 3.9258, time 92.61ms, mfu 10.60%',\n",
       " 'step 250: train loss 3.8540, val loss 4.9136',\n",
       " 'saving checkpoint to out-shakespeare',\n",
       " 'iter 250: loss 3.9744, time 16453.32ms, mfu 9.55%',\n",
       " 'iter 260: loss 3.8296, time 92.62ms, mfu 9.65%',\n",
       " 'iter 270: loss 3.7680, time 92.65ms, mfu 9.75%',\n",
       " 'iter 280: loss 3.7572, time 91.82ms, mfu 9.84%',\n",
       " 'iter 290: loss 3.7601, time 91.85ms, mfu 9.92%',\n",
       " 'iter 300: loss 3.7516, time 92.63ms, mfu 9.99%',\n",
       " 'iter 310: loss 3.8182, time 92.47ms, mfu 10.05%',\n",
       " 'iter 320: loss 3.5779, time 92.32ms, mfu 10.11%',\n",
       " 'iter 330: loss 3.5799, time 92.37ms, mfu 10.16%',\n",
       " 'iter 340: loss 3.5791, time 92.63ms, mfu 10.20%',\n",
       " 'iter 350: loss 3.4809, time 92.16ms, mfu 10.24%',\n",
       " 'iter 360: loss 3.4716, time 92.74ms, mfu 10.28%',\n",
       " 'iter 370: loss 3.4929, time 92.69ms, mfu 10.31%',\n",
       " 'iter 380: loss 3.3926, time 93.18ms, mfu 10.33%',\n",
       " 'iter 390: loss 3.4139, time 92.31ms, mfu 10.36%',\n",
       " 'iter 400: loss 3.3256, time 92.33ms, mfu 10.38%',\n",
       " 'iter 410: loss 3.3003, time 92.65ms, mfu 10.40%',\n",
       " 'iter 420: loss 3.1742, time 92.71ms, mfu 10.42%',\n",
       " 'iter 430: loss 3.2849, time 92.46ms, mfu 10.44%',\n",
       " 'iter 440: loss 3.1730, time 92.33ms, mfu 10.46%',\n",
       " 'iter 450: loss 3.2233, time 92.75ms, mfu 10.47%',\n",
       " 'iter 460: loss 3.0994, time 93.03ms, mfu 10.48%',\n",
       " 'iter 470: loss 3.0946, time 91.82ms, mfu 10.50%',\n",
       " 'iter 480: loss 3.1007, time 92.70ms, mfu 10.50%',\n",
       " 'iter 490: loss 2.9293, time 92.17ms, mfu 10.52%',\n",
       " 'step 500: train loss 2.7538, val loss 5.0743',\n",
       " 'iter 500: loss 3.0445, time 13779.22ms, mfu 9.47%',\n",
       " 'iter 510: loss 2.8878, time 92.35ms, mfu 9.59%',\n",
       " 'iter 520: loss 2.9778, time 93.16ms, mfu 9.68%',\n",
       " 'iter 530: loss 2.8506, time 92.75ms, mfu 9.77%',\n",
       " 'iter 540: loss 2.8877, time 92.57ms, mfu 9.85%',\n",
       " 'iter 550: loss 2.7851, time 92.29ms, mfu 9.93%',\n",
       " 'iter 560: loss 2.7865, time 91.82ms, mfu 10.00%',\n",
       " 'iter 570: loss 2.7766, time 93.21ms, mfu 10.06%',\n",
       " 'iter 580: loss 2.7324, time 92.35ms, mfu 10.11%',\n",
       " 'iter 590: loss 2.8223, time 92.31ms, mfu 10.16%',\n",
       " 'iter 600: loss 2.6526, time 92.97ms, mfu 10.20%',\n",
       " 'iter 610: loss 2.7829, time 92.68ms, mfu 10.24%',\n",
       " 'iter 620: loss 2.6743, time 92.61ms, mfu 10.27%',\n",
       " 'iter 630: loss 2.5485, time 92.33ms, mfu 10.31%',\n",
       " 'iter 640: loss 2.5749, time 93.01ms, mfu 10.33%',\n",
       " 'iter 650: loss 2.4677, time 92.52ms, mfu 10.36%',\n",
       " 'iter 660: loss 2.5287, time 92.06ms, mfu 10.39%',\n",
       " 'iter 670: loss 2.4221, time 92.79ms, mfu 10.41%',\n",
       " 'iter 680: loss 2.4090, time 92.71ms, mfu 10.42%',\n",
       " 'iter 690: loss 2.5127, time 92.69ms, mfu 10.44%',\n",
       " 'iter 700: loss 2.4139, time 101.38ms, mfu 10.36%',\n",
       " 'iter 710: loss 2.2779, time 92.40ms, mfu 10.39%',\n",
       " 'iter 720: loss 2.3110, time 93.10ms, mfu 10.40%',\n",
       " 'iter 730: loss 2.3513, time 91.94ms, mfu 10.43%',\n",
       " 'iter 740: loss 2.2765, time 92.42ms, mfu 10.45%',\n",
       " 'step 750: train loss 1.7693, val loss 5.6252',\n",
       " 'iter 750: loss 2.1849, time 13813.05ms, mfu 9.41%',\n",
       " 'iter 760: loss 2.2213, time 92.44ms, mfu 9.53%',\n",
       " 'iter 770: loss 2.1767, time 92.43ms, mfu 9.64%',\n",
       " 'iter 780: loss 2.1622, time 92.41ms, mfu 9.73%',\n",
       " 'iter 790: loss 2.1507, time 91.99ms, mfu 9.83%',\n",
       " 'iter 800: loss 2.0854, time 91.92ms, mfu 9.91%',\n",
       " 'iter 810: loss 2.0116, time 92.61ms, mfu 9.98%',\n",
       " 'iter 820: loss 1.9885, time 93.87ms, mfu 10.02%',\n",
       " 'iter 830: loss 2.0331, time 92.42ms, mfu 10.08%',\n",
       " 'iter 840: loss 2.0158, time 91.99ms, mfu 10.14%',\n",
       " 'iter 850: loss 1.9564, time 92.66ms, mfu 10.18%',\n",
       " 'iter 860: loss 2.0105, time 92.79ms, mfu 10.22%',\n",
       " 'iter 870: loss 1.9484, time 91.94ms, mfu 10.27%',\n",
       " 'iter 880: loss 1.9052, time 92.51ms, mfu 10.30%',\n",
       " 'iter 890: loss 1.8556, time 92.26ms, mfu 10.33%',\n",
       " 'iter 900: loss 1.8172, time 92.43ms, mfu 10.36%',\n",
       " 'iter 910: loss 1.8430, time 92.75ms, mfu 10.38%',\n",
       " 'iter 920: loss 1.7502, time 92.40ms, mfu 10.40%',\n",
       " 'iter 930: loss 1.6899, time 91.95ms, mfu 10.43%',\n",
       " 'iter 940: loss 1.7826, time 92.38ms, mfu 10.45%',\n",
       " 'iter 950: loss 1.7415, time 92.04ms, mfu 10.47%',\n",
       " 'iter 960: loss 1.6633, time 92.68ms, mfu 10.48%',\n",
       " 'iter 970: loss 1.6969, time 92.63ms, mfu 10.49%',\n",
       " 'iter 980: loss 1.6085, time 92.85ms, mfu 10.50%',\n",
       " 'iter 990: loss 1.6721, time 92.48ms, mfu 10.51%',\n",
       " 'step 1000: train loss 0.9975, val loss 6.2472',\n",
       " 'iter 1000: loss 1.5755, time 13795.38ms, mfu 9.46%',\n",
       " 'iter 1010: loss 1.5770, time 92.59ms, mfu 9.58%',\n",
       " 'iter 1020: loss 1.5198, time 92.69ms, mfu 9.68%',\n",
       " 'iter 1030: loss 1.5292, time 93.15ms, mfu 9.76%',\n",
       " 'iter 1040: loss 1.5262, time 92.99ms, mfu 9.84%',\n",
       " 'iter 1050: loss 1.4627, time 92.03ms, mfu 9.92%',\n",
       " 'iter 1060: loss 1.4417, time 92.51ms, mfu 9.99%',\n",
       " 'iter 1070: loss 1.3992, time 93.18ms, mfu 10.04%',\n",
       " 'iter 1080: loss 1.4288, time 93.17ms, mfu 10.09%',\n",
       " 'iter 1090: loss 1.3779, time 92.41ms, mfu 10.14%',\n",
       " 'iter 1100: loss 1.3639, time 92.39ms, mfu 10.19%',\n",
       " 'iter 1110: loss 1.3825, time 92.71ms, mfu 10.23%',\n",
       " 'iter 1120: loss 1.4124, time 92.53ms, mfu 10.26%',\n",
       " 'iter 1130: loss 1.3415, time 93.05ms, mfu 10.29%',\n",
       " 'iter 1140: loss 1.2642, time 92.60ms, mfu 10.32%',\n",
       " 'iter 1150: loss 1.3074, time 92.71ms, mfu 10.35%',\n",
       " 'iter 1160: loss 1.2879, time 92.78ms, mfu 10.37%',\n",
       " 'iter 1170: loss 1.3220, time 92.69ms, mfu 10.39%',\n",
       " 'iter 1180: loss 1.2467, time 91.96ms, mfu 10.42%',\n",
       " 'iter 1190: loss 1.2713, time 92.67ms, mfu 10.43%',\n",
       " 'iter 1200: loss 1.2643, time 92.72ms, mfu 10.45%',\n",
       " 'iter 1210: loss 1.2312, time 92.59ms, mfu 10.46%',\n",
       " 'iter 1220: loss 1.1864, time 92.67ms, mfu 10.47%',\n",
       " 'iter 1230: loss 1.1618, time 92.28ms, mfu 10.49%',\n",
       " 'iter 1240: loss 1.1902, time 92.25ms, mfu 10.50%',\n",
       " 'step 1250: train loss 0.5619, val loss 6.8642',\n",
       " 'iter 1250: loss 1.2324, time 13780.55ms, mfu 9.46%',\n",
       " 'iter 1260: loss 1.1221, time 92.88ms, mfu 9.57%',\n",
       " 'iter 1270: loss 1.1922, time 92.65ms, mfu 9.67%',\n",
       " 'iter 1280: loss 1.1656, time 93.04ms, mfu 9.76%',\n",
       " 'iter 1290: loss 1.0993, time 92.56ms, mfu 9.84%',\n",
       " 'iter 1300: loss 1.0904, time 92.61ms, mfu 9.92%',\n",
       " 'iter 1310: loss 1.1043, time 92.34ms, mfu 9.99%',\n",
       " 'iter 1320: loss 1.0536, time 92.02ms, mfu 10.05%',\n",
       " 'iter 1330: loss 1.1346, time 92.27ms, mfu 10.11%',\n",
       " 'iter 1340: loss 1.0483, time 92.16ms, mfu 10.16%',\n",
       " 'iter 1350: loss 1.0243, time 92.38ms, mfu 10.21%',\n",
       " 'iter 1360: loss 1.0585, time 92.33ms, mfu 10.25%',\n",
       " 'iter 1370: loss 1.0100, time 92.11ms, mfu 10.29%',\n",
       " 'iter 1380: loss 1.1003, time 93.08ms, mfu 10.31%',\n",
       " 'iter 1390: loss 0.9948, time 92.68ms, mfu 10.34%',\n",
       " 'iter 1400: loss 0.9875, time 92.17ms, mfu 10.37%',\n",
       " 'iter 1410: loss 0.9960, time 92.15ms, mfu 10.40%',\n",
       " 'iter 1420: loss 0.9694, time 92.15ms, mfu 10.42%',\n",
       " 'iter 1430: loss 0.9306, time 92.02ms, mfu 10.44%',\n",
       " 'iter 1440: loss 0.9659, time 92.66ms, mfu 10.46%',\n",
       " 'iter 1450: loss 0.9165, time 93.05ms, mfu 10.47%',\n",
       " 'iter 1460: loss 0.9019, time 92.59ms, mfu 10.48%',\n",
       " 'iter 1470: loss 0.9042, time 92.62ms, mfu 10.49%',\n",
       " 'iter 1480: loss 0.9397, time 92.30ms, mfu 10.50%',\n",
       " 'iter 1490: loss 0.9208, time 92.63ms, mfu 10.51%',\n",
       " 'step 1500: train loss 0.3614, val loss 7.3177',\n",
       " 'iter 1500: loss 0.8365, time 13782.11ms, mfu 9.47%',\n",
       " 'iter 1510: loss 0.8576, time 92.62ms, mfu 9.58%',\n",
       " 'iter 1520: loss 0.9060, time 92.66ms, mfu 9.68%',\n",
       " 'iter 1530: loss 0.8146, time 92.53ms, mfu 9.77%',\n",
       " 'iter 1540: loss 0.8468, time 92.30ms, mfu 9.86%',\n",
       " 'iter 1550: loss 0.8529, time 92.33ms, mfu 9.93%',\n",
       " 'iter 1560: loss 0.7823, time 92.27ms, mfu 10.00%',\n",
       " 'iter 1570: loss 0.8448, time 92.62ms, mfu 10.06%',\n",
       " 'iter 1580: loss 0.8048, time 92.13ms, mfu 10.12%',\n",
       " 'iter 1590: loss 0.8147, time 92.70ms, mfu 10.16%',\n",
       " 'iter 1600: loss 0.7805, time 91.85ms, mfu 10.22%',\n",
       " 'iter 1610: loss 0.8157, time 92.66ms, mfu 10.25%',\n",
       " 'iter 1620: loss 0.8232, time 92.66ms, mfu 10.28%',\n",
       " 'iter 1630: loss 0.7840, time 92.20ms, mfu 10.32%',\n",
       " 'iter 1640: loss 0.7366, time 92.26ms, mfu 10.35%',\n",
       " 'iter 1650: loss 0.7926, time 91.82ms, mfu 10.38%',\n",
       " 'iter 1660: loss 0.7310, time 92.34ms, mfu 10.41%',\n",
       " 'iter 1670: loss 0.7925, time 92.32ms, mfu 10.43%',\n",
       " 'iter 1680: loss 0.7613, time 91.79ms, mfu 10.45%',\n",
       " 'iter 1690: loss 0.7606, time 91.84ms, mfu 10.48%',\n",
       " 'iter 1700: loss 0.7409, time 92.63ms, mfu 10.49%',\n",
       " 'iter 1710: loss 0.7555, time 92.15ms, mfu 10.50%',\n",
       " 'iter 1720: loss 0.7762, time 92.06ms, mfu 10.52%',\n",
       " 'iter 1730: loss 0.7314, time 92.27ms, mfu 10.53%',\n",
       " 'iter 1740: loss 0.7042, time 92.68ms, mfu 10.53%',\n",
       " 'step 1750: train loss 0.2590, val loss 7.6947',\n",
       " 'iter 1750: loss 0.7169, time 13779.39ms, mfu 9.49%',\n",
       " 'iter 1760: loss 0.7008, time 92.16ms, mfu 9.60%',\n",
       " 'iter 1770: loss 0.7420, time 92.11ms, mfu 9.71%',\n",
       " 'iter 1780: loss 0.7203, time 92.14ms, mfu 9.80%',\n",
       " 'iter 1790: loss 0.7156, time 92.15ms, mfu 9.88%',\n",
       " 'iter 1800: loss 0.6530, time 92.16ms, mfu 9.96%',\n",
       " 'iter 1810: loss 0.7088, time 92.16ms, mfu 10.03%',\n",
       " 'iter 1820: loss 0.6681, time 92.18ms, mfu 10.09%',\n",
       " 'iter 1830: loss 0.6463, time 92.14ms, mfu 10.14%',\n",
       " 'iter 1840: loss 0.6682, time 92.16ms, mfu 10.19%',\n",
       " 'iter 1850: loss 0.6533, time 92.18ms, mfu 10.24%',\n",
       " 'iter 1860: loss 0.6442, time 92.17ms, mfu 10.28%',\n",
       " 'iter 1870: loss 0.6415, time 92.14ms, mfu 10.31%',\n",
       " 'iter 1880: loss 0.6586, time 92.15ms, mfu 10.35%',\n",
       " 'iter 1890: loss 0.6514, time 92.15ms, mfu 10.38%',\n",
       " 'iter 1900: loss 0.6453, time 92.16ms, mfu 10.40%',\n",
       " 'iter 1910: loss 0.6409, time 92.13ms, mfu 10.43%',\n",
       " 'iter 1920: loss 0.6205, time 92.15ms, mfu 10.45%',\n",
       " 'iter 1930: loss 0.6317, time 92.15ms, mfu 10.47%',\n",
       " 'iter 1940: loss 0.6437, time 92.13ms, mfu 10.48%',\n",
       " 'iter 1950: loss 0.6139, time 92.16ms, mfu 10.50%',\n",
       " 'iter 1960: loss 0.6086, time 92.23ms, mfu 10.51%',\n",
       " 'iter 1970: loss 0.5992, time 92.14ms, mfu 10.53%',\n",
       " 'iter 1980: loss 0.5978, time 92.17ms, mfu 10.54%',\n",
       " 'iter 1990: loss 0.5824, time 92.14ms, mfu 10.55%',\n",
       " 'step 2000: train loss 0.1957, val loss 8.0881',\n",
       " 'iter 2000: loss 0.6227, time 13779.95ms, mfu 9.50%',\n",
       " 'iter 2010: loss 0.6079, time 92.15ms, mfu 9.61%',\n",
       " 'iter 2020: loss 0.6062, time 92.19ms, mfu 9.72%',\n",
       " 'iter 2030: loss 0.6064, time 92.17ms, mfu 9.81%',\n",
       " 'iter 2040: loss 0.5774, time 92.14ms, mfu 9.89%',\n",
       " 'iter 2050: loss 0.5952, time 92.11ms, mfu 9.97%',\n",
       " 'iter 2060: loss 0.5912, time 92.13ms, mfu 10.03%',\n",
       " 'iter 2070: loss 0.5965, time 92.14ms, mfu 10.09%',\n",
       " 'iter 2080: loss 0.5541, time 92.13ms, mfu 10.15%',\n",
       " 'iter 2090: loss 0.5694, time 92.14ms, mfu 10.20%',\n",
       " 'iter 2100: loss 0.5726, time 92.13ms, mfu 10.24%',\n",
       " 'iter 2110: loss 0.5414, time 92.14ms, mfu 10.28%',\n",
       " 'iter 2120: loss 0.5503, time 92.14ms, mfu 10.32%',\n",
       " 'iter 2130: loss 0.5421, time 92.14ms, mfu 10.35%',\n",
       " 'iter 2140: loss 0.5517, time 92.11ms, mfu 10.38%',\n",
       " 'iter 2150: loss 0.5241, time 92.13ms, mfu 10.41%',\n",
       " 'iter 2160: loss 0.5884, time 92.09ms, mfu 10.43%',\n",
       " 'iter 2170: loss 0.5732, time 92.13ms, mfu 10.45%',\n",
       " 'iter 2180: loss 0.5169, time 92.13ms, mfu 10.47%',\n",
       " 'iter 2190: loss 0.5094, time 92.12ms, mfu 10.49%',\n",
       " 'iter 2200: loss 0.5308, time 92.13ms, mfu 10.50%',\n",
       " 'iter 2210: loss 0.5156, time 92.17ms, mfu 10.52%',\n",
       " 'iter 2220: loss 0.5271, time 92.15ms, mfu 10.53%',\n",
       " 'iter 2230: loss 0.5117, time 92.14ms, mfu 10.54%',\n",
       " 'iter 2240: loss 0.5124, time 92.13ms, mfu 10.55%',\n",
       " 'step 2250: train loss 0.1641, val loss 8.3367',\n",
       " 'iter 2250: loss 0.5242, time 13773.15ms, mfu 9.50%',\n",
       " 'iter 2260: loss 0.5262, time 92.54ms, mfu 9.61%',\n",
       " 'iter 2270: loss 0.4904, time 92.16ms, mfu 9.71%',\n",
       " 'iter 2280: loss 0.4908, time 92.23ms, mfu 9.81%',\n",
       " 'iter 2290: loss 0.4802, time 92.00ms, mfu 9.89%',\n",
       " 'iter 2300: loss 0.5061, time 91.90ms, mfu 9.97%',\n",
       " 'iter 2310: loss 0.4981, time 92.56ms, mfu 10.03%',\n",
       " 'iter 2320: loss 0.4986, time 92.17ms, mfu 10.09%',\n",
       " 'iter 2330: loss 0.5028, time 92.63ms, mfu 10.14%',\n",
       " 'iter 2340: loss 0.4809, time 91.82ms, mfu 10.19%',\n",
       " 'iter 2350: loss 0.4937, time 92.69ms, mfu 10.23%',\n",
       " 'iter 2360: loss 0.4669, time 92.70ms, mfu 10.27%',\n",
       " 'iter 2370: loss 0.4735, time 92.34ms, mfu 10.30%',\n",
       " 'iter 2380: loss 0.4781, time 92.36ms, mfu 10.33%',\n",
       " 'iter 2390: loss 0.4951, time 91.82ms, mfu 10.37%',\n",
       " 'iter 2400: loss 0.4590, time 92.55ms, mfu 10.39%',\n",
       " 'iter 2410: loss 0.4743, time 92.32ms, mfu 10.41%',\n",
       " 'iter 2420: loss 0.4811, time 92.36ms, mfu 10.43%',\n",
       " 'iter 2430: loss 0.4212, time 92.46ms, mfu 10.45%',\n",
       " 'iter 2440: loss 0.4235, time 92.13ms, mfu 10.47%',\n",
       " 'iter 2450: loss 0.4440, time 92.15ms, mfu 10.49%',\n",
       " 'iter 2460: loss 0.4506, time 92.12ms, mfu 10.50%',\n",
       " 'iter 2470: loss 0.4882, time 92.13ms, mfu 10.52%',\n",
       " 'iter 2480: loss 0.4374, time 92.11ms, mfu 10.53%',\n",
       " 'iter 2490: loss 0.4340, time 92.14ms, mfu 10.54%',\n",
       " 'step 2500: train loss 0.1410, val loss 8.5485',\n",
       " 'iter 2500: loss 0.4279, time 13774.24ms, mfu 9.49%',\n",
       " 'iter 2510: loss 0.4222, time 92.13ms, mfu 9.61%',\n",
       " 'iter 2520: loss 0.4105, time 92.14ms, mfu 9.71%',\n",
       " 'iter 2530: loss 0.4346, time 92.14ms, mfu 9.80%',\n",
       " 'iter 2540: loss 0.4117, time 92.14ms, mfu 9.89%',\n",
       " 'iter 2550: loss 0.4335, time 92.17ms, mfu 9.96%',\n",
       " 'iter 2560: loss 0.4321, time 92.12ms, mfu 10.03%',\n",
       " 'iter 2570: loss 0.4129, time 92.12ms, mfu 10.09%',\n",
       " 'iter 2580: loss 0.4158, time 92.19ms, mfu 10.15%',\n",
       " 'iter 2590: loss 0.4476, time 92.13ms, mfu 10.20%',\n",
       " 'iter 2600: loss 0.4253, time 92.12ms, mfu 10.24%',\n",
       " 'iter 2610: loss 0.4036, time 92.14ms, mfu 10.28%',\n",
       " 'iter 2620: loss 0.4099, time 92.37ms, mfu 10.31%',\n",
       " 'iter 2630: loss 0.3923, time 92.21ms, mfu 10.35%',\n",
       " 'iter 2640: loss 0.3830, time 92.11ms, mfu 10.38%',\n",
       " 'iter 2650: loss 0.4182, time 92.12ms, mfu 10.40%',\n",
       " 'iter 2660: loss 0.4134, time 92.14ms, mfu 10.43%',\n",
       " 'iter 2670: loss 0.4087, time 92.20ms, mfu 10.45%',\n",
       " 'iter 2680: loss 0.4107, time 92.13ms, mfu 10.47%',\n",
       " 'iter 2690: loss 0.3805, time 92.48ms, mfu 10.48%',\n",
       " 'iter 2700: loss 0.3812, time 92.13ms, mfu 10.50%',\n",
       " 'iter 2710: loss 0.3881, time 92.14ms, mfu 10.51%',\n",
       " 'iter 2720: loss 0.3898, time 92.68ms, mfu 10.52%',\n",
       " 'iter 2730: loss 0.4012, time 92.71ms, mfu 10.52%',\n",
       " 'iter 2740: loss 0.3910, time 93.01ms, mfu 10.53%',\n",
       " 'step 2750: train loss 0.1258, val loss 8.7486',\n",
       " 'iter 2750: loss 0.3796, time 13771.09ms, mfu 9.48%',\n",
       " 'iter 2760: loss 0.3766, time 92.84ms, mfu 9.59%',\n",
       " 'iter 2770: loss 0.3873, time 92.67ms, mfu 9.69%',\n",
       " 'iter 2780: loss 0.3632, time 93.01ms, mfu 9.77%',\n",
       " 'iter 2790: loss 0.3588, time 92.57ms, mfu 9.85%',\n",
       " 'iter 2800: loss 0.3808, time 92.61ms, mfu 9.93%',\n",
       " 'iter 2810: loss 0.3612, time 92.30ms, mfu 10.00%',\n",
       " 'iter 2820: loss 0.3670, time 92.26ms, mfu 10.06%',\n",
       " 'iter 2830: loss 0.3922, time 91.92ms, mfu 10.12%',\n",
       " 'iter 2840: loss 0.3736, time 92.21ms, mfu 10.17%',\n",
       " 'iter 2850: loss 0.3831, time 92.52ms, mfu 10.21%',\n",
       " 'iter 2860: loss 0.3672, time 92.29ms, mfu 10.26%',\n",
       " 'iter 2870: loss 0.3651, time 93.15ms, mfu 10.28%',\n",
       " 'iter 2880: loss 0.3535, time 92.67ms, mfu 10.31%',\n",
       " 'iter 2890: loss 0.3696, time 92.30ms, mfu 10.34%',\n",
       " 'iter 2900: loss 0.3619, time 92.31ms, mfu 10.37%',\n",
       " 'iter 2910: loss 0.3649, time 92.58ms, mfu 10.39%',\n",
       " 'iter 2920: loss 0.3442, time 92.36ms, mfu 10.42%',\n",
       " 'iter 2930: loss 0.3579, time 92.59ms, mfu 10.43%',\n",
       " 'iter 2940: loss 0.3545, time 92.24ms, mfu 10.45%',\n",
       " 'iter 2950: loss 0.3484, time 92.54ms, mfu 10.47%',\n",
       " 'iter 2960: loss 0.3387, time 92.59ms, mfu 10.48%',\n",
       " 'iter 2970: loss 0.3495, time 92.34ms, mfu 10.49%',\n",
       " 'iter 2980: loss 0.3443, time 93.00ms, mfu 10.50%',\n",
       " 'iter 2990: loss 0.3428, time 92.65ms, mfu 10.51%',\n",
       " 'step 3000: train loss 0.1171, val loss 8.8926',\n",
       " 'iter 3000: loss 0.3422, time 13773.16ms, mfu 9.46%',\n",
       " 'iter 3010: loss 0.3483, time 92.70ms, mfu 9.57%',\n",
       " 'iter 3020: loss 0.3243, time 92.66ms, mfu 9.67%',\n",
       " 'iter 3030: loss 0.3500, time 96.63ms, mfu 9.72%',\n",
       " 'iter 3040: loss 0.3250, time 92.58ms, mfu 9.81%',\n",
       " 'iter 3050: loss 0.3383, time 91.85ms, mfu 9.90%',\n",
       " 'iter 3060: loss 0.3332, time 92.32ms, mfu 9.97%',\n",
       " 'iter 3070: loss 0.3328, time 92.67ms, mfu 10.03%',\n",
       " 'iter 3080: loss 0.3166, time 92.97ms, mfu 10.08%',\n",
       " 'iter 3090: loss 0.3288, time 92.45ms, mfu 10.13%',\n",
       " 'iter 3100: loss 0.3189, time 92.59ms, mfu 10.18%',\n",
       " 'iter 3110: loss 0.3394, time 92.38ms, mfu 10.22%',\n",
       " 'iter 3120: loss 0.3285, time 91.86ms, mfu 10.27%',\n",
       " 'iter 3130: loss 0.3136, time 92.50ms, mfu 10.30%',\n",
       " 'iter 3140: loss 0.3307, time 92.32ms, mfu 10.33%',\n",
       " 'iter 3150: loss 0.3349, time 91.91ms, mfu 10.37%',\n",
       " 'iter 3160: loss 0.3364, time 92.39ms, mfu 10.39%',\n",
       " 'iter 3170: loss 0.3173, time 91.83ms, mfu 10.42%',\n",
       " 'iter 3180: loss 0.3155, time 92.57ms, mfu 10.44%',\n",
       " 'iter 3190: loss 0.3316, time 92.36ms, mfu 10.45%',\n",
       " 'iter 3200: loss 0.3290, time 92.69ms, mfu 10.47%',\n",
       " 'iter 3210: loss 0.3217, time 92.27ms, mfu 10.48%',\n",
       " 'iter 3220: loss 0.3067, time 91.81ms, mfu 10.50%',\n",
       " 'iter 3230: loss 0.2965, time 91.81ms, mfu 10.52%',\n",
       " 'iter 3240: loss 0.3182, time 92.32ms, mfu 10.53%',\n",
       " 'step 3250: train loss 0.1081, val loss 9.0164',\n",
       " 'iter 3250: loss 0.2827, time 13770.67ms, mfu 9.48%',\n",
       " 'iter 3260: loss 0.3068, time 92.66ms, mfu 9.59%',\n",
       " 'iter 3270: loss 0.3144, time 92.26ms, mfu 9.70%',\n",
       " 'iter 3280: loss 0.3018, time 92.66ms, mfu 9.79%',\n",
       " 'iter 3290: loss 0.2933, time 93.02ms, mfu 9.86%',\n",
       " 'iter 3300: loss 0.3171, time 92.13ms, mfu 9.94%',\n",
       " 'iter 3310: loss 0.3107, time 92.13ms, mfu 10.01%',\n",
       " 'iter 3320: loss 0.3157, time 92.11ms, mfu 10.07%',\n",
       " 'iter 3330: loss 0.3107, time 92.13ms, mfu 10.13%',\n",
       " 'iter 3340: loss 0.3014, time 92.13ms, mfu 10.18%',\n",
       " 'iter 3350: loss 0.2982, time 92.16ms, mfu 10.23%',\n",
       " 'iter 3360: loss 0.2992, time 92.12ms, mfu 10.27%',\n",
       " 'iter 3370: loss 0.2963, time 92.14ms, mfu 10.31%',\n",
       " 'iter 3380: loss 0.2934, time 92.15ms, mfu 10.34%',\n",
       " 'iter 3390: loss 0.2972, time 92.14ms, mfu 10.37%',\n",
       " 'iter 3400: loss 0.2987, time 92.28ms, mfu 10.39%',\n",
       " 'iter 3410: loss 0.2908, time 92.11ms, mfu 10.42%',\n",
       " 'iter 3420: loss 0.2950, time 92.11ms, mfu 10.44%',\n",
       " 'iter 3430: loss 0.2843, time 92.15ms, mfu 10.46%',\n",
       " 'iter 3440: loss 0.2865, time 92.12ms, mfu 10.48%',\n",
       " 'iter 3450: loss 0.2832, time 92.14ms, mfu 10.50%',\n",
       " 'iter 3460: loss 0.2654, time 92.12ms, mfu 10.51%',\n",
       " 'iter 3470: loss 0.2780, time 92.08ms, mfu 10.52%',\n",
       " 'iter 3480: loss 0.2841, time 92.14ms, mfu 10.54%',\n",
       " 'iter 3490: loss 0.2635, time 92.14ms, mfu 10.55%',\n",
       " 'step 3500: train loss 0.1031, val loss 9.0987',\n",
       " 'iter 3500: loss 0.2847, time 13769.22ms, mfu 9.50%',\n",
       " 'iter 3510: loss 0.2776, time 92.14ms, mfu 9.61%',\n",
       " 'iter 3520: loss 0.2748, time 92.11ms, mfu 9.72%',\n",
       " 'iter 3530: loss 0.2938, time 92.62ms, mfu 9.80%',\n",
       " 'iter 3540: loss 0.2823, time 92.12ms, mfu 9.89%',\n",
       " 'iter 3550: loss 0.2784, time 92.13ms, mfu 9.96%',\n",
       " 'iter 3560: loss 0.2802, time 92.12ms, mfu 10.03%',\n",
       " 'iter 3570: loss 0.2749, time 92.15ms, mfu 10.09%',\n",
       " 'iter 3580: loss 0.2665, time 92.14ms, mfu 10.15%',\n",
       " 'iter 3590: loss 0.2607, time 92.40ms, mfu 10.19%',\n",
       " 'iter 3600: loss 0.2811, time 92.21ms, mfu 10.24%',\n",
       " 'iter 3610: loss 0.2812, time 92.13ms, mfu 10.28%',\n",
       " 'iter 3620: loss 0.2600, time 92.35ms, mfu 10.31%',\n",
       " 'iter 3630: loss 0.2774, time 92.14ms, mfu 10.34%',\n",
       " 'iter 3640: loss 0.2772, time 92.11ms, mfu 10.37%',\n",
       " 'iter 3650: loss 0.2714, time 92.55ms, mfu 10.40%',\n",
       " 'iter 3660: loss 0.2786, time 92.69ms, mfu 10.41%',\n",
       " 'iter 3670: loss 0.2645, time 92.33ms, mfu 10.44%',\n",
       " 'iter 3680: loss 0.2861, time 92.67ms, mfu 10.45%',\n",
       " 'iter 3690: loss 0.2734, time 92.68ms, mfu 10.46%',\n",
       " 'iter 3700: loss 0.2505, time 92.31ms, mfu 10.48%',\n",
       " 'iter 3710: loss 0.2645, time 92.31ms, mfu 10.49%',\n",
       " 'iter 3720: loss 0.2657, time 92.55ms, mfu 10.50%',\n",
       " 'iter 3730: loss 0.2546, time 92.70ms, mfu 10.51%',\n",
       " 'iter 3740: loss 0.2608, time 92.34ms, mfu 10.52%',\n",
       " 'step 3750: train loss 0.0977, val loss 9.2768',\n",
       " 'iter 3750: loss 0.2567, time 13772.30ms, mfu 9.48%',\n",
       " 'iter 3760: loss 0.2889, time 92.64ms, mfu 9.59%',\n",
       " 'iter 3770: loss 0.2673, time 92.63ms, mfu 9.69%',\n",
       " 'iter 3780: loss 0.2484, time 92.33ms, mfu 9.78%',\n",
       " 'iter 3790: loss 0.2572, time 92.31ms, mfu 9.86%',\n",
       " 'iter 3800: loss 0.2598, time 92.58ms, mfu 9.94%',\n",
       " 'iter 3810: loss 0.2519, time 92.13ms, mfu 10.01%',\n",
       " 'iter 3820: loss 0.2442, time 92.60ms, mfu 10.07%',\n",
       " 'iter 3830: loss 0.2485, time 92.35ms, mfu 10.12%',\n",
       " 'iter 3840: loss 0.2424, time 92.12ms, mfu 10.17%',\n",
       " 'iter 3850: loss 0.2546, time 92.12ms, mfu 10.22%',\n",
       " 'iter 3860: loss 0.2454, time 92.29ms, mfu 10.26%',\n",
       " 'iter 3870: loss 0.2669, time 93.00ms, mfu 10.29%',\n",
       " 'iter 3880: loss 0.2498, time 92.35ms, mfu 10.32%',\n",
       " 'iter 3890: loss 0.2453, time 92.05ms, mfu 10.35%',\n",
       " 'iter 3900: loss 0.2585, time 92.28ms, mfu 10.38%',\n",
       " 'iter 3910: loss 0.2429, time 92.71ms, mfu 10.40%',\n",
       " 'iter 3920: loss 0.2646, time 92.09ms, mfu 10.43%',\n",
       " 'iter 3930: loss 0.2304, time 92.98ms, mfu 10.44%',\n",
       " 'iter 3940: loss 0.2390, time 92.29ms, mfu 10.46%',\n",
       " 'iter 3950: loss 0.2432, time 92.66ms, mfu 10.47%',\n",
       " 'iter 3960: loss 0.2466, time 92.44ms, mfu 10.48%',\n",
       " 'iter 3970: loss 0.2466, time 92.63ms, mfu 10.49%',\n",
       " 'iter 3980: loss 0.2418, time 92.31ms, mfu 10.51%',\n",
       " 'iter 3990: loss 0.2456, time 92.65ms, mfu 10.51%',\n",
       " 'step 4000: train loss 0.0937, val loss 9.2830',\n",
       " 'iter 4000: loss 0.2512, time 13772.25ms, mfu 9.47%',\n",
       " 'iter 4010: loss 0.2441, time 92.61ms, mfu 9.58%',\n",
       " 'iter 4020: loss 0.2285, time 92.09ms, mfu 9.69%',\n",
       " 'iter 4030: loss 0.2416, time 92.33ms, mfu 9.78%',\n",
       " 'iter 4040: loss 0.2335, time 92.11ms, mfu 9.87%',\n",
       " 'iter 4050: loss 0.2281, time 92.51ms, mfu 9.94%',\n",
       " 'iter 4060: loss 0.2338, time 92.50ms, mfu 10.01%',\n",
       " 'iter 4070: loss 0.2205, time 92.29ms, mfu 10.07%',\n",
       " 'iter 4080: loss 0.2372, time 92.31ms, mfu 10.12%',\n",
       " 'iter 4090: loss 0.2419, time 91.87ms, mfu 10.18%',\n",
       " 'iter 4100: loss 0.2249, time 92.65ms, mfu 10.22%',\n",
       " 'iter 4110: loss 0.2220, time 91.82ms, mfu 10.26%',\n",
       " 'iter 4120: loss 0.2189, time 92.08ms, mfu 10.30%',\n",
       " 'iter 4130: loss 0.2345, time 92.39ms, mfu 10.33%',\n",
       " 'iter 4140: loss 0.2371, time 92.29ms, mfu 10.36%',\n",
       " 'iter 4150: loss 0.2434, time 92.15ms, mfu 10.39%',\n",
       " 'iter 4160: loss 0.2431, time 92.67ms, mfu 10.41%',\n",
       " 'iter 4170: loss 0.2316, time 91.82ms, mfu 10.44%',\n",
       " 'iter 4180: loss 0.2308, time 92.62ms, mfu 10.45%',\n",
       " 'iter 4190: loss 0.2277, time 92.63ms, mfu 10.46%',\n",
       " 'iter 4200: loss 0.2297, time 92.29ms, mfu 10.48%',\n",
       " 'iter 4210: loss 0.2128, time 92.35ms, mfu 10.49%',\n",
       " 'iter 4220: loss 0.2352, time 91.85ms, mfu 10.51%',\n",
       " 'iter 4230: loss 0.2224, time 91.82ms, mfu 10.53%',\n",
       " 'iter 4240: loss 0.2317, time 92.09ms, mfu 10.54%',\n",
       " 'step 4250: train loss 0.0903, val loss 9.3959',\n",
       " 'iter 4250: loss 0.2254, time 13774.58ms, mfu 9.49%',\n",
       " 'iter 4260: loss 0.2114, time 93.12ms, mfu 9.60%',\n",
       " 'iter 4270: loss 0.2285, time 92.11ms, mfu 9.70%',\n",
       " 'iter 4280: loss 0.2205, time 91.85ms, mfu 9.80%',\n",
       " 'iter 4290: loss 0.2137, time 92.11ms, mfu 9.88%',\n",
       " 'iter 4300: loss 0.2280, time 92.26ms, mfu 9.96%',\n",
       " 'iter 4310: loss 0.2217, time 92.47ms, mfu 10.02%',\n",
       " 'iter 4320: loss 0.2227, time 92.31ms, mfu 10.08%',\n",
       " 'iter 4330: loss 0.2162, time 92.94ms, mfu 10.13%',\n",
       " 'iter 4340: loss 0.2156, time 92.04ms, mfu 10.18%',\n",
       " 'iter 4350: loss 0.2173, time 92.64ms, mfu 10.22%',\n",
       " 'iter 4360: loss 0.2252, time 92.68ms, mfu 10.26%',\n",
       " 'iter 4370: loss 0.2239, time 92.62ms, mfu 10.29%',\n",
       " 'iter 4380: loss 0.2146, time 92.08ms, mfu 10.33%',\n",
       " 'iter 4390: loss 0.2223, time 92.17ms, mfu 10.36%',\n",
       " 'iter 4400: loss 0.2175, time 92.13ms, mfu 10.39%',\n",
       " 'iter 4410: loss 0.2250, time 92.17ms, mfu 10.41%',\n",
       " 'iter 4420: loss 0.2223, time 92.15ms, mfu 10.43%',\n",
       " 'iter 4430: loss 0.2156, time 92.11ms, mfu 10.45%',\n",
       " 'iter 4440: loss 0.2212, time 92.14ms, mfu 10.47%',\n",
       " 'iter 4450: loss 0.2201, time 92.11ms, mfu 10.49%',\n",
       " 'iter 4460: loss 0.2094, time 92.11ms, mfu 10.51%',\n",
       " 'iter 4470: loss 0.2122, time 92.14ms, mfu 10.52%',\n",
       " 'iter 4480: loss 0.2078, time 92.14ms, mfu 10.53%',\n",
       " 'iter 4490: loss 0.2019, time 92.13ms, mfu 10.54%',\n",
       " 'step 4500: train loss 0.0875, val loss 9.3998',\n",
       " 'iter 4500: loss 0.2078, time 13770.12ms, mfu 9.50%',\n",
       " 'iter 4510: loss 0.2235, time 92.15ms, mfu 9.61%',\n",
       " 'iter 4520: loss 0.2222, time 92.15ms, mfu 9.71%',\n",
       " 'iter 4530: loss 0.2164, time 92.17ms, mfu 9.81%',\n",
       " 'iter 4540: loss 0.2010, time 92.11ms, mfu 9.89%',\n",
       " 'iter 4550: loss 0.2120, time 92.17ms, mfu 9.96%',\n",
       " 'iter 4560: loss 0.2080, time 92.15ms, mfu 10.03%',\n",
       " 'iter 4570: loss 0.2156, time 92.15ms, mfu 10.09%',\n",
       " 'iter 4580: loss 0.2174, time 92.13ms, mfu 10.15%',\n",
       " 'iter 4590: loss 0.2096, time 92.10ms, mfu 10.20%',\n",
       " 'iter 4600: loss 0.2065, time 92.14ms, mfu 10.24%',\n",
       " 'iter 4610: loss 0.2164, time 92.16ms, mfu 10.28%',\n",
       " 'iter 4620: loss 0.2011, time 92.10ms, mfu 10.32%',\n",
       " 'iter 4630: loss 0.2021, time 92.11ms, mfu 10.35%',\n",
       " 'iter 4640: loss 0.2153, time 92.14ms, mfu 10.38%',\n",
       " 'iter 4650: loss 0.2218, time 92.13ms, mfu 10.41%',\n",
       " 'iter 4660: loss 0.2073, time 92.15ms, mfu 10.43%',\n",
       " 'iter 4670: loss 0.2159, time 92.21ms, mfu 10.45%',\n",
       " 'iter 4680: loss 0.2023, time 92.16ms, mfu 10.47%',\n",
       " 'iter 4690: loss 0.2003, time 92.16ms, mfu 10.49%',\n",
       " 'iter 4700: loss 0.2134, time 92.17ms, mfu 10.50%',\n",
       " 'iter 4710: loss 0.2058, time 92.14ms, mfu 10.51%',\n",
       " 'iter 4720: loss 0.2135, time 92.16ms, mfu 10.53%',\n",
       " 'iter 4730: loss 0.2077, time 92.12ms, mfu 10.54%',\n",
       " 'iter 4740: loss 0.2191, time 92.13ms, mfu 10.55%',\n",
       " 'step 4750: train loss 0.0856, val loss 9.4810',\n",
       " 'iter 4750: loss 0.2053, time 13772.51ms, mfu 9.50%',\n",
       " 'iter 4760: loss 0.2039, time 92.30ms, mfu 9.61%',\n",
       " 'iter 4770: loss 0.2038, time 92.03ms, mfu 9.72%',\n",
       " 'iter 4780: loss 0.1955, time 92.13ms, mfu 9.81%',\n",
       " 'iter 4790: loss 0.2091, time 92.38ms, mfu 9.89%',\n",
       " 'iter 4800: loss 0.2077, time 91.81ms, mfu 9.97%',\n",
       " 'iter 4810: loss 0.2041, time 92.43ms, mfu 10.03%',\n",
       " 'iter 4820: loss 0.2067, time 92.60ms, mfu 10.09%',\n",
       " 'iter 4830: loss 0.1958, time 92.31ms, mfu 10.14%',\n",
       " 'iter 4840: loss 0.2094, time 93.19ms, mfu 10.18%',\n",
       " 'iter 4850: loss 0.2076, time 91.83ms, mfu 10.23%',\n",
       " 'iter 4860: loss 0.2056, time 92.80ms, mfu 10.26%',\n",
       " 'iter 4870: loss 0.1978, time 92.80ms, mfu 10.29%',\n",
       " 'iter 4880: loss 0.1982, time 92.29ms, mfu 10.33%',\n",
       " 'iter 4890: loss 0.1999, time 91.83ms, mfu 10.36%',\n",
       " 'iter 4900: loss 0.1972, time 92.68ms, mfu 10.38%',\n",
       " 'iter 4910: loss 0.1930, time 92.08ms, mfu 10.41%',\n",
       " 'iter 4920: loss 0.2027, time 92.34ms, mfu 10.43%',\n",
       " 'iter 4930: loss 0.1920, time 92.67ms, mfu 10.45%',\n",
       " 'iter 4940: loss 0.1843, time 92.25ms, mfu 10.46%',\n",
       " 'iter 4950: loss 0.2070, time 92.49ms, mfu 10.48%',\n",
       " 'iter 4960: loss 0.1980, time 92.64ms, mfu 10.49%',\n",
       " 'iter 4970: loss 0.2010, time 92.64ms, mfu 10.50%',\n",
       " 'iter 4980: loss 0.1891, time 92.14ms, mfu 10.51%',\n",
       " 'iter 4990: loss 0.2009, time 92.63ms, mfu 10.52%',\n",
       " 'step 5000: train loss 0.0844, val loss 9.4588',\n",
       " 'iter 5000: loss 0.1943, time 13776.48ms, mfu 9.47%',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('train_shakespeare.out', 'r') as karpathy_result:\n",
    "    result = karpathy_result.read()\n",
    "result.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235526b0-7fb0-4fe7-9c83-508fb380fd6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 10, 'gradient_accumulation_steps': 1, 'flash': False, 'export': False, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-21 13:06:14,428 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:14,432 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:14,436 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 13:06:15.097201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-21 13:06:16,566 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,570 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,574 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,577 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-21_13:06:16\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,580 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,886 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,894 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,899 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,902 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,906 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 118309 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,909 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,912 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,922 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:16,957 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,133 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,180 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,199 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,276 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,293 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,304 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,391 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,403 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,491 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,501 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,591 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:17,616 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:18,087 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.51M\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:19,758 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:19,762 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:22,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.608381175994873\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:23,687 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.346301031112671\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:25,041 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.158055353164673\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:26,389 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 5.971330881118774\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:27,744 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 5.765323734283447\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:29,097 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 5.629354524612427\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:30,445 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 5.487095499038697\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:31,800 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 5.386763668060302\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:33,154 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.260457611083984\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:34,509 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.105164289474487\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:35,860 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.035233879089356\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:37,215 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 4.95036096572876\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:38,571 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.822760105133057\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:39,923 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.727867841720581\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:41,278 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.598275995254516\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:42,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.34987850189209\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:43,989 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 3.9748345851898192\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:45,319 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 3.3186208963394166\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:46,654 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 2.733143377304077\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:48,007 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 2.3163334369659423\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:49,353 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 1.935312020778656\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:50,708 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 1.6525158524513244\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:52,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 1.4436611771583556\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:53,417 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 1.2658702611923218\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:06:54,771 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 1.056493729352951\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:03,139 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/10: 1.1734434366226196\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:03,142 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m1.056493729352951\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:03,731 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:06:16__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:03,734 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:05,178 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.9664419412612915\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:06,530 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.953937155008316\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:07,885 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.903748619556427\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:09,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.8951196730136871\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:10,596 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.8810203433036804\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:11,945 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.840756744146347\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:13,298 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.8266213953495025\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:14,650 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.8144483447074891\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:16,004 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.7706876158714294\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:17,358 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.7751279890537262\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:18,703 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.7511259317398071\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:20,055 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.7417667865753174\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:21,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.703726989030838\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:22,763 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.6922036409378052\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:24,120 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.6706977546215057\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:25,477 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.6807100594043731\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:26,833 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.6733692586421967\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:28,189 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.6422507464885712\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:29,544 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.6286012530326843\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:30,897 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.6129437625408173\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:32,252 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.6091564714908599\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:33,605 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.5954651534557343\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:34,961 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.5736383557319641\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:36,316 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.5696245700120925\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:37,665 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.5516718685626983\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:46,036 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/10: 0.7254965305328369\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:46,040 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.5516718685626983\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:46,650 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:06:16__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:46,653 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:48,099 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.5225139677524566\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:49,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.5405037343502045\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:50,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.5488684505224228\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:52,150 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.5569551110267639\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:53,504 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.5282899469137192\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:54,858 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.5277098566293716\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:56,211 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.5538671255111695\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:57,564 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.5119339764118195\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:07:58,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.5405855923891068\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:00,265 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.535944226384163\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:01,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.5227920204401016\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:02,957 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.5319635421037674\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:04,312 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.5277522593736649\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:05,668 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.5208544701337814\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:07,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.5290393203496933\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:08,380 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.5305586516857147\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     27\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed=1337\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#same seed as karpathy\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         ]\n\u001b[1;32m     49\u001b[0m     qtransform\u001b[38;5;241m.\u001b[39mnotebook_run(args)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrain_qtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mtrain_qtransform\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m beta2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m \u001b[38;5;66;03m# make a bit bigger because number of tokens per iter is small\u001b[39;00m\n\u001b[1;32m     27\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed=1337\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#same seed as karpathy\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun=train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice=cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     ]\n\u001b[0;32m---> 49\u001b[0m \u001b[43mqtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:21\u001b[0m, in \u001b[0;36mnotebook_run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__init__.py:12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this app like amodule, Note: cfg is a Hydra config (OmegaConf Object)\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  __main__ \u001b[38;5;28;01mas\u001b[39;00m mn\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/__main__.py:44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:          \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqtransform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bench\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:108\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    105\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m    111\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:189\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, cfg, device, train_data_loader, eval_data_loader, optimizer, scheduler, timestamp)\u001b[0m\n\u001b[1;32m    185\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#dataloader always returns the same tensors after each epoch because it is casted inside of function call\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#therefore, cast it before training\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#TODO: find a more elegant solution, maybe by manipulating its seed with a torch.Generator?\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m## eval\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39meval_epoch_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eval_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qtransform-0.0.2.dev0-py3.10.egg/qtransform/run/train.py:246\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(cfg, device, model, train_data, optimizer, mini_run)\u001b[0m\n\u001b[1;32m    244\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(outputs, labels)\n\u001b[1;32m    245\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m gradient_accumulation_steps \u001b[38;5;66;03m#make all mini-batches account as one large batch\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m#clip gradients to prevent vanishing/exploding gradient problem\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# (https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m#nn.utils.clip_grad_value_(model.parameters(), clip_value=cfg.run.grad_clip)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m#karpathy uses norm gradient clipping which scales the pre-existing gradients with the grad_clip value\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_qtransform():\n",
    "    #from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py\n",
    "    #karpathy evaluates after 250 iterations, we implemented eval to do so after every epoch -> max_iters = 5000 / 200\n",
    "    eval_epoch_interval = 1 # keep frequent because we'll overfit\n",
    "    eval_iters = 200\n",
    "    max_iters = 250\n",
    "    epochs = 10 #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "    gradient_accumulation_steps = 1 #one large batch, potentially do gradient_accumulation_steps = 8 and batch_size = 8\n",
    "    batch_size = 64\n",
    "    block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "    # baby GPT model :)\n",
    "    n_layer = 6\n",
    "    n_head = 6\n",
    "    n_embd = 384\n",
    "    dropout = 0.2\n",
    "\n",
    "    learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "\n",
    "    #not implemented currently\n",
    "    lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "\n",
    "    #not used currently\n",
    "    min_lr = 1e-4 # learning_rate / 10 usually\n",
    "    beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "    args = [\n",
    "            \"seed=1337\", #same seed as karpathy\n",
    "            \"run=train\", \n",
    "            \"run.export=False\",\n",
    "            \"run.epochs=\"+str(epochs),\n",
    "            \"run.max_iters=\"+str(max_iters),\n",
    "            \"run.eval_epoch_interval=1\", \n",
    "            \"run.eval_iters=\"+str(eval_iters),\n",
    "            \"run.grad_clip=1.0\",\n",
    "            \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "            \"model.args.n_layer=\"+str(n_layer),\n",
    "            \"model.args.n_head=\"+str(n_head),\n",
    "            \"model.args.n_embd=\"+str(n_embd),\n",
    "            \"model.args.dropout=\"+str(dropout),\n",
    "            \"dataset=huggingface\", \n",
    "            \"dataset/tokenizer=tiktoken\",\n",
    "            \"dataset.tokenizer.encoding=gpt2\",\n",
    "            \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "            \"dataset.name=tiny_shakespeare\",\n",
    "            \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "            \"device=cuda\"\n",
    "        ]\n",
    "    qtransform.notebook_run(args)\n",
    "train_qtransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d4129-d575-4f8e-b5a6-ef1f73be77ec",
   "metadata": {},
   "source": [
    "### LayerNorm and GELU, just like Karpathy's model. Learning rate is set to decay after 5 epochs out of 10\n",
    "#### We overfit after the first epoch, however the eval losses are significantly lower compared to karpathy (karpathy had an eval loss about 10 times as high as his training loss)\n",
    "#### Our eval loss stays roughly the same after the first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12ca2f3-da41-490a-9913-459aa0f6e333",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 64}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1337, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'LayerNorm', 'flash': False}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.001, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 5, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 10, 'gradient_accumulation_steps': 1, 'flash': False, 'export': False, 'max_iters': 250, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 1.0, 'eval_epoch_interval': 1, 'eval_iters': 200}}\n",
      "[ \u001b[36m2024-02-21 13:08:16,847 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,850 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,853 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-02-21_13:08:16\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,858 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,862 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,875 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,879 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,883 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,886 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,889 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 118309 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,892 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,896 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,904 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mVocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:16,910 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,042 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,058 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,081 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,095 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,113 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,189 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,206 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,290 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,376 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,388 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,476 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,492 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:17,975 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:18,004 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:18,007 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:19,490 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 8.617336320877076\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:20,832 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 6.873014163970947\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:22,186 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 6.200290107727051\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:23,539 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 6.007695531845092\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:24,891 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 5.904830598831177\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:26,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 5.834317064285278\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:27,597 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 5.733217096328735\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:28,950 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 5.6174952507019045\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:30,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 5.458647394180298\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:31,655 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 5.297951602935791\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:33,006 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 5.194418668746948\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:34,356 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 5.099787664413452\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:35,686 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 4.949888134002686\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:37,023 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 4.86325511932373\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:38,374 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 4.793827915191651\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:39,726 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 4.716346359252929\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:41,079 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 4.709087133407593\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:42,428 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 4.577806615829468\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:43,782 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 4.483739185333252\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:45,133 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 4.439263486862183\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:46,483 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 4.31032395362854\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:47,842 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 4.179560136795044\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:49,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 4.024064373970032\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:50,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 3.7437366247177124\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:08:51,886 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 3.236054563522339\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:00,339 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 1/10: 2.9676074981689453\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:00,344 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m3.236054563522339\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:00,947 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:1\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:00,951 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 2/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:02,387 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 2.8025551319122313\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:03,737 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 2.4803247690200805\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:05,088 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 2.2093034029006957\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:06,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 2.0415209412574766\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:07,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 1.870394217967987\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:09,113 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 1.693783962726593\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:10,461 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 1.5842567443847657\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:11,810 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 1.4850234746932984\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:13,144 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 1.3921921491622924\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:14,472 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 1.351242995262146\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:15,818 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 1.2433575630187987\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:17,170 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 1.0886577248573304\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:18,520 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.8336531698703766\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:19,861 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.6526162624359131\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:21,189 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.5702128112316132\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:22,524 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.528004863858223\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:23,852 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.49741852283477783\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:25,180 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.45112851560115813\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:26,517 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.4016664057970047\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:27,846 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.35256220400333405\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:29,196 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.30113096833229064\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:30,545 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.2572552666068077\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:31,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.23647273182868958\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:33,245 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.22385796755552292\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:34,597 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.2045239359140396\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:43,025 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 2/10: 0.45264145731925964\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:43,029 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.2045239359140396\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:43,630 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:43,634 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 3/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:45,091 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.18430517762899398\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:46,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.18256212323904036\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:47,802 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.17604829221963883\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:49,152 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.16848496049642564\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:50,505 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.15850949883461\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:51,855 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.15002923756837844\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:53,205 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.1447013109922409\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:54,558 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.13727632239460946\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:55,910 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.13866283148527145\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:57,262 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.13491352945566176\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:58,615 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.1339341312646866\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:09:59,968 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.125163471698761\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:01,321 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.12576360255479813\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:02,672 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.1256664015352726\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:04,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.12537339255213736\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:05,338 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.11762663796544075\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:06,668 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.1220582477748394\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:07,999 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.11607517376542091\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:09,328 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.11851395294070244\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:10,658 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.12201786488294601\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:11,988 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.1204320639371872\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:13,317 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.11197666525840759\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:14,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.1170285925269127\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:15,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.1110974207520485\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:17,306 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.11505855545401573\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:25,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 3/10: 0.3343518078327179\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:25,749 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.11505855545401573\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:26,316 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:3\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:26,319 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 4/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:27,758 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.10745461285114288\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:29,111 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.10886368677020072\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:30,464 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.11494839265942573\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:31,817 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.11444461718201637\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:33,169 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.11251433342695236\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:34,522 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.10784393921494484\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:35,874 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.11539307236671448\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:37,227 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.11167211681604386\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:38,578 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.11298641785979271\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:39,930 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.11676192358136177\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:41,281 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.11026836261153221\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:42,630 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.10915960446000099\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:43,984 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.10787184908986092\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:45,341 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.11015079617500305\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:46,692 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.10605697855353355\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:48,049 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.11138258501887321\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:49,404 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.11870531365275383\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:50,764 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.1074389636516571\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:52,121 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.11176283285021782\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:53,474 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.10690734907984734\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:54,825 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.11036904826760292\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:56,184 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.11327295079827308\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:57,538 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.11343257799744606\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:10:58,895 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.10920192301273346\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:00,255 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.109053236246109\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:08,716 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 4/10: 0.3631870150566101\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:08,720 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.109053236246109\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:09,371 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:4\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:09,374 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 5/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:10,820 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.1058034859597683\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:12,173 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.10596395209431649\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:13,528 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.11329921782016754\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:14,882 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.10867793783545494\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:16,237 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.10573248714208602\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:17,589 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.1140502631664276\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:18,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.11320577710866928\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:20,291 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.10692966431379318\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:21,647 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.11176814958453178\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:22,995 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.1111711584031582\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:24,349 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.10598878636956215\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:25,700 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.10711091831326484\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:27,052 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.1096186175942421\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:28,406 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.10351037010550498\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:29,745 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.11375793516635894\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:31,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.10998420938849449\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:32,446 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.10693232715129852\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:33,797 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.11134813204407693\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:35,142 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.10508761703968048\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:36,491 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.10815237835049629\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:37,843 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.11146882772445679\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:39,197 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.10278815031051636\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:40,549 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.11325608938932419\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:41,901 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.10840946733951569\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:43,249 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.11193336248397827\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:51,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 5/10: 0.33802318572998047\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:51,688 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.11193336248397827\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:52,300 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:5\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:52,303 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 6/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:53,734 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.10856224000453948\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:55,063 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.10459395200014114\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:56,390 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.10627172961831093\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:57,733 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.0976499781012535\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:11:59,084 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.10466077476739884\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:00,433 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.10215826332569122\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:01,783 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.09468815103173256\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:03,130 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.09986722245812416\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:04,481 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.09550871178507805\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:05,834 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.09589750692248344\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:07,187 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.09179642349481583\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:08,540 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.09292086437344552\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:09,892 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.09730524495244026\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:11,241 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.09856158494949341\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:12,588 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.09285907819867134\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:13,939 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.09787482991814614\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:15,292 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.09275861904025078\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:16,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.09129416570067406\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:17,994 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.09500567764043807\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:19,346 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.09594396874308586\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:20,699 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.09329411238431931\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:22,048 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.09133555814623832\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:23,399 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.08982881680130958\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:24,748 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.09062132015824317\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:26,099 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.09309173300862313\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:34,543 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 6/10: 0.3355576992034912\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:34,548 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.09309173300862313\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:35,156 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:6\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:35,159 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 7/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:36,602 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.08768409639596939\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:37,956 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.0852283738553524\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:39,308 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.08836332857608795\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:40,662 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.08895371854305267\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:42,016 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.08860343843698501\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:43,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.08562808781862259\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:44,727 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.08671540915966033\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:46,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.08515140190720558\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:47,410 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.08542563021183014\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:48,739 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.09386366903781891\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:50,071 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07992459759116173\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:51,425 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.0888111300766468\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:52,777 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.08961407095193863\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:54,119 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.0848570205271244\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:55,449 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.08387313708662987\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:56,778 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.07898874953389168\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:58,107 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.08570840656757354\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:12:59,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.08268493711948395\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:00,765 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07800438776612281\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:02,094 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.08127629980444909\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:03,436 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.07722283527255058\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:04,784 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.08603845313191413\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:06,139 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07679262980818749\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:07,470 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07739427909255028\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:08,799 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07498135194182395\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:17,227 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 7/10: 0.3398352861404419\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:17,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07498135194182395\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:17,847 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:7\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:17,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 8/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:19,284 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07741412296891212\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:20,613 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.0755504995584488\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:21,941 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.07557670772075653\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:23,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.0733662024140358\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:24,603 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.07988397479057312\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:25,949 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.07888739556074142\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:27,278 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.07576272003352642\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:28,626 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.07261806428432464\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:29,978 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.07368740737438202\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:31,325 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07287783250212669\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:32,680 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07646933197975159\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:34,026 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.07545559927821159\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:35,371 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.07518097385764122\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:36,725 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.0758434310555458\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:38,080 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07314934283494949\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:39,431 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.07521371021866799\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:40,783 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.07358812391757966\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:42,139 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.07663150355219842\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:43,496 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07401178106665611\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:44,851 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.07339778617024421\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:46,208 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.07495573237538337\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:47,561 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.07418687343597412\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:48,916 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07324643209576606\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:50,270 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07144477665424347\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:13:51,624 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07387139797210693\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:00,068 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 8/10: 0.327086478471756\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:00,073 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07387139797210693\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:00,653 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:8\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:00,656 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 9/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:02,114 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07046245224773884\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:03,459 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.06835820078849793\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:04,810 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.07525918036699294\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:06,163 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.0692724909633398\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:07,516 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.07539785653352737\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:08,869 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.07355229631066322\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:10,219 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.0691937416791916\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:11,551 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.07134845033288002\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:12,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.0738032378256321\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:14,225 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07010369040071965\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:15,573 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07209363877773285\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:16,927 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.07465692907571793\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:18,280 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.06843298971652985\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:19,633 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.07140101902186871\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:20,986 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07241390123963357\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:22,322 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.0720407359302044\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:23,669 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.07285108342766762\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:25,020 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.06974868178367614\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:26,376 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.07160477861762046\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:27,732 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.0747091606259346\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:29,086 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.07269448414444923\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:30,432 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.06947516053915023\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:31,782 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.06928317844867707\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:33,135 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07133057862520217\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:34,489 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07144197523593902\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:42,942 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 9/10: 0.3330703377723694\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:42,947 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07144197523593902\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:43,553 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:9\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:43,557 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 10/10\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:45,008 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 0.07113952711224555\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:46,363 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 0.07063335850834847\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:47,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 0.06633522324264049\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:49,062 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 0.06852088421583176\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:50,412 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 0.07026690691709518\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:51,761 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 0.06811179332435131\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:53,112 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 0.06775029711425304\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:54,464 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 0.06815551370382308\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:55,816 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 0.06900151818990707\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:57,170 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 0.07156178168952465\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:58,523 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 0.07239833623170852\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:14:59,872 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 0.06982476972043514\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:01,217 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 0.07021746598184109\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:02,567 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 0.07103165052831173\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:03,919 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 0.07040540240705014\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:05,267 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 0.06964877285063267\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:06,619 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 0.06952939070761203\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:07,958 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 0.07308257520198821\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:09,304 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 0.06654922626912593\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:10,643 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 0.07275898680090905\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:11,972 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 0.07217981144785882\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:13,315 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 0.07185433208942413\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:14,668 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 0.07150369882583618\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:16,011 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 0.07116157934069633\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:17,340 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 0.07232859134674072\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:25,776 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAVERAGE EVAL LOSS FOR EPOCH 10/10: 0.3297448456287384\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:25,779 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m0.07232859134674072\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:15:26,338 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py\n",
    "#karpathy evaluates after 250 iterations, we implemented eval to do so after every epoch -> max_iters = 5000 / 200\n",
    "eval_epoch_interval = 1 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "max_iters = 250\n",
    "epochs = 10 #eval after every epoch, karpathy has 5000 max_iters in total -> epoch = max_iters / eval_interval \n",
    "gradient_accumulation_steps = 1 #one large batch, potentially do gradient_accumulation_steps = 8 and batch_size = 8\n",
    "batch_size = 64\n",
    "block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "# baby GPT model :)\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "\n",
    "#not implemented currently\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "\n",
    "#not used currently\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "\n",
    "args = [\n",
    "        \"seed=1337\", #same seed as karpathy\n",
    "        \"run=train\", \n",
    "        \"run.export=False\",\n",
    "        \"run.epochs=\"+str(epochs),\n",
    "        \"run.max_iters=\"+str(max_iters),\n",
    "        \"run.eval_epoch_interval=1\", \n",
    "        \"run.eval_iters=\"+str(eval_iters),\n",
    "        \"run.grad_clip=1.0\",\n",
    "        \"model=gpt_2_h2l2e256b64_GeLN\",\n",
    "        \"model.args.n_layer=\"+str(n_layer),\n",
    "        \"model.args.n_head=\"+str(n_head),\n",
    "        \"model.args.n_embd=\"+str(n_embd),\n",
    "        \"model.args.dropout=\"+str(dropout),\n",
    "        \"dataset=huggingface\", \n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\",\n",
    "        \"dataset.dataloader.batch_size=\"+str(batch_size),\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"optim.args.learning_rate=\"+str(learning_rate),\n",
    "        \"optim.scheduler.schedulers.1.args.step_size=\"+str(epochs//2),\n",
    "        \"device=cuda\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dec1c-93cd-4e33-913e-22f9d7a4580a",
   "metadata": {},
   "source": [
    "Inference is a lot better, but not quite there yet due to the newlines and the repeated nonsense tokens (FOLOLOL...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0825b2ad-9f80-4897-a30c-301e6e7a90a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-21 13:16:23,498 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,628 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,643 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,688 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,699 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,716 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,776 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,799 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,876 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,892 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,976 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:23,993 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:24,076 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:16:24,485 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OLANUS:\n",
      "I must\n",
      "\n",
      "CORFIDIUS:\n",
      "Whatest me to do be I Conspirator:\n",
      "How are speak, let me,\n",
      "Say, we do you are our house Tribunes?\n",
      "\n",
      "AUFIDIUS:\n",
      "Herere 'IUS:\n",
      "But!\n",
      "\n",
      "CORIOLANUS:\n",
      "Masters, my Aufidius,\n",
      "And I have\n",
      "And show'd,\n",
      "Even since you!\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll hear you answer'd, that thou Mars,\n",
      "Have thou thy eye, nor theoth talk'd of all's are,\n",
      "In time, there was, by's one,--\n",
      "\n",
      "AUFIDIUS:\n",
      "With the present Lord!\n",
      "\n",
      "AUFIDIUS:\n",
      "I have say,\n",
      "Than I am a soldier,\n",
      "I'll be drawn:\n",
      "Only the first, the god\n",
      "\n",
      "CORIOLANUS:\n",
      "I am as he?\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll hear thee, my part\n",
      "I would be a power,\n",
      "Here\n",
      "To give me of me!\n",
      "\n",
      "CORIOLANUS:\n",
      "I fear me not--\n",
      "\n",
      "Pray I'll persuade me, good sir!\n",
      "\n",
      "CORIOLANUS:\n",
      "What be mareech I do not.\n",
      "\n",
      "CORIOLANUS:\n",
      "If would, you would do!\n",
      "\n",
      "CORIOLANUS:\n",
      "IOLANUS:\n",
      "My tribunes to, my traitor, and by the people is consul?\n",
      "\n",
      "MENENIUS:\n",
      "No:\n",
      "I would not in you, sir, my noble nobleCORIOLANUS:\n",
      "Are anawnly and in time,\n",
      "Theiolanus,\n",
      "I never noCORIOLANUS:\n",
      "Well, go can that shall be no matter,\n",
      "I am it, sir, no Citizen:\n",
      "O, Caius:\n",
      "How, for our stake I should it may be been further:\n",
      "Where all.\n",
      "\n",
      "AUFIDIUS:\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOL\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FirstCORF dispatchUS:\n",
      "p of Cor Coriolanus\n",
      "\n",
      "AUFIDIUS:\n",
      "I'FIDIUS:\n",
      "What be not be revenged you have, I am banIUS:\n",
      "No you to hear us have be your ground\n",
      "CORIOLANUS:\n",
      "\n",
      "COMINIUS:\n",
      "I'll tell me, and in his faults,\n",
      "Have you be give nor place,\n",
      "And no other, their traitor,\n",
      "If you.\n",
      "\n",
      "CORIOLANUS:\n",
      "If you, where you shall lose the consul\n",
      "To do you have been voices,\n",
      "Have by if you are as youThird Citizen:\n",
      "And\n",
      "And not so.\n",
      "\n",
      "Second Citizen:\n",
      "And you he Citizen:\n",
      "We have incray, indeed, then so\n",
      "When we have lack'd like some without his lord.\n",
      "\n",
      "First Citizen:\n",
      "I Citizen:\n",
      "Pray, we, sir, I have been true, sir, sir, sir, I am\n",
      "dSecond Citizen:\n",
      "I would you have, you, I am.\n",
      "\n",
      "Third Citizen:\n",
      "I have you shall have,\n",
      "And so, you; sir, noay, sir, sir, you, sir, be,\n",
      "And, sir, where your voices to be have done.\n",
      "\n",
      "Third Citizen:\n",
      "How, sir, no more now know; have,\n",
      "And, here is your voices,\n",
      "And 'Third Citizen:\n",
      "Take my voices.\n",
      "\n",
      "Third Citizen:\n",
      "And you have you, I be this.\n",
      "\n",
      "poor I do you are have, then, go--\n",
      "\n",
      "Third Citizen:\n",
      "Well, if I should be, you come, be your voices.\n",
      "\n",
      "Third Citizen:\n",
      "I think they are be done; I have, be no voices,IA:\n",
      "I am\n",
      "IOLANUS:\n",
      "They shall be with the voices, no place\n",
      "A great voices,\n",
      "And so.\n",
      "\n",
      "First Citizen:\n",
      "No, no more: you think he'st?\n",
      "\n",
      "Both:\n",
      "A sir?\n",
      "\n",
      "CORIOLANUS:\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First Lord of theUMNIA.\n",
      "\n",
      "TYVALERIA:\n",
      "What' thele!\n",
      "\n",
      "VOLUMNIA:\n",
      "How!\n",
      "\n",
      "VOLUMNIA:\n",
      "My goodIA:\n",
      "Let Servingman:\n",
      "What's go?\n",
      "\n",
      "First Servingman:\n",
      "How!\n",
      "\n",
      "CORIOLANUS:\n",
      "How!\n",
      "\n",
      "CORIOLANUS:\n",
      "What's man?\n",
      "\n",
      "VOLUMNIA:\n",
      "For no!\n",
      "\n",
      "CORIOLANUS:\n",
      "How, he has that?\n",
      "\n",
      "CORIOLANUS:\n",
      "Butth an me have a cause.\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll be never I would with me.\n",
      "\n",
      "CORIOLANUS:\n",
      "No, sir,--\n",
      "\n",
      "CORIOLANUS:\n",
      "I do 'IOLANUS:\n",
      "I would not, to the do 'IUS:\n",
      "Aium, my tent by the people be done.\n",
      "\n",
      "CORIOLANUS:\n",
      "Well, true!\n",
      "What did be name of them to have been a ANNE:\n",
      "What is answer'd and the Vols and power,\n",
      "I have been thither me to see that I'll am, to see my lord?\n",
      "\n",
      "CORIOLANUS:\n",
      "Is in that\n",
      "That to send thee on,\n",
      "To did not not I have been not where,\n",
      "As thou wert, thoup and,\n",
      "To hang my praise thee, and my blood,\n",
      "And live, let thee, defend thee to thee.\n",
      "\n",
      "AUFADY ANKENBURYea,\n",
      "To call thee be but to leave of thee,\n",
      "And have with thee with thee my lord,\n",
      "And plAnose me, with me, but,\n",
      "'er them for me.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Even, and be good madam, and,\n",
      "That much an Edward's death, by his grace,\n",
      "But if thou wark is, my true, I in this: the queen of my happy Edward'st it's name:\n",
      "An flourishing flourishing device\n",
      "---------------\n",
      "\n",
      "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SecondOLANUS:\n",
      "IOLANUS:\n",
      "Is to-row:\n",
      "I would do do thee.\n",
      "\n",
      "CORIOLANUS:\n",
      "Are if you were, if you come,\n",
      "You have, no matter.\n",
      "\n",
      "CORIOLANUS:\n",
      "How, if you, I'll have all!\n",
      "\n",
      "CORIOLANUS:\n",
      "I cannot keep me.\n",
      "\n",
      "AUFIDIUS:\n",
      "IOLANUS:\n",
      "In gentle ostium; I pray you.\n",
      "You have been further.\n",
      "What comes.\n",
      "\n",
      "AUFIDIUS:\n",
      "Letius.\n",
      "\n",
      "COMINIUS:\n",
      "What is no matter.\n",
      "\n",
      "AUFIDIUS:\n",
      "How, noay, they\n",
      "CORIOLANUS:\n",
      "I would you have spoke.\n",
      "\n",
      "VOLUMNIA:\n",
      "If I'll have quiet, but I am, be a name's name?\n",
      "\n",
      "CORIOLANUS:\n",
      "When I am done, to him to have thom.\n",
      "\n",
      "VOLUMNIA:\n",
      "I shall.\n",
      "\n",
      "CORIOLANUS:\n",
      "How, indeed, sir, sir, till thou go to speak to time to thee; orNIA:\n",
      "What shall not better, sir, sir, sir, I will not, I, I will have voices,\n",
      "Pray he is for me?\n",
      "\n",
      "CORIOLANUS:\n",
      "I would but, for, go to you are be all to speak is be\n",
      "Ay, for\n",
      "You have, for you; if.\n",
      "\n",
      "CORIOLANUS:\n",
      "I am, I am here'st ant his country's, and nobl doubt, have to the people, and you all the people's\n",
      "CORIOLANUS:\n",
      "Well, and at,\n",
      "Not and your person,\n",
      "I will find you have to do,\n",
      "For you be here to wish\n",
      "Against them;\n",
      "Have you.\n",
      "\n",
      "CORIOLANUS:\n",
      "And, let me.\n",
      "\n",
      "AUFIDIUS:\n",
      "I'll so.\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OLANUS:\n",
      "Here is\n",
      "\n",
      "AUFIDIUS:\n",
      "What's hear to speak\n",
      "ToFOLANUS:\n",
      "What's hear me to his earood me to take a Jack on thee.\n",
      "\n",
      "CORIOLANUS:\n",
      "Go,--\n",
      "\n",
      "CORIOLANUS:\n",
      "Where'st time\n",
      "IOLANUS:\n",
      "Ay, sir, but, my teeth.\n",
      "\n",
      "AUFIDIUS:\n",
      "Let me see me in thy mostt'st, you shall you cannot allius,\n",
      "And speak I wish on theplace;\n",
      "I'll told me; take.\n",
      "\n",
      "AUFIDIUS:\n",
      "But I do it, my nobleIA:\n",
      "Have, and, I\n",
      "I would take fair wife' the general, you that would know a mother,\n",
      "How, I would,\n",
      "I'll have their mother, where you are for the Rome be your noble worthy part\n",
      "I will be been for you have been further, I am put to be be\n",
      "my or\n",
      "If you have been further.\n",
      "\n",
      "SICINIUS:\n",
      "What are\n",
      "VOLUMNIA:\n",
      "O too a word's true.\n",
      "\n",
      "SICINIUS:\n",
      "Even with 'tis you haveUMNIA:\n",
      "What\n",
      "But I'll be too home!\n",
      "\n",
      "VOLUMNIA:\n",
      "I'll be home;\n",
      "I am toILIA:\n",
      "He had been much in a house;\n",
      "To be come's house, this better?\n",
      "\n",
      "Second Servingman:\n",
      "O,\n",
      "I prit's for aoth more; where you had it's poor man's:\n",
      "What's gone's:\n",
      " pray I'll must will you.\n",
      "\n",
      "Third Servingman:\n",
      "Well, he's, this this!\n",
      "\n",
      "Second Servingman:\n",
      "A death, I are, noay, and oneman:\n",
      "How, I beseech you,\n",
      " do you to you to say you are here, to be I will go.\n",
      "\n",
      "Third Servingman:\n",
      "What so.\n",
      "\n",
      "First Servingman:\n",
      "He's I\n",
      "---------------\n",
      "\n",
      "OLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOL\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AUFOLOLID\n",
      "\n",
      "AUFID\n",
      "\n",
      "FIDCORIOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOLOL\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FUMai\n",
      "\n",
      "CORIOLOLANUS:\n",
      "How\n",
      "IOLANUS:\n",
      "HowBOLANUS:\n",
      "I them, you shall, Marcius!\n",
      "\n",
      "CORIOLANUS:\n",
      "IOLANUS:\n",
      "How heenius,--\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll with work\n",
      "A faceUS:\n",
      "Well, sir, I should they, I\n",
      "I am I'll be\n",
      "I beseech you?\n",
      "\n",
      "AUANUS:\n",
      "O, to hold no more,\n",
      "And you.\n",
      "\n",
      "CORIOLANUS:\n",
      "I do he should do pass way,\n",
      "I'll hear them\n",
      "And\n",
      "CORIOLANUS:\n",
      "I know yis\n",
      " custom, goodeeus'd\n",
      "That thou hast no doubt our wars you no doubt as you,\n",
      "I have no doubt\n",
      "For you were of you have\n",
      "I have beenBeing you.\n",
      "\n",
      "Second Citizen:\n",
      "I am\n",
      "I do, you,\n",
      "What, sir: you may said, sir, Marcius?\n",
      "\n",
      "First Citizen:\n",
      "I' thecius, if you for you that, shall:\n",
      "When much in good noble voices, to,\n",
      "re they,\n",
      "To,\n",
      "When you have you be\n",
      "And you have be sureity to be be, and o' you shall be.\n",
      "\n",
      "First Citizen:\n",
      "They have, I be\n",
      "Even your voices.\n",
      "\n",
      "First Citizen:\n",
      "What, you, and consul the Tower, you.\n",
      "\n",
      "Second Citizen:\n",
      "If you, Caius'd is be be for you are to Rome, you should\n",
      "Has have be be yours,\n",
      "Have you be sure, sir, sir, they shall be so Citizen:\n",
      "I am stay you\n",
      "' the people were,\n",
      "When you are\n",
      "Your voices.\n",
      "\n",
      "First Citizen:\n",
      "When you are slain.\n",
      "\n",
      "First Citizen:\n",
      "It's in all.\n",
      "But men to come, good pity nor a own,,\n",
      "Your voices:\n",
      "What is\n",
      "For you give's proud to hear you.\n",
      "\n",
      "First Citizen\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample(\"/home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:10\", karpathy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7d0ee1-a937-4de2-b733-3866addb803e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-21 13:19:28,063 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50256, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=True, flash=False, transformer_active_func='GELU', norm_layer='LayerNorm', single_output=False, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,191 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,203 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,222 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,293 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,319 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,388 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,414 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,487 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,510 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,586 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,613 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:28,679 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-21 13:19:29,109 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 33.52M\u001b[0m\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "First citizen::Before:Before citizen: citizen::::::::::::::::::::::: brace::::::::::::: citizen::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: cause::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: look::::::::::::::::::::::::: queen:::::::::::::::\n",
      "---------------\n",
      "First citizen::StayStayStayStayStayStayStay:::StayStay:::::Stay:StayStayStay:Stay:StayStayStayStayStayStay:::::::StayStay::Stay::::::StayStayStayStay::::::Stay::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: heavily::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::;:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::,::::::::::::\n",
      "---------------\n",
      "First citizen::::: citizen: citizen::::::::::::::::::::::: what,:::::::::::: mercy:::::::::::::::::::::::::::::::: so:::: where::::::: side:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::,::::::::::::::::: not:::::::::::::::::::::: then:::: come:::: look,tis:::::::::: not:::: they::: they::::: he, he:: he: he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "---------------\n",
      "First citizen:: citizen: citizen citizen: citizen::::::::::::::::: spirit:::: spirit:::::::::::::: citizen::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: on:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: where::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "---------------\n",
      "First citizen::::: citizen: citizen::::::::::::::::::::::: a:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: I:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: world: cause:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "---------------\n",
      "First citizen:::: walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls:: walls walls: walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls wallsbound walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls walls\n",
      "---------------\n",
      "First citizen:: caps: citizen citizen: citizen::::::::::::::::::::::: bloody::::::::::::: citizen:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: he::: when he::: tell he::: on:: our:: he: our: he: he he that: he he he he, he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "---------------\n",
      "First citizen:: storms: storms citizen: citizen::::: storms::::::::::: storms:::: storms when h delay::::::::::: storms::::::::::::::: mouths::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::,::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: when::::::::::::::::::::::::::\n",
      "---------------\n",
      "First citizen::StayStayStayStayStayStayStay::::Stay:::::Stay:StayStayStay:StayStayStayStayStayStayStay::::::::Stay:::Stay::::::StayStayStayStay::::::Stay::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::Because::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "---------------\n",
      "First citizen::::: citizen: citizen::::::::::::::::::::::: he::::::::::::: citizen::::::::::::::::::::::::::::::::::::::::: he he:: he,:: where:, he he he he he he he he he he he he he he he he he he he he he he he he he he he he he d: he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "sample(\"/home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:10\", karpathy = False, start=\"First citizen:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baec163-c19f-4c6d-9a23-805e121183b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(\"/home/mabot004/nanoGPT/GPT_2024-02-21_13:08:16__epoch:10\", karpathy = False, start=\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddcdea5-1983-42e9-8a9c-254c52d5f44b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 1e-06\n",
      "1.0000000000000002e-06 1e-06\n",
      "1.0000000000000002e-07 1e-06\n",
      "1.0000000000000004e-08 1e-06\n",
      "1.0000000000000005e-09 1e-06\n",
      "1.0000000000000006e-10 1e-06\n",
      "1.0000000000000006e-11 1e-06\n",
      "1.0000000000000006e-12 1e-06\n",
      "1.0000000000000007e-13 1e-06\n",
      "1.0000000000000008e-14 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "#our learning rate stagnates after some time due to the scheduler adjusting the learning rate to be negligible (1e-10)\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "linear = torch.nn.Linear(1,1)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=1e-4)\n",
    "min_lr = 1e-6\n",
    "scheduler =lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "steps = 10\n",
    "for step in range(steps):\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr()[0], min_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
