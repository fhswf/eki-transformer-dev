{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f3a38-8a86-4e92-ba1f-23bce71d5d3e",
   "metadata": {},
   "source": [
    "### Validating our inference and benchmarking scripts based on OpenAI's GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8ddb3d-dfe2-4a3b-89c8-a6fa0b45bb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import qtransform\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330015ad-7d74-4812-8300-570433699495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#qtransform.notebook_run([\"run=infer\", \"run.pretrained_model=gpt2\"], logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01c3db7-4069-4601-810b-c6b03c55ec37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run=bench',\n",
       " 'run.pretrained_model=gpt2',\n",
       " 'run.num_samples=1',\n",
       " 'dataset=huggingface',\n",
       " 'dataset.name=wikitext',\n",
       " 'dataset/tokenizer=tiktoken',\n",
       " 'dataset.tokenizer.encoding=\"gpt2\"',\n",
       " '+model.args.block_size=1024']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'run=bench run.pretrained_model=gpt2 run.num_samples=1 dataset=huggingface dataset.name=wikitext dataset/tokenizer=tiktoken dataset.tokenizer.encoding=\"gpt2\" +model.args.block_size=1024'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280887b2-3aa6-4fbf-b751-775740cfbaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'wikitext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.0, 'eval': 0.0, 'bench': 0.0}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'subset': 'wikitext-2-raw-v1', 'type': 'huggingface', 'splits': {'names': {'train': 'train', 'eval': 'validation', 'bench': 'test'}, 'sizes': {'train': 0.9, 'eval': 0.05, 'bench': 0.05}}, 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': True, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': False, 'args': {'block_size': 1024}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'bench', 'el': 2, 'num_samples': 5, 'out_dir': '', 'profile': True, 'checkpoint_dir': 'models', 'from_checkpoint': None, 'pretrained_model': 'gpt2', 'row_limit': 10, 'onnx_model': {'path': None, 'tokenizer': {'module': 'tiktoken', 'encoding': 'gpt2', 'meta_path': None}}}}\n",
      "[ \u001b[36m2024-03-07 16:38:22,719 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:22,726 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:22,729 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,362 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,365 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmarking\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,367 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,369 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-03-07_16:38:23\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,372 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mDevice specified: cuda. Using device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,378 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,688 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: wikitext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,695 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/train-wikitext-2-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,699 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 2501647 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,701 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/eval-wikitext-2-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,704 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 258896 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,707 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/wikitext/tokenized/gpt2/bench-wikitext-2-raw-v1-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,710 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 296271 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:23,714 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mUsing pretrained model gpt2\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:29,912 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mProperty meta_file omited in config. Assuming default: \"meta.pkl\"\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:30,078 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Benchmark fo 5 samples\u001b[0m\n",
      "[ \u001b[36m2024-03-07 16:38:30,081 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mDatalaoder length might not be correct\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-07 16:38:30 170824:170824 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-07 16:38:34,466 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mBenchmark results: \n",
      "┌────────────────────┬───────────┬────────────┐\n",
      "│ path               │   avg_ppl │   acc_in_% │\n",
      "├────────────────────┼───────────┼────────────┤\n",
      "│ hf-pretrained-gpt2 │   189.862 │    23.7988 │\n",
      "└────────────────────┴───────────┴────────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-07 16:38:34 170824:170824 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-03-07 16:38:34 170824:170824 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "[W collection.cpp:700] Warning: Failed to recover relationship between all profiler and kineto events: 5249 vs. 0  reassociated. (function reassociate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-03-07 16:38:35,098 \u001b[0m][\u001b[2;37mqtransform.run.bench\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             cudaMalloc        43.65%        1.678s        43.65%        1.678s       4.314ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           389  \n",
      "                                               cudaFree        33.93%        1.304s        33.93%        1.304s       4.064ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           321  \n",
      "                                  cudaStreamSynchronize        16.08%     617.998ms        16.08%     618.053ms       7.271ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            85  \n",
      "                                        cudaMemcpyAsync         5.81%     223.401ms         5.81%     223.401ms       2.628ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            85  \n",
      "                                       cudaLaunchKernel         0.45%      17.142ms         0.45%      17.142ms      10.204us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1680  \n",
      "                                          cudaHostAlloc         0.07%       2.697ms         0.07%       2.697ms     269.700us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            10  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.01%     356.000us         0.01%     356.000us       0.324us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1100  \n",
      "                               cudaPointerGetAttributes         0.00%     180.000us         0.00%     180.000us      10.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     109.00 Mb     109.00 Mb            18  \n",
      "                                  cudaStreamIsCapturing         0.00%      57.000us         0.00%      57.000us       0.148us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           386  \n",
      "                                  cudaDeviceSynchronize         0.00%      28.000us         0.00%      28.000us      28.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.844s\n",
      "Self CUDA time total: 1.506s\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "args_benchmarking = ['run=bench',\n",
    " 'run.pretrained_model=gpt2',\n",
    " 'run.num_samples=5',\n",
    " 'dataset=huggingface',\n",
    " 'dataset.name=wikitext',\n",
    " 'dataset.subset=wikitext-2-raw-v1',\n",
    " 'dataset/tokenizer=tiktoken',\n",
    " 'dataset.tokenizer.encoding=\"gpt2\"',\n",
    " '+model.args.block_size=1024']\n",
    "qtransform.notebook_run(args_benchmarking,logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e0f82e-fa62-4e18-8ced-9fa92779b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def measure_perplexity(logits: torch.Tensor, labels: torch.Tensor):\n",
    "    #cross entropy either expects the probabilities of tokens or a list of tokens\n",
    "    #(https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    return torch.exp(F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6add984-d373-43c2-8b35-0ceafd77f5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (471172194.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    labels = torch.arange(\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "labels = torch.arange("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
