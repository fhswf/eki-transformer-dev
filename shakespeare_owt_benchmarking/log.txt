[2024-02-13 13:44:13,308][numexpr.utils][INFO] - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
[2024-02-13 13:44:13,313][numexpr.utils][INFO] - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-02-13 13:44:13,316][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2024-02-13 13:44:13,562][qtransform.run.train][INFO] - ================
[2024-02-13 13:44:13,566][qtransform.run.train][INFO] - Running Training
[2024-02-13 13:44:13,569][qtransform.run.train][INFO] - ================
[2024-02-13 13:44:13,573][qtransform.run.train][INFO] - time is: 2024-02-13_13:44:13
[2024-02-13 13:44:13,577][qtransform][INFO] - Device specified: cuda. Using device: cuda
[2024-02-13 13:44:13,588][qtransform.run.train][INFO] - number of torch dataloader: 2
[2024-02-13 13:44:14,939][qtransform.dataset][INFO] - Loading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32
[2024-02-13 13:44:14,948][qtransform.dataset][INFO] - Attempting to retrieve tokenized dataset under "/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin"
[2024-02-13 13:44:14,954][qtransform.dataset][INFO] - Loaded data has 101408 tokens.
[2024-02-13 13:44:14,986][qtransform.dataset][INFO] - Attempting to retrieve tokenized dataset under "/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin"
[2024-02-13 13:44:14,991][qtransform.dataset][INFO] - Loaded data has 94647 tokens.
[2024-02-13 13:44:15,000][qtransform.run.train][WARNING] - Vocab size of model is larger than the tokenizer vocab. Setting vocab_size to: 50256 to prevent errors during inference
[2024-02-13 13:44:15,283][qtransform.model.gpt][INFO] - Model config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)
[2024-02-13 13:44:15,402][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:44:15,411][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:44:15,490][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:44:15,498][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:44:15,729][qtransform.model.gpt][INFO] - number of parameters: 14.98M
[2024-02-13 13:44:17,437][qtransform.run.train][INFO] - Starting new training
[2024-02-13 13:44:17,439][qtransform.run.train][INFO] - EPOCH: 1/100
[2024-02-13 13:44:18,634][qtransform.run.train][INFO] -   batch 0 loss: 1.0814786911010743
[2024-02-13 13:44:18,962][qtransform.run.train][INFO] -   batch 10 loss: 10.028734016418458
[2024-02-13 13:44:19,280][qtransform.run.train][INFO] -   batch 20 loss: 9.417726612091064
[2024-02-13 13:44:19,603][qtransform.run.train][INFO] -   batch 30 loss: 8.841564083099366
[2024-02-13 13:44:19,927][qtransform.run.train][INFO] -   batch 40 loss: 8.336726760864257
[2024-02-13 13:44:20,242][qtransform.run.train][INFO] -   batch 50 loss: 7.842315435409546
[2024-02-13 13:44:20,564][qtransform.run.train][INFO] -   batch 60 loss: 7.42349648475647
[2024-02-13 13:44:20,882][qtransform.run.train][INFO] -   batch 70 loss: 7.046977567672729
[2024-02-13 13:44:21,188][qtransform.run.train][INFO] -   batch 80 loss: 6.75757155418396
[2024-02-13 13:44:21,524][qtransform.run.train][INFO] -   batch 90 loss: 6.5278137683868405
[2024-02-13 13:44:21,857][qtransform.run.train][INFO] -   batch 100 loss: 6.345893001556396
[2024-02-13 13:44:22,180][qtransform.run.train][INFO] -   batch 110 loss: 6.116252470016479
[2024-02-13 13:44:22,480][qtransform.run.train][INFO] -   batch 120 loss: 6.0132341384887695
[2024-02-13 13:44:22,786][qtransform.run.train][INFO] -   batch 130 loss: 5.878851127624512
[2024-02-13 13:44:23,109][qtransform.run.train][INFO] -   batch 140 loss: 5.804667854309082
[2024-02-13 13:44:23,432][qtransform.run.train][INFO] -   batch 150 loss: 5.695879507064819
[2024-02-13 13:44:23,760][qtransform.run.train][INFO] -   batch 160 loss: 5.600269079208374
[2024-02-13 13:44:24,083][qtransform.run.train][INFO] -   batch 170 loss: 5.538007020950317
[2024-02-13 13:44:24,405][qtransform.run.train][INFO] -   batch 180 loss: 5.44656195640564
[2024-02-13 13:44:24,733][qtransform.run.train][INFO] -   batch 190 loss: 5.251713800430298
[2024-02-13 13:44:25,060][qtransform.run.train][INFO] -   batch 200 loss: 5.200865888595581
[2024-02-13 13:44:25,388][qtransform.run.train][INFO] -   batch 210 loss: 5.038680076599121
[2024-02-13 13:44:25,717][qtransform.run.train][INFO] -   batch 220 loss: 4.861286687850952
[2024-02-13 13:44:26,037][qtransform.run.train][INFO] -   batch 230 loss: 4.634791707992553
[2024-02-13 13:44:26,368][qtransform.run.train][INFO] -   batch 240 loss: 4.378215074539185
[2024-02-13 13:44:26,690][qtransform.run.train][INFO] -   batch 250 loss: 4.140535163879394
[2024-02-13 13:44:27,017][qtransform.run.train][INFO] -   batch 260 loss: 3.918185019493103
[2024-02-13 13:44:27,336][qtransform.run.train][INFO] -   batch 270 loss: 3.59971444606781
[2024-02-13 13:44:27,656][qtransform.run.train][INFO] -   batch 280 loss: 3.332096314430237
[2024-02-13 13:44:27,978][qtransform.run.train][INFO] -   batch 290 loss: 3.1224578857421874
[2024-02-13 13:44:28,306][qtransform.run.train][INFO] -   batch 300 loss: 2.891332507133484
[2024-02-13 13:44:28,633][qtransform.run.train][INFO] -   batch 310 loss: 2.6721574783325197
[2024-02-13 13:44:28,934][qtransform.run.train][INFO] -   batch 320 loss: 2.4845260620117187
[2024-02-13 13:44:29,259][qtransform.run.train][INFO] -   batch 330 loss: 2.2823659896850588
[2024-02-13 13:44:29,583][qtransform.run.train][INFO] -   batch 340 loss: 2.1804298162460327
[2024-02-13 13:44:29,909][qtransform.run.train][INFO] -   batch 350 loss: 2.009354305267334
[2024-02-13 13:44:30,237][qtransform.run.train][INFO] -   batch 360 loss: 1.8618500351905822
[2024-02-13 13:44:30,560][qtransform.run.train][INFO] -   batch 370 loss: 1.6786442637443542
[2024-02-13 13:44:30,885][qtransform.run.train][INFO] -   batch 380 loss: 1.5513007402420045
[2024-02-13 13:44:31,210][qtransform.run.train][INFO] -   batch 390 loss: 1.4092130661010742
[2024-02-13 13:44:31,538][qtransform.run.train][INFO] -   batch 400 loss: 1.327401328086853
[2024-02-13 13:44:31,861][qtransform.run.train][INFO] -   batch 410 loss: 1.2216348052024841
[2024-02-13 13:44:32,186][qtransform.run.train][INFO] -   batch 420 loss: 1.1183194756507873
[2024-02-13 13:44:32,512][qtransform.run.train][INFO] -   batch 430 loss: 1.0603752493858338
[2024-02-13 13:44:32,828][qtransform.run.train][INFO] -   batch 440 loss: 0.9550049722194671
[2024-02-13 13:44:33,122][qtransform.run.train][INFO] -   batch 450 loss: 0.8526090204715728
[2024-02-13 13:44:33,442][qtransform.run.train][INFO] -   batch 460 loss: 0.8235867023468018
[2024-02-13 13:44:33,765][qtransform.run.train][INFO] -   batch 470 loss: 0.7416079103946686
[2024-02-13 13:44:34,089][qtransform.run.train][INFO] -   batch 480 loss: 0.6766059696674347
[2024-02-13 13:44:34,418][qtransform.run.train][INFO] -   batch 490 loss: 0.623449730873108
[2024-02-13 13:44:34,747][qtransform.run.train][INFO] -   batch 500 loss: 0.5610007286071778
[2024-02-13 13:44:35,076][qtransform.run.train][INFO] -   batch 510 loss: 0.5322279185056686
[2024-02-13 13:44:35,400][qtransform.run.train][INFO] -   batch 520 loss: 0.4816823750734329
[2024-02-13 13:44:35,723][qtransform.run.train][INFO] -   batch 530 loss: 0.4518648833036423
[2024-02-13 13:44:36,048][qtransform.run.train][INFO] -   batch 540 loss: 0.4204030305147171
[2024-02-13 13:44:36,369][qtransform.run.train][INFO] -   batch 550 loss: 0.37947399616241456
[2024-02-13 13:44:36,681][qtransform.run.train][INFO] -   batch 560 loss: 0.35556192696094513
[2024-02-13 13:44:37,000][qtransform.run.train][INFO] -   batch 570 loss: 0.3440380185842514
[2024-02-13 13:44:37,321][qtransform.run.train][INFO] -   batch 580 loss: 0.29768029451370237
[2024-02-13 13:44:37,648][qtransform.run.train][INFO] -   batch 590 loss: 0.28025399148464203
[2024-02-13 13:44:37,972][qtransform.run.train][INFO] -   batch 600 loss: 0.26523620039224627
[2024-02-13 13:44:38,293][qtransform.run.train][INFO] -   batch 610 loss: 0.2554860457777977
[2024-02-13 13:44:38,617][qtransform.run.train][INFO] -   batch 620 loss: 0.23417816907167435
[2024-02-13 13:44:38,937][qtransform.run.train][INFO] -   batch 630 loss: 0.21371423751115798
[2024-02-13 13:44:39,270][qtransform.run.train][INFO] -   batch 640 loss: 0.1988332137465477
[2024-02-13 13:44:39,605][qtransform.run.train][INFO] -   batch 650 loss: 0.19074835479259492
[2024-02-13 13:44:39,922][qtransform.run.train][INFO] -   batch 660 loss: 0.18297459036111832
[2024-02-13 13:44:40,240][qtransform.run.train][INFO] -   batch 670 loss: 0.17245704233646392
[2024-02-13 13:44:40,570][qtransform.run.train][INFO] -   batch 680 loss: 0.17341131418943406
[2024-02-13 13:44:40,897][qtransform.run.train][INFO] -   batch 690 loss: 0.1595330536365509
[2024-02-13 13:44:41,220][qtransform.run.train][INFO] -   batch 700 loss: 0.15104628428816796
[2024-02-13 13:44:41,545][qtransform.run.train][INFO] -   batch 710 loss: 0.14839548915624617
[2024-02-13 13:44:41,869][qtransform.run.train][INFO] -   batch 720 loss: 0.14153633862733841
[2024-02-13 13:44:42,192][qtransform.run.train][INFO] -   batch 730 loss: 0.1328538939356804
[2024-02-13 13:44:42,515][qtransform.run.train][INFO] -   batch 740 loss: 0.12690432518720626
[2024-02-13 13:44:42,827][qtransform.run.train][INFO] -   batch 750 loss: 0.13007508590817451
[2024-02-13 13:44:43,121][qtransform.run.train][INFO] -   batch 760 loss: 0.125824736058712
[2024-02-13 13:44:43,415][qtransform.run.train][INFO] -   batch 770 loss: 0.12251279503107071
[2024-02-13 13:44:43,709][qtransform.run.train][INFO] -   batch 780 loss: 0.1187422126531601
[2024-02-13 13:44:44,003][qtransform.run.train][INFO] -   batch 790 loss: 0.11048740148544312
[2024-02-13 13:44:44,317][qtransform.run.train][INFO] -   batch 800 loss: 0.10810237675905228
[2024-02-13 13:44:44,641][qtransform.run.train][INFO] -   batch 810 loss: 0.10765649899840354
[2024-02-13 13:44:44,969][qtransform.run.train][INFO] -   batch 820 loss: 0.10224309861660004
[2024-02-13 13:44:45,292][qtransform.run.train][INFO] -   batch 830 loss: 0.10713213160634041
[2024-02-13 13:44:45,623][qtransform.run.train][INFO] -   batch 840 loss: 0.09892426878213882
[2024-02-13 13:44:45,951][qtransform.run.train][INFO] -   batch 850 loss: 0.10035960376262665
[2024-02-13 13:44:46,275][qtransform.run.train][INFO] -   batch 860 loss: 0.09816799089312553
[2024-02-13 13:44:46,600][qtransform.run.train][INFO] -   batch 870 loss: 0.09627232924103737
[2024-02-13 13:44:46,924][qtransform.run.train][INFO] -   batch 880 loss: 0.09459140226244926
[2024-02-13 13:44:47,236][qtransform.run.train][INFO] -   batch 890 loss: 0.09456081539392472
[2024-02-13 13:44:47,561][qtransform.run.train][INFO] -   batch 900 loss: 0.09531142190098763
[2024-02-13 13:44:47,883][qtransform.run.train][INFO] -   batch 910 loss: 0.09356032237410546
[2024-02-13 13:44:48,205][qtransform.run.train][INFO] -   batch 920 loss: 0.0900658719241619
[2024-02-13 13:44:48,530][qtransform.run.train][INFO] -   batch 930 loss: 0.09381850361824036
[2024-02-13 13:44:48,853][qtransform.run.train][INFO] -   batch 940 loss: 0.08983872383832932
[2024-02-13 13:44:49,177][qtransform.run.train][INFO] -   batch 950 loss: 0.089760223031044
[2024-02-13 13:44:49,503][qtransform.run.train][INFO] -   batch 960 loss: 0.08956152871251107
[2024-02-13 13:44:49,829][qtransform.run.train][INFO] -   batch 970 loss: 0.08698741123080253
[2024-02-13 13:44:50,153][qtransform.run.train][INFO] -   batch 980 loss: 0.08797898441553116
[2024-02-13 13:44:50,475][qtransform.run.train][INFO] -   batch 990 loss: 0.08725197464227677
[2024-02-13 13:44:50,792][qtransform.run.train][INFO] -   batch 1000 loss: 0.08584734052419662
[2024-02-13 13:44:51,113][qtransform.run.train][INFO] -   batch 1010 loss: 0.08436140418052673
[2024-02-13 13:44:51,436][qtransform.run.train][INFO] -   batch 1020 loss: 0.08557359799742699
[2024-02-13 13:44:51,756][qtransform.run.train][INFO] -   batch 1030 loss: 0.08337687328457832
[2024-02-13 13:44:52,080][qtransform.run.train][INFO] -   batch 1040 loss: 0.08435980305075645
[2024-02-13 13:44:52,388][qtransform.run.train][INFO] -   batch 1050 loss: 0.08416210934519767
[2024-02-13 13:44:52,681][qtransform.run.train][INFO] -   batch 1060 loss: 0.08072719871997833
[2024-02-13 13:44:52,974][qtransform.run.train][INFO] -   batch 1070 loss: 0.08021606504917145
[2024-02-13 13:44:53,268][qtransform.run.train][INFO] -   batch 1080 loss: 0.07771769538521767
[2024-02-13 13:44:53,584][qtransform.run.train][INFO] -   batch 1090 loss: 0.08004260808229446
[2024-02-13 13:44:53,908][qtransform.run.train][INFO] -   batch 1100 loss: 0.07954467162489891
[2024-02-13 13:44:54,232][qtransform.run.train][INFO] -   batch 1110 loss: 0.08120676279067993
[2024-02-13 13:44:54,555][qtransform.run.train][INFO] -   batch 1120 loss: 0.07973136603832245
[2024-02-13 13:44:54,875][qtransform.run.train][INFO] -   batch 1130 loss: 0.08074760884046554
[2024-02-13 13:44:55,198][qtransform.run.train][INFO] -   batch 1140 loss: 0.08167456313967705
[2024-02-13 13:44:55,524][qtransform.run.train][INFO] -   batch 1150 loss: 0.08013327717781067
[2024-02-13 13:44:55,845][qtransform.run.train][INFO] -   batch 1160 loss: 0.08386159390211105
[2024-02-13 13:44:56,168][qtransform.run.train][INFO] -   batch 1170 loss: 0.08124819472432136
[2024-02-13 13:44:56,493][qtransform.run.train][INFO] -   batch 1180 loss: 0.07815293446183205
[2024-02-13 13:44:56,814][qtransform.run.train][INFO] -   batch 1190 loss: 0.08068719729781151
[2024-02-13 13:44:57,140][qtransform.run.train][INFO] -   batch 1200 loss: 0.07527995184063911
[2024-02-13 13:44:57,460][qtransform.run.train][INFO] -   batch 1210 loss: 0.0792896255850792
[2024-02-13 13:44:57,777][qtransform.run.train][INFO] -   batch 1220 loss: 0.07909879982471466
[2024-02-13 13:44:58,098][qtransform.run.train][INFO] -   batch 1230 loss: 0.080222849547863
[2024-02-13 13:44:58,416][qtransform.run.train][INFO] -   batch 1240 loss: 0.07985053658485412
[2024-02-13 13:44:58,737][qtransform.run.train][INFO] -   batch 1250 loss: 0.07990247905254363
[2024-02-13 13:44:59,058][qtransform.run.train][INFO] -   batch 1260 loss: 0.07967272847890854
[2024-02-13 13:44:59,378][qtransform.run.train][INFO] -   batch 1270 loss: 0.07683225870132446
[2024-02-13 13:44:59,700][qtransform.run.train][INFO] -   batch 1280 loss: 0.07696817442774773
[2024-02-13 13:45:00,020][qtransform.run.train][INFO] -   batch 1290 loss: 0.07847564071416854
[2024-02-13 13:45:00,338][qtransform.run.train][INFO] -   batch 1300 loss: 0.07635548561811448
[2024-02-13 13:45:00,663][qtransform.run.train][INFO] -   batch 1310 loss: 0.07755953297019005
[2024-02-13 13:45:00,980][qtransform.run.train][INFO] -   batch 1320 loss: 0.08147314414381981
[2024-02-13 13:45:01,299][qtransform.run.train][INFO] -   batch 1330 loss: 0.0745748072862625
[2024-02-13 13:45:01,624][qtransform.run.train][INFO] -   batch 1340 loss: 0.07801730483770371
[2024-02-13 13:45:01,940][qtransform.run.train][INFO] -   batch 1350 loss: 0.07652391642332076
[2024-02-13 13:45:02,259][qtransform.run.train][INFO] -   batch 1360 loss: 0.07694418206810952
[2024-02-13 13:45:02,584][qtransform.run.train][INFO] -   batch 1370 loss: 0.07574513629078865
[2024-02-13 13:45:02,902][qtransform.run.train][INFO] -   batch 1380 loss: 0.08034205883741379
[2024-02-13 13:45:03,228][qtransform.run.train][INFO] -   batch 1390 loss: 0.07813942767679691
[2024-02-13 13:45:03,541][qtransform.run.train][INFO] -   batch 1400 loss: 0.07715454623103142
[2024-02-13 13:45:03,851][qtransform.run.train][INFO] -   batch 1410 loss: 0.07432392910122872
[2024-02-13 13:45:04,172][qtransform.run.train][INFO] -   batch 1420 loss: 0.07728920727968216
[2024-02-13 13:45:04,496][qtransform.run.train][INFO] -   batch 1430 loss: 0.07314848452806473
[2024-02-13 13:45:04,821][qtransform.run.train][INFO] -   batch 1440 loss: 0.07422819398343564
[2024-02-13 13:45:05,153][qtransform.run.train][INFO] -   batch 1450 loss: 0.07614408656954766
[2024-02-13 13:45:05,481][qtransform.run.train][INFO] -   batch 1460 loss: 0.07296678051352501
[2024-02-13 13:45:05,800][qtransform.run.train][INFO] -   batch 1470 loss: 0.07522793412208557
[2024-02-13 13:45:06,124][qtransform.run.train][INFO] -   batch 1480 loss: 0.07485603466629982
[2024-02-13 13:45:06,447][qtransform.run.train][INFO] -   batch 1490 loss: 0.07659061178565026
[2024-02-13 13:45:06,768][qtransform.run.train][INFO] -   batch 1500 loss: 0.07550743147730828
[2024-02-13 13:45:07,088][qtransform.run.train][INFO] -   batch 1510 loss: 0.07389946877956391
[2024-02-13 13:45:07,408][qtransform.run.train][INFO] -   batch 1520 loss: 0.07652528509497643
[2024-02-13 13:45:07,733][qtransform.run.train][INFO] -   batch 1530 loss: 0.07560347318649292
[2024-02-13 13:45:08,056][qtransform.run.train][INFO] -   batch 1540 loss: 0.07402549535036088
[2024-02-13 13:45:08,380][qtransform.run.train][INFO] -   batch 1550 loss: 0.074910619109869
[2024-02-13 13:45:08,704][qtransform.run.train][INFO] -   batch 1560 loss: 0.07472982481122017
[2024-02-13 13:45:09,024][qtransform.run.train][INFO] -   batch 1570 loss: 0.07554230615496635
[2024-02-13 13:45:09,362][qtransform.run.train][INFO] -   batch 1580 loss: 0.07694078162312508
[2024-02-13 13:46:00,888][qtransform.run.train][INFO] - AVERAGE EVAL LOSS FOR EPOCH 1/100: 0.133059561252594
[2024-02-13 13:46:00,893][qtransform.run.train][INFO] - 0.07694078162312508
[2024-02-13 13:46:01,172][qtransform.utils.helper][INFO] - Model checkpoint saved to /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-13_13:44:13__epoch:1
[2024-02-13 13:46:01,175][qtransform.run.train][INFO] - EPOCH: 2/100
[2024-02-13 13:46:01,348][qtransform.run.train][INFO] -   batch 0 loss: 0.005953484401106835
[2024-02-13 13:46:01,673][qtransform.run.train][INFO] -   batch 10 loss: 0.07082101553678513
[2024-02-13 13:46:01,995][qtransform.run.train][INFO] -   batch 20 loss: 0.07401572242379188
[2024-02-13 13:46:02,315][qtransform.run.train][INFO] -   batch 30 loss: 0.07121982723474503
[2024-02-13 13:46:02,635][qtransform.run.train][INFO] -   batch 40 loss: 0.07029064297676087
[2024-02-13 13:46:02,951][qtransform.run.train][INFO] -   batch 50 loss: 0.06943750269711017
[2024-02-13 13:46:03,271][qtransform.run.train][INFO] -   batch 60 loss: 0.07120902799069881
[2024-02-13 13:46:03,584][qtransform.run.train][INFO] -   batch 70 loss: 0.06863513328135014
[2024-02-13 13:46:03,903][qtransform.run.train][INFO] -   batch 80 loss: 0.07117352485656739
[2024-02-13 13:46:04,228][qtransform.run.train][INFO] -   batch 90 loss: 0.06729949042201042
[2024-02-13 13:46:04,552][qtransform.run.train][INFO] -   batch 100 loss: 0.07023796513676643
[2024-02-13 13:46:04,871][qtransform.run.train][INFO] -   batch 110 loss: 0.07243234217166901
[2024-02-13 13:46:05,196][qtransform.run.train][INFO] -   batch 120 loss: 0.0731443539261818
[2024-02-13 13:46:05,520][qtransform.run.train][INFO] -   batch 130 loss: 0.0650293543934822
[2024-02-13 13:46:05,842][qtransform.run.train][INFO] -   batch 140 loss: 0.07004907317459583
[2024-02-13 13:46:06,164][qtransform.run.train][INFO] -   batch 150 loss: 0.07083249166607856
[2024-02-13 13:46:06,487][qtransform.run.train][INFO] -   batch 160 loss: 0.06878587603569031
[2024-02-13 13:46:06,804][qtransform.run.train][INFO] -   batch 170 loss: 0.06674405559897423
[2024-02-13 13:46:07,128][qtransform.run.train][INFO] -   batch 180 loss: 0.06930300667881965
[2024-02-13 13:46:07,433][qtransform.run.train][INFO] -   batch 190 loss: 0.07282493859529496
[2024-02-13 13:46:07,748][qtransform.run.train][INFO] -   batch 200 loss: 0.0702348593622446
[2024-02-13 13:46:08,074][qtransform.run.train][INFO] -   batch 210 loss: 0.06613621674478054
[2024-02-13 13:46:08,393][qtransform.run.train][INFO] -   batch 220 loss: 0.065633849427104
[2024-02-13 13:46:08,717][qtransform.run.train][INFO] -   batch 230 loss: 0.07187155038118362
[2024-02-13 13:46:09,040][qtransform.run.train][INFO] -   batch 240 loss: 0.07209198959171773
[2024-02-13 13:46:09,365][qtransform.run.train][INFO] -   batch 250 loss: 0.07056076899170875
[2024-02-13 13:46:09,687][qtransform.run.train][INFO] -   batch 260 loss: 0.0731478177011013
[2024-02-13 13:46:10,008][qtransform.run.train][INFO] -   batch 270 loss: 0.06732679381966591
[2024-02-13 13:46:10,330][qtransform.run.train][INFO] -   batch 280 loss: 0.06573506109416485
[2024-02-13 13:46:10,640][qtransform.run.train][INFO] -   batch 290 loss: 0.07029695585370063
[2024-02-13 13:46:10,964][qtransform.run.train][INFO] -   batch 300 loss: 0.06923590525984764
[2024-02-13 13:46:11,288][qtransform.run.train][INFO] -   batch 310 loss: 0.06766384989023208
[2024-02-13 13:46:11,611][qtransform.run.train][INFO] -   batch 320 loss: 0.06799498610198498
[2024-02-13 13:46:11,936][qtransform.run.train][INFO] -   batch 330 loss: 0.06996757984161377
[2024-02-13 13:46:12,255][qtransform.run.train][INFO] -   batch 340 loss: 0.06888032145798206
[2024-02-13 13:46:12,580][qtransform.run.train][INFO] -   batch 350 loss: 0.06892801485955716
[2024-02-13 13:46:12,883][qtransform.run.train][INFO] -   batch 360 loss: 0.0686967846006155
[2024-02-13 13:46:13,177][qtransform.run.train][INFO] -   batch 370 loss: 0.07068811431527137
[2024-02-13 13:46:13,488][qtransform.run.train][INFO] -   batch 380 loss: 0.06987420693039895
[2024-02-13 13:46:13,805][qtransform.run.train][INFO] -   batch 390 loss: 0.06994570903480053
[2024-02-13 13:46:14,130][qtransform.run.train][INFO] -   batch 400 loss: 0.06992909088730812
[2024-02-13 13:46:14,452][qtransform.run.train][INFO] -   batch 410 loss: 0.06451450362801552
[2024-02-13 13:46:14,778][qtransform.run.train][INFO] -   batch 420 loss: 0.0671445544809103
[2024-02-13 13:46:15,104][qtransform.run.train][INFO] -   batch 430 loss: 0.0680693306028843
[2024-02-13 13:46:15,430][qtransform.run.train][INFO] -   batch 440 loss: 0.07334598451852799
[2024-02-13 13:46:15,752][qtransform.run.train][INFO] -   batch 450 loss: 0.06892706342041492
[2024-02-13 13:46:16,072][qtransform.run.train][INFO] -   batch 460 loss: 0.06757157891988755
[2024-02-13 13:46:16,398][qtransform.run.train][INFO] -   batch 470 loss: 0.07052591480314732
[2024-02-13 13:46:16,723][qtransform.run.train][INFO] -   batch 480 loss: 0.06981403641402721
[2024-02-13 13:46:17,048][qtransform.run.train][INFO] -   batch 490 loss: 0.06944975815713406
[2024-02-13 13:46:17,375][qtransform.run.train][INFO] -   batch 500 loss: 0.07077371105551719
[2024-02-13 13:46:17,698][qtransform.run.train][INFO] -   batch 510 loss: 0.06996183507144452
[2024-02-13 13:46:18,023][qtransform.run.train][INFO] -   batch 520 loss: 0.0694230530411005
[2024-02-13 13:46:18,345][qtransform.run.train][INFO] -   batch 530 loss: 0.06557997539639474
[2024-02-13 13:46:18,664][qtransform.run.train][INFO] -   batch 540 loss: 0.06862628795206546
[2024-02-13 13:46:18,984][qtransform.run.train][INFO] -   batch 550 loss: 0.0700732484459877
[2024-02-13 13:46:19,293][qtransform.run.train][INFO] -   batch 560 loss: 0.06813231781125069
[2024-02-13 13:46:19,618][qtransform.run.train][INFO] -   batch 570 loss: 0.06853835545480251
[2024-02-13 13:46:19,936][qtransform.run.train][INFO] -   batch 580 loss: 0.06956844478845596
[2024-02-13 13:46:20,261][qtransform.run.train][INFO] -   batch 590 loss: 0.06586511470377446
[2024-02-13 13:46:20,582][qtransform.run.train][INFO] -   batch 600 loss: 0.07010632418096066
[2024-02-13 13:46:20,905][qtransform.run.train][INFO] -   batch 610 loss: 0.06835015043616295
[2024-02-13 13:46:21,232][qtransform.run.train][INFO] -   batch 620 loss: 0.06821108944714069
[2024-02-13 13:46:21,557][qtransform.run.train][INFO] -   batch 630 loss: 0.06943667121231556
[2024-02-13 13:46:21,884][qtransform.run.train][INFO] -   batch 640 loss: 0.0673607874661684
[2024-02-13 13:46:22,205][qtransform.run.train][INFO] -   batch 650 loss: 0.06755397357046604
[2024-02-13 13:46:22,523][qtransform.run.train][INFO] -   batch 660 loss: 0.06880321577191353
[2024-02-13 13:46:22,840][qtransform.run.train][INFO] -   batch 670 loss: 0.06871512234210968
[2024-02-13 13:46:23,163][qtransform.run.train][INFO] -   batch 680 loss: 0.07124297022819519
[2024-02-13 13:46:23,482][qtransform.run.train][INFO] -   batch 690 loss: 0.06964963674545288
[2024-02-13 13:46:23,804][qtransform.run.train][INFO] -   batch 700 loss: 0.07146930769085884
[2024-02-13 13:46:24,129][qtransform.run.train][INFO] -   batch 710 loss: 0.06947363130748271
[2024-02-13 13:46:24,448][qtransform.run.train][INFO] -   batch 720 loss: 0.06855200305581093
[2024-02-13 13:46:24,772][qtransform.run.train][INFO] -   batch 730 loss: 0.06658084094524383
[2024-02-13 13:46:25,090][qtransform.run.train][INFO] -   batch 740 loss: 0.06878070905804634
[2024-02-13 13:46:25,413][qtransform.run.train][INFO] -   batch 750 loss: 0.07016171887516975
[2024-02-13 13:46:25,736][qtransform.run.train][INFO] -   batch 760 loss: 0.07204182855784894
[2024-02-13 13:46:26,064][qtransform.run.train][INFO] -   batch 770 loss: 0.06919909864664078
[2024-02-13 13:46:26,359][qtransform.run.train][INFO] -   batch 780 loss: 0.0710775252431631
[2024-02-13 13:46:26,667][qtransform.run.train][INFO] -   batch 790 loss: 0.07189789228141308
[2024-02-13 13:46:26,986][qtransform.run.train][INFO] -   batch 800 loss: 0.07093326337635517
[2024-02-13 13:46:27,312][qtransform.run.train][INFO] -   batch 810 loss: 0.07051902897655964
[2024-02-13 13:46:27,634][qtransform.run.train][INFO] -   batch 820 loss: 0.07366400584578514
[2024-02-13 13:46:27,958][qtransform.run.train][INFO] -   batch 830 loss: 0.0664932906627655
[2024-02-13 13:46:28,283][qtransform.run.train][INFO] -   batch 840 loss: 0.06879346258938313
[2024-02-13 13:46:28,608][qtransform.run.train][INFO] -   batch 850 loss: 0.06913681291043758
[2024-02-13 13:46:28,932][qtransform.run.train][INFO] -   batch 860 loss: 0.069281155616045
[2024-02-13 13:46:29,254][qtransform.run.train][INFO] -   batch 870 loss: 0.06843326278030873
[2024-02-13 13:46:29,572][qtransform.run.train][INFO] -   batch 880 loss: 0.06893916204571723
[2024-02-13 13:46:29,894][qtransform.run.train][INFO] -   batch 890 loss: 0.06852278523147107
[2024-02-13 13:46:30,215][qtransform.run.train][INFO] -   batch 900 loss: 0.07043468467891216
[2024-02-13 13:46:30,534][qtransform.run.train][INFO] -   batch 910 loss: 0.07027224525809288
[2024-02-13 13:46:30,860][qtransform.run.train][INFO] -   batch 920 loss: 0.06835661269724369
[2024-02-13 13:46:31,189][qtransform.run.train][INFO] -   batch 930 loss: 0.07088794261217117
[2024-02-13 13:46:31,509][qtransform.run.train][INFO] -   batch 940 loss: 0.07069383189082146
[2024-02-13 13:46:31,834][qtransform.run.train][INFO] -   batch 950 loss: 0.06545544117689132
[2024-02-13 13:46:32,155][qtransform.run.train][INFO] -   batch 960 loss: 0.06914999186992646
[2024-02-13 13:46:32,480][qtransform.run.train][INFO] -   batch 970 loss: 0.06767004244029522
[2024-02-13 13:46:32,799][qtransform.run.train][INFO] -   batch 980 loss: 0.07045739553868771
[2024-02-13 13:46:33,120][qtransform.run.train][INFO] -   batch 990 loss: 0.0689571488648653
[2024-02-13 13:46:33,438][qtransform.run.train][INFO] -   batch 1000 loss: 0.06914092190563678
[2024-02-13 13:46:33,764][qtransform.run.train][INFO] -   batch 1010 loss: 0.06643678843975068
[2024-02-13 13:46:34,087][qtransform.run.train][INFO] -   batch 1020 loss: 0.06644399277865887
[2024-02-13 13:46:34,412][qtransform.run.train][INFO] -   batch 1030 loss: 0.06971023604273796
[2024-02-13 13:46:34,737][qtransform.run.train][INFO] -   batch 1040 loss: 0.07272332422435283
[2024-02-13 13:46:35,060][qtransform.run.train][INFO] -   batch 1050 loss: 0.06667447723448276
[2024-02-13 13:46:35,388][qtransform.run.train][INFO] -   batch 1060 loss: 0.06759239658713341
[2024-02-13 13:46:35,713][qtransform.run.train][INFO] -   batch 1070 loss: 0.06913544759154319
[2024-02-13 13:46:36,041][qtransform.run.train][INFO] -   batch 1080 loss: 0.07417276315391064
[2024-02-13 13:46:36,363][qtransform.run.train][INFO] -   batch 1090 loss: 0.06717519052326679
[2024-02-13 13:46:36,688][qtransform.run.train][INFO] -   batch 1100 loss: 0.06821254156529903
[2024-02-13 13:46:37,012][qtransform.run.train][INFO] -   batch 1110 loss: 0.06578771919012069
[2024-02-13 13:46:37,336][qtransform.run.train][INFO] -   batch 1120 loss: 0.06996179968118668
[2024-02-13 13:46:37,655][qtransform.run.train][INFO] -   batch 1130 loss: 0.066846938803792
[2024-02-13 13:46:37,976][qtransform.run.train][INFO] -   batch 1140 loss: 0.07002154067158699
[2024-02-13 13:46:38,294][qtransform.run.train][INFO] -   batch 1150 loss: 0.06956944465637208
[2024-02-13 13:46:38,616][qtransform.run.train][INFO] -   batch 1160 loss: 0.0701913632452488
[2024-02-13 13:46:38,935][qtransform.run.train][INFO] -   batch 1170 loss: 0.07032300047576427
[2024-02-13 13:46:39,251][qtransform.run.train][INFO] -   batch 1180 loss: 0.06583375632762908
[2024-02-13 13:46:39,547][qtransform.run.train][INFO] -   batch 1190 loss: 0.06707629524171352
[2024-02-13 13:46:39,846][qtransform.run.train][INFO] -   batch 1200 loss: 0.06304956637322903
[2024-02-13 13:51:48,022][qtransform.qtransform.__main__][DEBUG] - DEBUG ENABLED
[2024-02-13 13:51:48,049][qtransform.run.infer][INFO] - =================
[2024-02-13 13:51:48,051][qtransform.run.infer][INFO] - Running Inference
[2024-02-13 13:51:48,053][qtransform.run.infer][INFO] - =================
[2024-02-13 13:51:48,055][qtransform][INFO] - Device specified: cpu. Using device: cpu
[2024-02-13 13:51:48,056][qtransform.run.infer][INFO] - using device: cpu
[2024-02-13 13:51:48,059][qtransform.utils.helper][ERROR] - Checkpoint /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/GPT_2024-02-13_13:44:13__epoch:1 is not a file
[2024-02-13 13:52:43,381][hydra.core.utils][DEBUG] - Setting JobRuntime:name=app
[2024-02-13 13:52:43,567][qtransform.qtransform.__main__][DEBUG] - DEBUG ENABLED
[2024-02-13 13:52:43,571][qtransform.run.infer][INFO] - =================
[2024-02-13 13:52:43,574][qtransform.run.infer][INFO] - Running Inference
[2024-02-13 13:52:43,577][qtransform.run.infer][INFO] - =================
[2024-02-13 13:52:43,580][qtransform][INFO] - Device specified: cpu. Using device: cpu
[2024-02-13 13:52:43,584][qtransform.run.infer][INFO] - using device: cpu
[2024-02-13 13:52:43,587][qtransform.utils.helper][INFO] - Loading checkpoint from /home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/outputs/models/GPT_2024-02-13_13:44:13__epoch:1
[2024-02-13 13:52:43,752][qtransform.model][DEBUG] - get_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50256, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}
[2024-02-13 13:52:43,755][qtransform.model][DEBUG] - Loading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)
[2024-02-13 13:52:43,765][qtransform.model.gpt][INFO] - Model config: GPTConfig(block_size=64, vocab_size=50256, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, custom_ln=False)
[2024-02-13 13:52:43,866][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:52:43,878][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:52:43,895][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:52:43,912][qtransform.model.modules][WARNING] - WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2
[2024-02-13 13:52:44,230][qtransform.model.gpt][INFO] - number of parameters: 14.98M
[2024-02-13 13:52:44,235][qtransform.dataset.tokenizer][DEBUG] - Attempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}
[2024-02-13 13:52:44,238][qtransform.dataset.tokenizer][DEBUG] - Loading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)
[2024-02-13 13:52:44,244][qtransform.dataset.tokenizer][DEBUG] - Passing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>
[2024-02-13 13:52:44,246][qtransform.dataset.tokenizer.tokenizer][DEBUG] - Creating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken', 'meta': {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}}
[2024-02-13 13:52:44,250][qtransform.run.infer][DEBUG] - {'max_token_value': 50256, 'encoding': 'gpt2', 'dtype': 'float32', 'num_tokens': 338027, 'module': 'tiktoken'}
[2024-02-13 13:52:44,261][qtransform.run.infer][INFO] - Writing to file: "/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out"
[2024-02-13 13:52:44,263][qtransform.run.infer][INFO] - Running inference from CHECKPOINT.
[2024-02-13 13:53:10,202][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  198,   202,   317,   363,   385,   406,   563,   589,   879,   880,
          962,  1010,  1152,  1267,  1369,  1408,  1487,  1860,  2143,  2186,
         2244,  2266,  2283,  2394,  2566,  2889,  2988,  3209,  3297,  3385,
         3599,  3708,  3765,  3770,  3843,  3889,  4252,  4256,  4388,  4393,
         4410,  4611,  4692,  4891,  4930,  5398,  5399,  5507,  5675,  5785,
         5801,  5933,  6006,  6100,  6111,  6458,  6481,  6596,  6625,  6760,
         6783,  6821,  6868,  7134,  7248,  7251,  7491,  7555,  7840,  7845,
         9030,  9041,  9067,  9194,  9341,  9352,  9388,  9477,  9839,  9855,
         9912, 10180, 10458, 10546, 10605, 10643, 10687, 10827, 10873, 11037,
        11325, 11415, 11481, 11507, 11520, 11717, 11748, 11769, 11788, 11793,
        11872, 12002, 12476, 12579, 12659, 12694, 12711, 13030, 13072, 13091,
        13178, 13201, 13216, 13255, 13346, 13451, 13509, 13562, 13563, 13635,
        13675, 13816, 13858, 13903, 13905, 13949, 14001, 14073, 14202, 14327,
        14383, 14505, 14515, 14532, 14725, 14840, 14855, 14921, 15073, 15096,
        15121, 15222, 15304, 15314, 15433, 15794, 15860, 15861, 15989, 16035,
        16067, 16392, 16416, 16437, 16527, 16824, 16862, 16999, 17013, 17108,
        17113, 17318, 17556, 17574, 17602, 17667, 17672, 17856, 18135, 18198,
        18250, 18301, 18333, 18362, 18371, 18471, 18734, 18926, 18948, 18976,
        19104, 19145, 19296, 19306, 19393, 19524, 19762, 19899, 19949, 20126,
        20128, 20188, 20326, 20517, 20518, 20627, 20656, 20706, 20856, 20893,
        21354, 21369, 21381, 21567, 21694, 21710, 21722, 21910, 22076, 22425,
        22530, 22568, 22591, 22634, 22968, 23201, 23230, 23253, 23480, 23530,
        23678, 23689, 23738, 23852, 24144, 24208, 24403, 24513, 24758, 25452,
        25492, 25580, 25810, 25867, 26008, 26248, 26268, 26476, 26770, 26944,
        27156, 27611, 27620, 27800, 27858, 27884, 27923, 28165, 28205, 28563,
        28682, 28699, 28705, 28992, 29037, 29094, 29116, 29445, 29483, 29485,
        29557, 29746, 29963, 30178, 30365, 30468, 30521, 30872, 30962, 30966,
        31103, 31107, 31456, 31510, 31643, 31673, 31736, 31788, 32060, 32336,
        32493, 32728, 32780, 32829, 33010, 33161, 33174, 33280, 33324, 33535,
        33727, 33769, 33871, 33877, 33883, 34004, 34042, 34227, 34273, 34278,
        34311, 34373, 34412, 34571, 34626, 34937, 35030, 35075, 35418, 35814,
        35903, 36082, 36114, 36220, 36315, 36492, 36673, 36697, 36873, 37061,
        37090, 37206, 37691, 37728, 38118, 38356, 38385, 38432, 38535, 38538,
        38687, 38798, 38965, 39075, 39232, 39377, 39384, 39403, 39698, 39700,
        39781, 39801, 39805, 39984, 40009, 40062, 40078, 40388, 40427, 40430,
        40508, 40599, 40604, 40693, 40701, 40713, 40755, 40850, 40869, 40901,
        41318, 41534, 41550, 41736, 41762, 41924, 42139, 42192, 42248, 42259,
        42452, 42585, 42598, 42834, 42993, 43034, 43045, 43189, 43218, 43289,
        43426, 43503, 43865, 43888, 43901, 43956, 43984, 44153, 44159, 44205,
        44360, 44624, 44678, 44679, 44730, 44822, 45186, 45208, 45384, 45479,
        45529, 45567, 45589, 45627, 45638, 45674, 45754, 45789, 46087, 46153,
        46241, 46253, 46304, 46403, 46414, 46581, 46696, 46807, 46912, 46944,
        47316, 47470, 47653, 47746, 47871, 47892, 47894, 47992, 48013, 48112,
        48305, 48377, 48479, 48614, 48664, 48752, 48763, 48999, 49026, 49100,
        49143, 49153, 49533, 49615, 49792, 49945, 49969, 50158, 50160]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448]))
[2024-02-13 13:53:10,207][qtransform.run.infer][INFO] - Writing sample: 0/10
[2024-02-13 13:53:32,705][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  153,   198,   251,   317,   650,   824,   829,   832,   878,   901,
          919,   962,  1195,  1267,  1270,  1451,  1473,  1683,  1710,  1772,
         1858,  1937,  2048,  2053,  2143,  2375,  2491,  2537,  2878,  2881,
         2898,  2989,  3006,  3134,  3267,  3322,  3357,  3383,  3408,  3538,
         3542,  3599,  3770,  3901,  3914,  3921,  4388,  4421,  4688,  4719,
         4746,  4831,  5128,  5291,  5420,  5664,  5884,  5942,  6035,  6113,
         6295,  6536,  6718,  6760,  6783,  6851,  6969,  6994,  7027,  7056,
         7111,  7123,  7134,  7251,  7358,  7449,  7496,  7546,  7575,  7590,
         7596,  8361,  8455,  8489,  8590,  8604,  8689,  8773,  8967,  9029,
         9194,  9404,  9538,  9717,  9943,  9976, 10282, 10308, 10310, 10403,
        10547, 10691, 10715, 10930, 10946, 11089, 11286, 11584, 11682, 11701,
        11818, 11862, 11885, 11903, 12259, 12490, 12559, 12587, 12694, 12729,
        12990, 13018, 13084, 13324, 13361, 13502, 13792, 13822, 13871, 13929,
        13949, 14001, 14073, 14138, 14331, 14379, 14455, 14478, 14560, 14594,
        14725, 14866, 14992, 15023, 15073, 15275, 15559, 15582, 15646, 15685,
        15828, 16009, 16025, 16156, 16161, 16313, 16465, 16501, 16665, 16670,
        16869, 16980, 16983, 17047, 17054, 17218, 17228, 17278, 17312, 17323,
        17601, 17681, 17866, 17986, 18150, 18447, 18494, 18511, 18551, 18689,
        18760, 18764, 18778, 18948, 18981, 19209, 19285, 19313, 19350, 19500,
        19924, 20326, 20333, 20401, 20478, 20514, 20518, 20570, 20627, 20662,
        20856, 20881, 20920, 21008, 21122, 21485, 21489, 21604, 21797, 22233,
        22358, 22440, 22917, 22926, 22968, 23230, 23312, 23378, 23421, 23689,
        23768, 23775, 23848, 24155, 24181, 24471, 24513, 24545, 24626, 24803,
        25039, 25087, 25100, 25305, 25312, 25324, 25452, 25467, 25492, 25532,
        25535, 25643, 25716, 25810, 25824, 25833, 26071, 26111, 26153, 26239,
        26405, 26417, 26424, 26872, 27156, 27310, 27413, 27694, 27775, 27922,
        27924, 27951, 28079, 28273, 28444, 28563, 28595, 28747, 28796, 29090,
        29094, 29116, 29353, 29405, 29533, 29572, 29663, 29706, 29774, 30000,
        30241, 30383, 30558, 30815, 30923, 30982, 31102, 31103, 31156, 31324,
        31360, 31618, 31717, 31883, 32045, 32068, 32260, 32275, 32452, 33028,
        33099, 33158, 33371, 33418, 33658, 33698, 33831, 33834, 34311, 34575,
        34709, 34779, 34835, 34937, 34977, 35030, 35044, 35096, 35273, 35299,
        35330, 35335, 35709, 35750, 35818, 35978, 36073, 36113, 36117, 36315,
        36394, 36436, 36492, 36529, 36599, 36672, 36772, 36851, 36984, 37342,
        37376, 37405, 37515, 37653, 37717, 37914, 37984, 38069, 38141, 38487,
        38687, 38754, 38839, 38977, 39027, 39153, 39178, 39204, 39230, 39329,
        39426, 39427, 39908, 39984, 39995, 40037, 40172, 40388, 40396, 40426,
        40790, 40817, 41105, 41171, 41173, 41181, 41406, 41477, 41484, 41550,
        41572, 41598, 41843, 41936, 41989, 41990, 42220, 42228, 42252, 42283,
        42487, 42734, 42796, 42798, 42816, 42834, 42993, 43036, 43289, 43389,
        43469, 43699, 43826, 43934, 43969, 43984, 44193, 44360, 44678, 44831,
        44884, 45336, 45558, 45560, 45638, 45657, 45674, 45760, 45802, 45938,
        45975, 46040, 46304, 46601, 46767, 46782, 47102, 47264, 47387, 47447,
        47560, 47667, 47733, 47821, 47928, 48013, 48259, 48282, 48312, 48364,
        48537, 48752, 48930, 48953, 49355, 49406, 49467, 49655, 49673, 49716,
        49798, 50118]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449, 450, 451]))
[2024-02-13 13:53:32,711][qtransform.run.infer][INFO] - Writing sample: 1/10
[2024-02-13 13:53:54,693][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  198,   369,   784,   788,   821,   847,   869,   901,   962,   998,
         1020,  1025,  1147,  1179,  1193,  1201,  1267,  1299,  1370,  1408,
         1433,  1508,  1563,  1729,  1783,  1903,  2173,  2288,  2300,  2334,
         2566,  2621,  2690,  2878,  2898,  2950,  3134,  3396,  3478,  3631,
         3652,  3726,  3757,  3925,  3976,  4096,  4157,  4268,  4367,  4388,
         4421,  4423,  4624,  4704,  4898,  5227,  5720,  5831,  5933,  5942,
         5994,  6035,  6222,  6229,  6395,  6830,  6921,  6922,  6980,  6994,
         7025,  7027,  7071,  7134,  7424,  7568,  7623,  7648,  7705,  8125,
         8206,  8442,  8489,  8535,  8967,  9082,  9145,  9194,  9260,  9791,
        10043, 10163, 10180, 10328, 10511, 10685, 10801, 10912, 10969, 11286,
        11412, 11481, 11626, 11646, 11748, 11769, 11851, 12093, 12374, 13011,
        13018, 13128, 13154, 13216, 13415, 13470, 13509, 13518, 13562, 13577,
        13581, 13740, 13982, 14001, 14043, 14192, 14280, 14379, 14533, 14730,
        14874, 14921, 15222, 15313, 15323, 15350, 15582, 15636, 15670, 15712,
        15976, 15982, 16098, 16457, 16665, 16699, 16742, 16754, 16798, 17062,
        17108, 17178, 17312, 17381, 17798, 17852, 17871, 17918, 17963, 18074,
        18091, 18127, 18133, 18184, 18347, 18364, 18869, 18895, 18981, 18997,
        19009, 19021, 19072, 19140, 19165, 19171, 19189, 19215, 19296, 19306,
        19386, 19393, 19762, 19916, 19938, 19988, 20048, 20214, 20319, 20468,
        20478, 20491, 21086, 21093, 21253, 21287, 21550, 21932, 22193, 22317,
        22358, 22540, 22633, 22644, 22879, 23253, 23378, 23405, 23412, 23456,
        23627, 23903, 24030, 24065, 24208, 24300, 24306, 24454, 24513, 24718,
        24798, 24940, 25039, 25492, 25496, 25564, 25613, 25810, 26034, 26102,
        26111, 26248, 26362, 26405, 26670, 26680, 26727, 26872, 26930, 27089,
        27162, 27262, 27413, 27765, 27923, 28212, 28337, 28408, 28496, 28563,
        28821, 28918, 29094, 29353, 29550, 29559, 29594, 29657, 29869, 30017,
        30099, 30139, 30202, 30260, 30483, 30631, 30915, 30946, 31021, 31103,
        31107, 31288, 31340, 31360, 31561, 31575, 31851, 31975, 32123, 32168,
        32228, 32263, 32443, 32452, 32619, 32860, 33150, 33158, 33562, 33599,
        33610, 33687, 33698, 33736, 33740, 34227, 34278, 34591, 34627, 34805,
        34811, 34911, 34937, 35030, 35177, 35202, 35290, 35316, 35385, 35466,
        35494, 35589, 35867, 35902, 35997, 36222, 36315, 36377, 36509, 36873,
        37053, 37117, 37294, 37342, 37344, 37464, 37536, 37608, 37810, 37864,
        37994, 38079, 38128, 38248, 38304, 38336, 38356, 38510, 38591, 38985,
        39057, 39177, 39427, 39538, 39789, 39797, 39801, 39805, 39907, 39984,
        39995, 40009, 40028, 40108, 40198, 40223, 40401, 40549, 40600, 40740,
        40755, 40869, 40952, 40976, 41148, 41199, 41274, 41364, 41572, 41632,
        41672, 41973, 41990, 42184, 42341, 42470, 42522, 42540, 42598, 42656,
        42798, 42851, 43130, 43289, 43648, 43815, 43895, 43988, 44205, 44265,
        44333, 44427, 44589, 44598, 44707, 44738, 44978, 45002, 45017, 45041,
        45186, 45384, 45479, 45529, 45564, 45627, 45642, 45650, 45674, 45677,
        45739, 45752, 45967, 46259, 46408, 46636, 46811, 46969, 47264, 47333,
        47392, 47558, 47571, 47746, 47916, 47986, 48013, 48285, 48437, 48622,
        48805, 48822, 48877, 48949, 49026, 49153, 49261, 49716, 49720, 49747,
        49798, 49923, 49945, 50040, 50243]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444]))
[2024-02-13 13:53:54,698][qtransform.run.infer][INFO] - Writing sample: 2/10
[2024-02-13 13:54:18,603][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([   54,    96,   135,   198,   317,   343,   590,   694,   788,   824,
          829,   879,   933,   962,  1010,  1020,  1113,  1267,  1416,  1620,
         1645,  1656,  1733,  1926,  2334,  2451,  2616,  2872,  2874,  2889,
         3063,  3073,  3298,  3322,  3478,  3510,  3534,  3546,  3629,  3726,
         3759,  3770,  3929,  4062,  4283,  4294,  4422,  4641,  4671,  4716,
         4719,  4795,  4930,  5065,  5359,  5476,  5533,  5578,  5720,  5748,
         6013,  6087,  6336,  6760,  6783,  6810,  6932,  6968,  7027,  7134,
         7160,  7226,  7552,  7568,  7672,  7943,  8035,  8489,  8978,  9260,
         9388,  9439,  9538,  9674,  9717,  9746,  9870,  9912, 10282, 10503,
        10573, 10607, 11182, 11265, 11481, 11544, 11584, 11588, 11647, 11748,
        11769, 11780, 11793, 11870, 11872, 11885, 11974, 12346, 12407, 12489,
        12587, 12703, 12723, 13444, 13451, 13460, 13502, 13656, 13684, 13725,
        13807, 13822, 13939, 14073, 14105, 14186, 14268, 14614, 14725, 14836,
        14992, 15073, 15222, 15350, 15501, 15609, 15872, 16031, 16067, 16457,
        16634, 16665, 16722, 16941, 16992, 16998, 17013, 17098, 17210, 17228,
        17233, 17247, 17323, 17452, 17575, 17913, 17945, 18356, 18556, 18577,
        18659, 18699, 18758, 18789, 18839, 18948, 19064, 19145, 19402, 19480,
        19485, 19601, 19751, 19762, 19949, 19990, 20048, 20060, 20326, 20478,
        20627, 20751, 20893, 20920, 21020, 21079, 21117, 21246, 21253, 21404,
        21420, 21459, 21544, 21567, 21714, 21723, 21819, 21864, 21910, 22080,
        22089, 22317, 22451, 22568, 22633, 22880, 23194, 23316, 23412, 23678,
        23689, 23695, 23798, 23821, 23868, 23982, 24065, 24431, 24513, 24550,
        24666, 24798, 24872, 24917, 24991, 25089, 25100, 25452, 25492, 25546,
        25595, 25734, 25755, 25810, 26119, 26153, 26343, 26476, 26493, 26660,
        26920, 27085, 27277, 27319, 27775, 28003, 28252, 28256, 28563, 28583,
        28821, 29127, 29405, 29440, 29502, 29565, 29594, 29908, 30000, 30241,
        30260, 30315, 30381, 30558, 30571, 30641, 30716, 30812, 30837, 30886,
        31103, 31288, 31353, 31618, 31705, 31851, 31974, 32228, 32262, 32443,
        32650, 32652, 32674, 32876, 33174, 33216, 33520, 33687, 33769, 33780,
        33905, 34004, 34051, 34213, 34373, 34433, 34586, 34604, 34706, 34761,
        34766, 34801, 34895, 34899, 35044, 35316, 35335, 35466, 35494, 35559,
        35589, 35749, 35997, 36017, 36110, 36418, 36439, 36453, 36486, 36492,
        36570, 36673, 36694, 36857, 36998, 37061, 37116, 37163, 37282, 37536,
        37637, 37653, 37835, 37887, 37898, 37914, 37988, 37994, 38118, 38125,
        38128, 38131, 38139, 38244, 38461, 38518, 38520, 38640, 38687, 38819,
        38839, 38889, 39067, 39074, 39109, 39189, 39258, 39311, 39315, 39427,
        39775, 39984, 40062, 40300, 40323, 40388, 40401, 40604, 40783, 40817,
        40828, 40976, 41265, 41442, 41762, 41906, 41957, 41962, 42120, 42259,
        42298, 42413, 42457, 42834, 42845, 43058, 43295, 43298, 43320, 43815,
        43826, 43851, 43863, 43895, 43910, 43960, 44024, 44265, 44427, 44477,
        44598, 44738, 44796, 44856, 44970, 45186, 45384, 45389, 45477, 45479,
        45558, 45599, 45805, 45829, 45831, 46304, 46345, 46355, 46420, 46807,
        47130, 47227, 47287, 47335, 47698, 47894, 47990, 48000, 48119, 48212,
        48215, 48312, 48479, 48629, 48659, 48789, 49100, 49119, 49214, 49655,
        49792, 49798, 49813, 49823, 50087, 50216]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445]))
[2024-02-13 13:54:18,609][qtransform.run.infer][INFO] - Writing sample: 3/10
[2024-02-13 13:54:40,214][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  198,   590,   824,  1073,  1074,  1096,  1147,  1179,  1594,  1800,
         1880,  2139,  2244,  2300,  2451,  2552,  2690,  2898,  3063,  3101,
         3322,  3363,  3403,  3564,  3599,  3756,  3770,  3834,  3889,  4206,
         4283,  4367,  4628,  4804,  5097,  5209,  5348,  5476,  5528,  5578,
         5630,  5675,  5755,  5892,  5923,  5957,  5965,  6011,  6035,  6051,
         6060,  6229,  6442,  6549,  6578,  6604,  6684,  6895,  6994,  7027,
         7054,  7118,  7251,  7598,  7623,  7962,  8024,  8137,  8149,  8334,
         8482,  8532,  8593,  8789,  9067,  9071,  9232,  9369,  9814,  9912,
        10193, 10266, 10282, 10360, 10362, 10458, 10668, 10687, 10794, 10839,
        10958, 10970, 11244, 11268, 11609, 11723, 11941, 12046, 12203, 12307,
        12481, 12694, 12866, 13011, 13012, 13154, 13172, 13361, 13380, 13431,
        13527, 13528, 13562, 13699, 13713, 13733, 13939, 14001, 14270, 14532,
        14725, 14750, 14842, 14866, 14988, 15323, 15353, 15396, 15540, 15636,
        15651, 15712, 15782, 15794, 15796, 15863, 16031, 16099, 16450, 16495,
        16541, 16665, 16666, 16989, 17218, 17356, 17464, 17535, 17575, 17759,
        17875, 17906, 17986, 18145, 18307, 18316, 18333, 18869, 18895, 18926,
        18948, 18997, 19115, 19337, 19485, 19856, 19921, 19988, 20048, 20060,
        20223, 20257, 20305, 20468, 20478, 20481, 20483, 20517, 20518, 20575,
        20627, 20746, 20881, 20983, 21121, 21246, 21273, 21301, 21308, 21404,
        21604, 21753, 21925, 21980, 22025, 22049, 22080, 22233, 22274, 22346,
        22372, 22566, 22591, 22633, 22657, 22798, 22858, 23121, 23128, 23162,
        23201, 23253, 23449, 23678, 23690, 23852, 23938, 23965, 24216, 24275,
        24335, 24345, 24396, 24422, 24479, 24826, 25039, 25168, 25412, 25532,
        25622, 25631, 25679, 25716, 25810, 25839, 25939, 25977, 26008, 26119,
        26322, 26517, 26657, 26694, 26748, 26843, 27020, 27022, 27413, 27497,
        27644, 27775, 28058, 28068, 28131, 28328, 28334, 28335, 28337, 28563,
        28617, 28669, 28682, 28740, 29124, 29150, 29305, 29350, 29353, 29533,
        29558, 29594, 29783, 29847, 30035, 30241, 30381, 30449, 30458, 30468,
        30741, 30947, 31103, 31107, 31145, 31185, 31239, 31264, 31322, 31328,
        31360, 31430, 31534, 31822, 31851, 31922, 32147, 32532, 32739, 33028,
        33032, 33056, 33061, 33093, 33174, 33222, 33293, 33346, 33914, 34004,
        34268, 34311, 34575, 34805, 34937, 35082, 35097, 35566, 35589, 36011,
        36073, 36116, 36248, 36401, 36492, 36582, 36599, 36673, 36786, 36865,
        36873, 36977, 37059, 37294, 37308, 37612, 38064, 38141, 38150, 38322,
        38385, 38436, 39027, 39067, 39107, 39426, 39514, 39653, 39808, 39826,
        39856, 39901, 39908, 39984, 40014, 40201, 40219, 40426, 40557, 40599,
        40693, 40752, 40755, 40817, 41112, 41318, 41472, 41505, 41550, 41572,
        41630, 41632, 41762, 42119, 42120, 42220, 42298, 42452, 42470, 42528,
        42554, 42614, 42760, 42790, 42821, 42851, 42880, 42967, 43079, 43136,
        43289, 43391, 43499, 43554, 43643, 43766, 43815, 43851, 43930, 43956,
        43984, 44123, 44130, 44153, 44214, 44402, 44427, 44459, 44562, 44600,
        44678, 44711, 44978, 45041, 45134, 45336, 45529, 45627, 45752, 45760,
        45877, 45896, 46046, 46153, 46251, 46960, 47145, 47172, 47277, 47675,
        47745, 47803, 47851, 47856, 48013, 48171, 48215, 48230, 48479, 48528,
        48529, 48622, 49026, 49100, 49129, 49312, 49355, 49425, 49449, 49453,
        49486, 49602, 49615, 49732, 49744, 49765, 49900, 49910, 49923, 50160]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))
[2024-02-13 13:54:40,293][qtransform.run.infer][INFO] - Writing sample: 4/10
[2024-02-13 13:55:01,601][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  198,   259,   285,   317,   406,   521,   653,   664,   829,   901,
          962,   972,  1010,  1248,  1267,  1508,  1616,  1774,  1880,  2005,
         2173,  2207,  2235,  2300,  2454,  2485,  2653,  2898,  2950,  2956,
         3061,  3355,  3383,  3480,  3534,  3569,  3776,  3889,  4157,  4294,
         4388,  4400,  4429,  4491,  4624,  4641,  4712,  4725,  4804,  4930,
         5195,  5291,  5305,  5371,  5399,  5443,  5507,  5630,  5905,  5960,
         5965,  6011,  6035,  6175,  6274,  6442,  6480,  6557,  6596,  6760,
         6775,  7027,  7118,  7251,  7324,  7450,  8188,  8248,  8334,  8345,
         8347,  8354,  8569,  8705,  8744,  8804,  8854,  8892,  9153,  9209,
         9323,  9328,  9433,  9452,  9589,  9906,  9918, 10010, 10076, 10282,
        10328, 10379, 10404, 11015, 11087, 11244, 11515, 11557, 11588, 11796,
        11872, 11911, 11968, 12282, 12723, 12782, 12941, 13030, 13084, 13154,
        13169, 13178, 13415, 13455, 13562, 13586, 13611, 13733, 13851, 13903,
        14001, 14043, 14335, 14532, 14563, 14741, 14794, 14850, 14855, 14996,
        15203, 15302, 15360, 15382, 15441, 15470, 15582, 15636, 16031, 16098,
        16148, 16392, 16454, 16501, 16600, 16624, 16891, 16918, 17005, 17062,
        17067, 17189, 17249, 17323, 17647, 17918, 17986, 18054, 18386, 18411,
        18795, 18840, 18895, 18926, 18948, 18958, 19047, 19313, 19332, 19402,
        19485, 19637, 19716, 19745, 19890, 20060, 20128, 20199, 20468, 20514,
        20662, 20777, 20817, 20966, 21089, 21206, 21239, 21273, 21400, 21457,
        21714, 21910, 22043, 22358, 22431, 22553, 22591, 22633, 22968, 23014,
        23214, 23230, 23316, 23412, 23647, 23689, 23769, 23815, 23962, 24065,
        24545, 24623, 24654, 25275, 25412, 25622, 25716, 25800, 25810, 26093,
        26248, 26261, 26282, 26523, 26560, 26752, 26814, 26850, 26944, 27122,
        27270, 27331, 27444, 27445, 27488, 27924, 28014, 28079, 28153, 28221,
        28393, 28561, 28563, 28566, 28682, 28699, 28918, 29006, 29094, 29116,
        29141, 29353, 29405, 29744, 29884, 29888, 30465, 30468, 30565, 30757,
        30816, 30921, 30946, 31040, 31103, 31107, 31200, 31360, 31427, 31621,
        31713, 32228, 32350, 32409, 32443, 32452, 32699, 32889, 33020, 33032,
        33099, 33286, 33409, 33479, 33569, 33727, 33889, 33893, 34051, 34130,
        34278, 34439, 34581, 34767, 35030, 35096, 35290, 35341, 35410, 35748,
        35805, 35814, 35866, 36116, 36117, 36173, 36241, 36315, 36377, 36492,
        36582, 36599, 36665, 36814, 37218, 37994, 38060, 38067, 38089, 38118,
        38141, 38356, 38487, 38516, 38645, 38687, 38701, 38852, 39073, 39128,
        39177, 39291, 39426, 39480, 39495, 39877, 39931, 39984, 40198, 40237,
        40249, 40430, 40580, 40676, 40686, 40693, 40772, 40782, 40817, 40976,
        41308, 41414, 41632, 41736, 41966, 42184, 42254, 42334, 42418, 42470,
        42798, 42845, 43079, 43130, 43142, 43232, 43298, 43377, 43444, 43481,
        43489, 43699, 43805, 43814, 43986, 44156, 44290, 44402, 44580, 44678,
        44738, 44992, 45024, 45069, 45260, 45291, 45456, 45569, 45772, 45999,
        46161, 46420, 46434, 46577, 46610, 46776, 46876, 46885, 46951, 46954,
        47113, 47227, 47254, 47335, 47449, 47629, 47667, 47678, 47697, 47698,
        47733, 47845, 47847, 47986, 48013, 48134, 48306, 48307, 48350, 48395,
        48442, 48489, 48644, 48664, 48929, 48962, 48999, 49100, 49190, 49272,
        49355, 49462, 49630, 49674, 49765, 49792, 49875, 49969, 50095, 50116]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449]))
[2024-02-13 13:55:01,605][qtransform.run.infer][INFO] - Writing sample: 5/10
[2024-02-13 13:55:23,793][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([   31,   153,   198,   211,   227,   317,   371,   433,   481,   486,
          527,   589,   633,   857,   879,   880,   962,  1020,  1025,  1074,
         1408,  1422,  1428,  1861,  2019,  2138,  2266,  2280,  2300,  2334,
         2379,  2451,  2743,  2897,  2950,  3182,  3383,  3599,  3757,  3770,
         4012,  4157,  4162,  4388,  4459,  4491,  4514,  4628,  4858,  4930,
         4971,  5020,  5109,  5119,  5134,  5209,  5373,  5578,  5675,  5758,
         5929,  6458,  6596,  6657,  6687,  6717,  6760,  6783,  6868,  6950,
         6968,  6975,  6983,  7038,  7056,  7111,  7118,  7134,  7324,  7546,
         8095,  8125,  8188,  8250,  8277,  8334,  8609,  8684,  8906,  9035,
         9145,  9323,  9511,  9621,  9672,  9855,  9901,  9959, 10085, 10180,
        10404, 10460, 10804, 10993, 11430, 11530, 11588, 11654, 11717, 12025,
        12282, 12307, 12539, 12582, 12587, 12680, 12694, 12889, 13012, 13084,
        13168, 13172, 13224, 13448, 13509, 13523, 13562, 13611, 13628, 13822,
        13927, 14001, 14041, 14043, 14532, 14554, 14603, 14666, 14725, 14866,
        14919, 15073, 15135, 15222, 15472, 15501, 15630, 15860, 15894, 15976,
        16084, 16180, 16818, 16880, 16937, 17047, 17164, 17278, 17320, 17842,
        17906, 18032, 18237, 18283, 18336, 18831, 18869, 18948, 19148, 19171,
        19215, 19337, 19369, 19371, 19559, 19716, 19745, 20326, 20478, 20512,
        20518, 20627, 20757, 20785, 20792, 20881, 20900, 21105, 21121, 21184,
        21266, 21404, 21420, 21457, 21637, 21800, 21910, 21985, 22136, 22319,
        22393, 22568, 22657, 22709, 22954, 23128, 23412, 23628, 24065, 24479,
        24798, 24935, 24999, 25100, 25256, 25528, 25532, 25562, 25716, 25810,
        25833, 25939, 26146, 26153, 26381, 26413, 26537, 26551, 26645, 26670,
        26852, 27115, 27123, 27211, 27496, 27647, 27946, 28011, 28056, 28099,
        28121, 28165, 28237, 28388, 28561, 28769, 29116, 29130, 29224, 29292,
        29445, 29488, 29558, 29565, 29594, 29657, 29717, 29743, 30025, 30026,
        30241, 30301, 30327, 30449, 30621, 30658, 30797, 30812, 30852, 31261,
        31264, 31328, 31394, 31519, 31682, 31824, 31862, 31887, 32111, 32371,
        32635, 32690, 32717, 32754, 32843, 33174, 33187, 33376, 33549, 33594,
        33660, 34236, 34304, 34398, 34575, 34662, 34761, 34805, 34929, 35065,
        35166, 35290, 35335, 35417, 35418, 35420, 35466, 35515, 35618, 36241,
        36250, 36377, 36492, 36529, 36570, 36673, 36795, 37053, 37198, 37294,
        37324, 37342, 37345, 37405, 37464, 37468, 37540, 37691, 37776, 37988,
        38064, 38089, 38114, 38128, 38209, 38289, 38336, 38346, 38356, 38376,
        38416, 38518, 38687, 38739, 38764, 38777, 39047, 39109, 39119, 39250,
        39296, 39384, 39495, 39750, 39801, 39805, 39812, 40009, 40037, 40138,
        40568, 40693, 40772, 40795, 40952, 40955, 40982, 41062, 41297, 41374,
        41463, 41472, 41511, 41550, 41669, 41776, 41892, 42120, 42199, 42295,
        42298, 42457, 42776, 42845, 42928, 42993, 43006, 43022, 43045, 43047,
        43173, 43243, 43286, 43289, 43469, 43605, 43689, 43702, 43815, 43826,
        44065, 44176, 44245, 44310, 44362, 44427, 44540, 44543, 44598, 44678,
        44849, 45002, 45260, 45384, 45423, 45441, 45585, 45605, 45627, 45657,
        45802, 45837, 45959, 46241, 46314, 46324, 46367, 46407, 46680, 46952,
        47243, 47448, 47629, 47706, 47866, 48013, 48364, 48523, 48622, 48624,
        48823, 48880, 48929, 49026, 49178, 49235, 49256, 49486, 49792, 49811,
        49963, 49969, 50144, 50166, 50236]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449, 450, 451, 452, 453, 454]))
[2024-02-13 13:55:23,797][qtransform.run.infer][INFO] - Writing sample: 6/10
[2024-02-13 13:55:46,895][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([   40,    47,   198,   208,   370,   476,   486,   518,   563,   590,
          761,   801,   854,   879,   901,   962,  1005,  1077,  1127,  1264,
         1267,  1533,  1563,  1772,  1860,  1880,  1887,  1992,  2055,  2127,
         2457,  2660,  2690,  2694,  2832,  2874,  2950,  3196,  3322,  3341,
         3355,  3363,  3444,  3510,  3593,  3665,  3770,  3843,  3877,  4223,
         4256,  4328,  4333,  4367,  4388,  4491,  4620,  4624,  4641,  4692,
         4740,  4831,  5072,  5276,  5320,  5373,  5385,  5399,  5456,  5630,
         5905,  6229,  6760,  6783,  6830,  6994,  7027,  7111,  7134,  7256,
         7330,  7344,  7553,  7648,  8124,  8555,  8590,  8609,  8648,  8689,
         8874,  8915,  9190,  9194,  9323,  9352,  9369,  9450,  9694,  9795,
         9855,  9901,  9925, 10403, 10435, 10669, 10691, 10719, 10769, 10858,
        10946, 11089, 11143, 11286, 11872, 11885, 12207, 12283, 12490, 12673,
        12739, 12955, 13027, 13283, 13346, 13548, 13618, 13635, 13695, 13822,
        14013, 14064, 14073, 14280, 14464, 14532, 14643, 14725, 14988, 15222,
        15244, 15314, 15559, 15636, 15737, 15794, 15860, 16035, 16081, 16410,
        16614, 16716, 16998, 17046, 17067, 17101, 17151, 17191, 17218, 17278,
        17575, 17654, 17759, 17829, 17852, 17875, 17883, 17913, 17928, 17986,
        18032, 18237, 18307, 18331, 18347, 18731, 18849, 18895, 18948, 19072,
        19215, 19274, 19313, 19374, 19384, 19409, 19524, 19548, 19593, 19813,
        19916, 20017, 20048, 20060, 20303, 20360, 20393, 20402, 20518, 20627,
        20706, 20882, 21103, 21117, 21239, 21345, 21395, 21457, 21496, 21824,
        21841, 22182, 22233, 22633, 23194, 23230, 23320, 23347, 24275, 24349,
        24431, 24872, 24991, 24999, 25306, 25357, 25468, 25535, 25612, 25677,
        25716, 25810, 25904, 25934, 25939, 26008, 26111, 26362, 26405, 26471,
        26493, 26500, 26670, 26791, 26890, 26944, 27037, 27331, 27479, 27488,
        27496, 27498, 27576, 27775, 27898, 28252, 28264, 28332, 28379, 28489,
        28491, 28563, 28682, 28844, 28919, 29094, 29116, 29233, 29313, 29353,
        29485, 29488, 29509, 29550, 29678, 29774, 29869, 29894, 29984, 29988,
        30000, 30041, 30103, 30274, 30808, 30858, 30946, 31040, 31103, 31433,
        31547, 31676, 31686, 31815, 32068, 32266, 32324, 32443, 32588, 32591,
        32650, 32860, 32900, 33032, 33187, 33199, 33475, 33827, 34064, 34150,
        34171, 34278, 34311, 34507, 34575, 34825, 34937, 34951, 35030, 35220,
        35335, 35631, 35709, 35723, 35790, 35814, 35929, 35962, 36082, 36116,
        36117, 36255, 36259, 36599, 36673, 36779, 37053, 37344, 37405, 37536,
        37568, 37588, 37994, 38128, 38172, 38210, 38253, 38438, 38516, 38585,
        38586, 38598, 38839, 38883, 38903, 39027, 39081, 39311, 39348, 39426,
        39495, 39622, 39696, 39789, 39826, 39984, 40198, 40388, 40422, 40676,
        40693, 40817, 40907, 40911, 40960, 40976, 41435, 41572, 41681, 41863,
        41962, 42350, 42550, 42635, 42702, 42784, 42821, 42834, 42919, 42991,
        43045, 43173, 43513, 43664, 43699, 43788, 43805, 44105, 44214, 44427,
        44738, 44953, 44976, 45024, 45327, 45372, 45585, 45752, 45764, 45849,
        46241, 46304, 46678, 47110, 47200, 47264, 47821, 47913, 47936, 47951,
        48000, 48013, 48108, 48309, 48479, 48523, 48612, 48664, 48935, 49026,
        49045, 49085, 49091, 49105, 49715, 49783, 50156, 50160, 50195, 50232]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439]))
[2024-02-13 13:55:46,899][qtransform.run.infer][INFO] - Writing sample: 7/10
[2024-02-13 13:56:07,707][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([  107,   198,   220,   230,   608,   821,   879,   901,   948,   962,
         1108,  1248,  1267,  1668,  1714,  1760,  1858,  1880,  1887,  1928,
         1964,  2027,  2152,  2173,  2199,  2288,  2483,  2690,  2804,  2877,
         2878,  2894,  2966,  2989,  3101,  3582,  3599,  3708,  3741,  3809,
         3829,  4096,  4115,  4549,  4584,  4590,  4746,  4804,  5053,  5369,
         5496,  5531,  5559,  5578,  5588,  5675,  5922,  6011,  6175,  6229,
         6267,  6363,  6512,  6515,  6619,  6717,  6760,  6783,  6886,  6968,
         6975,  6994,  7009,  7056,  7118,  7130,  7251,  7588,  7623,  7948,
         8057,  8073,  8193,  8206,  8334,  8354,  8482,  8554,  8609,  8853,
         9029,  9145,  9153,  9250,  9323,  9328,  9538,  9642,  9700,  9746,
         9747, 10017, 10212, 10289, 10824, 11481, 11584, 11667, 11769, 11811,
        11818, 11858, 11886, 12207, 12255, 12402, 12582, 12694, 12716, 12729,
        12905, 12913, 13028, 13154, 13201, 13223, 13270, 13355, 13444, 13576,
        13611, 13635, 13676, 13733, 13939, 14073, 14243, 14383, 14541, 14606,
        14685, 14794, 14842, 14988, 14992, 15023, 15275, 15768, 15826, 15889,
        15965, 15989, 16009, 16030, 16131, 16180, 16384, 16504, 16746, 16853,
        17053, 17100, 17323, 17535, 17566, 17654, 17687, 17852, 18113, 18142,
        18221, 18313, 18671, 18706, 18926, 18948, 19296, 19350, 19365, 19385,
        19398, 19485, 19587, 19684, 19875, 19916, 19924, 19988, 20222, 20281,
        20326, 20424, 20468, 20517, 20518, 20674, 20751, 20785, 20856, 20881,
        21020, 21027, 21121, 21185, 21287, 21420, 21461, 21485, 21710, 21723,
        21783, 21835, 22078, 22136, 22393, 22568, 22633, 22879, 22968, 23012,
        23013, 23107, 23201, 23278, 23344, 23689, 23893, 24065, 24154, 24398,
        24479, 24536, 24550, 24798, 25053, 25085, 25268, 25288, 25412, 25447,
        25529, 25672, 25716, 25810, 26034, 26143, 26322, 26346, 26657, 26670,
        26727, 26906, 27775, 27924, 28235, 28332, 28444, 28496, 28563, 28682,
        28858, 28913, 28993, 29103, 29353, 29572, 29594, 29663, 30103, 30468,
        30534, 30558, 30621, 30677, 30682, 30698, 30893, 31011, 31103, 31561,
        31623, 31686, 31789, 32123, 32161, 32228, 32443, 32455, 32532, 32614,
        32666, 32674, 32731, 33040, 33099, 33112, 33187, 33300, 33403, 33460,
        33548, 33563, 33631, 33670, 33736, 33871, 34278, 34311, 34356, 34591,
        34694, 34766, 34977, 34979, 35030, 35150, 35220, 35335, 35492, 35541,
        35559, 35696, 35723, 35805, 35806, 35939, 36222, 36315, 36377, 36492,
        36599, 36673, 36772, 37053, 37116, 37151, 37233, 37282, 37294, 37308,
        37436, 37612, 37688, 37717, 37914, 38067, 38118, 38128, 38251, 38385,
        38522, 38568, 38591, 38687, 39291, 39327, 39427, 39696, 39700, 39801,
        39812, 39824, 39881, 39960, 40033, 40062, 40105, 40325, 40355, 40507,
        40538, 40693, 40772, 40803, 40828, 41081, 41111, 41151, 41279, 41454,
        41681, 41831, 42204, 42220, 42298, 42413, 42546, 42661, 42747, 42784,
        42798, 42882, 43130, 43289, 43292, 43463, 43469, 43489, 43814, 43815,
        43895, 43907, 43949, 44049, 44214, 44427, 44463, 44477, 44543, 44738,
        44932, 45038, 45159, 45186, 45599, 45653, 45985, 46017, 46101, 46153,
        46188, 46347, 46417, 46553, 46579, 46602, 46647, 46685, 46696, 46811,
        46969, 47070, 47447, 47534, 47542, 47560, 47629, 47667, 47916, 47933,
        48023, 48026, 48032, 48112, 48479, 48715, 48763, 48774, 48933, 48985,
        49039, 49301, 49355, 49629, 49744, 49923, 49945, 50008, 50086, 50161]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]))
[2024-02-13 13:56:07,712][qtransform.run.infer][INFO] - Writing sample: 8/10
[2024-02-13 13:56:27,803][qtransform.run.infer][DEBUG] - Uniquely generated tokens, sorted in ascending order: torch.return_types.sort(
values=tensor([   40,   122,   137,   153,   198,   415,   433,   589,   650,   655,
          901,   919,  1025,  1030,  1044,  1074,  1187,  1267,  1418,  1599,
         1641,  1714,  1903,  2138,  2300,  2334,  2451,  2470,  2566,  2690,
         2890,  2905,  3006,  3478,  3652,  3708,  3741,  3757,  3770,  3793,
         3925,  4160,  4430,  4491,  4611,  4730,  4804,  4818,  5482,  5507,
         5578,  5675,  5708,  5785,  5809,  5957,  6317,  6456,  6619,  6760,
         6907,  6968,  7027,  7056,  7160,  7334,  7369,  7568,  7583,  7623,
         7712,  7739,  7753,  8082,  8216,  8248,  8334,  8347,  8832,  9145,
         9155,  9167,  9194,  9260,  9323,  9538,  9541,  9627,  9683,  9943,
        10060, 10076, 10180, 10282, 10404, 10622, 10719, 10738, 10800, 11286,
        11788, 11796, 11872, 11885, 11970, 12046, 12307, 12396, 12476, 12514,
        12582, 12641, 12729, 13158, 13172, 13173, 13204, 13346, 13364, 13415,
        13542, 13650, 13939, 14001, 14073, 14243, 14268, 14725, 14730, 14842,
        14855, 14879, 14988, 15169, 15222, 15226, 15275, 15636, 15737, 16040,
        16180, 16457, 16504, 16600, 16631, 16877, 16918, 17098, 17256, 17320,
        17323, 17416, 17483, 17558, 17842, 17866, 17949, 17951, 18092, 18375,
        18681, 18869, 18948, 18979, 18989, 19044, 19277, 19296, 19645, 19687,
        19746, 20060, 20202, 20257, 20303, 20457, 20468, 20478, 20517, 20706,
        20741, 20881, 20893, 20998, 21155, 21400, 21472, 21481, 21614, 21636,
        21800, 21910, 21998, 22136, 22317, 22442, 22591, 22633, 22657, 23024,
        23087, 23102, 23201, 23297, 23347, 23428, 23530, 23553, 23769, 23788,
        23852, 23901, 24065, 24361, 24506, 24513, 24550, 25039, 25452, 25492,
        25532, 25535, 25683, 25716, 25734, 25755, 25782, 25786, 25810, 25904,
        25962, 26008, 26176, 26385, 26670, 26791, 26900, 27136, 27455, 27474,
        27488, 27516, 27891, 28068, 28271, 28563, 28682, 28705, 29047, 29116,
        29458, 29543, 29546, 29565, 29572, 29663, 29777, 29975, 30009, 30182,
        30558, 30757, 30851, 30946, 31040, 31043, 31215, 31365, 31398, 31510,
        31571, 31575, 31581, 31822, 31845, 31975, 32129, 32443, 32452, 32514,
        32532, 32674, 32776, 32829, 33148, 33161, 33687, 33736, 34064, 34222,
        34321, 34339, 34373, 34472, 34561, 34592, 34726, 34761, 34937, 35004,
        35007, 35030, 35098, 35130, 35166, 35174, 35284, 35290, 35324, 35335,
        35418, 35468, 35814, 35929, 36082, 36116, 36134, 36217, 36220, 36236,
        36280, 36305, 36315, 36316, 36372, 36394, 36582, 36769, 36821, 36835,
        37318, 37319, 37405, 37495, 37539, 37838, 37887, 37895, 37914, 37988,
        38009, 38118, 38121, 38128, 38169, 38336, 38356, 38436, 38481, 38516,
        38777, 38798, 38839, 38885, 39067, 39176, 39209, 39427, 39801, 39872,
        39895, 39960, 39984, 40009, 40014, 40062, 40118, 40125, 40153, 40330,
        40401, 40521, 40599, 40828, 41025, 41135, 41171, 41318, 41414, 41721,
        41736, 41744, 41951, 41962, 42120, 42155, 42328, 42503, 42575, 42595,
        42598, 42798, 42805, 43045, 43130, 43189, 43251, 43287, 43532, 43895,
        44017, 44104, 44176, 44190, 44310, 44362, 44378, 44402, 44752, 44778,
        45291, 45336, 45347, 45384, 45441, 45564, 45585, 45627, 45676, 45849,
        45938, 46331, 46379, 46608, 46701, 46939, 47165, 47388, 47489, 47614,
        47651, 47891, 47916, 48013, 48268, 48420, 48479, 48573, 48622, 48739,
        48929, 48952, 48953, 49026, 49076, 49156, 49223, 49299, 49339, 49732,
        49923]),
indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
        448, 449, 450]))
[2024-02-13 13:56:27,809][qtransform.run.infer][INFO] - Writing sample: 9/10
[2024-02-13 13:56:27,812][qtransform.run.infer][INFO] - Finished writing into file "/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/INFER_2024-02-13_13:52:44_CHECKPOINT.out".
