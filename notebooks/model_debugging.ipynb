{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d4054d-7c7c-414d-b0c9-a4b358028cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 08:18:09.716186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9852b28-0f51-41d7-aa4e-de5dbe2f68ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf3c26f-ea7a-48a6-827c-e660f8e46be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743c84fd-5183-4c98-a49f-a6d5a0f2cd79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db81779-1ac7-4f7f-8415-1c7c740b1d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
    "train_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"test\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1afd09-4044-483b-ac03-3ca051b5d8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "#test_dataset_text = [test_dataset[i : i + batch_size][\"text\"] for i in range(0, len(test_dataset), batch_size)]\n",
    "#def batch_iterator():\n",
    "#    for i in range(0, len(dataset), batch_size):\n",
    "#        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02c6e0e-b2a5-4cb6-bed1-44ee96911daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf2aba9-a081-442f-8030-dd75a939d9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "block_size = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09907928-ab94-4d4c-b878-dc1478baf8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2318\n",
      "240\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "print(len(train_dataset))\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "print(len(eval_dataset))\n",
    "test_dataset = lm_datasets[\"test\"]\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e55e414d-54fc-4673-a48c-ef3a32571b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " when the search party loses radio contact with Lieber, they must make a decision : do they sit out \n",
      " when the search party loses radio contact with Lieber, they must make a decision : do they sit out \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for index in random.sample(range(len(test_dataset)), 1):\n",
    "    print(tokenizer.decode(test_dataset[index]['input_ids'])[:100])\n",
    "    print(tokenizer.decode(test_dataset[index]['labels'])[:100])\n",
    "    #print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86581a8b-a5e0-4ea1-abba-2603637c8f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "20\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders creation:\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "print(len(train_dataloader))\n",
    "print(len(eval_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc1222-2208-41bb-8c90-40e9e5a5b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d07d217-7462-4a24-815d-20f302b25816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(3.4247)\n",
      "1\n",
      "tensor(3.4102)\n",
      "2\n",
      "tensor(3.4637)\n",
      "3\n",
      "tensor(3.5491)\n",
      "epoch 0: perplexity: 31.87777491065038 eval_loss: 3.461909055709839\n",
      "{'perplexity': 31.87777491065038, 'eval_loss': tensor(3.4619), 'epoch': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for epoch in range(0, 1):\n",
    "    active_dataloader = test_dataloader\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        #print(batch)\n",
    "        if step > 2:\n",
    "            break\n",
    "        print(step)\n",
    "        #print(batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch['input_ids'], labels=batch['labels'])\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        print(loss)\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    \n",
    "import math   \n",
    "try:\n",
    "    eval_loss = torch.mean(losses)\n",
    "    perplexity = math.exp(eval_loss)\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "\n",
    "print((f\"epoch {epoch}: perplexity: {perplexity} loss: {eval_loss}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6c8ca9e-8f88-4046-b992-117ab04f723a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: perplexity: 31.87777491065038 eval_loss: 3.461909055709839\n",
      "{'perplexity': 31.87777491065038, 'eval_loss': tensor(3.4619), 'epoch': 0}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33087326-bbfe-487c-afb5-dae220fa884f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ec5b8bb-a109-4eb2-97b9-bbe358d61fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DatasetSplits:\n",
    "    \"\"\"\n",
    "        Dataclass containing the datasets for training, eval, testing, benchmark along with the name of the dataset.\n",
    "        After construction, a simple type check is done with the __post_init__ hook.\n",
    "    \"\"\"\n",
    "    train: str = None\n",
    "    eval: str = None\n",
    "    test: str = None\n",
    "    bench: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de04494b-c06e-49d0-8dad-6d43f2822b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DatasetSplits' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataclasses \u001b[38;5;241m=\u001b[39m DatasetSplits()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdataclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DatasetSplits' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dataclasses = DatasetSplits()\n",
    "dataclasses['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc7109-f9b3-43f3-85b1-300e9efff901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
