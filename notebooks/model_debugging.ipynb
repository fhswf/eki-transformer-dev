{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d4054d-7c7c-414d-b0c9-a4b358028cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9852b28-0f51-41d7-aa4e-de5dbe2f68ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 12:43:13.041242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name '_LazyModule' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/file_utils.py:2777\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:37\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# isort: split\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     default_hp_search_backend,\n\u001b[1;32m     39\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     40\u001b[0m     hp_params,\n\u001b[1;32m     41\u001b[0m     is_fairscale_available,\n\u001b[1;32m     42\u001b[0m     is_optuna_available,\n\u001b[1;32m     43\u001b[0m     is_ray_tune_available,\n\u001b[1;32m     44\u001b[0m     is_sigopt_available,\n\u001b[1;32m     45\u001b[0m     is_wandb_available,\n\u001b[1;32m     46\u001b[0m     run_hp_search_optuna,\n\u001b[1;32m     47\u001b[0m     run_hp_search_ray,\n\u001b[1;32m     48\u001b[0m     run_hp_search_sigopt,\n\u001b[1;32m     49\u001b[0m     run_hp_search_wandb,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyModule\n\u001b[1;32m     19\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawq\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuse_awq_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace_with_awq_linear\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeftAdapterMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     80\u001b[0m }\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_LazyModule' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/file_utils.py:2767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   2766\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 2767\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/file_utils.py:2779\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2780\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2781\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name '_LazyModule' from 'transformers.utils' (/opt/conda/lib/python3.10/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23a93e-d3e1-4919-ad1b-6c597950e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf3c26f-ea7a-48a6-827c-e660f8e46be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743c84fd-5183-4c98-a49f-a6d5a0f2cd79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db81779-1ac7-4f7f-8415-1c7c740b1d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b5be03d1a0400f8b372582e1c219cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bb61c0c756485682f7c315de896d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc608d64ac724d51b5f9804fe95eada3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
    "train_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"test\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1afd09-4044-483b-ac03-3ca051b5d8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "#test_dataset_text = [test_dataset[i : i + batch_size][\"text\"] for i in range(0, len(test_dataset), batch_size)]\n",
    "#def batch_iterator():\n",
    "#    for i in range(0, len(dataset), batch_size):\n",
    "#        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02c6e0e-b2a5-4cb6-bed1-44ee96911daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf2aba9-a081-442f-8030-dd75a939d9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9468b1a789f64916be9af2d4965c7d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dcd318bfd940b4b7f6e3ff66b616bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac07dcf99c99486b808449752e0fc13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "block_size = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09907928-ab94-4d4c-b878-dc1478baf8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2318\n",
      "240\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "print(len(train_dataset))\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "print(len(eval_dataset))\n",
    "test_dataset = lm_datasets[\"test\"]\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55e414d-54fc-4673-a48c-ef3a32571b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " due east from Gould City to Naubinway and then along the lake to Epoufette. The former route throug\n",
      " due east from Gould City to Naubinway and then along the lake to Epoufette. The former route throug\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for index in random.sample(range(len(eval_dataset)), 1):\n",
    "    print(tokenizer.decode(eval_dataset[index]['input_ids'])[:100])\n",
    "    print(tokenizer.decode(eval_dataset[index]['labels'])[:100])\n",
    "    #print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86581a8b-a5e0-4ea1-abba-2603637c8f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "314\n",
      "364\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders creation:\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=default_data_collator, batch_size=batch_size\n",
    ")\n",
    "print(len(train_dataloader))\n",
    "print(len(eval_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d07d217-7462-4a24-815d-20f302b25816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     active_dataloader \u001b[38;5;241m=\u001b[39m eval_dataloader\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m     losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(active_dataloader):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for epoch in range(0, 1):\n",
    "    active_dataloader = eval_dataloader\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        print(step)\n",
    "        print(batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        print(loss)\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    try:\n",
    "        eval_loss = torch.mean(losses)\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "    print({\n",
    "            \"perplexity\": perplexity,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": completed_steps,\n",
    "        },\n",
    "        step=completed_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c8165a-8f81-4e22-9c72-5869f1d705a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c8ca9e-8f88-4046-b992-117ab04f723a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: perplexity: 30.63013824217404 eval_loss: 3.4219844341278076\n",
      "{'perplexity': 30.63013824217404, 'eval_loss': tensor(3.4220), 'epoch': 0}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    eval_loss = torch.mean(losses)\n",
    "    perplexity = math.exp(eval_loss)\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "\n",
    "print((f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\"))\n",
    "\n",
    "print({\n",
    "        \"perplexity\": perplexity,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da9d1ef6-ae61-4dff-9804-5583c37a9050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaForCausalLM' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     LlamaForCausalLM,\n\u001b[1;32m      3\u001b[0m     LlamaTokenizer,\n\u001b[1;32m      4\u001b[0m     LlamaConfig,\n\u001b[1;32m      5\u001b[0m     default_data_collator,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create DataLoaders for the training and validation dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     10\u001b[0m     dataset_train,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mbatch_size_training,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdefault_data_collator,\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaForCausalLM' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "  \n",
    "# Create DataLoaders for the training and validation dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=train_config.batch_size_training,\n",
    "    num_workers=train_config.num_workers_dataloader,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler if train_sampler else None,\n",
    "    drop_last=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c20e6e7-cbdd-44d9-a031-1aff50f644fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, text = (tokenized_datasets[\"train\"][\"input_ids\"][10], train_dataset[\"text\"][10])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "990c0c2e-adbb-4261-bbe6-6abcd1d08a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model_lmhead = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53e4078e-8c9a-4d17-9110-ef457c1730a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_accuracy(inputs, labels):\n",
    "    _, accuracy =  inputs.max(dim=-1).indices.eq(labels).unique(return_counts=True)\n",
    "    #if length is one, then accuracy only contains false values\n",
    "    accuracy = accuracy[1] / (accuracy[0] + accuracy[1]) * 100 if len(accuracy) == 2 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5835ee4c-bd91-43cc-bea6-0af4efda6835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f03e8f0-7b6d-4511-b8e0-c70b306994d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model_lmhead(torch.Tensor(tokens).to(dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "741d537d-e3be-4238-a820-335ab2d4c484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3758e-04, 3.6045e-04, 1.8377e-05,  ..., 2.4572e-06, 1.0243e-06,\n",
       "         6.1406e-04],\n",
       "        [7.2173e-05, 2.8046e-05, 9.7994e-07,  ..., 4.2035e-08, 1.1729e-07,\n",
       "         1.3800e-05],\n",
       "        [4.1298e-05, 2.8809e-04, 1.3808e-04,  ..., 2.2889e-08, 5.0886e-08,\n",
       "         6.8176e-05],\n",
       "        ...,\n",
       "        [6.7620e-06, 1.0283e-05, 2.5769e-06,  ..., 1.5717e-09, 8.4749e-11,\n",
       "         3.3997e-03],\n",
       "        [1.1863e-04, 3.0943e-04, 2.7778e-05,  ..., 2.4461e-08, 2.7869e-08,\n",
       "         6.0270e-03],\n",
       "        [1.8355e-05, 7.0140e-04, 9.1146e-05,  ..., 5.1784e-09, 1.4598e-09,\n",
       "         6.4578e-03]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(out.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e263625-05aa-4607-889f-2e5e0551508b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "gen_text = tokenizer.decode(probs.max(dim=-1).indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79297fe-c8fb-49da-85da-b3266599a5de",
   "metadata": {},
   "source": [
    "## generated text looks nothing like input text, i think i am using the forward pass incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74790157-2d32-4db2-9559-491b01444fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n isThe story system is which gameastersksron, , and a over from from theiscria\\'s .\\n the, the can a character from a unique- andlevel- arrow. the battlefield..\\n selected unit is selected, the unit can to unit to the battlefield using a person-@ order view\\n character can be be in per mission-@ position. and can can be moved a @ of a same of a characters.s.\\n character has a unique of a to 1 that to the own Pointsge . The to three characters can be selected to a single field at Each a , the can be upon to they is to them or and as a character drops or ), ) or depleted or their hit down of a attacks .\\n character has a actionsspecialions \" that which that to each character , Each can: into three Pot \" \" , \" is used abilities that can activealtered for changed specified by the player . the be be or hinder the character\\'s or \" Special Potentialentials \" , which are abilities- the game . are have aons to characters character . Each gain more Potentials , players character must a specific Battle Battle \" \" , which table of-@ that on that that can be used to determine additional use up Battle. Each can have a Pot that can bonuses special buffs to their battlefield , \" Von use a Battle Attack \" , \" his the battlefield in usingactiv his health Gau . . but same can- can activate her a \" Directalkyira \" \" , use a , and thea can use a enemies units with her \" attack , The\\xa0\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e221efa-383a-4036-97e1-3ea41e724c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = measure_accuracy(probs, torch.Tensor(tokens).to(dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec281a89-87d8-42ac-a789-0404ea71bd13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3086)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb349587-d55a-468a-bd80-180d14bd89da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
