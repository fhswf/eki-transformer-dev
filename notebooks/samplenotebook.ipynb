{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015aa5ab-928b-4d8e-8a29-0d3f390a9c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in case it is not been done before, install the package:\n",
    "#!pip install -e .\n",
    "# alternativly via git:\n",
    "#!pip install git+https://github.com/fhswf/eki-transformer-dev.git@develop#subdirectory=qtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d87df4-a4c0-4d0a-9dbc-39f9d7806798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076401d2-93b4-4969-9cea-9994bfdd52ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d302a961-db64-4b6b-8d91-d6aa7092cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/makuh001/eki/eki-transformer-dev/qtransform/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa48bd-ca65-4099-b33d-25473d172a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2058296-a377-4ad2-bc30-814a26532404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## compute config and run qtransform.main\n",
    "\n",
    "#with initialize_config_dir(version_base=None, config_dir=qtransform.get_module_config_path()):\n",
    "#    args = [\n",
    "#        \"run=train\", \n",
    "#        \"model=gpt_2_small\",\n",
    "#        \"dataset=huggingface\", \n",
    "#        \"dataset.name=tiny_shakespeare\",\n",
    "#        \"+export=True\",\n",
    "#        \"run.epochs=1\",\n",
    "#        \"run.max_iters=300\",\n",
    "#        \"dataset/tokenizer=tiktoken\",\n",
    "#        \"dataset.tokenizer.encoding=gpt2\"\n",
    "#    ]\n",
    "#    cfg = compose(config_name=\"config.yaml\", overrides=args)\n",
    "#    print(\"Hydra Config:\", cfg)\n",
    "#    qtransform.run(cfg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65354c98-0e04-4217-b10f-bf95ef8f35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': False, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.05, 'bench': 0.3}, 'tokenizer': {'dataset_dir': ['${dataset.root_path}', '${dataset.name}'], 'name': '${dataset.name}', 'dtype': '${data.dtype}', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False}}, 'quantization': {'quantize': False, 'device': '${device}'}, 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'init_from': 'scratch', 'model_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'validation_epoch_interval': 1000, 'validation_iters': 200, 'export_fn': 'export_qonnx', 'opset_version': 16, 'do_constant_folding': True}, 'export': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:42:00.783927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-01-08 08:42:02,458 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,462 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,465 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,713 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,716 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,719 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,723 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2024-01-08_08:42:02\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,825 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mUsing device: cuda\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:02,849 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:04,432 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50304, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm')\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:04,579 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:04,655 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:04,888 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.99M\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:07,206 \u001b[0m][\u001b[2;37mqtransform.dataset.huggingface\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-01-08 08:42:07,211 \u001b[0m][\u001b[2;37mqtransform.dataset.huggingface\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mBegin tokenizing dataset as file: /home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7955d5c0c7e4755a1b80f104a98289e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the dataset:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note that multirun is not supported in notebooks\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada9928-1ab5-4848-9706-a06124124146",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeLN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0695ee4-53e7-47d0-bc39-350371e51d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_ReBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08780ce-0dde-47a9-b3a2-2d61abac74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_ReLN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e4b72-3290-4d67-9c58-63f7897f09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== one bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1c23e-569a-4e44-8d15-f8727520c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h4l4e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44729263-b778-4dcd-9cf5-bc9bf59aa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h4l4e256b64_GeLN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65b866-6489-4457-9484-85536c408167",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h4l4e256b64_ReBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd975f87-0748-40cf-abb4-f477545fd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h4l4e256b64_ReLN\",\n",
    "        \"dataset=huggingface\", \n",
    "        # \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
