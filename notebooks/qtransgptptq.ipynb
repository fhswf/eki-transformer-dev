{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4387a8f5-ced8-4c22-9749-11fbb57ebec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12ddc68-ab4e-495b-93f1-0dd322137cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b6e4b3-71a4-4185-ae72-065a0f9a1a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-23 12:08:59,837 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=UNKNOWN_NAME\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:08:59,839 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,032 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,034 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,113 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,118 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,120 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,123 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,124 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,327 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,335 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,338 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,340 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,343 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,346 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,349 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 552924748, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,350 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 552924748\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,353 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 645078874 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,355 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,357 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,359 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,361 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,364 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,367 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:09:00,371 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mMetadata contains keys {'fast': True}.They are not supported in tiktoken. Removing them.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import qtransform\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=1\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "@qtransform.with_config(args, logging.DEBUG)\n",
    "def get_dataloader(cfg):\n",
    "    log = logging.getLogger(\"__name__\")\n",
    "    \n",
    "    from qtransform.dataset import get_data, get_loader, DatasetWrapper\n",
    "    data_wrapper: DatasetWrapper = get_data(cfg.dataset)\n",
    "    data_wrapper.load_dataset()\n",
    "    \n",
    "    dataset_train = data_wrapper.dataset_info.train\n",
    "    dataset_eval = data_wrapper.dataset_info.eval\n",
    "    if cfg.dataset.sizes.train >= 1.0:\n",
    "        log.warning(f'Training on the entirety of the dataset without leaving some data for testing.')\n",
    "    #check if batch_size batches are going to be performed\n",
    "    from torch.utils.data import Dataset\n",
    "    def check_dataset_size(name: str, dataset: Dataset):\n",
    "        batch_size = cfg.dataset.dataloader.batch_size\n",
    "        #model which is not an llm is loaded\n",
    "        if cfg.dataset.args.get('block_size') is None:\n",
    "            log.info(f'Model for dataset {name} presumably is not an LLM as the block size has not been specified')\n",
    "            return\n",
    "        block_size = cfg.dataset.args.block_size\n",
    "        if batch_size * block_size > len(dataset):\n",
    "            log.warning(f'The product of batch_size {batch_size} and block_size {block_size} is larger than the dataset {name}, causing the dataloader to skip batches. Maybe check the split size?')\n",
    "    check_dataset_size(\"train\", dataset_train)\n",
    "    train_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_train)\n",
    "    if dataset_eval is not None:\n",
    "        check_dataset_size(\"eval\", dataset_eval)\n",
    "        eval_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_eval)\n",
    "    else:\n",
    "        eval_dataloader = None\n",
    "\n",
    "    #update tokenizer config with metadata to save it in model checkpoints\n",
    "    data_wrapper.tokenizer.load_metadata(filepath=os.path.join(data_wrapper.tokenized_dir, cfg.dataset.tokenizer.meta_file))\n",
    "    with open_dict(cfg.dataset.tokenizer):\n",
    "        cfg.dataset.tokenizer[\"meta\"] = data_wrapper.tokenizer.meta\n",
    "        \n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "train_d, eval_d = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215f6156-6a9d-4a40-939d-ed422785da81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@qtransform.with_config(args, logging.DEBUG)\n",
    "def get_optim(cfg):\n",
    "    from qtransform.optim import get_optim, get_scheduler\n",
    "    log.debug(f\"optim config: {cfg.optim}\")\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=cfg.optim.learning_rate)\n",
    "    optimizer = get_optim(model=model, optim_cfg=cfg.optim)\n",
    "    log.debug(f'Configured optimizer ({type(optimizer)}): {optimizer}')\n",
    "    scheduler = get_scheduler(optimizer=optimizer, scheduler_cfg = cfg.optim.scheduler)\n",
    "    log.debug(f'Scheduler: {scheduler}')\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7e7322-b8c1-4437-8297-7a8dc7cb75c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qtransform.run.train import train\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=1\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "@qtransform.with_config(args, logging.DEBUG)\n",
    "def _train(cfg, model, device, train_dataloader, eval_dataloader, optimizer,scheduler, timestamp):\n",
    "    last_checkpoint = None\n",
    "    # lets go\n",
    "    quant_cfg = cfg.get('quantization')\n",
    "    if quant_cfg and quant_cfg.quantize:    \n",
    "        log.debug(f'Running quantized model')\n",
    "        from qtransform.quantization import get_quantizer\n",
    "        quantizer, model_quant_cfg = get_quantizer(quant_cfg, model=model)\n",
    "        model, replace_layers_later = quantizer.get_quantized_model(model_quant_cfg, inplace=True)\n",
    "        # TODO make this a decorator so it can return stuff\n",
    "        last_checkpoint = quantizer.train_qat(model, train, [cfg, device, train_dataloader, eval_dataloader, optimizer,scheduler, timestamp])\n",
    "        #quantize last layers (batchnorm). parmams last saved checkpoint do not entirely reflect current model anymore \n",
    "        if replace_layers_later is not None:\n",
    "            model = quantizer.get_quantized_model(replace_layers_later)\n",
    "    else:\n",
    "        #if hasattr(log,\"trace\"): log.trace(model)\n",
    "        last_checkpoint = train(cfg=cfg, device=device, model=model, train_data_loader=train_dataloader, eval_data_loader=eval_dataloader, optimizer=optimizer, scheduler=scheduler, timestamp=timestamp)\n",
    "    # maybe subsequent jobs can be managed by hydra in the future?\n",
    "    # when this paradigm comes up more frequently we have to make this a thing ....\n",
    "    log.debug(\"Finished training model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46259ae8-fbe6-486f-8352-4b6656c7d2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-23 12:24:06,170 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,358 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 1, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,362 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,364 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,371 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50304, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='GELU', norm_layer='BatchNorm', single_output=False, use_weight_tying=True, custom_ln=False)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,472 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,490 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,501 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,509 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,817 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 27.87M\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:06,830 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,013 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,017 \u001b[0m][\u001b[2;37m__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,020 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,026 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'betas', 'lr', 'weight_decay'}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,029 \u001b[0m][\u001b[2;37m__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.00015\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,033 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGetting scheduler\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,036 \u001b[0m][\u001b[2;37mqtransform.optim.scheduler\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mGoing through scheduler: StepLR with args: {'step_size': 1, 'gamma': 0.1}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,040 \u001b[0m][\u001b[2;37m__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mScheduler: <torch.optim.lr_scheduler.SequentialLR object at 0x7f6731e65360>\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,053 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "[ \u001b[36m2024-02-23 12:24:07,259 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 1, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model()\n\u001b[1;32m     14\u001b[0m optimizer, scheduler \u001b[38;5;241m=\u001b[39m get_optim()\n\u001b[0;32m---> 15\u001b[0m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki/eki-transformer-dev/qtransform/qtransform/__init__.py:60\u001b[0m, in \u001b[0;36mwith_config.<locals>.wrapper_decorator.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     59\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHydra compose config is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36m_train\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     28\u001b[0m         model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mget_quantized_model(replace_layers_later)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#if hasattr(log,\"trace\"): log.trace(model)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m train(cfg\u001b[38;5;241m=\u001b[39mcfg, device\u001b[38;5;241m=\u001b[39mdevice, model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, train_data_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader, eval_data_loader\u001b[38;5;241m=\u001b[39meval_dataloader, optimizer\u001b[38;5;241m=\u001b[39moptimizer, scheduler\u001b[38;5;241m=\u001b[39mscheduler, timestamp\u001b[38;5;241m=\u001b[39mtimestamp)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# maybe subsequent jobs can be managed by hydra in the future?\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# when this paradigm comes up more frequently we have to make this a thing ....\u001b[39;00m\n\u001b[1;32m     34\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from qtransform import device_singleton\n",
    "device = device_singleton.device\n",
    "@qtransform.with_config(args, logging.DEBUG)\n",
    "def get_model(cfg):\n",
    "    from qtransform.model import get_model\n",
    "    model = get_model(cfg.model)\n",
    "    model.train()\n",
    "    #only parameters (type torch.nn.parameter.Parameter) are moved to the device, not non-named Tensors\n",
    "    #this is a problem if a layer uses a non-named Tensor during the forward pass\n",
    "    model.to(device=device)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "optimizer, scheduler = get_optim()\n",
    "_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549f16-5c9e-4e82-9168-b5eaa6890ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
