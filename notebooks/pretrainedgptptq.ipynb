{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee903d26-5721-466f-9bdc-285b6b8d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb28efe-1f6a-46fb-92a9-f4f61382b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"The Sun was bright that day,\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=42)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edd09e7-239d-4ec8-89de-75028ef8d8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11,   290,   262,  4252,   373, 22751,    13,   198,   198,\n",
       "             1,    40,  1101,  7926,    11,   475,   314,  1101,   407,  1016,\n",
       "           284,   307,  1498,   284,   766,   345,   757,   526,   198,   198,\n",
       "             1,    40,  1101,  7926,    11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output = model(**encoded_input)\n",
    "output = model.generate(**encoded_input, do_sample=False, max_length=75)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c27b450-0a54-4dbb-a398-c8604dcc82bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The Sun was bright that day, and the sun was shining.\\n\\n\"I\\'m sorry, but I\\'m not going to be able to see you again.\"\\n\\n\"I\\'m sorry,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022b7be0-9e9d-4035-b027-647f03602b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'omegaconf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtokenizer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_wrapper\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmeta\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki/eki-transformer-dev/qtransform/qtransform/__init__.py:56\u001b[0m, in \u001b[0;36mwith_config.<locals>.wrapper_decorator.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 56\u001b[0m     initialize_config_dir, qtransform, compose \u001b[38;5;241m=\u001b[39m jls_extract_def(hydra, \u001b[43momegaconf\u001b[49m, Loader)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m initialize_config_dir(version_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, config_dir\u001b[38;5;241m=\u001b[39mconfig_path):\n\u001b[1;32m     58\u001b[0m         cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39marg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'omegaconf' is not defined"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import qtransform\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "@qtransform.with_config(args)\n",
    "def get_dataloader(cfg):\n",
    "    log = logging.getLogger(\"__name__\")\n",
    "    \n",
    "    from qtransform.dataset import get_data, get_loader, DatasetWrapper\n",
    "    data_wrapper: DatasetWrapper = get_data(cfg.dataset)\n",
    "    data_wrapper.load_dataset()\n",
    "    \n",
    "    dataset_train = data_wrapper.dataset_info.train\n",
    "    dataset_eval = data_wrapper.dataset_info.eval\n",
    "    if cfg.dataset.sizes.train >= 1.0:\n",
    "        log.warning(f'Training on the entirety of the dataset without leaving some data for testing.')\n",
    "    #check if batch_size batches are going to be performed\n",
    "    from torch.utils.data import Dataset\n",
    "    def check_dataset_size(name: str, dataset: Dataset):\n",
    "        batch_size = cfg.dataset.dataloader.batch_size\n",
    "        #model which is not an llm is loaded\n",
    "        if cfg.dataset.args.get('block_size') is None:\n",
    "            log.info(f'Model for dataset {name} presumably is not an LLM as the block size has not been specified')\n",
    "            return\n",
    "        block_size = cfg.dataset.args.block_size\n",
    "        if batch_size * block_size > len(dataset):\n",
    "            log.warning(f'The product of batch_size {batch_size} and block_size {block_size} is larger than the dataset {name}, causing the dataloader to skip batches. Maybe check the split size?')\n",
    "    check_dataset_size(\"train\", dataset_train)\n",
    "    train_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_train)\n",
    "    if dataset_eval is not None:\n",
    "        check_dataset_size(\"eval\", dataset_eval)\n",
    "        eval_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_eval)\n",
    "    else:\n",
    "        eval_dataloader = None\n",
    "\n",
    "    #update tokenizer config with metadata to save it in model checkpoints\n",
    "    data_wrapper.tokenizer.load_metadata(filepath=os.path.join(data_wrapper.tokenized_dir, cfg.dataset.tokenizer.meta_file))\n",
    "    with open_dict(cfg.dataset.tokenizer):\n",
    "        cfg.dataset.tokenizer[\"meta\"] = data_wrapper.tokenizer.meta\n",
    "    pass\n",
    "\n",
    "get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510d4093-cd40-45c7-bcec-ae8d7e5c3014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph  \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mgptq\u001b[38;5;241m.\u001b[39mgptq_mode(model) \u001b[38;5;28;01mas\u001b[39;00m gptq:\n\u001b[1;32m      5\u001b[0m         gptq_model \u001b[38;5;241m=\u001b[39m gptq\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from brevitas import graph  \n",
    "\n",
    "with torch.no_grad():\n",
    "    with graph.gptq.gptq_mode(model) as gptq:\n",
    "        gptq_model = gptq.model\n",
    "        for i in tqdm(range(gptq.num_layers)):\n",
    "            for img, t in calib_loader:\n",
    "                img = img.cuda()\n",
    "                gptq_model(img)\n",
    "            gptq.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
