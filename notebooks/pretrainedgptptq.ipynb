{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee903d26-5721-466f-9bdc-285b6b8d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb28efe-1f6a-46fb-92a9-f4f61382b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"The Sun was bright that day,\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=42)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edd09e7-239d-4ec8-89de-75028ef8d8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11,   290,   262,  4252,   373, 22751,    13,   198,   198,\n",
       "             1,    40,  1101,  7926,    11,   475,   314,  1101,   407,  1016,\n",
       "           284,   307,  1498,   284,   766,   345,   757,   526,   198,   198,\n",
       "             1,    40,  1101,  7926,    11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output = model(**encoded_input)\n",
    "output = model.generate(**encoded_input, do_sample=False, max_length=75)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c27b450-0a54-4dbb-a398-c8604dcc82bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The Sun was bright that day, and the sun was shining.\\n\\n\"I\\'m sorry, but I\\'m not going to be able to see you again.\"\\n\\n\"I\\'m sorry,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022b7be0-9e9d-4035-b027-647f03602b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-23 10:29:34,581 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=UNKNOWN_NAME\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:34,583 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:34,783 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,325 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,327 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,329 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,798 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,878 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'cfg': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}} to class: <class 'qtransform.dataset.huggingface.HuggingfaceDatasetWrapper'>\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,884 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mAttempting to retrieve tokenizer with cfg: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,885 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.tokenizer.TikTokenizer(parent: <class 'qtransform.dataset.tokenizer.tokenizer.Tokenizer'>)\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,888 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mPassing arguments {'tokenizer_cfg': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'memmap': None} to class: <class 'qtransform.dataset.tokenizer.tiktoken.TikTokenizer'>\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:35,890 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mCreating Tokenizer with parameters: {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,244 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,250 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,253 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,255 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,258 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,260 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,262 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 552924748, start is 0.3, end is 0.35\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,263 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 552924748\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,265 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 645078874 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,267 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,269 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,270 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 1843082498.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,272 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,275 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,277 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 10:29:36,280 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mMetadata contains keys {'fast': True}.They are not supported in tiktoken. Removing them.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import qtransform\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "@qtransform.with_config(args, logging.DEBUG)\n",
    "def get_dataloader(cfg):\n",
    "    log = logging.getLogger(\"__name__\")\n",
    "    \n",
    "    from qtransform.dataset import get_data, get_loader, DatasetWrapper\n",
    "    data_wrapper: DatasetWrapper = get_data(cfg.dataset)\n",
    "    data_wrapper.load_dataset()\n",
    "    \n",
    "    dataset_train = data_wrapper.dataset_info.train\n",
    "    dataset_eval = data_wrapper.dataset_info.eval\n",
    "    if cfg.dataset.sizes.train >= 1.0:\n",
    "        log.warning(f'Training on the entirety of the dataset without leaving some data for testing.')\n",
    "    #check if batch_size batches are going to be performed\n",
    "    from torch.utils.data import Dataset\n",
    "    def check_dataset_size(name: str, dataset: Dataset):\n",
    "        batch_size = cfg.dataset.dataloader.batch_size\n",
    "        #model which is not an llm is loaded\n",
    "        if cfg.dataset.args.get('block_size') is None:\n",
    "            log.info(f'Model for dataset {name} presumably is not an LLM as the block size has not been specified')\n",
    "            return\n",
    "        block_size = cfg.dataset.args.block_size\n",
    "        if batch_size * block_size > len(dataset):\n",
    "            log.warning(f'The product of batch_size {batch_size} and block_size {block_size} is larger than the dataset {name}, causing the dataloader to skip batches. Maybe check the split size?')\n",
    "    check_dataset_size(\"train\", dataset_train)\n",
    "    train_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_train)\n",
    "    if dataset_eval is not None:\n",
    "        check_dataset_size(\"eval\", dataset_eval)\n",
    "        eval_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_eval)\n",
    "    else:\n",
    "        eval_dataloader = None\n",
    "\n",
    "    #update tokenizer config with metadata to save it in model checkpoints\n",
    "    data_wrapper.tokenizer.load_metadata(filepath=os.path.join(data_wrapper.tokenized_dir, cfg.dataset.tokenizer.meta_file))\n",
    "    with open_dict(cfg.dataset.tokenizer):\n",
    "        cfg.dataset.tokenizer[\"meta\"] = data_wrapper.tokenizer.meta\n",
    "        \n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "train_d, eval_d = get_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510d4093-cd40-45c7-bcec-ae8d7e5c3014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1788.)\n",
      "  return super().rename(names)\n",
      "100%|██████████| 1/1 [02:43<00:00, 163.53s/it]\n"
     ]
    }
   ],
   "source": [
    "from brevitas.graph import gptq, quantize\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "#_ = quantize.preprocess_for_quantize(model, trace_model=True)\n",
    "#model = quantize.quantize(model)\n",
    "model = quantize.layerwise_quantize(model)\n",
    "\n",
    "# dont update weights during farward pass, only use gptq_mode.update()\n",
    "with torch.no_grad():\n",
    "    with gptq.gptq_mode(model) as gptq_mode:\n",
    "        gptq_model = gptq_mode.model\n",
    "        print(gptq_mode.num_layers)\n",
    "        for i in tqdm(range(gptq_mode.num_layers)):\n",
    "            for j, batch in enumerate(train_d):\n",
    "                for seq in batch:\n",
    "                    #t = t.cuda()\n",
    "                    gptq_model(seq)\n",
    "                gptq_mode.update()\n",
    "                # five batches for calibration?\n",
    "                if j > 5:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91690fd-4795-4424-8594-fe3b9f65fae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11,   290,   262,  4252,   373, 22751,  6016,   326,  1110,\n",
       "            13,   198,   198,     1,    40,   373,   287,   262,  3504,   286,\n",
       "           262,  1755,    11,   290,   314,  2497,   257,   582,   351,   257,\n",
       "          2485,    13,   679,   373,  5762]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**encoded_input, do_sample=False, max_length=75)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da823a5-9498-47fd-a893-25e0d089791e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The Sun was bright that day, and the sun was shining bright that day.\\n\\n\"I was in the middle of the night, and I saw a man with a gun. He was wearing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
