{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee903d26-5721-466f-9bdc-285b6b8d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb28efe-1f6a-46fb-92a9-f4f61382b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"The Sun was bright that day,\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=42)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edd09e7-239d-4ec8-89de-75028ef8d8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256,   464,  3825,   373,  6016,   326,\n",
       "          1110,    11,   290,   262,  4252,   373, 22751,    13,   198,   198,\n",
       "             1,    40,  1101,  7926,    11,   475,   314,  1101,   407,  1016,\n",
       "           284,   307,  1498,   284,   766,   345,   757,   526,   198,   198,\n",
       "             1,    40,  1101,  7926,    11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output = model(**encoded_input)\n",
    "output = model.generate(**encoded_input, do_sample=False, max_length=75)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c27b450-0a54-4dbb-a398-c8604dcc82bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The Sun was bright that day, and the sun was shining.\\n\\n\"I\\'m sorry, but I\\'m not going to be able to see you again.\"\\n\\n\"I\\'m sorry,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022b7be0-9e9d-4035-b027-647f03602b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2024-02-23 09:36:53,059 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mHydra compose config is: {'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'openwebtext', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'bench': 0.3}, 'tokenizer': {'dtype': '${data.dtype}', 'meta_file': 'meta.pkl', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2', 'module': 'tiktoken'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000, 'chunking': False, 'chunk_size': 100}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'GELU', 'norm_layer': 'BatchNorm', 'flash': False, 'single_output': False, 'use_weight_tying': True}}, 'quantization': {'quantize': False}, 'pipe': '/dev/null', 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}, 'scheduler': {'decay_lr': True, 'schedulers': {'1': {'name': 'StepLR', 'args': {'step_size': 1, 'gamma': 0.1}}}, 'milestones': None, 'warmup_epochs': 2}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'checkpoint_dir': 'models', 'epochs': 100, 'gradient_accumulation_steps': 1, 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'eval_epoch_interval': 1000, 'eval_iters': 200}, 'export': True}\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:53,908 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:53,911 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNote: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:53,913 \u001b[0m][\u001b[2;37mnumexpr.utils\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,789 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: openwebtext, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,796 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,799 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,802 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,830 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 645078874 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,832 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,834 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 552924749 tokens.\u001b[0m\n",
      "[ \u001b[36m2024-02-23 09:36:54,840 \u001b[0m][\u001b[2;37mqtransform.dataset.tokenizer.tokenizer\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mMetadata contains keys {'fast': True}.They are not supported in tiktoken. Removing them.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "import qtransform\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_h2l2e256b64_GeBN\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=openwebtext\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=100\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "@qtransform.with_config(args)\n",
    "def get_dataloader(cfg):\n",
    "    log = logging.getLogger(\"__name__\")\n",
    "    \n",
    "    from qtransform.dataset import get_data, get_loader, DatasetWrapper\n",
    "    data_wrapper: DatasetWrapper = get_data(cfg.dataset)\n",
    "    data_wrapper.load_dataset()\n",
    "    \n",
    "    dataset_train = data_wrapper.dataset_info.train\n",
    "    dataset_eval = data_wrapper.dataset_info.eval\n",
    "    if cfg.dataset.sizes.train >= 1.0:\n",
    "        log.warning(f'Training on the entirety of the dataset without leaving some data for testing.')\n",
    "    #check if batch_size batches are going to be performed\n",
    "    from torch.utils.data import Dataset\n",
    "    def check_dataset_size(name: str, dataset: Dataset):\n",
    "        batch_size = cfg.dataset.dataloader.batch_size\n",
    "        #model which is not an llm is loaded\n",
    "        if cfg.dataset.args.get('block_size') is None:\n",
    "            log.info(f'Model for dataset {name} presumably is not an LLM as the block size has not been specified')\n",
    "            return\n",
    "        block_size = cfg.dataset.args.block_size\n",
    "        if batch_size * block_size > len(dataset):\n",
    "            log.warning(f'The product of batch_size {batch_size} and block_size {block_size} is larger than the dataset {name}, causing the dataloader to skip batches. Maybe check the split size?')\n",
    "    check_dataset_size(\"train\", dataset_train)\n",
    "    train_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_train)\n",
    "    if dataset_eval is not None:\n",
    "        check_dataset_size(\"eval\", dataset_eval)\n",
    "        eval_dataloader = get_loader(dataloader_cfg = cfg.dataset.dataloader, data = dataset_eval)\n",
    "    else:\n",
    "        eval_dataloader = None\n",
    "\n",
    "    #update tokenizer config with metadata to save it in model checkpoints\n",
    "    data_wrapper.tokenizer.load_metadata(filepath=os.path.join(data_wrapper.tokenized_dir, cfg.dataset.tokenizer.meta_file))\n",
    "    with open_dict(cfg.dataset.tokenizer):\n",
    "        cfg.dataset.tokenizer[\"meta\"] = data_wrapper.tokenizer.meta\n",
    "        \n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510d4093-cd40-45c7-bcec-ae8d7e5c3014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph  \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mgptq\u001b[38;5;241m.\u001b[39mgptq_mode(model) \u001b[38;5;28;01mas\u001b[39;00m gptq:\n\u001b[1;32m      5\u001b[0m         gptq_model \u001b[38;5;241m=\u001b[39m gptq\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from brevitas import graph  \n",
    "\n",
    "with torch.no_grad():\n",
    "    with graph.gptq.gptq_mode(model) as gptq:\n",
    "        gptq_model = gptq.model\n",
    "        for i in tqdm(range(gptq.num_layers)):\n",
    "            for img, t in calib_loader:\n",
    "                img = img.cuda()\n",
    "                gptq_model(img)\n",
    "            gptq.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
