eval_interval : 1000
log_interval : 10
eval_iters : 200
eval_only : False # if True, script exits right after the first eval
always_save_checkpoint : True # if True, always save a checkpoint after each eval
init_from : 'scratch' # 'scratch' or 'resume' or 'gpt2*'

# wandb logging
wandb_log : False # disabled by default
wandb_project : 'owt'
wandb_run_name : 'gpt2' # 'run' + str(time.time())

# data
dataset : 'openwebtext'
gradient_accumulation_steps : 5 * 8 # used to simulate larger batch sizes
batch_size : 24 # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size : 1024


model:
  n_layer : 12
  n_head : 12
  n_embd : 768
  dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
  bias : False # do we use bias inside LayerNorm and Linear layers?

optimizer:
  type: AdamW
  learning_rate : 1.5e-4 # max learning rate
  max_iters : 600000 # total number of training iterations
  weight_decay : 1e-1
  beta1 : 0.9
  beta2 : 0.95
  grad_clip : 0.7 # clip gradients at this value, or disable if :: 0.0

# learning rate decay settings
decay_lr : True # whether to decay the learning rate
warmup_iters : 2000 # how many steps to warm up for
quantize : False # whether to quantize the model
lr_decay_iters : 600000 # should be ~: max_iters per Chinchilla
min_lr : 6e-5 # minimum learning rate, should be ~: learning_rate/10 per Chinchilla

# DDP settings
backend : 'nccl' # 'nccl', 'gloo', etc.

# system

device : 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
dtype : 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler, bfloat16 was nanoGPT default?