model:
  name: gpt_2_small
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: true
  quantize_weights: false
  block_size: 1024
  vocab_size: 50304
dataset:
  tokenizer:
    name: tiktoken
    encoding: ???
  name: openwebtext
  type: huggingface.dataset
  test_size: 0.05
  val_size: 0.05
  seed: null
  shuffle: true
  num_proc: 8
  train: true
  test: true
  val: true
  dtype: bfloat16
train:
  gradient_accumulation_steps: 5 * 8
  batch_size: 24
  block_size: 1024
  decay_lr: true
  warmup_iters: 2000
  lr_decay_iters: 600000
  min_lr: 6.0e-05
  eval_interval: 1000
  log_interval: 10
  eval_iters: 200
  eval_only: false
  always_save_checkpoint: true
  init_from: scratch
c: {}
device: cuda
