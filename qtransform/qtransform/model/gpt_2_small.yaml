cls: GPT
args:
  n_layer : 12
  n_head : 12
  n_embd : 768
  dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
  bias : True # do we use bias inside LayerNorm and Linear layers? # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  quantize : False
  block_size : 1024
  vocab_size : 50304
  transformer_active_func: ReLU