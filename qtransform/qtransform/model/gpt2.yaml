cls: GPT
calc_loss_in_model: True
model_name: gpt2-s${model.args.block_size}-t${model.args.vocab_size}-l${model.args.n_layer}-h${model.args.n_head}-e${model.args.n_embd}-A${model.args.transformer_active_func}-N${model.args.norm_layer}-P${model.args.pos_layer}
cstr: # example: Mgpt2-s256-t2048-l2-h4-e512-AReLU-NBatchNormTranspose-Plearned
type: CHECKPOINT
args:
  n_layer : ???
  n_head : ???
  n_embd : ???
  dropout : 0.1 # for pretraining 0 is good, for finetuning try 0.1+
  bias : True # do we use bias inside LayerNorm and Linear layers? # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  block_size : ???
  vocab_size : 2048 # tied to tokenizer and overwritten?
  transformer_active_func: ReLU
  norm_layer: BatchNormTranspose
  flash: False
  single_output: False
  use_weight_tying: False
  shift_targets: True
  pos_layer: "learned"
