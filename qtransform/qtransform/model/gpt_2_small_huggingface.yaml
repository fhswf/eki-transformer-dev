
#issue: inference and benchmarking require either onnx model or checkpoint
#issue 2: dataset args (block_size) are set before validating size of model
#         block_size is infered from model.args.block_size

type: PRETRAINED
cls: GPT2LMHeadModel #name of base transformer class
model_id: gpt2 #gpt2-medium, gpt2-large, gpt2-xl
from_file: #basically load checkpoint, unsure if something different should happen for pretrained model wrapper

#generated by get_model_wrapper
calc_loss_in_model: True
args:
  n_layer : 2
  n_head : 2
  n_embd : 256
  dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
  bias : True # do we use bias inside LayerNorm and Linear layers? # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  block_size : 64
  vocab_size : 50304
  transformer_active_func: GELU
  norm_layer: BatchNorm
  flash: False
  single_output: False
  use_weight_tying: True
  shift_targets: False



#using custom wrapper around huggingface model in order to make return values of forward pass consistent
#cls: PreTrainedGPT2 #use huggingface model
#pretrained: False
#calc_loss_in_model: True
#shift_targets: True
#args: #TODO: generate hydra config from pretrained hf model before using it
#  version: gpt2
#  block_size: 1024
#  vocab_size: 50257