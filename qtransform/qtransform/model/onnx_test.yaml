type: ONNX # ONNX or DYNAMIC_CHECKPOINT CHECKPOINT NEW FINN_ONNX same as onny but uses other runtime execution
from_file: "qonnx_1_128_BENCH_gpt2_ReBNT_tiny_roneneldan__TinyStories_2024-03-22_09:33:54__epoch:1__steps:None.onnx" #torch checkpoint or ONNX model path
runtime: "finn"  #qonnx, finn 
#model args
cls: GPT
calc_loss_in_model: False
args:
  n_layer : 4
  n_head : 8
  n_embd : 512
  dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
  bias : True # do we use bias inside LayerNorm and Linear layers? # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  block_size : 128
  vocab_size : 50304
  transformer_active_func: ReLU
  norm_layer: BatchNormTranspose
  flash: False
  single_output: False
  use_weight_tying: True
  shift_targets: False