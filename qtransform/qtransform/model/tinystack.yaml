cls: GPT
calc_loss_in_model: True
type: checkpoint
args:
  n_layer : 2
  n_head : 4
  n_embd : 256
  dropout : 0.1
  bias : True
  block_size : 512
  vocab_size : 12631
  transformer_active_func: ReLU
  norm_layer: BatchNormTranspose
  flash: False
  single_output: False
  use_weight_tying: False
  shift_targets: True
  pos_layer: "learned"
