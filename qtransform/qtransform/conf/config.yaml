hydra:
  run:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}
  #verbose: [__main__] # only for hydra debug logging, otherwise use debug=True
  searchpath:
    - pkg://qtransform
defaults:
  - _self_
  - model: 
  - dataset: 
  - optim: default
  - optim/scheduler: default
  - run: ???
  - quantization: None
  - override hydra/job_logging: perlevel
  - override hydra/help: main
  #- override hydra/job_logging: colorlog
  #- override hydra/hydra_logging: colorlog

data:
  dtype: 'float32' # 'float32', 'bfloat16', or 'float16'. if llms are used, the dtype has to be supported by torch.Tensor and numpy
device : 'cuda'
debug: False

dataset:
  wrapper: ??? #name of dataset wrapper to use which returns an instance of type Dataset
  module: ??? #name of python file which declares wrapper class
  name: ???
  root_path: ~/.qtransform/datasets
  #points to the directory containing the dataset used for training/validation etc. 
  #the list is composed of subentries under root_path (root_path included) in hierarchical order
  #the filename of the dataset is not included as it could be split into multiple files (take MNIST for example)
  dataset_dir: #structure something along the lines of: ~/.qtransform/datasets/huggingface/tiny_shakespeare
    - ${dataset.root_path}
    - ${dataset.module}
    - ${dataset.name}
  sizes:
    train: 0.0 #all dataset splits are empty by default
    eval: 0.0
    bench: 0.0
  tokenizer: 
      dtype: ${data.dtype}
      meta_file: meta.pkl #filename of metadata, contains encoding and vocab if character tokenization is used
seed: 1234567890

# DDP settings
#backend : 'nccl' # 'nccl', 'gloo', etc.

model: 
  calc_loss_in_model: False

quantization:
  quantize: False

#filepath of named pipe. currently, the path of saved checkpoints and (q)onnx models are written into it
#by default, write into /dev/null
pipe: /dev/null