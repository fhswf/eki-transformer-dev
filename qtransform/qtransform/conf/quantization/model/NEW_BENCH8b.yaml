cls: gpt ## has to match cls in config for qtransform.model ? it is arbitrary, as the model is passed to the quantizer during training
throw_errors_on_duplicate : False #If the same layer has multiple configs within this file, pick the last one and throw warnings. Otherwise, throw an error and exit

# CUDA_VISIBLE_DEVICES=3 python -m qtransform run=train dataset=tsV2 dataset.dataloader.batch_size=32 tokenizer=TS2k run.epochs=1 model=gpt2 model.cstr="MGPT-s512-t2048-l2-h4-e512-AReLU-NBatchNormTranspose-Plearned" quantization=qat quantization/model=NEW_BENCH8b
layers:
  # please write this in order of sequence for readability
  ##########################
  # Model input (embeddings)
  ##########################

  # regex for both position and token emb
  #   note that quantizing this layer has a significant impact on model performace (acc/loss/etc...)
  transformer.r'wte': 
    quantize: True
    layer_type: Embedding
    quantizers: 
      weight: 
        #type: weight
        #order of overriding: default_quantizer, then template, then args
        default_quantizer: Int8WeightPerTensorFloat
        args:
          bit_width: 8

  #transformer.r'w[tp]e': 
  #  quantize: True
  #  layer_type: Embedding
  #  quantizers: 
  #    weight: 
  #      #type: weight
  #      #order of overriding: default_quantizer, then template, then args
  #      default_quantizer: Int8WeightPerTensorFloat
  #      args:
  #        bit_width: 32

  # dummy layer for embeddings addition so that we can add quantizer
  transformer.emb_add:
    quantize: True
    layer_type: EltwiseAdd
    quantizers: 
      input: 
        default_quantizer: Int8ActPerTensorFloat
        args:
          bit_width: 8
      output:
      #  default_quantizer: Int8ActPerTensorFloat
      #  args:
      #    bit_width: 8
    #args:
    #  return_quant_tensor: True

  # dropout layer get replaced with an empty ident from brevitas, needs no params
  # QuantDropout is deprecated? It is no longer present in the current brevitas dev branch!
  #transformer.dropout: 
  #  quantize: True
  #  layer_type: Dropout 

  ####################
  # Transformer Blocks
  ####################

  ###  block entry dummy layer
  #transformer.layer.r'[0-9]+'.in_id:
  #  quantize: True
  #  layer_type: Identity
  #  quantizers: 
  #    #input: 
  #    #  default_quantizer: Int8ActPerTensorFloat
  #    #  args:
  #    #    bit_width: 2
  #    act: 
  #    #  default_quantizer: Int8ActPerTensorFloat
  #    #  args:
  #    #    bit_width: 2
  #    # output Left empty on purpose to be None
  #    output:
  #  #args:
  #  #  return_quant_tensor: True

  ###  residual1 is around MHA and residual2 is around MLP
  # transformer element add, to be reaplced with brevtas Quant Version to handle residuals
  transformer.layer.r'[0-9]+'.residual1:
    quantize: True
    layer_type: EltwiseAdd
    quantizers: 
      input: 
        default_quantizer: Int8ActPerTensorFloat
        args:
          bit_width: 8
      # output Left empty on purpose to be None
      output:
    #args:
    #  return_quant_tensor: True

  transformer.layer.r'[0-9]+'.residual2:
    quantize: True
    layer_type: EltwiseAdd
    quantizers: 
      input: 
        default_quantizer: Int8ActPerTensorFloat
        args:
          bit_width: 8
      # output Left empty on purpose to be None
      output: 
    #args:
    #  return_quant_tensor: True

  ##########################
  # Transformer Norm Layers

  #transformer.layer.r'[0-9]+'.ln_1: #r'ln_[0-9]':
  #  quantize: True
  #  #it still is open whether a custom layer with their own weights and biases shoud entirely replace batchnorm (CustomBatchNorm1d)
  #  #or the current params of the batchnorm layer should be merged into the previous layer
  #  #depending on the option, the quantargs would be applied on different layers
  #  layer_type: BatchNorm1d
  #  quantizers:
  #    weight:
  #      default_quantizer: Int8WeightPerTensorFloat
  #      args: 
  #        bit_width: 2
  #    input:
  #      default_quantizer: Int8ActPerTensorFloat
  #      args: 
  #        bit_width: 2
  #    bias:
  #      default_quantizer: Int32Bias
  #      args: 
  #        bit_width: 2
  #  args:
  #    replace_bn: True #replace with CustomBatchNorm1d, lose running_mean and running_var
  #    merge_bn:  custom_ln1 #name of the layer that batchnorm should be merged with.

  #transformer.layer.r'[0-9]+'.ln_2: #r'ln_[0-9]':
  #  quantize: True
  #  layer_type: BatchNorm1d
  #  quantizers:
  #    weight:
  #      default_quantizer: Int8WeightPerTensorFloat
  #      args: 
  #        bit_width: 2
  #    input:
  #      default_quantizer: Int8ActPerTensorFloat
  #      args: 
  #        bit_width: 2
  #    bias:
  #      default_quantizer: Int32Bias
  #      args: 
  #        bit_width: 2
  #  args:
  #    replace_bn: True
  #    merge_bn: custom_ln2

  ### for Transposed Norm or other cusotm norem with ident
  ### right now this layer causes double quant back to back wich finn does not like 

  # transformer Ident after BatchNorm heads 
  # transformer.layer.r'[0-9]+'.r'ln_[0-9].id':
  #   quantize: True
  #   layer_type: Identity
  #   quantizers: 
  #     #weight:
  #     #  default_quantizer: Int8WeightPerTensorFloat
  #     #  args:
  #     #    bit_width: 2
  #     act:
  #       default_quantizer: Int8ActPerTensorFloat
  #       args:
  #         bit_width: 2
  #   args:
  #     return_quant_tensor: True

  ##################
  #  Transformer MLP

  #Feed forward neural network
  transformer.layer.r'[0-9]+'.mlp.c_fc:
    quantize: True
    layer_type: Linear
    quantizers: 
      weight:
        default_quantizer: Int8WeightPerTensorFloat
        args:
          bit_width: 8
      input:
        default_quantizer: Int8ActPerTensorFloat
        args:
          bit_width: 8
      # either: use input_quant=Int8ActPerTensorFloat as an input quantizer for reusing the same scaler as the weight quantizer
      # or: use a bias quantizer with its own scaling: e.g. bias_quant=Int8BiasPerTensorFloatInternalScaling
      bias:
        default_quantizer: Int8Bias
        args:
          bit_width: 8
    args:
      return_quant_tensor: True

  transformer.layer.r'[0-9]+'.mlp.active:
    quantize: True
    layer_type: ReLU #no GeLU for brevitas currently
    quantizers:
      act:
        default_quantizer: Uint8ActPerTensorFloat
        args:
          bit_width: 8
      input:
      #  default_quantizer: Int8ActPerTensorFloat
      #  args:
      #    bit_width: 2
    args:
      return_quant_tensor: True

  transformer.layer.r'[0-9]+'.mlp.c_proj:
    quantize: True
    layer_type: Linear
    quantizers: 
      weight:
        default_quantizer: Int8WeightPerTensorFloat
        args:
          bit_width: 8
      # input already quantizewd from activation layer output
      input:
      #  default_quantizer: Int8ActPerTensorFloat
      #  args:
      #    bit_width: 2
      # either: use input_quant=Int8ActPerTensorFloat as an input quantizer for reusing the same scaler as the weight quantizer
      # or: use a bias quantizer with its own scaling: e.g. bias_quant=Int8BiasPerTensorFloatInternalScaling
      #output_quant:
      #  default_quantizer: Int8ActPerTensorFloat
      #  args:
      #    bit_width: 2
      bias:
        default_quantizer: Int8Bias
        args:
          bit_width: 8
    args:
      return_quant_tensor: True

  transformer.layer.r'[0-9]+'.mlp.dropout:
    quantize: True
    layer_type: Dropout

  #######################
  # Transformer MHA
  
  #Multi Head attention, all default args except bit width of 2
  transformer.layer.r'[0-9]+'.attn.r'.+dropout':
    quantize: True
    layer_type: Dropout

  transformer.layer.r'[0-9]+'.attn.mha: 
    quantize: True
    layer_type: MultiheadAttention
    quantizers:
      in_proj_input_quant:
        default_quantizer: Int8ActPerTensorFloat
        args: 
          bit_width: 8
      in_proj_weight_quant:
        default_quantizer: Int8WeightPerTensorFloat
        args: 
          bit_width: 8
      in_proj_bias_quant:
        default_quantizer: Int32Bias
        args: 
          bit_width: 8        
      #softmax_input_quant=None,
      attn_output_weights_quant:
        default_quantizer: Int8ActPerTensorFloat
        args: 
            bit_width: 8
      q_scaled_quant:
        type: act
        default_quantizer: Int8ActPerTensorFloat
        args: 
          bit_width: 8
      k_transposed_quant:
        type: act
        default_quantizer: Int8ActPerTensorFloat
        args: 
          bit_width: 8
      v_quant:
        type: act
        default_quantizer: Int8ActPerTensorFloat
        args: 
          bit_width: 8
      out_proj_input_quant:
        default_quantizer: Int8ActPerTensorFloat
        args: 
          bit_width: 8
      out_proj_weight_quant:
        default_quantizer: Int8WeightPerTensorFloat
        args: 
          bit_width: 8
      out_proj_bias_quant:
        default_quantizer: Int32Bias
        args: 
          bit_width: 8
      out_proj_output_quant:
        #none
    ## extra args:
    args:
      packed_in_proj: False
      batch_first: True
      return_quant_tensor: True


  #########################################################
  # logits layer (linear layer for next token prediction) #
  #########################################################

  linear_out:
    quantize: True
    layer_type: Linear
    quantizers: 
      input: 
        default_quantizer: Int8ActPerTensorFloat
        args:
          bit_width: 8
      #output:
      #  default_quantizer: Int8ActPerTensorFloat
      #  args:
      #    bit_width: 8
    args:
      return_quant_tensor: False