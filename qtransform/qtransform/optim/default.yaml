# learning rate decay settings
#decay_lr : True # whether to decay the learning rate. uses torch.optim.lr_scheduler schedulers
#warmup_iters : 2000 # how many steps to warm up for
#lr_decay_iters : 600000 # should be ~: max_iters per Chinchilla
#min_lr : 6e-5 # minimum learning rate, should be ~: learning_rate/10 per Chinchilla
# optimizer
optimizer: AdamW
args: 
  learning_rate : 1.5e-4 # max learning rate
  #max_iters : 600000 # total number of training iterations
  # L2 loss for parameter regularization , maybe we need more = higher value 
  weight_decay : 1e-1
  #instead of beta1, beta2, ..., use betas
  betas : 
    - 0.9
    - 0.95