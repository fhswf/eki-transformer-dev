name: mnist
modules:
  model: #name of submodule
    conv1: #name of layer
      quantize: True
      layer_type: Conv2d
      #quantization for different kinds, e.g. weight
      #not all layers support quantization of all kinds, e.g. activation functions do not have weight quantization
      weight: 
        #order of overriding: default_quantizer, then template, then args
        default_quantizer: Int8WeightPerTensorFloat
        template: "" #config in yaml files under ./templates
        args:
          quant_type : INT #Integer, binary, ternary, fixed point integer
          bit_width_impl_type : CONST #is the bit width backpropagated and optimised
          float_to_int_impl_type : ROUND #how should the quantized values be clipped to fit into the quantized datatype
          narrow_range : TRUE #clip max value of data type (e.g. for 8 bits: -128:127 instead of -127:127)
          signed : True #can quantized values take on negative values
          #zero_point_impl : ZeroZeroPoint #how is zero point infered

          scaling_impl_type : STATS #how the scale qparam should be calculated, for now: from statistics (weight tensors of layer)

          #attributes only applied when scaling_impl_type is statistics
          scaling_stats_op : MIN_MAX #max value, minmax etc.
          scaling_min_val : 1e-10 #minimum value that the scale is going to have during calibration

          scaling_per_output_channel : True #per tensor or per channel quantization
          restrict_scaling_type : FP #restrict range of values that scale qparam can have
          bit_width : 6 #bit width of quantized values
    relu1:
      quantize: True
      layer_type: ReLU
      act: 
        default_quantizer: Uint8ActPerTensorFloat
    conv2:
      quantize: True
      layer_type: Conv2d
      weight: 
        default_quantizer: Int8WeightPerTensorFloat
    relu2:
      quantize: True
      layer_type: ReLU
      act: 
        default_quantizer: Uint8ActPerTensorFloat
    #for some reason, some layers like maxpool2d do not have quantizer for parameters
    maxpool2d:
      quantize: False
      #layer_type: MaxPool2d
    dropout1: 
      quantize: True
      layer_type: Dropout
    flatten: 
      quantize: False
    fc1:
      quantize: True
      layer_type: Linear
      weight:
        default_quantizer: Int8WeightPerTensorFloat
      act:
        default_quantizer: Uint8ActPerTensorFloat
      bias:
        default_quantizer: Int8Bias
      input:
        default_quantizer: Uint8ActPerTensorFloat
      output:
        default_quantizer: Uint8ActPerTensorFloat
    relu3:
      quantize: True
      layer_type: ReLU
      act: 
        default_quantizer: Uint8ActPerTensorFloat   
    dropout2: 
      quantize: True
      layer_type: Dropout
    fc2:
      quantize: True
      layer_type: Linear
      weight:
        default_quantizer: Int8WeightPerTensorFloat
      act:
        default_quantizer: Uint8ActPerTensorFloat
      bias:
        default_quantizer: Int8Bias
      input:
        default_quantizer: Uint8ActPerTensorFloat
      output:
        default_quantizer: Uint8ActPerTensorFloat
    