
# template for quantization of models 
# in this template, you can specify the layers and modules that should be quantized. for easy debugging, you can change the property "quantize" to toggle quantization
# for that layer/ module.
# the config is composed as follows: as the first property, the name of the module along with a dict of name modules is defined. in modules, you define
# all submodules (of type nn.Module) that you want to quantize. it is important that the name of the submodule corresponds to the name of the model within the python class.
# within each submodule, you define layers whose names correspond to the according layers. within each layer, you define whether the layer should be quantized or not.
# if a layer is not mentioned within this config, it is not quantized. the configs within each layer can theoretically contain quantization options for 
# weights, biases, activations, inputs or outputs depending on which layer is defined. for example, a linear layer supports all options, while activation functions only
# support quantization for activation. note that in order to quantize the bias, the input needs to be quantized as well. 
# brevitas disables all types of quantization by default except for the corresponding layer, e.g. weights for linear and act for activations. if you want to use the default
# config, define a property containing nothing.




#TODO: explain difference between act quantization and input/output quantization
#activation functions are defined within this config as layers.

name: gpt
modules:
  transformer: #name of submodule
    wte: #name of layer
      quantize: True
      #quantization for different kinds, e.g. weight
      #not all layers support quantization of all kinds, e.g. activation functions do not have weight quantization
      weight: 
        #order of overriding: default_quantizer, then template, then args
        default_quantizer: Uint8ActPerTensorFloat
        template: ""
        args:
          quant_type : INT #Integer, binary, ternary, fixed point integer
          bit_width_impl_type : CONST #is the bit width backpropagated and optimised
          float_to_int_impl_type : ROUND #how should the quantized values be clipped to fit into the quantized datatype
          narrow_range : TRUE #clip max value of data type (e.g. for 8 bits: -128:127 instead of -127:127)
          signed : True #can quantized values take on negative values
          #zero_point_impl : ZeroZeroPoint #how is zero point infered

          scaling_impl_type : STATS #how the scale qparam should be calculated, for now: from statistics (weight tensors of layer)

          #attributes only applied when scaling_impl_type is statistics
          scaling_stats_op : MIN_MAX #max value, minmax etc.
          scaling_min_val : 1e-10 #minimum value that the scale is going to have during calibration

          scaling_per_output_channel : True #per tensor or per channel quantization
          restrict_scaling_type : FP #restrict range of values that scale qparam can have
          bit_width : 6 #bit width of quantized values
    gelu:
      quantize: True
      act:
        default_quantizer: Int8ActPerTensorFloat
        template: "" 
        args:
          min_value: 0.0
          max_value: 100.0
    #you can skip certain layers by setting quantize to False or implicitly skip them by not listing them in the yaml file
    another_layer:
      quantize: False