cls: gpt ## has to match cls in config for qtransform.model ? it is arbitrary, as the model is passed to the quantizer during training
#scope:
#  transformer.layer:
#    type: container
#    quantized_layre: attn.mha
#    scope: 0,1,2,4,5 #0-5 also possible
layers:
  #<module>.<submodule>.<submodule>.<quantized_layer>
  #-> quantized layer at the bottom, always
  transformer.wte: #name of layer
    #quantize: True #assume that layer should be quantized if it is in config
    layer_type: Embedding
    quantizers: 
      weight: 
        #type: weight
        #order of overriding: default_quantizer, then template, then args
        default_quantizer: Int8WeightPerTensorFloat
        template: weight_round_minmax  #config in yaml files under ./templates
        args:
          quant_type : INT #Integer, binary, ternary, fixed point integer
          bit_width_impl_type : CONST #is the bit width backpropagated and optimised
          float_to_int_impl_type : ROUND #how should the quantized values be clipped to fit into the quantized datatype
          narrow_range : TRUE #clip max value of data type (e.g. for 8 bits: -128:127 instead of -127:127)
          signed : True #can quantized values take on negative values
          #zero_point_impl : ZeroZeroPoint #how is zero point infered

          scaling_impl_type : STATS #how the scale qparam should be calculated, for now: from statistics (weight tensors of layer)

          #attributes only applied when scaling_impl_type is statistics
          scaling_stats_op : MIN_MAX #max value, minmax etc.
          scaling_min_val : 1e-10 #minimum value that the scale is going to have during calibration

          scaling_per_output_channel : True #per tensor or per channel quantization
          restrict_scaling_type : FP #restrict range of values that scale qparam can have
          #bit_width : 6 #bit width of quantized values

  #transformer.gelu:
  #  layer_type: ReLU
  #  act:
  #    default_quantizer: Int8ActPerTensorFloat
  #    template: "" 
  #    args:
  #      min_val: 0.0
  #      max_val: 100.0
  
  #covering all modules inside of a sublayer is going to be difficult
  #currently, each entry inside of the config applied to exactly one layer that should be quantized
  #if wider coverage is needed, another property beside layers could be implemented
  #transformer.layer.attn.mha: # does .layer (it is an array) cover all Moduels in this torch.nn.ModuleList? 
  #  layer_type: MultiheadAttention
  #  weight:
  #    default_quantizer: Int8WeightPerTensorFloat
    

  #all other layers: weight, act, bias, input, output
  #mha: in_proc_weight, out_proc_weight, <prefix>_act, ...
  #problem: structure of config reflects structure of dataclass
  #if fields are not present as attributes in dataclass, error occurs


#type, subtype, quant_cfg
#type, quant_cfg
#goal: make config as flat as possible
#problem: dataclass structure
#best to not create attributes corresponding to layers, but dict entries
#however: dict entries create hierarchy
#problem: currently, type checking and cleanup is done through dataclasses.fields -> dependent on attributes

  transformer.layer.1.attn.mha: # only layver with id 1? yes
    quantize: True
    layer_type: MultiheadAttention
    #-----------option 1
    #keep current structure, layerquantargs has to contain more attributes
    #or contain a dict and cleanup during modelquantargs
    #since some quantizer names are different, need to explicitly define name in config
    quantizers: 
      weight:
        type: weight #if unspecified, infer type from name 
        default_quantizer: Int8WeightPerTensorFloat
        template: ""
        args:
      in_proc_weight_quant: #mha has multiple weight quantizers, each having a different name (in_proj_weight_quant, out_proc_weight_quant)
        default_quantizer: Int8WeightPerTensorFloat
        template: ""
        args:
    #-----------option 2
    #dict of quantbase
    #weight: #quantizer options is a dict since some layers e.g. multi head attention have more than one quantizer for a certain type
    #  weight:
    #    default_quantizer: Int8WeightPerTensorFloat
    #    template: ""
    #    args:
    #  in_proc: 
    #    default_quantizer: Int8WeightPerTensorFloat
    #    template: ""
    #    args:
    # no args at all = brevitas default, so only weight and nothing else ?
    # basically yes, usually the quantized layers only have a default quantizer for weights
    # it sometimes depends on the implementation of the layer in brevitas, for example
    # activation functions have a default activation quantizer and no weight quantizer