cls: gpt ## has to match cls in config for qtransform.model ? it is arbitrary, as the model is passed to the quantizer during training
layers:
  #<module>.<submodule>.<submodule>.<quantized_layer>
  #-> quantized layer at the bottom, always
  transformer.wte: #name of layer
    layer_type: Embedding
    weight: 
      #order of overriding: default_quantizer, then template, then args
      default_quantizer: Int8WeightPerTensorFloat
      template: ""  #config in yaml files under ./templates
      args:
        # None is default? default values of default_quantizer

  #transformer.gelu:
  #  layer_type: ReLU
  #  act:
  #    default_quantizer: Int8ActPerTensorFloat
  #    template: "" 
  #    args:
  #      min_val: 0.0
  #      max_val: 100.0
  
  #covering all modules inside of a sublayer is going to be difficult
  #currently, each entry inside of the config applied to exactly one layer that should be quantized
  #if wider coverage is needed, another property beside layers could be implemented
  #transformer.layer.attn.mha: # does .layer (it is an array) cover all Moduels in this torch.nn.ModuleList? 
  #  layer_type: MultiheadAttention
  #  weight:
  #    default_quantizer: Int8WeightPerTensorFloat
    

  transformer.layer.1.attn.mha: # only layver with id 1? yes
    layer_type: MultiheadAttention
    # no args at all = brevitas default, so only weight and nothing else ?
    # basically yes, usually the quantized layers only have a default quantizer for weights
    # it sometimes depends on the implementation of the layer in brevitas, for example
    # activation functions have a default activation quantizer and no weight quantizer