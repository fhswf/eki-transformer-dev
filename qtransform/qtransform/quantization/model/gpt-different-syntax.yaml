
# template for quantization of models 
# the syntax is different from gpt.yaml in the composition of quantization parameters differentiated for weights, biases and activations
# here: quant: list -> entry 1: kind: weight, quant_options: ...
name: gpt
modules:
  transformer: #name of submodule
    wte: #name of layer
      quantize: True
      quant:
        #config defined for e.g. weight overrides template config
        - kind: weight
          default_quantizer: Uint8ActPerTensorFloat #A default quantizer has to be used, otherwise the program will crash if certain properties are not set
          quantize: True #toggle if certain options should be quantized. takes priority over quantize option of layer
          options:
            template: "" #path to template
            quant_type : INT #Integer, binary, ternary, fixed point integer
            bit_width_impl_type : CONST #is the bit width backpropagated and optimised
            float_to_int_impl_type : ROUND #how should the quantized values be clipped to fit into the quantized datatype
            narrow_range : TRUE #clip max value of data type (e.g. for 8 bits: -128:127 instead of -127:127)
            signed : True #can quantized values take on negative values
            #zero_point_impl : ZeroZeroPoint #how is zero point infered

            scaling_impl_type : STATS #how the scale qparam should be calculated, for now: from statistics (weight tensors of layer)

            #attributes only applied when scaling_impl_type is statistics
            scaling_stats_op : MIN_MAX #max value, minmax etc.
            scaling_min_val : 1e-10 #minimum value that the scale is going to have during calibration

            scaling_per_output_channel : True #per tensor or per channel quantization
            restrict_scaling_type : FP #restrict range of values that scale qparam can have
            bit_width : 6 #bit width of quantized values

    gelu:
      quantize: True
      quant: 
        - kind: act
          template: ""
          options:
            min_value: 0.0
            max_value: 100.0
    #you can skip certain layers by setting quantize to False or implicitly skip them by not listing them in the yaml file
    another_layer:
      quantize: False