cls: gpt ## has to match cls in config for qtransform.model ? it is arbitrary, as the model is passed to the quantizer during training
throw_errors_on_duplicate : False #If the same layer has multiple configs within this file, pick the last one and throw warnings. Otherwise, throw an error and exit
layers:
    transformer.r'w[tp]e': 
      quantize: True
      layer_type: Embedding
      quantizers: 
        weight: 
          #type: weight
          #order of overriding: default_quantizer, then template, then args
          default_quantizer: Int8WeightPerTensorFloat
          args:
            bit_width: 32
    transformer.dropout: 
      quantize: True
      layer_type: Dropout #no quantizers for dropout possible, what is the default behavior then?

    transformer.emb_add:
      quantize: True
      layer_type: EltwiseAdd
      quantizers: 
        input: 
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8
        output:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8

    # transformer element add, to be reaplced with brevtas Quant Version to handle residuals
    transformer.layer.r'[0-9]+'.residual1:
      quantize: True
      layer_type: EltwiseAdd
      quantizers: 
        input: 
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8
       
        output:
      #args:
      #  return_quant_tensor: True
    transformer.layer.r'[0-9]+'.residual2:
      quantize: True
      layer_type: EltwiseAdd
      quantizers: 
        input: 
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8
         # output Left empty on purpose
        output: 
      #args:
      #  return_quant_tensor: True


    # last layer of our model, does the projection towards embeddings
    # TODO curretnly does prevent the model from converging
    linear_out:
      quantize: True
      layer_type: Linear
      quantizers: 
        #weight:
        #  default_quantizer: Int8WeightPerTensorFloat
        #  args:
        #    bit_width: 32
        input:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 32
        #bias:
        #  default_quantizer: Int8BiasPerTensorFloatInternalScaling
          #args:
          #  bit_width: 8

    # transformer Ident after BatchNorm heads 
    transformer.layer.r'[0-9]+'.r'ln_[0-9].id':
      quantize: True
      layer_type: Identity
      quantizers: 
        #weight:
        #  default_quantizer: Int8WeightPerTensorFloat
        #  args:
        #    bit_width: 8
        act:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8
      args:
        return_quant_tensor: True

    #Feed forward neural network
    transformer.layer.r'[0-9]+'.mlp.r'c_.+':
      quantize: True
      layer_type: Linear
      quantizers: 
        weight:
          default_quantizer: Int8WeightPerTensorFloat
          args:
            bit_width: 8
        input:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8
        # either: use input_quant=Int8ActPerTensorFloat as an input quantizer for reusing the same scaler as the weight quantizer
        # or: use a bias quantizer with its own scaling: e.g. bias_quant=Int8BiasPerTensorFloatInternalScaling
        bias:
          default_quantizer: Int8Bias
          args:
            bit_width: 8

    transformer.layer.r'[0-9]+'.mlp.active:
      quantize: True
      layer_type: ReLU #no GeLU for brevitas currently
      quantizers:
        act:
          default_quantizer: Uint8ActPerTensorFloat
          args:
            bit_width: 8
        input:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 8

    transformer.layer.r'[0-9]+'.mlp.dropout:
      quantize: True
      layer_type: Dropout

    #Multi Head attention, all default args except bit width of 8
    transformer.layer.r'[0-9]+'.attn.r'.+dropout':
      quantize: True
      layer_type: Dropout

    transformer.layer.r'[0-9]+'.attn.mha: 
      quantize: True
      layer_type: MultiheadAttention
      quantizers:
        in_proj_input_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 8
        in_proj_weight_quant:
          default_quantizer: Int8WeightPerTensorFloat
          args: 
            bit_width: 8
        in_proj_bias_quant:
          default_quantizer: Int32Bias
          args: 
            bit_width: 8        
        #softmax_input_quant=None,
        attn_output_weights_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
              bit_width: 8
        q_scaled_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 8
        k_transposed_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 8
        v_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 8
        out_proj_input_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 8
        out_proj_weight_quant:
          default_quantizer: Int8WeightPerTensorFloat
          args: 
            bit_width: 8
        out_proj_bias_quant:
          default_quantizer: Int32Bias
          args: 
            bit_width: 8
       # out_proj_output_quant=None,
      ## extra args:
      args:
        packed_in_proj: False
        batch_first: True