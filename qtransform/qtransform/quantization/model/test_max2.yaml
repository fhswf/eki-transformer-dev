cls: gpt ## has to match cls in config for qtransform.model ? it is arbitrary, as the model is passed to the quantizer during training
throw_errors_on_duplicate : False #If the same layer has multiple configs within this file, pick the last one and throw warnings. Otherwise, throw an error and exit
layers:
    transformer.r'w[tp]e': 
      quantize: True
      layer_type: Embedding
      quantizers: 
        weight: 
          #type: weight
          #order of overriding: default_quantizer, then template, then args
          default_quantizer: Int8WeightPerTensorFloat
          args:
            bit_width: 4
    transformer.dropout: 
      quantize: True
      layer_type: Dropout #no quantizers for dropout possible, what is the default behavior then?

    # transformer element add, to be reaplced with brevtas Quant Version to handle residuals
    transformer.layer.r'[0-9]+'.residual1:
      quantize: True
      layer_type: EltwiseAdd
      quantizers: 
        input: 
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 4
       
        output:
      args:
        return_quant_tensor: True
    transformer.layer.r'[0-9]+'.residual2:
      quantize: True
      layer_type: EltwiseAdd
      quantizers: 
        input: 
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 4
         # output Left empty on purpose
        output: 
      args:
        return_quant_tensor: True

    #transformer.ln_out:
    #  quantize: True
    #  layer_type: BatchNorm1d
    #transformer.ln_out:
    #  quantize: True
    #  layer_type: Identity
    #  quantizers: 
    #    weight:
    #      default_quantizer: Int8WeightPerTensorFloat
    #      args:
    #        bit_width: 4

    # last layer of our model, does the projection towards embeddings
    # TODO curretnly does prevent the model from converging
    #linear_out:
    #  quantize: True
    #  layer_type: Linear
    #  quantizers: 
    #    weight:
    #      default_quantizer: Int8WeightPerTensorFloat
          #args:
          #  bit_width: 8
        #input:
        #  default_quantizer: Int8ActPerTensorFloat
        #  args:
        #    bit_width: 8
        #bias:
        #  default_quantizer: Int8BiasPerTensorFloatInternalScaling
          #args:
          #  bit_width: 8

    #transformer Norm for  heads 
    #transformer.layer.r'[0-9]+'.r'ln_[0-9]':
    #  quantize: True
    #  layer_type: BatchNorm1d

    #transformer.layer.r'[0-9]+'.r'ln_[0-9]':
    #  quantize: True
    #  layer_type: Identity
    #  quantizers: 
    #    weight:
    #      default_quantizer: Int8WeightPerTensorFloat
    #      args:
    #        bit_width: 4

    #Feed forward neural network
    transformer.layer.r'[0-9]+'.mlp.r'c_.+':
      quantize: True
      layer_type: Linear
      quantizers: 
        weight:
          default_quantizer: Int8WeightPerTensorFloat
          args:
            bit_width: 4
        input:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 4
        # either: use input_quant=Int8ActPerTensorFloat as an input quantizer for reusing the same scaler as the weight quantizer
        # or: use a bias quantizer with its own scaling: e.g. bias_quant=Int8BiasPerTensorFloatInternalScaling
        bias:
          default_quantizer: Int8Bias
          args:
            bit_width: 4

    transformer.layer.r'[0-9]+'.mlp.active:
      quantize: True
      layer_type: ReLU #no GeLU for brevitas currently
      quantizers:
        act:
          default_quantizer: Uint8ActPerTensorFloat
          args:
            bit_width: 4
        input:
          default_quantizer: Int8ActPerTensorFloat
          args:
            bit_width: 4

    transformer.layer.r'[0-9]+'.mlp.dropout:
      quantize: True
      layer_type: Dropout

    #Multi Head attention, all default args except bit width of 4
    transformer.layer.r'[0-9]+'.attn.r'.+dropout':
      quantize: True
      layer_type: Dropout

    transformer.layer.r'[0-9]+'.attn.mha: 
      quantize: True
      layer_type: MultiheadAttention
      quantizers:
        in_proj_input_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 4
        in_proj_weight_quant:
          default_quantizer: Int8WeightPerTensorFloat
          args: 
            bit_width: 4
        in_proj_bias_quant:
          default_quantizer: Int32Bias
          args: 
            bit_width: 4        
        #softmax_input_quant=None,
        attn_output_weights_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
              bit_width: 4
        q_scaled_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 4
        k_transposed_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 4
        v_quant:
          type: act
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 4
        out_proj_input_quant:
          default_quantizer: Int8ActPerTensorFloat
          args: 
            bit_width: 4
        out_proj_weight_quant:
          default_quantizer: Int8WeightPerTensorFloat
          args: 
            bit_width: 4
        out_proj_bias_quant:
          default_quantizer: Int32Bias
          args: 
            bit_width: 4
       # out_proj_output_quant=None,
      ## extra args:
      args:
        packed_in_proj: False
        batch_first: True