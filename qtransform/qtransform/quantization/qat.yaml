quantize: True
kind: qat #currently, only qat is supported
type: BrevitasQuantizer #The Framework that supports quantization, appended with Quantizer (corresponds to the classes in this package)
model: ???

#architecture of each quant config in brevitas: 
#quant: define data type, bit width, scaling, fixed bit width, signed or unsigned etc. e.g.: IntQuant
#scaling: specify how the scale qparam is calculated (from min/max values of weight tensors ) e.g.: ParamFromRuntimePercentileScaling
#scope: specify for how many weights the scaling qparam is applied to (all weights within layer or for each node within layer) e.g.:PerTensorFloatScaling8bit 
#solver: apparently translates all configs into one class or something, TODO: investigate
weights_args:
    signed: True
    bit_width: 8 #if pytorch is used: bit_width restricted to 8 or 32 for int, 16 for float
    #https://pytorch.org/docs/stable/quantization-support.html#quantized-dtypes-and-quantization-schemes
    scheme: affine #affine or symmetric
    #TODO: apparently for brevitas, minmax is applied with two properties instead of one
    #   -> scaling_impl_type = ScalingImplType.STATS to let the scale be based off of statisstics
    #   -> scaling_stats_op = StatsOp.MAX to make the clipping range be based off of minmax values
    scaling_impl_type: minmax #specify how the clipping range for the scale qparam is applied. minmax: take min and max value of scope
    #scope: either tensor or channel
    #tensor: qparams (scale and zero value) is applied to an entire layer; 
    #channel: qparams are applied for each weight within the layer
    scope: tensor
    ######PARAMS BELOW ONLY SUPPORTED BY BREVITAS#####
    quant_type: INT #INT, BINARY, TERNARY, FP (floating point)
    bit_width_learnable: False #if set to true: bit width is backpropagated and optimized (e.g. from 8 bits to 17 bits after calibration)
    float_to_int_impl_type: ROUND #sets how quantized values are clipped, other options: CEIL, FLOOR, ROUND_TO_ZERO, DPU, LEARNED_ROUND
    zero_point_impl: ZeroZeroPoint #ZeroZeroPoint: zero qparam is always 0, other options: classes from brevitas.core.zero_point
act_args:
    #act_args should be somewhat the same as weight args
    #optional parameters to limit the output of activation function
    #in yaml, use null to omit values and not limit act outputs
    max_value: null
    min_value: null