#for now, the type of tokenized and untokenized data is the same

#type: ??? 
#name: 
#  path: ???
#  args:
#    subset: 
type: ${dataset.tokenized.type}
#for tokenization
splits: 
  names: #mapping of our split names to huggingface split names
    train: train
    eval: validation
    bench: test
  sizes:
    train: 0.9 
    eval: 0.05
    bench: 0.05
args:
  cache_dir: #directorry where cached datasets are stored. default: ~/.cache/huggingface
  data_column_name: text #name of the column that contains the training data. usually "text"
  batches: 1000 #split dataset into shards to perform tokenization more efficiently
  chunking: True #if True, split long sentences after chunk_size characters for faster tokenization. Default: False
  chunk_size: 100 

tokenizer_cfg: ${tokenizer} #create tokenizer from specified hydra config