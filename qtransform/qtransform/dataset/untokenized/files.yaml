type: files
splits: 
  train:
    size: 0.9
    exists: False
  eval:
    size: 0.05
    exists: False
  bench:
    size: 0.05
    exists: False
args:
  cache_dir: ~/.cache/huggingface/datasets/
  batches: 1000 #split dataset into shards to perform tokenization more efficiently
  chunking: True #if True, split long sentences after chunk_size characters for faster tokenization. Default: False
  chunk_size: 100 