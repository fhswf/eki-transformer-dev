wrapper: TransformersTokenizer
pretrained_tokenizer: #if a custom encoding should be used (e.g. Kristijan/wikitext-103-tokenizer), specify base Model (GPT2TokenizerFast) here
encoding: ??? #the name of the pretrained model, e.g. bert-base-uncased for BertTokenizer or gpt2
module: transformers
fast: True #(https://huggingface.co/docs/transformers/main_classes/tokenizer)
#available encoders: 
  #t5: T5Tokenizer (T5 model)
  #distilbert: DistilBertTokenizer (DistilBert model)
  #albert: AlbertTokenizer (ALBERT model)
  #camembert: CamembertTokenizer (CamemBERT model)
  #xlm-roberta: XLMRobertaTokenizer (XLM-RoBERTa model)
  #longformer: LongformerTokenizer (AllenAI Longformer model)
  #roberta: RobertaTokenizer (RoBERTa model)
  #bert-base-japanese: BertJapaneseTokenizer (Bert model)
  #bert: BertTokenizer (Bert model)
  #openai-gpt: OpenAIGPTTokenizer (OpenAI GPT model)
  #gpt2: GPT2Tokenizer (OpenAI GPT-2 model)
  #transfo-xl: TransfoXLTokenizer (Transformer-XL model)
  #xlnet: XLNetTokenizer (XLNet model)
  #xlm: XLMTokenizer (XLM model)
  #ctrl: CTRLTokenizer (Salesforce CTRL model)
  #electra: ElectraTokenizer (Google ELECTRA model)