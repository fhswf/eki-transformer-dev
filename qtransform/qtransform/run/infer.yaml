command: infer

checkpoint_dir: models #either absolute path to the directory containing checkpoints or relative path under qtransform/output

num_samples: 10 #generate num_samples 
max_new_tokens: 500
temperature: 0.8
top_k: 200
start: "\n" #generates text starting with #start (\n, "<|endoftext|>" or etc.) Can also specify a file, use as: "FILE:prompt.txt" where prompt.txt is absolute
compile: False #use torch.compile

#path of directory create files containing the infered samples. if empty, write onto stdout. 
#the file name is composed as: INFER_<datetime>_<ONNX or CHECKPOINT>.out
out_dir: 

onnx_model: #absolute path of onnx model to run inference on. either use onnx_model or from_checkpoint
  path: 
  tokenizer: #specify tokenizer config manually as it cannot be saved in onnx model
    module: tiktoken
    encoding: gpt2
    meta_path: #usually only necessary for character tokenization
from_checkpoint:  #filename of checkpoint to load

debug: False #use karpathy's inference