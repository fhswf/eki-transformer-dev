command: train

always_save_checkpoint : True # if True, always save a checkpoint after each eval
init_from : 'scratch' # 'scratch' or 'resume' or 'gpt2*'
# model_chkpt_dir: ${oc.env:HOME}
model_dir: models
epochs: 10
gradient_accumulation_steps : 5 * 8 # used to simulate larger batch sizes, leave empty for no accumulation
flash: False
export: True ## perform an export after #epochs are done



## old stuff:

validation_epoch_interval : 1000
save_epoch_interval: 1
log_steps_interval : 10
validation_iters : 200

# learning rate decay settings
decay_lr : True # whether to decay the learning rate
warmup_iters : 2000 # how many steps to warm up for
lr_decay_iters : 600000 # should be ~: max_iters per Chinchilla
min_lr : 6e-5 # minimum learning rate, should be ~: learning_rate/10 per Chinchilla
# optimizer
optimizer: AdamW
learning_rate : 1.5e-4 # max learning rate
max_iters : 600000 # total number of training iterations
weight_decay : 1e-1
beta1 : 0.9
beta2 : 0.95
grad_clip : 0.7 # clip gradients at this value, or disable if :: 0.0

