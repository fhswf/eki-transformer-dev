{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General testing notebook for qtransform and quantization\n",
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from logging import getLogger\n",
    "import os\n",
    "from omegaconf import DictConfig\n",
    "from brevitas import nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with dataclasses and python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod, ABC\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\"\n",
    "\n",
    "class Foo(ABC):\n",
    "    def __init__(self, encoding: str):\n",
    "        self.metadata: Metadata = Metadata(encoding)\n",
    "\n",
    "\n",
    "    def load_metadata():\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def test(self, file: str):\n",
    "        file += \"   padding\"\n",
    "\n",
    "class Bar(Foo):\n",
    "    def __init__(self, encoding: str):\n",
    "        super().__init__()\n",
    "        self.metadata: BarMetadata\n",
    "\n",
    "    def test(self, file: str):\n",
    "        super().test(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Foo(a=1, b=2, c=0.0, d=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if some keys can be omited within a dict if they are default params within the dataclass\n",
    "@dataclass\n",
    "class Foo():\n",
    "    a: float = 0.0\n",
    "    b: float = 0.0\n",
    "    c: float = 0.0\n",
    "    d: float = 0.0\n",
    "    \n",
    "args = {\"a\": 1, \"b\": 2}\n",
    "\n",
    "Foo(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarMetadata(encoding='gpt2', other='Bruh')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BarMetadata(encoding=\"gpt2\", other=\"ok\")\n",
    "import dataclasses\n",
    "dataclasses.replace(test, **{\"other\": \"Bruh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.BarMetadata() argument after ** must be a mapping, not Metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test: Metadata \u001b[39m=\u001b[39m Metadata(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test: BarMetadata \u001b[39m=\u001b[39m BarMetadata(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest, other\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.BarMetadata() argument after ** must be a mapping, not Metadata"
     ]
    }
   ],
   "source": [
    "test: Metadata = Metadata(\"gpt2\")\n",
    "test: BarMetadata = BarMetadata(**test, other=\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = test\n",
    "params = set(inspect.signature(Metadata.__init__).parameters.keys()) - set(['self'])\n",
    "{x:getattr(obj, x) for x in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, \n",
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#test if inner functions can access member attributes\n",
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def function(self):\n",
    "        def other():\n",
    "            print(self.a)\n",
    "        other()\n",
    "\n",
    "Foo().function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# padding does not get appended to the parameter as it is a seperate function\n",
    "Bar().test(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tests with torch framework to gain familiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b,c,e = 4, 5,6\n",
    "tensor_3d = torch.arange(b*c*e).reshape(b,c,e)\n",
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch has 5 rows, only want 3 \n",
    "index = torch.tile(torch.arange(3).reshape(3,1), (b,1,e))\n",
    "#you only consider the first batch\n",
    "torch.gather(tensor_3d, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 0,  1,  2,  3,  4]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [30, 31, 32, 33, 34]],\n",
       "\n",
       "        [[60, 61, 62, 63, 64],\n",
       "         [60, 61, 62, 63, 64]],\n",
       "\n",
       "        [[90, 91, 92, 93, 94],\n",
       "         [90, 91, 92, 93, 94]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#objective: retrieve first rows of tensor_3d -> if we specify dim=1, we collapse along the rows (we perform indexing for each row)\n",
    "#b,c,e = 4,5,6\n",
    "#i always want the first row -> specify by row, dim=1\n",
    "#how do i reduce the amount of rows if the index tensor has to be of the same dimension?\n",
    "#dimension has to be the same but not the shape\n",
    "#torch.zeros(4,1,6) gets the first row of the tensor, but it is problematic if i want multiple rows as i \n",
    "#then use the same index (0) while having the output shape that i want\n",
    "#solution: arange\n",
    "#index=torch.zeros(4,1,6) -> if we use 5 instead of 6, each row has 5 columns\n",
    "#meaning: we need a row containing the same index \n",
    "tensor_3d.gather(dim=1, index=torch.zeros(4,2,6, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ]\n",
    "   ])\n",
    "#size is: 3, 4, 3. if you collapse in the first dimension (dim=0), the result tensor becomes of size 4,3. if you collapse it in the second dimension, you get a tensor of size 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)\n",
    "#in transformers, we usually have tensors of shape b,c,e (batch_size, context, embedding_dimension).\n",
    "#if we specify dim=0, we perform the operation along the entire batch, in dim=1 along the context and in dim=2 along the embedding dimension.\n",
    "#if we were to sum the tensors together, sum(dim=1) will yield the sum of the embeddings of each word.\n",
    "#think of it as squishing a dimension together so that it is of size 1, meaning that we have to squeeze in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "#test if torch.tile and tensor.repeat are the same\n",
    "c = 2 #simulate two words\n",
    "a = torch.arange(c).reshape((c,1)).repeat((3,1,4))\n",
    "b = torch.tile(torch.arange(c).reshape((c,1)), (3,1,4))\n",
    "print(a.equal(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 7],\n",
       "        [23]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"experiments with torch.gather\"\n",
    "M = torch.tensor([[1,2,3], [4,7,18], [19,9,23]])\n",
    "#if there is more than one value inside of the last dimension, continue along current index\n",
    "#meaning at dim=1:\n",
    "#[1,1,1] -> 2,7,9\n",
    "#[0,0,0] -> 1,4,19\n",
    "#increments along the current dimension\n",
    "#at new row, reset counter ->\n",
    "#[1] -> 2\n",
    "#[1] -> 2\n",
    "indexes = torch.tensor([1,1,2]).view(-1,1) \n",
    "\n",
    "dimension = 0 #2d, meaning dim=0 along rows, dim=1 along columns\n",
    "out = M.gather(dimension ,indexes) #dim=0: , dim=1: tensor([[ 2],[ 7],[23]])\n",
    "M.gather(1, torch.Tensor([[1],[1],[2]]).to(dtype=torch.long)) #counter along the current dimension for the dimension of index\n",
    "#M.gather(1, torch.tensor([[0,0,0],[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BatchNorm with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are: same\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding\n",
    "\"test if padding does not lower values\"\n",
    "#first word of each batch -> gather by column\n",
    "#result tensor: (3, 1, 64)\n",
    "#retrieving an index from the dimension increases the counter along index of said dimension by one\n",
    "#e.g. indexing 0 twice will retrieve two different values\n",
    "FEATURES = 16\n",
    "EMBEDDINGS = 64\n",
    "BATCH_SIZE = 3\n",
    "bn = torch.nn.BatchNorm1d(FEATURES)\n",
    "#get first word embeddings of three batches\n",
    "embedding_layer = torch.nn.Embedding(FEATURES, EMBEDDINGS)\n",
    "batch = embedding_layer(torch.randint(16, (BATCH_SIZE, FEATURES)))\n",
    "index = torch.arange(1).repeat(BATCH_SIZE,1,EMBEDDINGS).to(dtype=torch.long)\n",
    "embd_first_word = torch.gather(batch, index=index, dim=1)\n",
    "padding_bn = BatchNormWithPadding(FEATURES,bias=True)\n",
    "norm_padding = padding_bn(embd_first_word)\n",
    "norm = bn(batch)\n",
    "#check if values are the same\n",
    "print(f'Values are: {\"same\" if torch.gather(norm, index=index, dim=1).equal(norm_padding) else \"different\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#test if huggingface datasets can be created from text files\n",
    "import datasets\n",
    "\n",
    "BASEDIR = '/home/mabot004/.qtransform/datasets/files/shakespeare/untokenized/'\n",
    "#number of rows depends on the amount of files\n",
    "files = [os.path.join(BASEDIR, 'shakespeare.txt'), os.path.join(BASEDIR, 'shakespeare_2.txt')]\n",
    "#does the same as huggingface mapping but now with files\n",
    "def gen_text():\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as file:\n",
    "            yield {\"text\": file.read()}\n",
    "\n",
    "#chunk size from config, default 100\n",
    "def chunk_examples(examples):\n",
    "                #splits the text of each row into chunks of length chunk_length. currently it is only used\n",
    "                #for character tokenization to avoid feeding large samples to the tokenizer\n",
    "    chunk_length = 100\n",
    "                #perform tokenization on a handful of characters at a time\n",
    "                #from: https://huggingface.co/docs/datasets/process#split-long-examples            \n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in examples[\"text\"]:\n",
    "        new_chunks = [sentence[i:i + chunk_length] for i in range(0, len(sentence), chunk_length)]\n",
    "        chunks.extend(new_chunks)\n",
    "    return {\"chunks\": chunks}\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335f8a35994846dca3abf8e8cbb29aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "chunks = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "rotten_tomatoes = datasets.load_dataset('rotten_tomatoes')\n",
    "rotten_tomatoes[\"train\"].shard(num_shards=1000, index=0, contiguous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok: 100%|██████████| 100/100 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# status bar like huggingface dataset map process\n",
    "from tqdm import tqdm\n",
    "msg = 'ok'\n",
    "for i in tqdm(range(100), desc=f'{msg}'):\n",
    "    msg = str(i)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i, data in tqdm(enumerate(range(10)), desc='test progress bar and other stdout stuff'):\n",
    "    print(data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 13952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 1745\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e85d125997141e9aff299774dc53747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd5be2fac8a4e98af374c8771baa1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29862032dbf1442fa5b3ee6e43f03da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash eve'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#error occurs because the splits have more than one feature and this function changes the amount of samples in each split of one feature without changint the other\n",
    "#so: 5 samples, 2 features. after mapping: text has 10 samples, other feature still has 5 features\n",
    "#from: https://github.com/huggingface/datasets/issues/1817#issuecomment-774066254\n",
    "rt_chunks = rotten_tomatoes.select_columns(\"text\").map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "print(rt_chunks)\n",
    "#tokenize\n",
    "rt_chunks = rt_chunks.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")\n",
    "rt_chunks.save_to_disk('/home/mabot004/custom_hf_datasets/')\n",
    "\"test if tokenizing is correct\"\n",
    "tokenizer.decode(rt_chunks[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d1411c88164121bfb64ff217c471df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170153f358c9444c95ba1d014ca57337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1fd3b4fac54f56ac9c6bcc2e6f90ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "block_size = 64\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "rt_grouped = rt_chunks.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "owt = datasets.load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 3429\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 434\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(rt_grouped[\"train\"], Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_hf = DataLoader(rt_grouped[\"train\"], batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1392248133.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[58], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(len(data[\"input_ids\"])\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader_hf))\n",
    "print(len(data[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "shakespeare = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "shakespeare = shakespeare.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.concatenate(shakespeare[:3][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memmap(memmap, start, end, data):\n",
    "    memmap[start:end] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test generating huggingface datasets from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 163 examples [00:00, 31912.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def gen_text():\n",
    "    for i in range(163):\n",
    "        yield {\"text\": i}\n",
    "\n",
    "test_threading = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 163\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.rename_column(\"text\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.shard(num_shards=30, index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "num_threads = 3 #os.cpu_count // 2\n",
    "batch_size = 30\n",
    "num_samples = len(test_threading)\n",
    "# 163 // 30 shards\n",
    "# -> 3 threads, each having a batch size of 30 samples\n",
    "# dataset has 163 samples -> each thread should have around 50-60 samples max\n",
    "# -> divide samples of dataset with num_threads\n",
    "# -> each thread should have the entire dataset as an arg, but split differently\n",
    "# range of splitting should be specified as an arg in thread -> index arg in parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why should you use multithreading? the writing process is I/O based\n",
    "#if anything, the amount of write requests increases with the amount of threads\n",
    "memmap = np.memmap('test', mode='w+', shape=(163,), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: 'abcd'\n"
     ]
    }
   ],
   "source": [
    "#playing around with error messages\n",
    "try:\n",
    "    int(\"abcd\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test memory usage in worst case scenarios\n",
    "\"\"\"\n",
    "\n",
    "#no high memory usage as memmap values are lazily loaded, only overhead is the pages (around 5MB per memmap )\n",
    "memmap = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap2 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap3 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap4 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "import psutil\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709600997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform.dataset import MemmapDataset\n",
    "#token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0\n",
    "memmap_ds = MemmapDataset(\n",
    "    token_file='/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin',\n",
    "    dtype=np.float32,\n",
    "    block_size=64,\n",
    "    start=0.0,\n",
    "    end=0.3\n",
    ")\n",
    "len(memmap_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(memmap_ds, batch_size=12, num_workers=8)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    input, labels = data\n",
    "    print(f'{input.size()}, {labels.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing batchnorm quant\n",
    "#https://github.com/Xilinx/brevitas/issues/542\n",
    "#https://github.com/Xilinx/brevitas/issues/363\n",
    "#test merge_bn from qtransform\n",
    "from qtransform.model.modules import CausalSelfAttention\n",
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding, MLP\n",
    "from qtransform.model.gpt import GPTConfig\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import utils as qutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from brevitas.quant import scaled_int\n",
    "#simulate values from embedding, skip positional encoding\n",
    "wte = torch.nn.Embedding(16,64)\n",
    "tokens = torch.randint(16, (3,16))\n",
    "embeddings = wte(tokens)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if quantized layers having return_quant_tensor set to True are compatible with torch operations \n",
    "quant_tensor_linear = qnn.QuantLinear(1,1,True,return_quant_tensor=True)\n",
    "quant_tensor_linear(torch.Tensor(8,1)) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug loading quantized checkpoint\n",
    "CHECKPOINT = '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:9'\n",
    "#doesnt work since qtransform.dataset cannot be found\n",
    "#but module info about tokenizers is not saved in checkpoint, only their names\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.layer.0.attn.attn_mask',\n",
       " 'transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.attn.attn_mask',\n",
       " 'transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if info about quant params are even saved within checkpoint\n",
    "import re\n",
    "keys = checkpoint[\"model_state_dict\"].keys()\n",
    "#quant param that exists within checkpoint: \n",
    "#transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value \n",
    "weights_and_biases = list(filter(lambda x: re.search(r'.+\\.(weight|bias)$', x), keys))\n",
    "def find(x):\n",
    "    if not re.search(r'.+\\.(weight|bias)$', x):\n",
    "        return x\n",
    "other_keys = list(filter(find, keys))\n",
    "len(keys) == len(weights_and_biases) # not only weights and biases in state dict\n",
    "#only scaling_impl is saved in state dict\n",
    "#no multiheadattention though\n",
    "#in gpt quant config, every single layer has a quantizer (most commonly Int8WeightPerTensorFloat)\n",
    "#that quantizer has ScalingImplType STATS\n",
    "#the layers with scaling_impl had an activation quantizer named Int8ActPerTensorFloat\n",
    "#it had the ScalingImplType PARAMETER_FROM_STATS\n",
    "other_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6414)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if qparam is not one \n",
    "checkpoint[\"model_state_dict\"][\"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(3.1415, requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if scaling_impl params exist within model\n",
    "test_mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=256)\n",
    "#simulate some learning steps for param\n",
    "print(test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value = torch.nn.Parameter(torch.tensor(3.1415))\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_mha.state_dict(), 'mha.chpt')\n",
    "#v_quant etc. not appearing within state_dict\n",
    "test_mha.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[0.9874]])), ('bias', tensor([-0.8623]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if brevitas layers relevant for Transformers return qparams in state_dict\n",
    "print(qnn.QuantLinear(1,1,True,input_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantIdentity(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantReLU(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 5), match='allo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?!hallo|welt).*$', \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if storing checkpoints of quantized models even is working\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.ModuleDict(dict(\n",
    "            wte = qnn.QuantEmbedding(32, 128),\n",
    "            pos = qnn.QuantEmbedding(16, 128),\n",
    "            logic = nn.ModuleDict(dict(\n",
    "                layer1 = qnn.QuantLinear(128, 16, True),\n",
    "                layer2 = qnn.QuantLinear(16,1, True))\n",
    "            )\n",
    "        ))\n",
    "    def forward(self, x):\n",
    "        embd = self.network.wte(x)\n",
    "        b,t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "        pos = self.network.pos(pos)\n",
    "        output = embd + pos\n",
    "        for name, layer in self.network.logic.items():\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3944],\n",
       "         [ 0.2287],\n",
       "         [-0.5937],\n",
       "         [-0.8445],\n",
       "         [ 0.4049],\n",
       "         [-0.1961],\n",
       "         [ 0.2558],\n",
       "         [ 0.5325],\n",
       "         [-0.2270],\n",
       "         [ 0.0485],\n",
       "         [-0.5637],\n",
       "         [ 0.1862],\n",
       "         [ 0.7595],\n",
       "         [-0.2511],\n",
       "         [ 0.1841],\n",
       "         [-0.3207]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(torch.randint(32, (1,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#doesnt work, Quantizer cannot be found in brevitas.inject\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#why are they being searched for in inject if they are in brevitas.quant.scaled_int\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m'\u001b[39;49m\u001b[39mquantized_test\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:831\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    829\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    830\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 831\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    832\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    833\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed"
     ]
    }
   ],
   "source": [
    "#doesnt work, Quantizer cannot be found in brevitas.inject\n",
    "#why are they being searched for in inject if they are in brevitas.quant.scaled_int\n",
    "torch.save(model, 'quantized_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform import DeviceSingleton\n",
    "#check if value from class is set in object\n",
    "DeviceSingleton.device = 'cuda'\n",
    "singleton = DeviceSingleton()\n",
    "singleton.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Batchnorm and Conv merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.49767026\n"
     ]
    }
   ],
   "source": [
    "#from: \n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv1d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) )\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\tfusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )\n",
    "\t#\n",
    "\t# we're done\n",
    "\treturn fusedconv\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = (16, 64, 256)\n",
    "x = torch.randn(16, 64, 256)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=(256,256)),\n",
    "    torch.nn.BatchNorm1d(64)\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy.load_state_dict(cv1.state_dict())\n",
    "bn1 = torch.nn.BatchNorm1d(5)\n",
    "qnn.utils.merge_bn(cv1, bn1)\n",
    "input = torch.Tensor(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 is cv1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9793e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9794e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is the same without batchnorm, why?\n",
    "output = cv1_copy(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272],\n",
       "        [ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bn1(input)\n",
    "cv1_copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(30, (3,5,20)).to(dtype=torch.float32) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5470e+00,  1.5270e-01, -2.5464e-01,  2.9082e-01,  1.2001e+00,\n",
       "           6.7265e-01,  7.3291e-01, -1.8250e-01, -1.2105e+00, -1.0728e+00,\n",
       "           3.1590e-01,  2.5334e+00, -1.3411e-01,  2.5901e+00,  6.9120e-02,\n",
       "          -5.1725e-01,  1.9493e-01,  2.2703e+00],\n",
       "         [-3.4492e-01, -9.6099e-01, -9.2788e-01, -5.6099e-01, -2.0823e+00,\n",
       "           8.8297e-01,  4.6034e-01,  9.3609e-01,  1.8312e+00, -8.3214e-01,\n",
       "          -1.0253e+00, -1.3361e+00, -1.3721e+00,  4.9575e-01, -6.1378e-01,\n",
       "           3.7313e-01, -1.6607e+00, -7.7247e-01],\n",
       "         [ 1.8919e+00,  8.4047e-01,  5.7258e-01,  3.1601e-01,  1.4367e-01,\n",
       "          -4.8590e-01,  7.8809e-01,  5.6231e-01, -9.9302e-01, -5.6623e-01,\n",
       "          -2.1978e-01, -8.2209e-01, -2.8324e-02,  9.5371e-01,  9.6952e-02,\n",
       "          -6.7169e-01, -8.7423e-02, -4.8495e-02],\n",
       "         [-2.3595e+00, -4.5535e-01,  1.1663e+00,  1.6639e+00,  5.8315e-01,\n",
       "          -4.0086e-01, -4.8103e-01, -2.0247e+00,  4.6665e-01,  2.1141e-01,\n",
       "           2.7884e-02,  1.7325e+00,  2.3958e+00,  7.5227e-01,  6.7972e-02,\n",
       "           5.4808e-01,  1.2512e+00,  8.5656e-01],\n",
       "         [-6.4959e-01,  6.6814e-01,  4.8005e-01, -2.0198e+00,  4.6459e-01,\n",
       "          -8.1610e-01, -2.8203e-01,  3.0454e-01,  1.0447e+00,  4.4882e-02,\n",
       "          -9.1393e-01,  1.3076e+00, -2.0745e+00, -3.1045e-01,  6.2427e-01,\n",
       "           5.3256e-01,  4.6315e-01, -1.1006e+00]],\n",
       "\n",
       "        [[ 1.9405e-01,  9.4450e-01, -2.1945e-01, -1.0464e-01, -2.1462e+00,\n",
       "          -1.8074e-01, -5.1549e-01,  2.3760e-01, -4.5039e-01,  2.9090e-02,\n",
       "          -2.0757e-01,  1.0382e+00, -1.5610e-01,  5.3232e-01, -9.5934e-01,\n",
       "           2.4156e-01,  1.1666e+00, -3.6109e-01],\n",
       "         [ 1.9643e+00, -3.5404e-02,  2.8985e-01,  1.2767e+00,  1.5938e+00,\n",
       "           2.8329e-01,  1.3378e-01, -5.0787e-01,  1.2574e+00, -6.1935e-01,\n",
       "          -8.2427e-01,  4.4821e-01, -2.4590e-01, -8.7280e-01,  1.4246e+00,\n",
       "           8.9188e-02,  1.6821e-01, -2.1289e+00],\n",
       "         [-6.4067e-02, -3.6310e-01, -5.1032e-01,  4.1864e-01,  5.1348e-01,\n",
       "          -2.7373e+00,  2.9017e-01,  2.4135e+00, -7.8918e-01, -1.9854e-01,\n",
       "           2.0203e+00, -3.6310e-01, -5.2306e-01,  8.9957e-01, -1.4630e+00,\n",
       "           1.2373e-02, -1.2238e+00,  9.9299e-01],\n",
       "         [ 9.1042e-01,  1.4595e-01, -1.4738e+00, -1.5985e+00, -1.7431e+00,\n",
       "           3.1330e-02,  4.3972e-01, -7.1936e-01,  5.5935e-01,  3.6048e-01,\n",
       "           6.7992e-01,  1.2452e+00, -1.1293e+00, -6.2311e-02,  7.0498e-01,\n",
       "           1.4889e+00,  8.2691e-02, -1.0297e+00],\n",
       "         [-2.0543e-01,  5.3759e-01, -9.8586e-01,  2.0672e-01,  6.3649e-01,\n",
       "           2.3649e+00, -1.7379e+00, -6.4168e-01,  1.0911e+00,  3.2361e-01,\n",
       "          -2.0871e+00,  1.4637e+00, -1.4250e+00, -1.5364e-01,  4.2754e-01,\n",
       "          -4.6797e-01,  1.4504e+00, -5.4709e-01]],\n",
       "\n",
       "        [[-8.9680e-02, -1.4670e+00,  1.2023e+00,  6.6737e-01, -1.5057e+00,\n",
       "          -4.8822e-01,  2.4024e-01, -1.5994e+00,  1.7873e+00, -5.9907e-01,\n",
       "          -8.3309e-01,  5.0197e-01, -7.8162e-01,  1.8217e-01, -4.5251e-02,\n",
       "          -1.7203e+00, -8.5721e-02, -5.5332e-01],\n",
       "         [ 1.1663e+00, -1.0131e+00,  8.3614e-02,  6.0756e-01,  1.2145e+00,\n",
       "           1.1872e+00,  1.1804e-01, -2.2524e-01,  3.7509e-01, -2.6787e-01,\n",
       "           4.7772e-01,  2.2624e-01, -1.5147e-01,  6.2887e-01,  1.1935e-01,\n",
       "           1.5600e+00, -8.1634e-02, -2.2099e+00],\n",
       "         [ 5.1029e-01, -1.4743e+00,  9.7388e-01,  3.3264e-01,  2.0862e+00,\n",
       "          -1.2319e+00, -2.1481e+00,  7.9234e-01, -7.6193e-01,  1.6490e-01,\n",
       "           8.2065e-01,  6.9478e-04,  4.6818e-01, -1.6127e+00, -4.3211e-01,\n",
       "           4.9885e-02, -7.4565e-01,  6.3875e-01],\n",
       "         [-4.3844e-01,  2.0107e-01, -7.5047e-03,  1.8040e-01, -1.4647e+00,\n",
       "           7.5665e-01,  1.1131e+00, -6.5673e-01, -2.1358e-01, -8.7125e-01,\n",
       "          -5.8282e-01,  1.0587e-01, -1.6796e+00, -2.1013e-01, -6.5641e-01,\n",
       "          -8.1833e-01, -2.7934e-01,  6.2668e-01],\n",
       "         [ 1.6464e+00,  4.3653e-01, -1.2452e+00, -5.8335e-02, -3.4749e-01,\n",
       "           4.8509e-01,  3.7072e-01, -3.7842e-01,  3.8690e-01,  8.0157e-01,\n",
       "          -8.9019e-01, -1.9134e-02, -1.5724e-01,  2.2794e-01, -6.8663e-01,\n",
       "           8.4329e-01,  2.0301e+00, -1.4638e+00]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.nn.BatchNorm1d(5)(qnn.QuantConv1d(5,5,kernel_size=3)(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5805,  1.6874,  1.4232,  0.4953,  0.3392,  1.6921,  0.4289,\n",
       "           0.6056,  1.4143, -0.3349,  0.7054,  1.2688,  0.7755,  0.8200,\n",
       "           0.8781,  1.1979,  0.5057,  0.3852],\n",
       "         [-0.6557,  0.1111,  0.1168,  0.1569, -0.1044, -0.7017, -0.4960,\n",
       "          -0.0382, -0.2186,  0.1632,  0.3991, -0.2319, -0.1412, -1.0671,\n",
       "          -0.2227, -0.4349,  0.0598, -0.0862],\n",
       "         [-1.8093, -1.3581, -1.4218, -1.9729, -2.4638, -1.5891, -2.7596,\n",
       "          -1.0707, -0.9489, -1.2964, -0.4576, -2.2601, -2.3817, -2.9702,\n",
       "          -2.1696, -1.4183, -1.6867, -1.5476],\n",
       "         [ 0.0087,  0.9669,  0.2861, -0.0195,  1.7805, -0.1940,  1.5801,\n",
       "           0.4090,  0.1231, -0.3474,  0.4677,  0.5724,  0.8208,  1.0850,\n",
       "           1.3689,  0.1611,  0.5700, -0.0545],\n",
       "         [-1.7327, -0.0812, -0.3896, -1.2272, -0.7673, -1.1279, -1.3454,\n",
       "          -0.1006,  0.1854, -0.1570, -0.0988, -0.2189, -0.5590, -1.7284,\n",
       "          -0.0418, -0.0416,  0.0663, -0.0783]],\n",
       "\n",
       "        [[ 1.6639,  0.7003,  1.2008,  0.9851,  0.8164,  1.4518,  1.4647,\n",
       "          -0.1788,  2.0859,  0.5120,  0.8852,  0.7612,  0.6098,  0.6913,\n",
       "           0.9692,  0.6834,  1.3616,  0.9310],\n",
       "         [-0.8825, -0.3148, -0.2431, -0.5726, -0.2170, -0.3808, -0.1197,\n",
       "          -0.2470, -0.1361, -0.0082,  0.8303, -0.6241,  0.2627,  0.3186,\n",
       "          -0.6388, -0.2168, -0.9436,  0.2259],\n",
       "         [-1.2239, -2.1880, -1.9660, -2.3704, -0.7485, -0.4421, -1.5090,\n",
       "          -2.1500, -0.3124, -1.8201, -1.7527, -2.3098, -1.7470, -2.1069,\n",
       "          -1.3791, -1.5358, -2.2366, -2.0583],\n",
       "         [ 0.6760,  1.1154,  0.6760,  1.2122,  0.3283,  0.1619, -0.1343,\n",
       "           0.7418,  0.9900,  0.0270,  0.7077,  0.9982,  0.1400,  1.7570,\n",
       "          -0.0549,  1.0549,  0.4207,  0.7455],\n",
       "         [-1.1979, -1.6723, -1.1881, -1.0570, -0.1860,  0.5241, -1.5507,\n",
       "          -0.7745, -0.1433, -0.6879, -0.4526, -0.6777, -1.1168, -0.2085,\n",
       "          -0.0175, -0.5790, -1.0503, -0.2322]],\n",
       "\n",
       "        [[ 1.5133,  1.3983,  1.5074,  1.0723,  0.7788,  0.7588,  0.6164,\n",
       "           1.0027,  1.2222,  1.5164,  1.4553,  0.7161,  0.7796,  1.6876,\n",
       "           0.4413,  2.1108,  1.6400,  0.9259],\n",
       "         [-0.9756,  0.0696, -0.2441, -0.4958,  0.0807,  0.5572,  0.0222,\n",
       "          -0.4592, -0.5887, -0.3081, -0.0168, -0.1240,  0.1828, -0.1130,\n",
       "          -0.5912, -1.0130, -0.6067,  0.7823],\n",
       "         [-1.9029, -1.1918, -1.7631, -2.7777, -1.8128, -0.9612, -1.9680,\n",
       "          -2.2855, -1.8218, -1.8851, -1.3642, -2.3375, -1.5278, -1.6818,\n",
       "          -2.4809, -1.1818, -1.5407, -1.7131],\n",
       "         [ 0.4436,  0.8576,  0.8286,  1.5920,  0.6813,  1.2457,  0.7904,\n",
       "           0.3165,  0.7779,  1.4541, -0.0767,  1.1432,  0.8329,  0.7933,\n",
       "           0.9158,  1.3503, -0.0775,  0.6725],\n",
       "         [-0.9279, -0.1795, -1.8110, -1.0357, -0.6544,  0.1664, -0.4564,\n",
       "          -0.7024, -0.6100, -1.0599, -1.0029, -1.0985, -0.9784, -0.8995,\n",
       "          -1.0086, -0.7555, -0.4560, -0.3629]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if QuantMHA is learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug QuantMultiheadAttention and merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_bn_mha(layer, bn, output_channel_dim=0):\n",
    "    \n",
    "    #retrieve learnable parameters from batchnorm (scale + bias)\n",
    "    mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "        bn_mean=bn.running_mean,\n",
    "        bn_var=bn.running_var,\n",
    "        bn_eps=bn.eps,\n",
    "        bn_weight=bn.weight.data.clone(),\n",
    "        bn_bias=bn.bias.data.clone())\n",
    "    #out_proj is QuantLinear(in_features=embd_dim, out_features=embd_dim)\n",
    "    out_ch_weight_shape = qutils.compute_channel_view_shape(layer.weight, output_channel_dim)\n",
    "    #apply batchnorm during after forward pass of layer, before returning result\n",
    "    \n",
    "    #!!\n",
    "    layer.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "    #!!\n",
    "    \n",
    "    if layer.out_proj.bias is not None:\n",
    "        out_ch_bias_shape = qutils.compute_channel_view_shape(layer.out_proj.bias, channel_dim=0)\n",
    "        layer.out_proj.bias.data.mul_(mul_factor.view(out_ch_bias_shape))\n",
    "        layer.out_proj.bias.data.add_(add_factor.view(out_ch_bias_shape))\n",
    "    else:\n",
    "        layer.out_proj.bias = nn.Parameter(add_factor)\n",
    "    if (hasattr(layer, 'out_proj_weight_quant') and\n",
    "            isinstance(layer.out_proj_weight_quant, WeightQuantProxyFromInjector)):\n",
    "        layer.out_proj_weight_quant.init_tensor_quant()\n",
    "    if (hasattr(layer, 'out_proj_bias_quant') and isinstance(layer.out_proj_bias_quant, BiasQuantProxyFromInjector)):\n",
    "        layer.out_proj_bias_quant.init_tensor_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_linear = torch.nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "test_linear.weight = torch.nn.parameter.Parameter(torch.ones((embed_dim, embed_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         [ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         ...,\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         [ -8.1817,  -8.1817,  -8.1817,  ...,  -8.1817,  -8.1817,  -8.1817],\n",
       "         [  1.8297,   1.8297,   1.8297,  ...,   1.8297,   1.8297,   1.8297]],\n",
       "\n",
       "        [[-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670],\n",
       "         [ -3.0146,  -3.0146,  -3.0146,  ...,  -3.0146,  -3.0146,  -3.0146],\n",
       "         [  2.3900,   2.3900,   2.3900,  ...,   2.3900,   2.3900,   2.3900],\n",
       "         ...,\n",
       "         [  9.5870,   9.5870,   9.5870,  ...,   9.5870,   9.5870,   9.5870],\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [ -5.1709,  -5.1709,  -5.1709,  ...,  -5.1709,  -5.1709,  -5.1709]],\n",
       "\n",
       "        [[ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [ -0.0878,  -0.0878,  -0.0878,  ...,  -0.0878,  -0.0878,  -0.0878],\n",
       "         [ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         ...,\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [  4.6854,   4.6854,   4.6854,  ...,   4.6854,   4.6854,   4.6854],\n",
       "         [-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670]]],\n",
       "       grad_fn=<UnsafeViewBackward0>, names=('L', 'N', None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "heads = 2\n",
    "embed_dim = 64\n",
    "context = 16\n",
    "quant_mha = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "#pass the same input tensor to merged and unmerged mha + batchnorm and compare results\n",
    "quant_mha_merged = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "from brevitas import config\n",
    "#qparams not imported from state dict\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "quant_mha_merged.load_state_dict(quant_mha.state_dict())\n",
    "#feature length not critical\n",
    "bn = torch.nn.BatchNorm1d(context)\n",
    "#test if quantmha works\n",
    "assert quant_mha(embeddings, embeddings, embeddings)[0].size() == embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLinear(\n",
       "  in_features=64, out_features=64, bias=True\n",
       "  (input_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "      (activation_impl): Identity()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "          (stats_input_view_shape_impl): OverTensorView()\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsPercentile()\n",
       "          )\n",
       "          (restrict_scaling): _RestrictValue(\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (clamp_scaling): _ClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "          )\n",
       "          (restrict_inplace_preprocess): Identity()\n",
       "          (restrict_preprocess): Identity()\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (weight_quant): WeightQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): RescalingIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClampSte()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (scaling_impl): StatsFromParameterScaling(\n",
       "        (parameter_list_stats): _ParameterListStats(\n",
       "          (first_tracked_param): _ViewParameterWrapper(\n",
       "            (view_shape_impl): OverTensorView()\n",
       "          )\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsMax()\n",
       "          )\n",
       "        )\n",
       "        (stats_scaling_impl): _StatsScaling(\n",
       "          (affine_rescaling): Identity()\n",
       "          (restrict_clamp_scaling): _RestrictClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (restrict_scaling_pre): Identity()\n",
       "        )\n",
       "      )\n",
       "      (int_scaling_impl): IntScaling()\n",
       "      (zero_point_impl): ZeroZeroPoint(\n",
       "        (zero_point): StatelessBuffer()\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bias_quant): BiasQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): PrescaledRestrictIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClamp()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "      (zero_point): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_mha.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0926,  0.1662,  0.0606,  ...,  0.0211,  0.0027,  0.6270],\n",
       "          [-0.4505,  0.2013,  0.0885,  ..., -0.1597, -0.3558,  0.4523],\n",
       "          [-0.3425, -0.0519,  0.0775,  ..., -0.1996, -0.2760, -0.2787],\n",
       "          ...,\n",
       "          [-0.0925,  0.2803, -0.1011,  ...,  0.2038, -0.0426,  0.4598],\n",
       "          [ 0.1152,  0.0099, -0.0458,  ...,  0.0279, -0.2678, -0.3036],\n",
       "          [ 0.0462,  0.4055, -0.4288,  ...,  1.0150,  0.1507, -0.0658]],\n",
       " \n",
       "         [[-0.1684,  0.2740, -0.0567,  ...,  0.1945, -0.1586,  0.4784],\n",
       "          [-0.2414,  0.2263,  0.0706,  ..., -0.2693, -0.4626,  0.4440],\n",
       "          [-0.3649,  0.0976, -0.0294,  ..., -0.0971, -0.1957, -0.1036],\n",
       "          ...,\n",
       "          [-0.0461,  0.1246, -0.1975,  ...,  0.0336,  0.2553,  0.3853],\n",
       "          [ 0.0460,  0.0541, -0.0979,  ..., -0.0303, -0.3825, -0.3351],\n",
       "          [-0.0157,  0.2822, -0.4208,  ...,  0.5638,  0.3149,  0.0858]],\n",
       " \n",
       "         [[-0.0181,  0.1725, -0.0047,  ...,  0.1487,  0.1976,  0.5633],\n",
       "          [-0.1927,  0.1515, -0.0845,  ..., -0.2242, -0.4402,  0.5122],\n",
       "          [-0.1058, -0.1061,  0.0588,  ..., -0.2105, -0.3280, -0.3857],\n",
       "          ...,\n",
       "          [-0.0667,  0.1150, -0.3250,  ..., -0.0771,  0.1500,  0.4351],\n",
       "          [ 0.0051,  0.1560, -0.1228,  ...,  0.3762, -0.2658,  0.0824],\n",
       "          [ 0.0911,  0.2581, -0.2653,  ...,  0.3346,  0.2105,  0.3753]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.2267, 0.3770, 0.3974],\n",
       "          [0.2486, 0.5163, 0.2363],\n",
       "          [0.2704, 0.3674, 0.3619]],\n",
       " \n",
       "         [[0.4780, 0.2718, 0.2513],\n",
       "          [0.2445, 0.2800, 0.4767],\n",
       "          [0.3401, 0.3811, 0.2773]],\n",
       " \n",
       "         [[0.3292, 0.3783, 0.2923],\n",
       "          [0.2991, 0.5053, 0.1953],\n",
       "          [0.4384, 0.2035, 0.3578]],\n",
       " \n",
       "         [[0.2322, 0.4357, 0.3333],\n",
       "          [0.2759, 0.4958, 0.2281],\n",
       "          [0.2827, 0.2062, 0.5122]],\n",
       " \n",
       "         [[0.3565, 0.4220, 0.2199],\n",
       "          [0.4971, 0.1079, 0.3933],\n",
       "          [0.3510, 0.3333, 0.3155]],\n",
       " \n",
       "         [[0.3292, 0.2636, 0.4070],\n",
       "          [0.2868, 0.3141, 0.3988],\n",
       "          [0.3059, 0.3414, 0.3537]],\n",
       " \n",
       "         [[0.2759, 0.2800, 0.4439],\n",
       "          [0.3701, 0.2035, 0.4248],\n",
       "          [0.2144, 0.4534, 0.3319]],\n",
       " \n",
       "         [[0.3414, 0.4220, 0.2376],\n",
       "          [0.2213, 0.4070, 0.3715],\n",
       "          [0.2581, 0.3974, 0.3442]],\n",
       " \n",
       "         [[0.4644, 0.2827, 0.2540],\n",
       "          [0.4138, 0.0970, 0.4889],\n",
       "          [0.3005, 0.3360, 0.3633]],\n",
       " \n",
       "         [[0.3578, 0.2144, 0.4275],\n",
       "          [0.2295, 0.5545, 0.2172],\n",
       "          [0.3947, 0.1980, 0.4084]],\n",
       " \n",
       "         [[0.3865, 0.2103, 0.4029],\n",
       "          [0.2827, 0.5286, 0.1885],\n",
       "          [0.3906, 0.4398, 0.1694]],\n",
       " \n",
       "         [[0.3169, 0.3169, 0.3674],\n",
       "          [0.3169, 0.3169, 0.3674],\n",
       "          [0.3974, 0.3974, 0.2049]],\n",
       " \n",
       "         [[0.3251, 0.3496, 0.3251],\n",
       "          [0.3428, 0.3155, 0.3428],\n",
       "          [0.3251, 0.3496, 0.3251]],\n",
       " \n",
       "         [[0.4999, 0.2240, 0.2759],\n",
       "          [0.3770, 0.4097, 0.2131],\n",
       "          [0.4439, 0.3797, 0.1762]],\n",
       " \n",
       "         [[0.3578, 0.4275, 0.2144],\n",
       "          [0.3947, 0.4084, 0.1980],\n",
       "          [0.2295, 0.2172, 0.5545]],\n",
       " \n",
       "         [[0.0819, 0.4111, 0.5053],\n",
       "          [0.3046, 0.3387, 0.3565],\n",
       "          [0.3920, 0.3988, 0.2090]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#added a print statement after out_proj to see if the shape is changed afterwards\n",
    "quant_mha(embeddings,embeddings,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test if qparams are the same without copying them. they should have value 1 \n",
    "MISSING_QPARAMS =  [\"in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]\n",
    "for missing_qparam in MISSING_QPARAMS:\n",
    "    #omit \".value\" to get submodule \n",
    "    submodule = missing_qparam[:-len(\".value\")]\n",
    "    assert quant_mha.get_submodule(submodule).value == quant_mha_merged.get_submodule(submodule).value, f'qparam {missing_qparam} is different' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0486e+05, 4.5555e-41, 1.2967e-05],\n",
      "        [3.0866e-41, 4.4842e-44, 0.0000e+00]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(test(tensor))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'"
     ]
    }
   ],
   "source": [
    "import brevitas\n",
    "test = brevitas.core.quant.IntQuant(True, True)\n",
    "tensor = torch.Tensor(2,3)\n",
    "print(tensor)\n",
    "print(test(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if creating a custom activation can simulate batchnorm\n",
    "#### Alternative: Implement custom Quantizer which performs normalization based on a scale and add factor which can be passed in its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "def QuantIdentityWithWeights(qnn.QuantIdentity):\n",
    "    def __init__(self,\n",
    "            act_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        super().__init__(self,\n",
    "            act_quant = act_quant\n",
    "            return_quant_tensor = return_quant_tensor,\n",
    "            **kwargs):\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        return super().forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from brevitas.nn.quant_layer import ActQuantType\n",
    "from torch import Tensor\n",
    "from brevitas.quant.scaled_int import *\n",
    "from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL\n",
    "\n",
    "class QuantCustom(QuantNLAL):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mul_factor: Tensor,\n",
    "            add_factor: Tensor,\n",
    "            act_quant: Optional[ActQuantType] = Uint8ActPerTensorFloat,\n",
    "            input_quant: Optional[ActQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        QuantNLAL.__init__(\n",
    "            self,\n",
    "            act_impl=None,\n",
    "            passthrough_act=True,\n",
    "            input_quant=input_quant,\n",
    "            act_quant=act_quant,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "        self.mul_factor = mul_factor\n",
    "        self.add_factor = add_factor\n",
    "    \n",
    "\n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        input = self.unpack_input(input)\n",
    "        quant_input = self.input_quant(input)\n",
    "        # shortcut execution through the export impl during export\n",
    "        if self.export_mode:\n",
    "            out = self.export_handler(quant_input.value)\n",
    "            self._set_global_is_quant_layer(False)\n",
    "            return out\n",
    "        out = self.act_quant(quant_input)\n",
    "        \n",
    "        out = self.pack_output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL\n",
    "from torch.nn import Module as TorchModule\n",
    "from brevitas.nn.mixin import * #WeightQuantType, BiasQuantType\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat\n",
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "import torch\n",
    "#test if a quantized layer can be implemented which basically scales the values along a tensor and adds a bias, thereby simulating batch normalization\n",
    "class QuantBatchnorm1d(QuantWBIOL, TorchModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
    "            bias_quant: Optional[BiasQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs) -> None:\n",
    "        TorchModule.__init__(self)\n",
    "        if not isinstance(num_features, int) or num_features <= 0:\n",
    "            raise AttributeError()\n",
    "        #do the same as quantidentity\n",
    "        self.weight = torch.ones(num_features)\n",
    "        self.bias = torch.zeros(num_features)\n",
    "        QuantWBIOL.__init__(\n",
    "            self,\n",
    "            weight_quant=weight_quant,\n",
    "            bias_quant=bias_quant,\n",
    "            input_quant=None,\n",
    "            output_quant=None,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "    \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:\n",
    "        return self.forward_impl(input)\n",
    "    \n",
    "    def inner_forward_impl(self, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n",
    "        #inner_forward_impl is apparently the actual forward pass of the layer \n",
    "        return x * quant_weight[:,None] + quant_bias[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test functionality of custom Quantized layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.]), tensor([0., 0., 0., 0.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "#mul_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "#add_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "batch, context, embeds = 2,4,8\n",
    "#should do nothing as weights are 1 and biases are 0\n",
    "custom_quant_layer = QuantBatchnorm1d(num_features=context)\n",
    "print(f'{custom_quant_layer.weight}, {custom_quant_layer.bias}\\n')\n",
    "input = torch.randint(30, (batch, context, embeds))\n",
    "output = custom_quant_layer(input)\n",
    "assert input.equal(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### compare results of custom Batchnorm with regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True, False, False,  True,  True,  True, False,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True, False,  True, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True, False, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False,  True,  True, False,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embeddings = torch.randn(batch, sentence_length, embedding_dim)\n",
    "custom_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "regular_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "#simulate some forward passes to change the mean and standard deviation\n",
    "for _ in range(10):\n",
    "    regular_bn(embeddings)\n",
    "#test if merge_bn works\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=regular_bn.running_mean,\n",
    "    bn_var=regular_bn.running_var,\n",
    "    bn_eps=regular_bn.eps,\n",
    "    bn_weight=regular_bn.weight.data.clone(),\n",
    "    bn_bias=regular_bn.bias.data.clone())\n",
    "assert custom_quant_bn.weight.size() == mul_factor.size()\n",
    "assert custom_quant_bn.bias.size() == add_factor.size()\n",
    "#change weight and bias of quantized batchnorm\n",
    "custom_quant_bn.weight = mul_factor\n",
    "custom_quant_bn.bias = add_factor\n",
    "#test if results are at least somewhat similiar\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_bn.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "qtransform_quant_bn = merge_quant_bn(regular_bn)\n",
    "out_qtransform_quant_bn = qtransform_quant_bn(embeddings)\n",
    "print((out_regular_bn - out_qtransform_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn.utils import merge_bn\n",
    "custom_merged_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "merge_bn(layer=custom_merged_quant_bn, bn = regular_bn)\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy pasted from brevitas.nn.utils\n",
    "def compute_channel_view_shape(tensor: torch.Tensor, channel_dim: int):\n",
    "    #create a list containing ones with length of the tensor dimension (for mha: always length of 3)\n",
    "    broadcast_shape = [1] * len(tensor.size())\n",
    "    #why is that important\n",
    "    broadcast_shape[channel_dim] = -1\n",
    "    return tuple(broadcast_shape)\n",
    "\n",
    "def mul_add_from_bn(bn_mean, bn_var, bn_eps, bn_weight, bn_bias):\n",
    "    denom = torch.sqrt(bn_var + bn_eps)\n",
    "    mul_factor = bn_weight / denom\n",
    "    add_factor = -bn_mean * mul_factor + bn_bias\n",
    "    return mul_factor, add_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layernorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = embeddings.rename(None)\n",
    "for i in range(10):\n",
    "    layernorm(tensor)\n",
    "    bn(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerNorm' object has no attribute 'running_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test if mul_add_from_bn works for layernorm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layernorm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embed_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m ln_mul, ln_add \u001b[38;5;241m=\u001b[39m qutils\u001b[38;5;241m.\u001b[39mmul_add_from_bn(\n\u001b[0;32m----> 4\u001b[0m     bn_mean\u001b[38;5;241m=\u001b[39m\u001b[43mlayernorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m,\n\u001b[1;32m      5\u001b[0m     bn_var\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mrunning_var,\n\u001b[1;32m      6\u001b[0m     bn_eps\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m      7\u001b[0m     bn_weight\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m      8\u001b[0m     bn_bias\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerNorm' object has no attribute 'running_mean'"
     ]
    }
   ],
   "source": [
    "# test if mul_add_from_bn works for layernorm\n",
    "layernorm = torch.nn.LayerNorm(embed_dim, bias=True)\n",
    "ln_mul, ln_add = qutils.mul_add_from_bn(\n",
    "    bn_mean=layernorm.running_mean,\n",
    "    bn_var=layernorm.running_var,\n",
    "    bn_eps=layernorm.eps,\n",
    "    bn_weight=layernorm.weight.data.clone(),\n",
    "    bn_bias=layernorm.bias.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, -1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basically a list of ones with a -1 at index channel_dim\n",
    "compute_channel_view_shape(torch.Tensor(3,3,3,3,3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quant_mha_merged.out_proj.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (-1, 1), mul_factor view: torch.Size([16, 1])\n",
      "quant_mha_merged.out_proj.weight.data shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#disassemble functionality of merge_bn_mha\n",
    "#one dimensional tensor to scale all values of a batch with a corresponding scalar\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=bn.running_mean,\n",
    "    bn_var=bn.running_var,\n",
    "    bn_eps=bn.eps,\n",
    "    bn_weight=bn.weight.data.clone(),\n",
    "    bn_bias=bn.bias.data.clone())\n",
    "assert mul_factor.size() == add_factor.size()\n",
    "assert mul_factor.size()[0] == add_factor.size()[0] == context\n",
    "output_channel_dim = 0\n",
    "#out_proj is QuantLinear -> 2d Tensor, [-1, 1]\n",
    "#meaning: reverse shape of mul_factor tensor\n",
    "#currently: [1,context] now: [context, 1]\n",
    "out_ch_weight_shape = qutils.compute_channel_view_shape(quant_mha_merged.out_proj.weight, output_channel_dim)\n",
    "#out_proj is a linear layer applying a scaling factor to quantize outputs -> inputs: n_embd, outputs: n_embd\n",
    "#batchnorm applied normalization along second dimension, linear layer along third\n",
    "#-> could work if batchnorm normalizes along embeddings\n",
    "assert mul_factor.view(out_ch_weight_shape).size() != quant_mha_merged.out_proj.weight.data.size()\n",
    "print(f'shape: {out_ch_weight_shape}, mul_factor view: {mul_factor.view(out_ch_weight_shape).size()}')\n",
    "print(f'quant_mha_merged.out_proj.weight.data shape: {quant_mha_merged.out_proj.weight.data.size()}')\n",
    "#quant_mha_merged.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "#merge params of bn in quant_mha_merged\n",
    "#merge_bn_mha(quant_mha_merged, bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_factor: torch.Size([16]), add_factor: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#each value of mul_factor is the scale with which the embedding of one word is multiplied with\n",
    "#in total: context amount of words\n",
    "#linear layer calculates the sum of each embedding of a word with a weight\n",
    "#problem: weights are the same for every word\n",
    "print(f'mul_factor: {mul_factor.size()}, add_factor: {add_factor.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qutils.merge_bn(qnn.QuantLinear(context, context, True), bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <p>Rows: Context, Columns: Embeddings</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Row</th>\n",
       "            <th>Embedding 1</th>\n",
       "            <th>Embedding 2</th>\n",
       "            <th>Embedding 3</th>\n",
       "            <th>Embedding 4</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>-0.6182</td>\n",
       "            <td>0.6397</td>\n",
       "            <td>-0.6141</td>\n",
       "            <td>0.8668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>0.4140</td>\n",
       "            <td>0.1806</td>\n",
       "            <td>-1.1200</td>\n",
       "            <td>-0.3160</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\n",
    "    \"\"\"<body>\n",
    "    <p>Rows: Context, Columns: Embeddings</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Row</th>\n",
    "            <th>Embedding 1</th>\n",
    "            <th>Embedding 2</th>\n",
    "            <th>Embedding 3</th>\n",
    "            <th>Embedding 4</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>-0.6182</td>\n",
    "            <td>0.6397</td>\n",
    "            <td>-0.6141</td>\n",
    "            <td>0.8668</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>0.4140</td>\n",
    "            <td>0.1806</td>\n",
    "            <td>-1.1200</td>\n",
    "            <td>-0.3160</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each row of a prompt needs to be multiplied with the same scalar from mul_factor, the position of the row determines the index of mul_factor\\\n",
    "basically: row 1 * mul_factor[0], row 2 * mul_factor[2] etc.\\\n",
    "remember the bit_width value from quant config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6182,  0.6397, -0.6141,  0.1633,  0.1303,  0.9678, -0.0135, -0.3592,\n",
      "         1.0851,  1.2883, -1.4943, -1.3554, -1.2857,  0.5534, -0.9053, -0.2538,\n",
      "         2.0112,  1.5106, -0.5143,  0.0181, -1.1853, -0.1291,  1.1889, -0.2304,\n",
      "        -0.1677, -1.0456, -0.1630,  0.8798,  0.4793,  1.3267,  0.9272,  0.4181,\n",
      "        -1.6796, -0.2393,  0.7780, -1.7058, -1.1486, -1.5907,  0.9055, -0.6892,\n",
      "        -0.3182,  1.7268,  1.3576, -0.0698,  0.5315, -0.9513,  0.0850, -0.0770,\n",
      "         0.6108, -0.9660, -0.2021, -0.8171, -0.4134, -0.9940, -1.1997,  1.1844,\n",
      "         2.1950, -0.9969, -0.6415,  2.3817, -0.1636,  0.8668, -1.2963,  0.2591],\n",
      "       grad_fn=<SelectBackward0>, names=('E',)), \n",
      "tensor([-0.6476,  0.6723, -0.6434,  0.1723,  0.1378,  1.0165, -0.0132, -0.3759,\n",
      "         1.1397,  1.3528, -1.5670, -1.4212, -1.3480,  0.5817, -0.9490, -0.2653,\n",
      "         2.1113,  1.5861, -0.5386,  0.0200, -1.2427, -0.1344,  1.2486, -0.2408,\n",
      "        -0.1750, -1.0961, -0.1700,  0.9242,  0.5039,  1.3931,  0.9740,  0.4398,\n",
      "        -1.7614, -0.2501,  0.8174, -1.7889, -1.2042, -1.6681,  0.9512, -0.7222,\n",
      "        -0.3328,  1.8130,  1.4256, -0.0722,  0.5587, -0.9972,  0.0902, -0.0798,\n",
      "         0.6419, -1.0126, -0.2110, -0.8563, -0.4327, -1.0420, -1.2578,  1.2439,\n",
      "         2.3043, -1.0450, -0.6721,  2.5002, -0.1707,  0.9105, -1.3592,  0.2729],\n",
      "       grad_fn=<AddBackward0>, names=('E',))\n"
     ]
    }
   ],
   "source": [
    "#perform normalization like so:\n",
    "print(f'{embeddings[0][0]}, \\n{mul_factor[0] * embeddings[0][0] + add_factor[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n",
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_no_merge \u001b[39m=\u001b[39m bn(output_no_merge)\n\u001b[1;32m      4\u001b[0m output_merge \u001b[39m=\u001b[39m quant_mha_merged(q,k,v)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m output_no_merge\u001b[39m.\u001b[39mequal(output_merge)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q,k,v = [embeddings for _ in range(3)]\n",
    "output_no_merge = quant_mha(q,k,v)[0]\n",
    "output_no_merge = bn(output_no_merge)\n",
    "output_merge = quant_mha_merged(q,k,v)[0]\n",
    "assert output_no_merge.equal(output_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test what happens if values are transposed after merging QuantLinear with BatchNorm\n",
    "quant_linear = qnn.QuantLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if transposing inputs changes values for batchnorm with two different feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "transposing does not work",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m out_context \u001b[39m=\u001b[39m bn_context(\u001b[39minput\u001b[39m)\n\u001b[1;32m      6\u001b[0m out_embedding \u001b[39m=\u001b[39m bn_embedding(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39massert\u001b[39;00m out_context\u001b[39m.\u001b[39mequal(out_embedding), \u001b[39m\"\u001b[39m\u001b[39mtransposing does not work\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: transposing does not work"
     ]
    }
   ],
   "source": [
    "bn_context = torch.nn.BatchNorm1d(context)\n",
    "bn_embedding = torch.nn.BatchNorm1d(embed_dim)\n",
    "input = embeddings\n",
    "input = input.rename(None)\n",
    "out_context = bn_context(input)\n",
    "out_embedding = bn_embedding(input.transpose(-1,-2))\n",
    "#after transposing, normalize along embeddings instead of along words\n",
    "assert out_context.equal(out_embedding), \"transposing does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if linear transformation can simulate the functionality of batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9493,  2.7542,  9.5925,  7.8767,  1.7580,  3.1383,  6.2312,  5.3196,\n",
       "          7.8936,  1.0332],\n",
       "        [12.1436, 11.1988, 15.5672, 13.1937,  5.2797,  7.7320, 12.6711,  8.1652,\n",
       "         13.6740,  6.0436],\n",
       "        [ 4.6590,  9.2941,  0.4616, -0.8165,  1.4461,  7.0240, -2.8188,  0.2568,\n",
       "          0.8149,  6.8677]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randint(30, (3,10)).to(dtype=torch.float) * 0.7\n",
    "linear = torch.nn.Linear(3, 3)\n",
    "linear(test.transpose(-1,0)).transpose(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before attention calculation, q,k,v are quantized\n",
    "#shape: n_embd * 3 as q,k,v are the same and of shape n_embd\n",
    "quant_mha_merged.in_proj.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64]), torch.Size([3, 16, 64]), torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "#code copied from forward pass of QuantMultiheadAttention\n",
    "#function is called if in_proj quantization has been set\n",
    "#TODO: find out what it does\n",
    "def chunk(x, num=3, dim=-1):\n",
    "    _len, _bsz, _dim = x.shape\n",
    "    x = x.reshape(_len, _bsz, num, dim)\n",
    "    return x[:, :, 0, :], x[:, :, 1, :], x[:, :, 2, :]\n",
    "assert attn_no_batchfirst.in_proj is not None\n",
    "from brevitas.nn.utils import check_tensors_same_ptr\n",
    "#no idea what it does, it has to be True or else an Exception will be thrown\n",
    "assert check_tensors_same_ptr([embeddings, embeddings, embeddings]) == True\n",
    "torch._C._get_tracing_state()\n",
    "\n",
    "query = embeddings\n",
    "query.rename_('L', 'N', 'E')\n",
    "#no idea why q,k,v are infered from the query and params key and value are still used\n",
    "#this is an issue if no in_proj is specified i think\n",
    "q,k,v = chunk(attn_no_batchfirst.in_proj(query))\n",
    "print(f'{q.size()}, {k.size()}, {v.size()}')\n",
    "#issue with wrong shapes could be that batch size is transposed instead of embedding dimension\n",
    "#q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]), \n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(9).reshape(3,3)\n",
    "#columns become rows, rows become columns\n",
    "print(f'{tensor}, \\n{tensor.transpose(1,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "small_attn_cpy = CausalSelfAttention(GPTConfig(block_size=16, n_embd=64, n_head=2))\n",
    "#if batchnorm and mha are merged together, padding should not be necessary for inference\n",
    "small_attn_cpy.mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=64)\n",
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True #copy state dict does not return brevitas qparams\n",
    "small_attn_cpy.load_state_dict(small_attn.state_dict())\n",
    "#qparams from state dict are set to 1 at first\n",
    "print(small_attn.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "print(small_attn_cpy.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "idea from: https://github.com/Xilinx/brevitas/issues/542#issuecomment-1446338490\n",
    "merge_bn does not delete current batchnorm, meaning that one model has to be initialiized without bn and the parameters from the trained model\n",
    "have to be copied to the model without bn\n",
    "TODO: find more ressource efficient ways\n",
    "\"\"\"\n",
    "#at one step in merge_bn_mha, layer.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape)) is performed\n",
    "#weight is of shape (embd_dim, embd_dim), mul_factor is of (shape features, 1)\n",
    "#meaning that batchnorm probably normalizes along the embeddings instead of each sentence\n",
    "\"bn_alt feature length is 64 (embedding dimension)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     88\u001b[0m out_ch_weight_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mweight, output_channel_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     out_ch_bias_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias, channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#merge_bn_mha appends batchnorm to mha, TODO: prepend it (maybe use input_quant_tensor or something)\n",
    "#problem: merged and unmerged outputs are not the same, possibly since feature length is different\n",
    "no_merge_attn_output = small_attn(embeddings)\n",
    "no_merge_bn_output = bn(no_merge_attn_output)\n",
    "try:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=0)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=1)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=2)\n",
    "merge_attn_output = small_attn(embeddings)\n",
    "assert torch.equal(no_merge_bn_output, merge_attn_output) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mul_:\n",
      "\n",
      "mul_(...) method of torch.Tensor instance\n",
      "    mul_(value) -> Tensor\n",
      "    \n",
      "    In-place version of :meth:`~Tensor.mul`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer.out_proj.weight.data.mul_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a.mul_(tensor) basically is a = a * tensor\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.mul_(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_attn(torch.Tensor(3,16,64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function brevitas.nn.utils.merge_bn(layer, bn, output_channel_dim=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv1d and batchnorm1d merge\n",
    "\n",
    "qnn.quant_layer.merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9917, 0.4984, 0.6176, 0.5039, 0.8158, 0.8521, 0.0155, 0.1858,\n",
       "          0.8048],\n",
       "         [0.1621, 0.4298, 0.3947, 0.5427, 0.8238, 0.9419, 0.7478, 0.4333,\n",
       "          0.0647],\n",
       "         [0.0897, 0.2927, 0.9780, 0.6710, 0.0377, 0.8199, 0.1301, 0.8592,\n",
       "          0.8216],\n",
       "         [0.2074, 0.6790, 0.2042, 0.7838, 0.5414, 0.5088, 0.8481, 0.2490,\n",
       "          0.1760],\n",
       "         [0.0197, 0.6737, 0.1897, 0.2794, 0.4024, 0.3306, 0.8610, 0.8641,\n",
       "          0.6871],\n",
       "         [0.7651, 0.4413, 0.9831, 0.4328, 0.2344, 0.0799, 0.4901, 0.1151,\n",
       "          0.9380]],\n",
       "\n",
       "        [[0.4503, 0.5180, 0.3012, 0.7354, 0.2637, 0.9073, 0.9226, 0.7925,\n",
       "          0.0674],\n",
       "         [0.9067, 0.1654, 0.9186, 0.1072, 0.0438, 0.4049, 0.1374, 0.3990,\n",
       "          0.6381],\n",
       "         [0.3767, 0.8549, 0.5588, 0.2489, 0.2599, 0.6461, 0.5800, 0.1559,\n",
       "          0.0832],\n",
       "         [0.9381, 0.2192, 0.7259, 0.7615, 0.1411, 0.1472, 0.9268, 0.6733,\n",
       "          0.9049],\n",
       "         [0.1468, 0.8668, 0.3151, 0.5401, 0.4347, 0.5541, 0.0995, 0.6333,\n",
       "          0.1252],\n",
       "         [0.4964, 0.4591, 0.3443, 0.2972, 0.6705, 0.2664, 0.4867, 0.5302,\n",
       "          0.6139]],\n",
       "\n",
       "        [[0.3803, 0.7252, 0.5246, 0.4232, 0.7195, 0.7118, 0.8266, 0.7990,\n",
       "          0.3631],\n",
       "         [0.1904, 0.0903, 0.8097, 0.7286, 0.2548, 0.3355, 0.7833, 0.9820,\n",
       "          0.1257],\n",
       "         [0.6124, 0.5454, 0.4477, 0.6442, 0.5862, 0.4324, 0.3639, 0.6780,\n",
       "          0.3984],\n",
       "         [0.4506, 0.1190, 0.4191, 0.3696, 0.6122, 0.7606, 0.1218, 0.3204,\n",
       "          0.6428],\n",
       "         [0.0765, 0.5835, 0.6852, 0.1305, 0.3852, 0.2372, 0.3551, 0.0528,\n",
       "          0.6343],\n",
       "         [0.0852, 0.0078, 0.7411, 0.6070, 0.8316, 0.9041, 0.3005, 0.5442,\n",
       "          0.2225]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand((3,6,9))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5546, -0.3177,  0.1346, -0.2969,  0.8870,  1.0246, -2.1506,\n",
       "          -1.5043,  0.8454],\n",
       "         [-0.9784, -0.1143, -0.2279,  0.2499,  1.1573,  1.5384,  0.9118,\n",
       "          -0.1030, -1.2930],\n",
       "         [-1.5225, -0.7464,  1.8741,  0.7002, -1.7215,  1.2696, -1.3680,\n",
       "           1.4198,  1.2762],\n",
       "         [-1.0571,  0.6570, -1.0687,  1.0382,  0.1571,  0.0385,  1.2718,\n",
       "          -0.9059, -1.1712],\n",
       "         [-1.5085,  0.9969, -0.8575, -0.5138, -0.0426, -0.3174,  1.7148,\n",
       "           1.7266,  1.0484],\n",
       "         [ 1.0773, -0.1349,  1.8936, -0.1669, -0.9097, -1.4882,  0.0477,\n",
       "          -1.3565,  1.7249]],\n",
       "\n",
       "        [[-0.5003, -0.2434, -1.0663,  0.5820, -1.2088,  1.2342,  1.2923,\n",
       "           0.7987, -1.9539],\n",
       "         [ 1.4249, -0.9680,  1.4631, -1.1558, -1.3605, -0.1948, -1.0582,\n",
       "          -0.2139,  0.5578],\n",
       "         [-0.4253,  1.4034,  0.2712, -0.9140, -0.8717,  0.6049,  0.3522,\n",
       "          -1.2693, -1.5475],\n",
       "         [ 1.5987, -1.0141,  0.8276,  0.9569, -1.2980, -1.2758,  1.5576,\n",
       "           0.6364,  1.4780],\n",
       "         [-1.0216,  1.7369, -0.3769,  0.4850,  0.0815,  0.5390, -1.2028,\n",
       "           0.8424, -1.1045],\n",
       "         [ 0.0714, -0.0682, -0.4982, -0.6744,  0.7233, -0.7897,  0.0349,\n",
       "           0.1978,  0.5111]],\n",
       "\n",
       "        [[-0.7660,  0.5431, -0.2185, -0.6032,  0.5214,  0.4922,  0.9280,\n",
       "           0.8232, -0.8312],\n",
       "         [-0.8871, -1.2103,  1.1116,  0.8499, -0.6794, -0.4189,  1.0267,\n",
       "           1.6678, -1.0960],\n",
       "         [ 0.4760,  0.2200, -0.1538,  0.5978,  0.3758, -0.2121, -0.4741,\n",
       "           0.7270, -0.3422],\n",
       "         [-0.1729, -1.3784, -0.2876, -0.4673,  0.4144,  0.9536, -1.3683,\n",
       "          -0.6463,  0.5256],\n",
       "         [-1.2911,  0.6514,  1.0409, -1.0840, -0.1082, -0.6752, -0.2235,\n",
       "          -1.3819,  0.8459],\n",
       "         [-1.4683, -1.7582,  0.9877,  0.4856,  1.3264,  1.5978, -0.6620,\n",
       "           0.2501, -0.9544]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized values along second dimension, meaning: along sentences\n",
    "#are \n",
    "torch.nn.BatchNorm1d(6)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor retains size, batchnorm essentially is a linear transformation to shift values to have a mean of 0 and a standard deviation of 1\n",
    "torch.nn.BatchNorm1d(10)(torch.Tensor(3,10,16)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = qnn.QuantIdentity()\n",
    "tensor = torch.Tensor(12,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1617e+35,  3.0907e-41, -1.5597e+37,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.1351e-43,  0.0000e+00],\n",
      "         [-1.5597e+37,  3.0907e-41, -3.0176e+34,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test if quantidentity is a simple wrapper around a tensor that does nothing\n",
    "#if so, it could be useful for merging with batchnorm\n",
    "tensor = torch.Tensor(2,3,4)\n",
    "print(tensor)\n",
    "print(\"\\n\" + 30* \"-\" + \"\\n\")\n",
    "print(qnn.QuantIdentity()(tensor).isclose(tensor).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = identity(tensor)\n",
    "output.size == tensor.size\n",
    "output == tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test custom batchnorm and merging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2659,  0.6091,  1.4836,  0.9439,  1.2065, -0.9223, -0.5282, -0.8455,\n",
      "         -0.6554, -1.1303],\n",
      "        [-1.3342, -0.6483, -1.0222,  1.5021, -0.2206,  0.6714, -0.7772,  1.1717,\n",
      "          1.8188, -0.7320],\n",
      "        [-0.2430, -2.0209,  0.5107, -1.3762,  0.5122, -0.6174, -0.2816, -0.9368,\n",
      "          0.0110, -0.6685],\n",
      "        [ 1.0439,  0.5637,  0.0682,  0.5662, -0.7288, -0.8823,  1.3416,  0.7040,\n",
      "         -0.4979,  1.0843],\n",
      "        [-0.8410, -0.4107,  1.9964, -0.3908, -1.1501, -1.4927, -1.0785, -0.1600,\n",
      "         -0.2757, -2.0370]], grad_fn=<SelectBackward0>)\n",
      "------------------------------ unquantized: tensor([[-1.2628,  0.6076,  1.4799,  0.9416,  1.2035, -0.9200, -0.5269, -0.8434,\n",
      "         -0.6538, -1.1275],\n",
      "        [-1.3309, -0.6467, -1.0197,  1.4984, -0.2201,  0.6698, -0.7753,  1.1687,\n",
      "          1.8142, -0.7302],\n",
      "        [-0.2424, -2.0158,  0.5095, -1.3728,  0.5109, -0.6158, -0.2809, -0.9344,\n",
      "          0.0109, -0.6668],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8389, -0.4097,  1.9914, -0.3899, -1.1472, -1.4889, -1.0758, -0.1596,\n",
      "         -0.2750, -2.0318]])\n",
      "------------------------------ quantized: tensor([[-1.2582,  0.6056,  1.4749,  0.9384,  1.1995, -0.9166, -0.5249, -0.8402,\n",
      "         -0.6513, -1.1234],\n",
      "        [-1.3332, -0.6480, -1.0215,  1.5007, -0.2206,  0.6707, -0.7768,  1.1705,\n",
      "          1.8171, -0.7316],\n",
      "        [-0.2421, -2.0135,  0.5089, -1.3712,  0.5104, -0.6151, -0.2805, -0.9333,\n",
      "          0.0110, -0.6660],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8408, -0.4106,  1.9961, -0.3907, -1.1498, -1.4923, -1.0783, -0.1599,\n",
      "         -0.2755, -2.0365]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "import torch\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "normal_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "unquantized_custom_bn = CustomBatchNorm1d(sentence_length)\n",
    "#simulate some forward passes, without updating learnable scale and bias\n",
    "for _ in range(100):\n",
    "    normal_bn(embedding)\n",
    "#before merging, do nothing\n",
    "assert embedding.equal(unquantized_custom_bn(embedding))\n",
    "unquantized_custom_bn = replace_bn(normal_bn, unquantized_custom_bn)\n",
    "out_normal_bn = normal_bn(embedding)\n",
    "out_unquantized_custom_bn = unquantized_custom_bn(embedding)\n",
    "print(out_normal_bn[0])\n",
    "print(f'{30*\"-\"} unquantized: {out_unquantized_custom_bn[0]}')\n",
    "quantized_custom_bn = QuantBatchnorm1d(sentence_length)\n",
    "assert embedding.equal(quantized_custom_bn(embedding))\n",
    "#replace\n",
    "quantized_custom_bn = replace_bn(normal_bn, quantized_custom_bn)\n",
    "out_quantized_custom_bn = quantized_custom_bn(embedding)\n",
    "print(f'{30*\"-\"} quantized: {out_quantized_custom_bn[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x TransformerBlock(\n",
      "        (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (mha): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (active): ReLU()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n",
      "ModuleDict(\n",
      "  (wte): Embedding(50304, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (mha): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (active): ReLU()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Embedding(50304, 768)\n",
      "Embedding(1024, 768)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "ModuleList(\n",
      "  (0-11): 12 x TransformerBlock(\n",
      "    (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attn): CausalSelfAttention(\n",
      "      (mha): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (active): ReLU()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=768, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for mn, module in gpt.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4834, -0.1027, -2.2402,  0.4352,  0.2077,  1.0181,  0.6446, -0.6509,\n",
      "          0.9757,  0.8973, -0.7394,  0.0084,  1.1972, -1.3001,  0.8528,  0.6930,\n",
      "          1.2027, -0.3247,  0.2913, -0.3543, -1.0235,  1.0952,  0.9958, -0.4298,\n",
      "          0.6690, -0.7751,  1.2495,  1.1023, -0.6781,  0.4847, -1.4741, -0.9532]])\n",
      "------------------------------\n",
      "tensor([[-0.4549, -0.9629, -2.8159, -0.4967, -0.6939,  0.0086, -0.3151, -1.4382,\n",
      "         -0.0281, -0.0961, -1.5149, -0.8666,  0.1639, -2.0010, -0.1346, -0.2732,\n",
      "          0.1687, -1.1554, -0.6214, -1.1810, -1.7612,  0.0755, -0.0107, -1.2465,\n",
      "         -0.2940, -1.5459,  0.2093,  0.0817, -1.4618, -0.4537, -2.1518, -1.7002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.quantization import quant_bn\n",
    "#one word with 32 embeddings\n",
    "input = torch.randn(1,32)\n",
    "#context is max. of 8 words\n",
    "weight, bias = torch.randn(2, 8)\n",
    "output = quant_bn.custom_bn1d(input, weight, bias)\n",
    "#input and output are the same, why?\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug custom_bn1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_shapes(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Checks if a tensor is of shape [C], [N,C] or [C,N] with N = 1 and C >= 1.\n",
    "    If tensor is of a different shape, a ValueError will be thrown.\n",
    "    The returning tensor will be of shape [C, 1].\n",
    "    \"\"\"\n",
    "    shape_tensor = tensor.size()\n",
    "    if len(shape_tensor) == 1:\n",
    "        tensor = tensor[:,None]\n",
    "    if len(shape_tensor) == 2:\n",
    "        if shape_tensor[0] > 1 and shape_tensor[1] > 1:\n",
    "            raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "        elif shape_tensor[0] == 1 and shape_tensor[1] > 1:\n",
    "            tensor = tensor.transpose(0,1)\n",
    "    elif len(shape_tensor) > 2:\n",
    "        raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def custom_bn1d(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass of custom BatchNorm implementation. It expects a Tensor x of size [N,C] or [N,C,L]\n",
    "    and both a weight and bias Tensor, each of size [C, 1] or of size [1,C] / [C].\n",
    "    Each row/ embedding of a sentence (dimension C) will be multiplied with one value from the index of the corresponding\n",
    "    weight tensor and added with the value of the bias tensor.\n",
    "\n",
    "    Output: tensor of shape [N,C] or [N,C,L], basically of the same size as the input tensor.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor) :\n",
    "        raise TypeError('Input is not a tensor')\n",
    "    elif not isinstance(weight, torch.Tensor):\n",
    "        raise TypeError('Weight is not a tensor')\n",
    "    elif not isinstance(bias, torch.Tensor):\n",
    "        raise TypeError('Bias is not a Tensor')\n",
    "    #make sure that weights and biases are of shape [C,1]\n",
    "    print(weight)\n",
    "    print(10*\"#\")\n",
    "    weight = check_shapes(weight)\n",
    "    bias = check_shapes(bias)\n",
    "    C_x = x.size()[0] if len(x.size()) == 2 else x.size()[1]\n",
    "    print(f'ok: {weight[:C_x]}')\n",
    "    out = x * weight[:C_x] + bias[:C_x]\n",
    "    #only return the first C_x rows of output tensor\n",
    "    return out[:,None:C_x] if len(x.size()) == 3 else out[:C_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "tensor([ 0,  8,  9, 14, 13,  3, 14,  9, 11,  8, 10,  6, 17, 15,  9,  3])\n",
      "tensor([[ 0, 16,  3,  2,  9, 19, 13,  7],\n",
      "        [10,  4,  1, 18, 13,  7,  8,  5],\n",
      "        [17, 19,  3,  5, 12, 19, 18, 11]])\n",
      "------------------------------\n",
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "##########\n",
      "ok: tensor([[ 6],\n",
      "        [10],\n",
      "        [10]])\n",
      "tensor([[  0,  96,  18,  12,  54, 114,  78,  42],\n",
      "        [108,  48,  18, 188, 138,  78,  88,  58],\n",
      "        [179, 199,  39,  59, 129, 199, 189, 119]])\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.randint(20, (16,))\n",
    "bias = torch.randint(20, (16,))\n",
    "input = torch.randint(20, (3,8))\n",
    "print(weight)\n",
    "print(bias)\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "out = custom_bn1d(input,weight,bias)\n",
    "print(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(3,3,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17],\n",
       "        [18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(9,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:,None:2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(input.size())\n",
    "output = custom_bn1d(input, weight, bias)\n",
    "custom_bn1d(torch.randn(3,1,32), weight, bias)\n",
    "output.size()\n",
    "assert output.equal(input * weight[0] + bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.4805e-10, 6.3011e-10, 6.6376e-07, 6.7212e-04, 1.7340e-07,\n",
       "          1.6594e-07, 6.4097e-10, 1.4580e-19, 1.1495e+24, 3.0956e-18,\n",
       "          5.8981e-10, 3.2506e+21, 1.0528e-11, 2.7625e-06, 6.4103e-10,\n",
       "          2.1744e+23, 1.2794e+22, 2.1574e-04, 3.3980e+21, 3.0818e-18,\n",
       "          3.1360e+27, 7.0800e+31, 3.1095e-18, 4.7851e+22, 2.8826e+32,\n",
       "          4.4248e+30, 7.2442e+22, 2.3086e-12, 7.1760e+22, 7.2250e+28,\n",
       "          1.5766e-19, 2.7447e-06]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.8091e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00, 9.1844e-41,\n",
       "          1.1551e-40, 4.5919e-41, 8.2957e-43, 2.9147e-43, 0.0000e+00,\n",
       "          6.7262e-44, 0.0000e+00]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((tensor,torch.Tensor(1,1,32)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MergeBatchNorm class from brevitas.graph.quantize.preprocess_for_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "import torch\n",
    "\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1150\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:140\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb)\u001b[38;5;66;03m# + pos_emb)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:184\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[0;32m--> 184\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    185\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:40\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#dirty workaround to avoid runtimeerrors by adding a padding if the input is smaller than the feature length\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#padding does not artificially lower mean as normalization is performed along the word embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     n,c,l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m#input tensor should always be three dimensional\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m-\u001b[39m c, l)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:437\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:300\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "#initialization of normalization layer dependent on whether batchnorm or layernorm is used\n",
    "#\n",
    "torch.fx.symbolic_trace(gpt, concrete_args= {'idx': torch.Tensor(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tracer' object has no attribute 'unpack_arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getLogger\n\u001b[1;32m      3\u001b[0m log \u001b[38;5;241m=\u001b[39m getLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m other_model \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_for_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixed_point\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MergeBatchNorm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model needs a graph attribute from torch.fx.symbolic_trace\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/quantize.py:272\u001b[0m, in \u001b[0;36mpreprocess_for_quantize\u001b[0;34m(model, trace_model, relu6_to_relu, equalize_iters, equalize_merge_bias, merge_bn, equalize_bias_shrinkage, equalize_scale_computation)\u001b[0m\n\u001b[1;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_model:\n\u001b[0;32m--> 272\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchFunctionalToModule()\u001b[38;5;241m.\u001b[39mapply(model)\n\u001b[1;32m    274\u001b[0m model \u001b[38;5;241m=\u001b[39m DuplicateSharedStatelessModule()\u001b[38;5;241m.\u001b[39mapply(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:133\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    131\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#assert block_size <= self.config.block_size, f\"Cannot forward sequence of length {block_size}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    136\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:53\u001b[0m, in \u001b[0;36m_gen_torch_fn_patches.<locals>.new_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tracers\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m orig_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_arg\u001b[49m(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer\u001b[38;5;241m.\u001b[39munpack_arg(kwargs))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnsetValueException:\n\u001b[1;32m     55\u001b[0m     value \u001b[38;5;241m=\u001b[39m _UNSET\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tracer' object has no attribute 'unpack_arg'"
     ]
    }
   ],
   "source": [
    "#preprocess_for_quantize needs access to a graph representation of the model\n",
    "from logging import getLogger\n",
    "log = getLogger(__name__)\n",
    "other_model = preprocess_for_quantize(gpt)\n",
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "#model needs a graph attribute from torch.fx.symbolic_trace\n",
    "#the purpose of that probably is the same as in https://github.com/pytorch/examples/blob/main/fx/replace_op.py\n",
    "\"\"\"\n",
    "it seems that control flow depending on arguments leads to this error\n",
    "https://discuss.tvm.apache.org/t/torch-fx-symbolic-trace-fails-for-most-encoder-decoder-nlp-models/16004\n",
    "\"\"\"\n",
    "try:\n",
    "    MergeBatchNorm().apply(gpt)\n",
    "except Exception as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'dummy_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;66;03m# is being used with transformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/utils/fx.py:1241\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(model, input_names, disable_check, tracer_cls)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03mPerforms symbolic tracing on the model.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     input_names \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1243\u001b[0m input_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_names)\n\u001b[1;32m   1244\u001b[0m concrete_args \u001b[38;5;241m=\u001b[39m get_concrete_args(model, input_names)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'dummy_inputs'"
     ]
    }
   ],
   "source": [
    "from transformers.utils.fx import symbolic_trace # is being used with transformers\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MinstConv(nn.Module):\n",
    "    def __init__(self, param = 10):\n",
    "        super(MinstConv, self).__init__()\n",
    "        #each model needs nn.module for quantization to work\n",
    "        self.model = nn.ModuleDict(dict(\n",
    "            conv1 = nn.Conv2d(1, 32, 3, 1),\n",
    "            relu1 = nn.ReLU(),\n",
    "            conv2 = nn.Conv2d(32, 64, 3, 1),\n",
    "            relu2 = nn.ReLU(),\n",
    "            maxpool2d = nn.MaxPool2d(kernel_size=2),\n",
    "            dropout1 = nn.Dropout(0.25),\n",
    "            flatten = nn.Flatten(),\n",
    "            fc1 = nn.Linear(9216, 128),\n",
    "            relu3 = nn.ReLU(),\n",
    "            dropout2 = nn.Dropout(0.5),\n",
    "            fc2 = nn.Linear(128, 10)\n",
    "        ))\n",
    "        #check symbolic traceability\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        #no exception\n",
    "        assert self.param > 0\n",
    "        #exception, meaning param checking during forward pass not possible\n",
    "        assert x.size()[-1] > 0\n",
    "        output = x\n",
    "        for layer_name, layer in self.model.items():\n",
    "            output = layer(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinstConv(\n",
       "  (model): Module(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout1): Dropout(p=0.25, inplace=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (relu3): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fx.symbolic_trace(MinstConv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:130\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    128\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot forward sequence of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, block size is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, t, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:495\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:352\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "from brevitas.fx.brevitas_tracer import symbolic_trace\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     l \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(l, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m traced \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1150\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(x):\n\u001b[1;32m     13\u001b[0m     l \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch.fx as fx\n",
    "\"\"\"\n",
    "using values from param leads to errors\n",
    "\"\"\"\n",
    "try:\n",
    "    fx.symbolic_trace(model)\n",
    "except:\n",
    "    pass\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "def test(x):\n",
    "    l = x.size(1)\n",
    "    return torch.arange(l, dtype=torch.long, device='cuda')\n",
    "traced = symbolic_trace(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, size: int = 10):\n",
    "        super().__init__()\n",
    "        #attributes can be param checked during symbolic tracing\n",
    "        self.size = size\n",
    "        self.mul = MultiplyModule()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #param checking fails for symbolic tracing, unless modules are leaf modules which are not traced\n",
    "        #assert isinstance(x,torch.Tensor), 'x is not a tensor'\n",
    "        #assert x.size()[-1] < 10, f'Size of input tensor {x.size()} not compatible with size {self.size}'\n",
    "        #custom modules for operations only necessary when param checking is performed\n",
    "        return self.mul.forward(x)\n",
    "    \n",
    "class MultiplyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "model = torch.fx.symbolic_trace(SimpleModel())\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "model = preprocess_for_quantize(model, trace_model = False)\n",
    "#forward pass works for model after preprocess_model_for_quantize, but not for gpt\n",
    "assert model(10) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleModel()]\n",
      "[SimpleModel(\n",
      "  (mul): MultiplyModule()\n",
      "), MultiplyModule()]\n"
     ]
    }
   ],
   "source": [
    "#submodules disappear form module list after preprocess_for_quantize\n",
    "print(list(model.modules()))\n",
    "print(list(SimpleModel().modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    size = x.size(1)\n",
      "    arange = self.arange(size);  size = None\n",
      "    getattr_1 = x.device;  x = None\n",
      "    to = arange.to(dtype = torch.int64, device = getattr_1);  arange = getattr_1 = None\n",
      "    return to\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/pytorch/pytorch/issues/51803#issuecomment-1104634592\n",
    "#experiments to make fx graphing work with transformers\n",
    "import torch\n",
    "\n",
    "from torch.fx import Tracer\n",
    "from torch.fx import symbolic_trace\n",
    "from torch.fx.graph_module import GraphModule\n",
    "\n",
    "\n",
    "class CustomedTracer(Tracer):\n",
    "    \"\"\"\n",
    "    ``Tracer`` is the class that implements the symbolic tracing functionality\n",
    "    of ``torch.fx.symbolic_trace``. A call to ``symbolic_trace(m)`` is equivalent\n",
    "    to ``Tracer().trace(m)``.\n",
    "    This Tracer override the ``is_leaf_module`` function to make symbolic trace\n",
    "    right in some cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, customed_leaf_module=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.customed_leaf_module = customed_leaf_module\n",
    "\n",
    "    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name : str) -> bool:\n",
    "        \"\"\"\n",
    "        A method to specify whether a given ``nn.Module`` is a \"leaf\" module.\n",
    "        Leaf modules are the atomic units that appear in\n",
    "        the IR, referenced by ``call_module`` calls. By default,\n",
    "        Modules in the PyTorch standard library namespace (torch.nn)\n",
    "        are leaf modules. All other modules are traced through and\n",
    "        their constituent ops are recorded, unless specified otherwise\n",
    "        via this parameter.\n",
    "        Args:\n",
    "            m (Module): The module being queried about\n",
    "            module_qualified_name (str): The path to root of this module. For example,\n",
    "                if you have a module hierarchy where submodule ``foo`` contains\n",
    "                submodule ``bar``, which contains submodule ``baz``, that module will\n",
    "                appear with the qualified name ``foo.bar.baz`` here.\n",
    "        \"\"\"\n",
    "        if self.customed_leaf_module and isinstance(m, self.customed_leaf_module):\n",
    "            return True\n",
    "        return m.__module__.startswith('torch.nn') and not isinstance(m, torch.nn.Sequential)\n",
    "\n",
    "\n",
    "\n",
    "class ArangeForFx(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.arange(x)\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.arange = ArangeForFx()\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = x.size(1)\n",
    "        return self.arange(l).to(dtype=torch.long, device=x.device)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "#all ops which need a param from a symbolically traced variable (from the function parameter) should be a leaf module\n",
    "#no idea why it fixes it though\n",
    "tracer = CustomedTracer(customed_leaf_module=(ArangeForFx,))\n",
    "graph = tracer.trace(model)\n",
    "#graph = symbolic_trace(model)\n",
    "name = model.__class__.__name__ if isinstance(model, torch.nn.Module) else model.__name__\n",
    "traced = GraphModule(tracer.root, graph, name)\n",
    "\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.model.modules import BatchNorm, LayerNorm\n",
    "#https://pytorch.org/docs/stable/fx.html#leaf-modules\n",
    "#leaf modules are not being traced through\n",
    "#could be problematic as the output of batch/layer norm are being passed into the attention and mlp layer\n",
    "tracer_gpt = CustomedTracer(customed_leaf_module=(BatchNorm,LayerNorm))\n",
    "from qtransform.model.gpt import GPT as qGPT, GPTConfig\n",
    "gpt = qGPT(GPTConfig())\n",
    "tokens = torch.randint(50304, (2, 1024))\n",
    "#assert statements should be nested inside of a custom module\n",
    "graph_gpt = tracer_gpt.trace(gpt, {\"idx\": tokens})\n",
    "name = gpt.__class__.__name__ if isinstance(gpt, torch.nn.Module) else gpt.__name__\n",
    "traced_gpt = GraphModule(tracer_gpt.root, graph_gpt, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preceding layer is merged with batchnorm layer\n",
    "#batchnorm, conv, linear and their quantized versions can be merged\n",
    "#if merging occurs before quantization, how can the learnable parameters from batchnorm be merged?\n",
    "#\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "processed_gpt = preprocess_for_quantize(traced_gpt, trace_model = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=1024, vocab_size=50304, n_layer=12, n_head=12, n_embd=768, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_symbolic_tracing = gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %idx_1 : [num_users=0] = placeholder[target=idx_1]\n",
      "    %targets : [num_users=1] = placeholder[target=targets](default=None)\n",
      "    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]\n",
      "    %transformer_wte : [num_users=1] = call_module[target=transformer.wte](args = (%_tensor_constant0,), kwargs = {})\n",
      "    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]\n",
      "    %transformer_wpe : [num_users=1] = call_module[target=transformer.wpe](args = (%_tensor_constant1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%transformer_wte, %transformer_wpe), kwargs = {})\n",
      "    %transformer_dropout : [num_users=2] = call_module[target=transformer.dropout](args = (%add,), kwargs = {})\n",
      "    %transformer_layer_0_ln_1 : [num_users=1] = call_module[target=transformer.layer.0.ln_1](args = (%transformer_dropout,), kwargs = {})\n",
      "    %transformer_layer_0_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.0.attn.attn_mask]\n",
      "    %transformer_layer_0_attn_mha : [num_users=2] = call_module[target=transformer.layer.0.attn.mha](args = (%transformer_layer_0_ln_1, %transformer_layer_0_ln_1, %transformer_layer_0_ln_1), kwargs = {attn_mask: %transformer_layer_0_attn_attn_mask, need_weights: False})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 0), kwargs = {})\n",
      "    %getitem_1 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 1), kwargs = {})\n",
      "    %add_1 : [num_users=2] = call_function[target=operator.add](args = (%transformer_dropout, %getitem), kwargs = {})\n",
      "    %transformer_layer_0_ln_2 : [num_users=1] = call_module[target=transformer.layer.0.ln_2](args = (%add_1,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_fc](args = (%transformer_layer_0_ln_2,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_active : [num_users=1] = call_module[target=transformer.layer.0.mlp.active](args = (%transformer_layer_0_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_proj](args = (%transformer_layer_0_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.0.mlp.dropout](args = (%transformer_layer_0_mlp_c_proj,), kwargs = {})\n",
      "    %add_2 : [num_users=2] = call_function[target=operator.add](args = (%add_1, %transformer_layer_0_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_1_ln_1 : [num_users=1] = call_module[target=transformer.layer.1.ln_1](args = (%add_2,), kwargs = {})\n",
      "    %transformer_layer_1_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.1.attn.attn_mask]\n",
      "    %transformer_layer_1_attn_mha : [num_users=2] = call_module[target=transformer.layer.1.attn.mha](args = (%transformer_layer_1_ln_1, %transformer_layer_1_ln_1, %transformer_layer_1_ln_1), kwargs = {attn_mask: %transformer_layer_1_attn_attn_mask, need_weights: False})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 0), kwargs = {})\n",
      "    %getitem_3 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 1), kwargs = {})\n",
      "    %add_3 : [num_users=2] = call_function[target=operator.add](args = (%add_2, %getitem_2), kwargs = {})\n",
      "    %transformer_layer_1_ln_2 : [num_users=1] = call_module[target=transformer.layer.1.ln_2](args = (%add_3,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_fc](args = (%transformer_layer_1_ln_2,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_active : [num_users=1] = call_module[target=transformer.layer.1.mlp.active](args = (%transformer_layer_1_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_proj](args = (%transformer_layer_1_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.1.mlp.dropout](args = (%transformer_layer_1_mlp_c_proj,), kwargs = {})\n",
      "    %add_4 : [num_users=2] = call_function[target=operator.add](args = (%add_3, %transformer_layer_1_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_2_ln_1 : [num_users=1] = call_module[target=transformer.layer.2.ln_1](args = (%add_4,), kwargs = {})\n",
      "    %transformer_layer_2_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.2.attn.attn_mask]\n",
      "    %transformer_layer_2_attn_mha : [num_users=2] = call_module[target=transformer.layer.2.attn.mha](args = (%transformer_layer_2_ln_1, %transformer_layer_2_ln_1, %transformer_layer_2_ln_1), kwargs = {attn_mask: %transformer_layer_2_attn_attn_mask, need_weights: False})\n",
      "    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 1), kwargs = {})\n",
      "    %add_5 : [num_users=2] = call_function[target=operator.add](args = (%add_4, %getitem_4), kwargs = {})\n",
      "    %transformer_layer_2_ln_2 : [num_users=1] = call_module[target=transformer.layer.2.ln_2](args = (%add_5,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_fc](args = (%transformer_layer_2_ln_2,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_active : [num_users=1] = call_module[target=transformer.layer.2.mlp.active](args = (%transformer_layer_2_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_proj](args = (%transformer_layer_2_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.2.mlp.dropout](args = (%transformer_layer_2_mlp_c_proj,), kwargs = {})\n",
      "    %add_6 : [num_users=2] = call_function[target=operator.add](args = (%add_5, %transformer_layer_2_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_3_ln_1 : [num_users=1] = call_module[target=transformer.layer.3.ln_1](args = (%add_6,), kwargs = {})\n",
      "    %transformer_layer_3_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.3.attn.attn_mask]\n",
      "    %transformer_layer_3_attn_mha : [num_users=2] = call_module[target=transformer.layer.3.attn.mha](args = (%transformer_layer_3_ln_1, %transformer_layer_3_ln_1, %transformer_layer_3_ln_1), kwargs = {attn_mask: %transformer_layer_3_attn_attn_mask, need_weights: False})\n",
      "    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 0), kwargs = {})\n",
      "    %getitem_7 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 1), kwargs = {})\n",
      "    %add_7 : [num_users=2] = call_function[target=operator.add](args = (%add_6, %getitem_6), kwargs = {})\n",
      "    %transformer_layer_3_ln_2 : [num_users=1] = call_module[target=transformer.layer.3.ln_2](args = (%add_7,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_fc](args = (%transformer_layer_3_ln_2,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_active : [num_users=1] = call_module[target=transformer.layer.3.mlp.active](args = (%transformer_layer_3_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_proj](args = (%transformer_layer_3_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.3.mlp.dropout](args = (%transformer_layer_3_mlp_c_proj,), kwargs = {})\n",
      "    %add_8 : [num_users=2] = call_function[target=operator.add](args = (%add_7, %transformer_layer_3_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_4_ln_1 : [num_users=1] = call_module[target=transformer.layer.4.ln_1](args = (%add_8,), kwargs = {})\n",
      "    %transformer_layer_4_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.4.attn.attn_mask]\n",
      "    %transformer_layer_4_attn_mha : [num_users=2] = call_module[target=transformer.layer.4.attn.mha](args = (%transformer_layer_4_ln_1, %transformer_layer_4_ln_1, %transformer_layer_4_ln_1), kwargs = {attn_mask: %transformer_layer_4_attn_attn_mask, need_weights: False})\n",
      "    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 0), kwargs = {})\n",
      "    %getitem_9 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 1), kwargs = {})\n",
      "    %add_9 : [num_users=2] = call_function[target=operator.add](args = (%add_8, %getitem_8), kwargs = {})\n",
      "    %transformer_layer_4_ln_2 : [num_users=1] = call_module[target=transformer.layer.4.ln_2](args = (%add_9,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_fc](args = (%transformer_layer_4_ln_2,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_active : [num_users=1] = call_module[target=transformer.layer.4.mlp.active](args = (%transformer_layer_4_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_proj](args = (%transformer_layer_4_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.4.mlp.dropout](args = (%transformer_layer_4_mlp_c_proj,), kwargs = {})\n",
      "    %add_10 : [num_users=2] = call_function[target=operator.add](args = (%add_9, %transformer_layer_4_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_5_ln_1 : [num_users=1] = call_module[target=transformer.layer.5.ln_1](args = (%add_10,), kwargs = {})\n",
      "    %transformer_layer_5_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.5.attn.attn_mask]\n",
      "    %transformer_layer_5_attn_mha : [num_users=2] = call_module[target=transformer.layer.5.attn.mha](args = (%transformer_layer_5_ln_1, %transformer_layer_5_ln_1, %transformer_layer_5_ln_1), kwargs = {attn_mask: %transformer_layer_5_attn_attn_mask, need_weights: False})\n",
      "    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 0), kwargs = {})\n",
      "    %getitem_11 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 1), kwargs = {})\n",
      "    %add_11 : [num_users=2] = call_function[target=operator.add](args = (%add_10, %getitem_10), kwargs = {})\n",
      "    %transformer_layer_5_ln_2 : [num_users=1] = call_module[target=transformer.layer.5.ln_2](args = (%add_11,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_fc](args = (%transformer_layer_5_ln_2,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_active : [num_users=1] = call_module[target=transformer.layer.5.mlp.active](args = (%transformer_layer_5_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_proj](args = (%transformer_layer_5_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.5.mlp.dropout](args = (%transformer_layer_5_mlp_c_proj,), kwargs = {})\n",
      "    %add_12 : [num_users=2] = call_function[target=operator.add](args = (%add_11, %transformer_layer_5_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_6_ln_1 : [num_users=1] = call_module[target=transformer.layer.6.ln_1](args = (%add_12,), kwargs = {})\n",
      "    %transformer_layer_6_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.6.attn.attn_mask]\n",
      "    %transformer_layer_6_attn_mha : [num_users=2] = call_module[target=transformer.layer.6.attn.mha](args = (%transformer_layer_6_ln_1, %transformer_layer_6_ln_1, %transformer_layer_6_ln_1), kwargs = {attn_mask: %transformer_layer_6_attn_attn_mask, need_weights: False})\n",
      "    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 0), kwargs = {})\n",
      "    %getitem_13 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 1), kwargs = {})\n",
      "    %add_13 : [num_users=2] = call_function[target=operator.add](args = (%add_12, %getitem_12), kwargs = {})\n",
      "    %transformer_layer_6_ln_2 : [num_users=1] = call_module[target=transformer.layer.6.ln_2](args = (%add_13,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_fc](args = (%transformer_layer_6_ln_2,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_active : [num_users=1] = call_module[target=transformer.layer.6.mlp.active](args = (%transformer_layer_6_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_proj](args = (%transformer_layer_6_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.6.mlp.dropout](args = (%transformer_layer_6_mlp_c_proj,), kwargs = {})\n",
      "    %add_14 : [num_users=2] = call_function[target=operator.add](args = (%add_13, %transformer_layer_6_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_7_ln_1 : [num_users=1] = call_module[target=transformer.layer.7.ln_1](args = (%add_14,), kwargs = {})\n",
      "    %transformer_layer_7_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.7.attn.attn_mask]\n",
      "    %transformer_layer_7_attn_mha : [num_users=2] = call_module[target=transformer.layer.7.attn.mha](args = (%transformer_layer_7_ln_1, %transformer_layer_7_ln_1, %transformer_layer_7_ln_1), kwargs = {attn_mask: %transformer_layer_7_attn_attn_mask, need_weights: False})\n",
      "    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 0), kwargs = {})\n",
      "    %getitem_15 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 1), kwargs = {})\n",
      "    %add_15 : [num_users=2] = call_function[target=operator.add](args = (%add_14, %getitem_14), kwargs = {})\n",
      "    %transformer_layer_7_ln_2 : [num_users=1] = call_module[target=transformer.layer.7.ln_2](args = (%add_15,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_fc](args = (%transformer_layer_7_ln_2,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_active : [num_users=1] = call_module[target=transformer.layer.7.mlp.active](args = (%transformer_layer_7_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_proj](args = (%transformer_layer_7_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.7.mlp.dropout](args = (%transformer_layer_7_mlp_c_proj,), kwargs = {})\n",
      "    %add_16 : [num_users=2] = call_function[target=operator.add](args = (%add_15, %transformer_layer_7_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_8_ln_1 : [num_users=1] = call_module[target=transformer.layer.8.ln_1](args = (%add_16,), kwargs = {})\n",
      "    %transformer_layer_8_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.8.attn.attn_mask]\n",
      "    %transformer_layer_8_attn_mha : [num_users=2] = call_module[target=transformer.layer.8.attn.mha](args = (%transformer_layer_8_ln_1, %transformer_layer_8_ln_1, %transformer_layer_8_ln_1), kwargs = {attn_mask: %transformer_layer_8_attn_attn_mask, need_weights: False})\n",
      "    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 0), kwargs = {})\n",
      "    %getitem_17 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 1), kwargs = {})\n",
      "    %add_17 : [num_users=2] = call_function[target=operator.add](args = (%add_16, %getitem_16), kwargs = {})\n",
      "    %transformer_layer_8_ln_2 : [num_users=1] = call_module[target=transformer.layer.8.ln_2](args = (%add_17,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_fc](args = (%transformer_layer_8_ln_2,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_active : [num_users=1] = call_module[target=transformer.layer.8.mlp.active](args = (%transformer_layer_8_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_proj](args = (%transformer_layer_8_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.8.mlp.dropout](args = (%transformer_layer_8_mlp_c_proj,), kwargs = {})\n",
      "    %add_18 : [num_users=2] = call_function[target=operator.add](args = (%add_17, %transformer_layer_8_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_9_ln_1 : [num_users=1] = call_module[target=transformer.layer.9.ln_1](args = (%add_18,), kwargs = {})\n",
      "    %transformer_layer_9_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.9.attn.attn_mask]\n",
      "    %transformer_layer_9_attn_mha : [num_users=2] = call_module[target=transformer.layer.9.attn.mha](args = (%transformer_layer_9_ln_1, %transformer_layer_9_ln_1, %transformer_layer_9_ln_1), kwargs = {attn_mask: %transformer_layer_9_attn_attn_mask, need_weights: False})\n",
      "    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 0), kwargs = {})\n",
      "    %getitem_19 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 1), kwargs = {})\n",
      "    %add_19 : [num_users=2] = call_function[target=operator.add](args = (%add_18, %getitem_18), kwargs = {})\n",
      "    %transformer_layer_9_ln_2 : [num_users=1] = call_module[target=transformer.layer.9.ln_2](args = (%add_19,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_fc](args = (%transformer_layer_9_ln_2,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_active : [num_users=1] = call_module[target=transformer.layer.9.mlp.active](args = (%transformer_layer_9_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_proj](args = (%transformer_layer_9_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.9.mlp.dropout](args = (%transformer_layer_9_mlp_c_proj,), kwargs = {})\n",
      "    %add_20 : [num_users=2] = call_function[target=operator.add](args = (%add_19, %transformer_layer_9_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_10_ln_1 : [num_users=1] = call_module[target=transformer.layer.10.ln_1](args = (%add_20,), kwargs = {})\n",
      "    %transformer_layer_10_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.10.attn.attn_mask]\n",
      "    %transformer_layer_10_attn_mha : [num_users=2] = call_module[target=transformer.layer.10.attn.mha](args = (%transformer_layer_10_ln_1, %transformer_layer_10_ln_1, %transformer_layer_10_ln_1), kwargs = {attn_mask: %transformer_layer_10_attn_attn_mask, need_weights: False})\n",
      "    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 0), kwargs = {})\n",
      "    %getitem_21 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 1), kwargs = {})\n",
      "    %add_21 : [num_users=2] = call_function[target=operator.add](args = (%add_20, %getitem_20), kwargs = {})\n",
      "    %transformer_layer_10_ln_2 : [num_users=1] = call_module[target=transformer.layer.10.ln_2](args = (%add_21,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_fc](args = (%transformer_layer_10_ln_2,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_active : [num_users=1] = call_module[target=transformer.layer.10.mlp.active](args = (%transformer_layer_10_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_proj](args = (%transformer_layer_10_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.10.mlp.dropout](args = (%transformer_layer_10_mlp_c_proj,), kwargs = {})\n",
      "    %add_22 : [num_users=2] = call_function[target=operator.add](args = (%add_21, %transformer_layer_10_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_11_ln_1 : [num_users=1] = call_module[target=transformer.layer.11.ln_1](args = (%add_22,), kwargs = {})\n",
      "    %transformer_layer_11_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.11.attn.attn_mask]\n",
      "    %transformer_layer_11_attn_mha : [num_users=2] = call_module[target=transformer.layer.11.attn.mha](args = (%transformer_layer_11_ln_1, %transformer_layer_11_ln_1, %transformer_layer_11_ln_1), kwargs = {attn_mask: %transformer_layer_11_attn_attn_mask, need_weights: False})\n",
      "    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 0), kwargs = {})\n",
      "    %getitem_23 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 1), kwargs = {})\n",
      "    %add_23 : [num_users=2] = call_function[target=operator.add](args = (%add_22, %getitem_22), kwargs = {})\n",
      "    %transformer_layer_11_ln_2 : [num_users=1] = call_module[target=transformer.layer.11.ln_2](args = (%add_23,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_fc](args = (%transformer_layer_11_ln_2,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_active : [num_users=1] = call_module[target=transformer.layer.11.mlp.active](args = (%transformer_layer_11_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_proj](args = (%transformer_layer_11_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.11.mlp.dropout](args = (%transformer_layer_11_mlp_c_proj,), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=operator.add](args = (%add_23, %transformer_layer_11_mlp_dropout), kwargs = {})\n",
      "    %transformer_ln_out : [num_users=1] = call_module[target=transformer.ln_out](args = (%add_24,), kwargs = {})\n",
      "    %linear_out : [num_users=3] = call_module[target=linear_out](args = (%transformer_ln_out,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%linear_out, -1), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%linear_out, -1, %size), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_method[target=view](args = (%targets, -1), kwargs = {})\n",
      "    %cross_entropy : [num_users=1] = call_function[target=torch.nn.functional.cross_entropy](args = (%view, %view_1), kwargs = {weight: None, size_average: None, ignore_index: -1, reduce: None, reduction: mean, label_smoothing: 0.0})\n",
      "    return (linear_out, cross_entropy)\n"
     ]
    }
   ],
   "source": [
    "print(processed_gpt.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 274, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"<eval_with_key>.21 from /home/mabot004/eki-transformer-dev/qtransform/qtransform/model/gpt.py:126 in forward\", line 159, in forward\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "AttributeError: 'NoneType' object has no attribute 'view'\n",
      "\n",
      "Call using an FX-traced Module, line 159 of the traced Module's generated forward function:\n",
      "    view = linear_out.view(-1, size);  size = None\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    cross_entropy = torch.nn.functional.cross_entropy(view, view_1, weight = None, size_average = None, ignore_index = -1, reduce = None, reduction = 'mean', label_smoothing = 0.0);  view = view_1 = None\n",
      "\n",
      "    return (linear_out, cross_entropy)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocessed_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:678\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:282\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_with_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m topmost_framesummary\u001b[38;5;241m.\u001b[39mfilename:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_WrappedCall\u001b[38;5;241m.\u001b[39m_generate_error_message(topmost_framesummary),\n\u001b[1;32m    281\u001b[0m           file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \n",
    "processed_gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mini transformer test\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm1d(64)\n",
    "        self.linear = torch.nn.Linear(256,256)\n",
    "        self.mha = torch.nn.MultiheadAttention(256, 2, batch_first = True)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        x, = self.mha(x,x,x, need_weights = False)\n",
    "        return self.softmax(x)\n",
    "#maybe the order of layers being called in the forward pass\n",
    "nodes = list(torch.fx.symbolic_trace(Layer()).graph.nodes)\n",
    "#nodes[1].args.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;28;01mas\u001b[39;00m brevitas_symbolic_trace\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m brevitas_symbolic_trace(Layer())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mMergeBatchNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/base.py:64\u001b[0m, in \u001b[0;36mUntilFixedPointGraphTransform.apply\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_model: GraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_converged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_model\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_model\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/fixed_point.py:112\u001b[0m, in \u001b[0;36mMergeBatchNorm.is_converged\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m layer \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[0;32m--> 112\u001b[0m bn \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#merging happens here\u001b[39;00m\n\u001b[1;32m    115\u001b[0m merge_bn(layer, bn, get_output_channel_dim(layer))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/nn/utils.py:37\u001b[0m, in \u001b[0;36mmerge_bn\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     35\u001b[0m mul_factor, add_factor \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     36\u001b[0m out_ch_weight_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mweight, output_channel_dim)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmul_factor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ch_weight_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     out_ch_bias_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mbias, channel_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "from brevitas.fx.brevitas_tracer import symbolic_trace as brevitas_symbolic_trace\n",
    "model = brevitas_symbolic_trace(Layer())\n",
    "MergeBatchNorm().apply(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.graph.utils import matches_module_pattern\n",
    "def is_converged(graph_model: GraphModule):\n",
    "        named_modules = dict(graph_model.named_modules())\n",
    "        for node in graph_model.graph.nodes:\n",
    "            for pattern in MergeBatchNorm.DEFAULT_PATTERNS:\n",
    "                if matches_module_pattern(pattern, node, named_modules):\n",
    "                    #potential error since node.args is a list containing tuples\n",
    "                    if len(node.args[0].users) > 1:\n",
    "                        continue\n",
    "                    layer = named_modules[node.args[0].target]\n",
    "                    bn = named_modules[node.target]\n",
    "                    #!!!! check if batchnorm is merged\n",
    "                    print(f'{layer}\\n{bn}')\n",
    "                    return -1\n",
    "                    #merging happens here\n",
    "                    merge_bn(layer, bn, get_output_channel_dim(layer))\n",
    "                    \n",
    "                    \n",
    "                    node.replace_all_uses_with(node.args[0])\n",
    "                    graph_model.graph.erase_node(node)\n",
    "                    del_module(graph_model, node.target)\n",
    "        graph_model.recompile()\n",
    "        graph_model.graph.lint()\n",
    "        return graph_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#not merged as it does not fit the patterns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m is_converged(model)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#not merged as it does not fit the patterns\n",
    "#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\n",
    "out = is_converged(model)\n",
    "assert out == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(64)\n",
    "for i in range(200):\n",
    "    bn(torch.randn(12,64,256))\n",
    "bn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = bn.running_mean\n",
    "bn(torch.randn(12,64,256))\n",
    "assert bn.running_mean.equal(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if merge_bn works with CustomBatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qtransform.quantization.quant_bn import CustomBatchNorm1d\n",
    "bn = torch.nn.BatchNorm1d(64)\n",
    "custom_bn = CustomBatchNorm1d(64)\n",
    "#some dummy values to make sure that weights and biases change\n",
    "bn.running_mean = torch.ones(64) - torch.randn(64)\n",
    "bn.running_var = torch.ones(64) - torch.randn(64)\n",
    "\n",
    "from brevitas.nn.utils import merge_bn\n",
    "merge_bn(layer=custom_bn, bn=bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3404, -0.2761,  3.7941,  0.7905,  2.1353,  2.2053,  1.4461,  0.5135,\n",
       "        -0.4257,  0.8978,  1.4404, -0.7516,  1.1945,  1.9911,  2.3707, -0.6096,\n",
       "         0.0236,  1.8476,  0.5917,  1.5003,  1.9186, -0.2508,  2.4155,  0.1087,\n",
       "         1.5802,  0.4824, -0.1152,  0.7643,  0.6944,  0.4769, -0.3377,  0.8450,\n",
       "         0.9591,  1.1728,  0.9848,  1.2537,  0.5495, -0.7393,  0.0230, -0.2664,\n",
       "         0.4891,  0.2027,  1.0734,  2.6990, -0.0286,  2.5351,  0.8789,  1.6932,\n",
       "         2.8497,  3.5356,  1.7280,  0.0991,  1.4326, -0.3052,  0.3073,  0.5748,\n",
       "         0.9516, -0.2996,  0.6284,  0.7912,  0.1061, -0.4269,  1.1209,  1.1498])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our pathced quantized batchnorm from brevitas is somewhat accurate to regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1788.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "bn = torch.nn.BatchNorm1d(64).to(device=device)\n",
    "bn_quant = qnn.quant_bn.BatchNorm1dToQuantScaleBias(64).to(device=device)\n",
    "x = torch.randn(12,64,256).to(device=device)\n",
    "chunks = x.chunk(12)\n",
    "out = bn(x)[0]\n",
    "#brevitas batchnorm cannot do batches and output shape is weird\n",
    "batches = torch.zeros(12).to(chunks[0].device)\n",
    "x = qnn.QuantIdentity().to(device=device)(x)\n",
    "out_quant = bn_quant(chunks[0]).squeeze(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNQUANTIZED: \n",
      "tensor([[ 0.3717,  0.2948,  0.6914,  ..., -0.5079, -0.7649, -0.9057],\n",
      "        [-1.1437, -1.2347,  1.8752,  ..., -0.0156, -2.3635,  0.8861],\n",
      "        [ 1.1881,  1.2583, -0.9642,  ..., -1.1639,  0.0508,  0.0539],\n",
      "        ...,\n",
      "        [-0.6113, -0.8401, -0.0366,  ..., -0.5180,  0.3790,  0.0749],\n",
      "        [-0.9505,  0.7660, -1.6229,  ...,  1.6479,  0.2074,  0.7169],\n",
      "        [ 0.9767, -0.1875, -0.4149,  ..., -0.1294, -1.6123, -0.1703]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "QUANTIZED: \n",
      "tensor([[ 0.3721,  0.2958,  0.6894,  ..., -0.5011, -0.7562, -0.8960],\n",
      "        [-1.1435, -1.2319,  1.7912,  ..., -0.0469, -2.3293,  0.8297],\n",
      "        [ 1.1743,  1.2445, -0.9773,  ..., -1.1769,  0.0374,  0.0405],\n",
      "        ...,\n",
      "        [-0.6131, -0.8418, -0.0385,  ..., -0.5198,  0.3772,  0.0731],\n",
      "        [-0.9197,  0.8135, -1.5987,  ...,  1.7039,  0.2494,  0.7639],\n",
      "        [ 0.9729, -0.2014, -0.4307,  ..., -0.1428, -1.6385, -0.1841]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "DIFFERENCE: tensor([[ 0.0004,  0.0009, -0.0020,  ...,  0.0068,  0.0087,  0.0097],\n",
      "        [ 0.0002,  0.0027, -0.0840,  ..., -0.0313,  0.0342, -0.0564],\n",
      "        [-0.0138, -0.0139, -0.0131,  ..., -0.0130, -0.0134, -0.0134],\n",
      "        ...,\n",
      "        [-0.0017, -0.0017, -0.0018,  ..., -0.0018, -0.0019, -0.0018],\n",
      "        [ 0.0308,  0.0475,  0.0242,  ...,  0.0561,  0.0421,  0.0470],\n",
      "        [-0.0038, -0.0139, -0.0159,  ..., -0.0134, -0.0262, -0.0137]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "\n",
      "DIFFERENCE IN EACH WORD: tensor([ 3.1339e-03, -3.0899e-02, -1.3413e-02, -2.1600e-02,  1.5570e-02,\n",
      "         2.8991e-02, -3.2045e-02, -5.1656e-05,  1.7745e-03,  3.2011e-03,\n",
      "        -2.7319e-02, -7.9251e-03,  6.6058e-03,  8.1829e-03, -2.0327e-03,\n",
      "        -1.0973e-02, -1.7600e-02, -1.7283e-02, -1.5700e-02,  7.0697e-03,\n",
      "        -3.9060e-03,  3.0110e-03, -4.8141e-03,  1.2407e-02, -5.6185e-03,\n",
      "         1.0696e-02, -2.4323e-02,  3.4389e-02,  1.9996e-02,  1.5830e-02,\n",
      "         1.4918e-02,  1.6751e-02,  1.7072e-02, -4.1615e-04, -2.2849e-02,\n",
      "        -7.2231e-03,  1.0098e-03, -4.1827e-02,  2.7265e-03,  1.8969e-02,\n",
      "         1.1765e-03, -1.2866e-02,  6.4182e-03, -3.2022e-03, -6.6441e-03,\n",
      "         1.4052e-02, -3.2617e-02,  1.2806e-02,  4.4178e-02,  5.5695e-03,\n",
      "        -1.3799e-02, -2.0418e-02, -1.4273e-02,  1.5722e-02, -2.3369e-02,\n",
      "        -1.0008e-02, -3.3145e-02, -3.1600e-02,  7.8381e-03, -9.2784e-04,\n",
      "         1.7084e-02, -1.8157e-03,  4.0906e-02, -1.2321e-02], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'UNQUANTIZED: \\n{out}\\n\\nQUANTIZED: \\n{out_quant}\\n\\nDIFFERENCE: {out_quant - out}\\n\\nDIFFERENCE IN EACH WORD: {(out_quant - out).mean(dim=-1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "def chunk_examples(examples, column_name = \"text\"):\n",
    "    \"\"\"\n",
    "            Splits the text of each row into chunks of length chunk_length. \n",
    "            It is useful when samples have large amounts of text in order to perform\n",
    "            mapping in batches more efficiently.\n",
    "            Parts of the code are inspired from: https://huggingface.co/docs/datasets/process#split-long-examples\n",
    "\n",
    "            Returns: {\"chunks\" : chunks} \n",
    "            where chunks is a list of sentences split after chunk_length characters.\n",
    "    \"\"\"\n",
    "    print(len(examples[column_name]))\n",
    "    chunks = []\n",
    "    CHUNK_LENGTH = 100\n",
    "    for sentence in examples[column_name]:\n",
    "        chunks += [sentence[i:i + CHUNK_LENGTH] for i in range(0, len(sentence), CHUNK_LENGTH)]\n",
    "    return {\"chunks\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rotten_tomatoes = datasets.load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3241354ff7a046da89ff96c2b476a262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0272c3f0ec1440ab9a0ee9978d2de20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015f1c7ba4614a79b1d494bdc28e78a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.map(\n",
    "    lambda x: print(len(x)), \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns =  [\"text\", \"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a7c3040474008af1f1e26ba40c502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6dfa7c3af4814906c6a2e638c9758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a5e8fe2e3422c9543dbcd20170543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.map(\n",
    "    chunk_examples, \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns = [\"text\", \"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 13952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 1745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67873aac4d4844b6b5e244497ecee27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787e32691604c79bf5c47d831e4c20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7e83fe7e04818ad2d150a3c8842e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.select_columns(\"text\").map(\n",
    "    chunk_examples, \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns = [\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13952, 1761, 1745]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(dataset_splits[x]) for x in dataset_splits.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a0b40bbc4347e994ee8e46c92a6188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e1e144e4ab4b66822342880ef832ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209dfcfcb8e0428aa348b438f9cd9125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "def encode_batch(batch):\n",
    "    batch_ids = [tokenizer.encode_ordinary(x) for x in batch[\"chunks\"]]\n",
    "    return {\"input_ids\": batch_ids, \"length\": [len(x) for x in batch_ids]}\n",
    "\n",
    "tokens = dataset_splits.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    batch_size = 1000, \n",
    "    remove_columns = \"chunks\",\n",
    "    desc=f'tokenizing from chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 13952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 1745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for split in tokens:\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = datasets.load_dataset(\"rotten_tomatoes\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-raw-v1', 'wikitext-103-v1', 'wikitext-2-raw-v1', 'wikitext-2-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-raw-v1')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikitext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1852\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1852\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:373\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:525\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    524\u001b[0m     example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    531\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo config specified, defaulting to the single config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilder_config\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-raw-v1', 'wikitext-103-v1', 'wikitext-2-raw-v1', 'wikitext-2-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-raw-v1')`"
     ]
    }
   ],
   "source": [
    "datasets.load_dataset(\"wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikitext = datasets.load_dataset(path = \"wikitext\", name = 'wikitext-103-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikitext[\"something_else\"] = wikitext.pop(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[43mwikitext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2357\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/dataset_dict.py:59\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     63\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "split = wikitext[\"train\"].train_test_split(test_size=0.05, seed=2357, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1711282\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 90068\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801350"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(split.num_rows.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our benchmarking script is faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "cfg_args =\n",
    " dict(\n",
    "    n_layer = 6,\n",
    "    n_head = 6,\n",
    "    n_embd = 384,\n",
    "    n_positions = 256,\n",
    "    n_inner = 384 * 4,\n",
    "    embd_pdrop  = 0.2,\n",
    "    attn_pdrop  = 0.2,\n",
    "    return_dict=False)\n",
    "hf_cfg = transformers.GPT2Config(**cfg_args)\n",
    "#gpt2_hf = transformers.GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gpt2_hf.__class__.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(gpt2_hf, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 384)\n",
       "  (wpe): Embedding(256, 384)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-5): 6 x GPT2Block(\n",
       "      (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ckpt = torch.load(\"test\")\n",
    "test_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Child(transformers.GPT2LMHeadModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[-0.0034, -0.0187, -0.0299,  ...,  0.0290, -0.0106,  0.0040],\n",
       "                      [ 0.0177, -0.0137, -0.0025,  ...,  0.0308,  0.0007, -0.0168],\n",
       "                      [-0.0055, -0.0008, -0.0139,  ..., -0.0303,  0.0064, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0141, -0.0004,  0.0128,  ...,  0.0113, -0.0113,  0.0003],\n",
       "                      [-0.0067,  0.0219,  0.0267,  ...,  0.0151, -0.0265,  0.0033],\n",
       "                      [-0.0253,  0.0030,  0.0068,  ..., -0.0190, -0.0058,  0.0044]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[ 0.0052, -0.0059,  0.0178,  ..., -0.0193,  0.0101, -0.0164],\n",
       "                      [ 0.0018, -0.0123, -0.0057,  ...,  0.0150,  0.0277, -0.0257],\n",
       "                      [ 0.0007,  0.0007,  0.0088,  ..., -0.0067, -0.0308, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0210,  0.0215,  0.0132,  ..., -0.0068,  0.0099,  0.0164],\n",
       "                      [-0.0072,  0.0063,  0.0115,  ...,  0.0059,  0.0046,  0.0108],\n",
       "                      [-0.0447,  0.0051,  0.0168,  ...,  0.0208, -0.0169, -0.0483]])),\n",
       "             ('transformer.h.0.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.0.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.attn.c_attn.weight',\n",
       "              tensor([[ 3.7256e-02, -5.7076e-03,  2.1655e-02,  ...,  1.8264e-02,\n",
       "                        1.3201e-02, -1.9385e-02],\n",
       "                      [-6.7957e-05, -1.8852e-02,  7.9665e-03,  ...,  1.3440e-02,\n",
       "                        1.0891e-02,  5.7556e-03],\n",
       "                      [ 5.4535e-03, -1.5160e-03,  9.2361e-03,  ..., -3.8783e-02,\n",
       "                       -1.8917e-02, -1.1734e-02],\n",
       "                      ...,\n",
       "                      [-3.4595e-02,  1.2689e-02,  2.9578e-02,  ..., -4.6025e-03,\n",
       "                       -3.1350e-02, -3.6187e-02],\n",
       "                      [-2.5702e-02, -3.5004e-02,  1.8364e-03,  ..., -1.3925e-02,\n",
       "                       -2.7972e-05,  2.6473e-03],\n",
       "                      [-4.5683e-03,  2.9496e-02,  1.8853e-02,  ...,  1.5937e-02,\n",
       "                        1.5186e-02,  1.1989e-02]])),\n",
       "             ('transformer.h.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.0.attn.c_proj.weight',\n",
       "              tensor([[ 0.0055, -0.0034, -0.0067,  ...,  0.0014,  0.0020, -0.0057],\n",
       "                      [-0.0002, -0.0059,  0.0052,  ..., -0.0030,  0.0109, -0.0065],\n",
       "                      [-0.0031,  0.0036,  0.0030,  ...,  0.0069,  0.0045,  0.0043],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0040,  0.0044,  ..., -0.0031, -0.0023,  0.0013],\n",
       "                      [ 0.0106,  0.0030, -0.0040,  ...,  0.0153, -0.0046, -0.0050],\n",
       "                      [ 0.0072,  0.0032,  0.0001,  ..., -0.0068,  0.0027,  0.0007]])),\n",
       "             ('transformer.h.0.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.0.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0186, -0.0079, -0.0231,  ..., -0.0090,  0.0049,  0.0112],\n",
       "                      [ 0.0156, -0.0173, -0.0105,  ...,  0.0267,  0.0082, -0.0050],\n",
       "                      [ 0.0273, -0.0035, -0.0220,  ..., -0.0092, -0.0019,  0.0142],\n",
       "                      ...,\n",
       "                      [ 0.0021, -0.0123,  0.0199,  ...,  0.0210, -0.0058, -0.0423],\n",
       "                      [-0.0195, -0.0100, -0.0093,  ...,  0.0105, -0.0299,  0.0084],\n",
       "                      [-0.0104, -0.0248,  0.0136,  ..., -0.0042,  0.0111, -0.0076]])),\n",
       "             ('transformer.h.0.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0087, -0.0064, -0.0061,  ..., -0.0067,  0.0004,  0.0036],\n",
       "                      [-0.0068, -0.0013,  0.0011,  ...,  0.0020,  0.0101,  0.0011],\n",
       "                      [ 0.0062, -0.0091, -0.0032,  ..., -0.0127, -0.0057,  0.0004],\n",
       "                      ...,\n",
       "                      [-0.0076,  0.0023,  0.0027,  ...,  0.0054,  0.0015,  0.0022],\n",
       "                      [-0.0053, -0.0024,  0.0004,  ..., -0.0049,  0.0007,  0.0042],\n",
       "                      [ 0.0012,  0.0019,  0.0059,  ..., -0.0032,  0.0003, -0.0041]])),\n",
       "             ('transformer.h.0.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.1.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0029, -0.0306,  0.0492,  ...,  0.0486,  0.0332,  0.0024],\n",
       "                      [-0.0033, -0.0012, -0.0049,  ..., -0.0322,  0.0253, -0.0083],\n",
       "                      [ 0.0086, -0.0182, -0.0170,  ...,  0.0254, -0.0096, -0.0118],\n",
       "                      ...,\n",
       "                      [-0.0220,  0.0099,  0.0235,  ...,  0.0151,  0.0061, -0.0051],\n",
       "                      [-0.0122, -0.0283,  0.0018,  ..., -0.0091,  0.0160, -0.0282],\n",
       "                      [-0.0208, -0.0002,  0.0039,  ...,  0.0543,  0.0217, -0.0133]])),\n",
       "             ('transformer.h.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.1.attn.c_proj.weight',\n",
       "              tensor([[-0.0073,  0.0082,  0.0022,  ..., -0.0110,  0.0025,  0.0077],\n",
       "                      [-0.0098,  0.0032, -0.0065,  ..., -0.0005, -0.0067,  0.0077],\n",
       "                      [-0.0028,  0.0072, -0.0022,  ...,  0.0061, -0.0002,  0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0030,  0.0010, -0.0073,  ...,  0.0025,  0.0019, -0.0107],\n",
       "                      [-0.0032,  0.0107,  0.0109,  ...,  0.0029, -0.0058,  0.0050],\n",
       "                      [-0.0080, -0.0057,  0.0047,  ..., -0.0043, -0.0038, -0.0035]])),\n",
       "             ('transformer.h.1.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.1.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0253,  0.0231,  0.0146,  ...,  0.0167,  0.0093, -0.0117],\n",
       "                      [ 0.0079,  0.0042, -0.0050,  ..., -0.0054, -0.0204, -0.0164],\n",
       "                      [ 0.0128,  0.0015, -0.0173,  ...,  0.0259,  0.0095,  0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0068, -0.0010,  0.0471,  ...,  0.0024,  0.0270, -0.0161],\n",
       "                      [ 0.0144,  0.0135, -0.0132,  ..., -0.0020,  0.0230, -0.0305],\n",
       "                      [-0.0324, -0.0064,  0.0334,  ...,  0.0325,  0.0288, -0.0200]])),\n",
       "             ('transformer.h.1.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.1.mlp.c_proj.weight',\n",
       "              tensor([[-0.0074, -0.0062, -0.0133,  ..., -0.0036,  0.0060,  0.0039],\n",
       "                      [-0.0006, -0.0023,  0.0043,  ...,  0.0049, -0.0170,  0.0050],\n",
       "                      [ 0.0014,  0.0034,  0.0025,  ...,  0.0023,  0.0062, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0065, -0.0053, -0.0045,  ..., -0.0061, -0.0049,  0.0043],\n",
       "                      [ 0.0011, -0.0056, -0.0066,  ...,  0.0026,  0.0019,  0.0039],\n",
       "                      [ 0.0068, -0.0038, -0.0037,  ..., -0.0021, -0.0046,  0.0030]])),\n",
       "             ('transformer.h.1.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.2.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.attn.c_attn.weight',\n",
       "              tensor([[-0.0011,  0.0015, -0.0151,  ..., -0.0326,  0.0005,  0.0041],\n",
       "                      [-0.0157, -0.0099,  0.0021,  ..., -0.0436, -0.0027,  0.0200],\n",
       "                      [ 0.0155,  0.0178, -0.0090,  ..., -0.0163, -0.0040,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0092,  0.0178, -0.0356,  ..., -0.0079, -0.0109,  0.0206],\n",
       "                      [-0.0144, -0.0005, -0.0236,  ..., -0.0450, -0.0089,  0.0058],\n",
       "                      [-0.0036, -0.0079,  0.0304,  ..., -0.0107,  0.0041,  0.0448]])),\n",
       "             ('transformer.h.2.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.2.attn.c_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0022,  0.0071,  ..., -0.0022, -0.0165, -0.0026],\n",
       "                      [ 0.0044,  0.0104,  0.0084,  ..., -0.0051,  0.0007, -0.0053],\n",
       "                      [-0.0053,  0.0002,  0.0077,  ..., -0.0005, -0.0017,  0.0043],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0057, -0.0060,  ...,  0.0025,  0.0005, -0.0043],\n",
       "                      [ 0.0010,  0.0026, -0.0043,  ..., -0.0096,  0.0031,  0.0100],\n",
       "                      [-0.0029,  0.0070,  0.0037,  ...,  0.0048,  0.0018,  0.0038]])),\n",
       "             ('transformer.h.2.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.2.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0315,  0.0344,  0.0009,  ..., -0.0097, -0.0009,  0.0069],\n",
       "                      [ 0.0277, -0.0016,  0.0410,  ..., -0.0333, -0.0150, -0.0140],\n",
       "                      [ 0.0352,  0.0106, -0.0107,  ..., -0.0025,  0.0187,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0449,  0.0141,  0.0310,  ...,  0.0090,  0.0016, -0.0281],\n",
       "                      [ 0.0204,  0.0066, -0.0013,  ...,  0.0184, -0.0160, -0.0169],\n",
       "                      [-0.0097,  0.0511,  0.0376,  ..., -0.0138, -0.0158, -0.0105]])),\n",
       "             ('transformer.h.2.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.2.mlp.c_proj.weight',\n",
       "              tensor([[-0.0015, -0.0046, -0.0029,  ...,  0.0009, -0.0026,  0.0063],\n",
       "                      [-0.0103,  0.0028, -0.0013,  ...,  0.0018,  0.0034, -0.0127],\n",
       "                      [-0.0063, -0.0044,  0.0020,  ..., -0.0087,  0.0043,  0.0091],\n",
       "                      ...,\n",
       "                      [ 0.0037, -0.0074,  0.0025,  ..., -0.0035,  0.0074, -0.0052],\n",
       "                      [-0.0033, -0.0035, -0.0060,  ...,  0.0044, -0.0033, -0.0029],\n",
       "                      [-0.0109,  0.0094,  0.0010,  ..., -0.0081,  0.0031, -0.0030]])),\n",
       "             ('transformer.h.2.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.3.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.attn.c_attn.weight',\n",
       "              tensor([[ 0.0231, -0.0003,  0.0229,  ...,  0.0518, -0.0003, -0.0184],\n",
       "                      [-0.0087,  0.0082,  0.0264,  ...,  0.0022, -0.0101, -0.0039],\n",
       "                      [ 0.0020,  0.0252, -0.0072,  ...,  0.0155,  0.0498,  0.0139],\n",
       "                      ...,\n",
       "                      [-0.0324, -0.0272, -0.0113,  ..., -0.0341, -0.0159, -0.0178],\n",
       "                      [-0.0347,  0.0082, -0.0143,  ..., -0.0131,  0.0047, -0.0249],\n",
       "                      [-0.0255,  0.0048,  0.0283,  ...,  0.0296, -0.0156,  0.0051]])),\n",
       "             ('transformer.h.3.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.3.attn.c_proj.weight',\n",
       "              tensor([[ 0.0015,  0.0085, -0.0011,  ...,  0.0015, -0.0030, -0.0037],\n",
       "                      [ 0.0030,  0.0064,  0.0003,  ...,  0.0150,  0.0033,  0.0081],\n",
       "                      [ 0.0059, -0.0078,  0.0050,  ..., -0.0024,  0.0047,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0108, -0.0008,  0.0020,  ..., -0.0033, -0.0005, -0.0044],\n",
       "                      [ 0.0033,  0.0042,  0.0002,  ..., -0.0029, -0.0072, -0.0088],\n",
       "                      [ 0.0019,  0.0048, -0.0003,  ..., -0.0069, -0.0043,  0.0092]])),\n",
       "             ('transformer.h.3.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.3.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.mlp.c_fc.weight',\n",
       "              tensor([[-0.0032,  0.0245,  0.0143,  ...,  0.0049,  0.0247,  0.0229],\n",
       "                      [ 0.0091, -0.0084, -0.0130,  ...,  0.0214, -0.0070, -0.0259],\n",
       "                      [ 0.0163,  0.0213,  0.0448,  ..., -0.0186, -0.0068,  0.0389],\n",
       "                      ...,\n",
       "                      [ 0.0025,  0.0045,  0.0160,  ...,  0.0050, -0.0262, -0.0125],\n",
       "                      [-0.0352, -0.0087, -0.0294,  ..., -0.0090, -0.0193, -0.0132],\n",
       "                      [-0.0088,  0.0146, -0.0012,  ...,  0.0272,  0.0011,  0.0364]])),\n",
       "             ('transformer.h.3.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.3.mlp.c_proj.weight',\n",
       "              tensor([[-4.6481e-03,  1.7276e-03,  6.5588e-03,  ...,  5.9177e-03,\n",
       "                        6.1392e-04, -8.3755e-03],\n",
       "                      [ 6.0920e-06,  5.6963e-04, -4.0137e-03,  ..., -4.3385e-03,\n",
       "                        1.2971e-02,  2.9268e-03],\n",
       "                      [-2.0432e-04, -6.1765e-03,  1.7358e-03,  ..., -6.7340e-03,\n",
       "                       -4.2019e-03,  2.4833e-03],\n",
       "                      ...,\n",
       "                      [-1.0416e-02,  2.1218e-03,  5.0554e-03,  ..., -4.8121e-03,\n",
       "                        4.1022e-03, -1.1739e-02],\n",
       "                      [-1.6727e-03, -5.9600e-03, -4.6248e-03,  ..., -6.9834e-04,\n",
       "                       -1.2813e-03,  2.7497e-03],\n",
       "                      [ 3.1807e-03, -2.7290e-03, -4.0406e-04,  ...,  5.0236e-03,\n",
       "                        3.1861e-03,  5.6393e-04]])),\n",
       "             ('transformer.h.3.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.4.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.attn.c_attn.weight',\n",
       "              tensor([[-0.0512, -0.0201,  0.0244,  ...,  0.0025, -0.0251,  0.0160],\n",
       "                      [ 0.0165,  0.0148, -0.0101,  ...,  0.0132,  0.0243,  0.0126],\n",
       "                      [-0.0276, -0.0339,  0.0122,  ..., -0.0051, -0.0143,  0.0240],\n",
       "                      ...,\n",
       "                      [-0.0311, -0.0140, -0.0056,  ...,  0.0076,  0.0414, -0.0005],\n",
       "                      [ 0.0043,  0.0368, -0.0142,  ..., -0.0159, -0.0248, -0.0092],\n",
       "                      [ 0.0179, -0.0112,  0.0143,  ...,  0.0363,  0.0040, -0.0308]])),\n",
       "             ('transformer.h.4.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.4.attn.c_proj.weight',\n",
       "              tensor([[-0.0038,  0.0040, -0.0114,  ...,  0.0142, -0.0011,  0.0030],\n",
       "                      [ 0.0027,  0.0003, -0.0029,  ..., -0.0066, -0.0062,  0.0026],\n",
       "                      [-0.0050, -0.0049,  0.0053,  ..., -0.0016,  0.0013, -0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0018, -0.0069,  0.0041,  ...,  0.0055,  0.0012, -0.0004],\n",
       "                      [ 0.0014, -0.0029, -0.0058,  ...,  0.0019, -0.0011, -0.0053],\n",
       "                      [ 0.0055, -0.0055, -0.0031,  ..., -0.0052, -0.0034,  0.0001]])),\n",
       "             ('transformer.h.4.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.4.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.mlp.c_fc.weight',\n",
       "              tensor([[-0.0723,  0.0188, -0.0038,  ...,  0.0066,  0.0168, -0.0271],\n",
       "                      [ 0.0071, -0.0258,  0.0235,  ...,  0.0219, -0.0212,  0.0034],\n",
       "                      [-0.0140,  0.0407, -0.0035,  ..., -0.0048, -0.0412, -0.0049],\n",
       "                      ...,\n",
       "                      [-0.0241, -0.0222, -0.0407,  ..., -0.0141,  0.0281,  0.0465],\n",
       "                      [-0.0203,  0.0182, -0.0042,  ..., -0.0216, -0.0052, -0.0103],\n",
       "                      [ 0.0189,  0.0079,  0.0197,  ...,  0.0155, -0.0180, -0.0184]])),\n",
       "             ('transformer.h.4.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.4.mlp.c_proj.weight',\n",
       "              tensor([[-0.0035,  0.0121, -0.0011,  ...,  0.0021, -0.0018,  0.0055],\n",
       "                      [-0.0042, -0.0052,  0.0045,  ..., -0.0051,  0.0037, -0.0032],\n",
       "                      [-0.0013,  0.0007, -0.0008,  ...,  0.0021, -0.0059,  0.0150],\n",
       "                      ...,\n",
       "                      [-0.0031, -0.0083,  0.0003,  ...,  0.0037, -0.0003, -0.0057],\n",
       "                      [-0.0052,  0.0024,  0.0038,  ...,  0.0010, -0.0028, -0.0038],\n",
       "                      [-0.0023, -0.0085,  0.0018,  ..., -0.0059,  0.0041, -0.0020]])),\n",
       "             ('transformer.h.4.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.5.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.attn.c_attn.weight',\n",
       "              tensor([[ 0.0065, -0.0118, -0.0069,  ...,  0.0399, -0.0374, -0.0160],\n",
       "                      [-0.0375, -0.0067,  0.0145,  ...,  0.0032, -0.0245,  0.0069],\n",
       "                      [-0.0346,  0.0048,  0.0188,  ..., -0.0069,  0.0224,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0006, -0.0080,  ..., -0.0500, -0.0187,  0.0186],\n",
       "                      [-0.0150, -0.0017, -0.0027,  ..., -0.0125,  0.0406, -0.0197],\n",
       "                      [ 0.0335,  0.0320,  0.0153,  ...,  0.0122, -0.0076,  0.0345]])),\n",
       "             ('transformer.h.5.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.5.attn.c_proj.weight',\n",
       "              tensor([[-4.4844e-03,  1.3152e-03, -3.4908e-03,  ...,  1.0732e-02,\n",
       "                        9.2446e-03,  1.2282e-02],\n",
       "                      [ 3.3737e-03, -7.5732e-03,  1.8066e-03,  ..., -4.9796e-03,\n",
       "                        6.8677e-03,  3.7573e-03],\n",
       "                      [-1.8318e-03, -1.2586e-03,  1.8209e-03,  ..., -5.5451e-03,\n",
       "                       -8.9172e-03,  6.3112e-03],\n",
       "                      ...,\n",
       "                      [-5.5658e-03, -3.5368e-03, -1.1666e-02,  ...,  1.0554e-03,\n",
       "                       -2.2873e-03, -6.9975e-03],\n",
       "                      [ 4.1811e-03,  5.0578e-03,  1.0962e-02,  ...,  2.1638e-03,\n",
       "                       -2.0211e-03, -4.3049e-03],\n",
       "                      [ 2.3144e-03, -3.2657e-05,  9.2031e-03,  ..., -1.0683e-02,\n",
       "                        6.9108e-03, -4.3645e-03]])),\n",
       "             ('transformer.h.5.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.5.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.mlp.c_fc.weight',\n",
       "              tensor([[-0.0117,  0.0228,  0.0034,  ...,  0.0170,  0.0223, -0.0357],\n",
       "                      [ 0.0081,  0.0052,  0.0225,  ..., -0.0146, -0.0184,  0.0036],\n",
       "                      [ 0.0248, -0.0153,  0.0256,  ..., -0.0205, -0.0131, -0.0301],\n",
       "                      ...,\n",
       "                      [-0.0035, -0.0120, -0.0019,  ..., -0.0074, -0.0068, -0.0182],\n",
       "                      [ 0.0216,  0.0306,  0.0594,  ...,  0.0326,  0.0241,  0.0049],\n",
       "                      [-0.0488,  0.0018,  0.0011,  ..., -0.0068, -0.0214, -0.0073]])),\n",
       "             ('transformer.h.5.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.5.mlp.c_proj.weight',\n",
       "              tensor([[-7.6916e-03, -7.8936e-03,  8.9618e-03,  ..., -5.4573e-03,\n",
       "                        4.4367e-04,  8.7935e-04],\n",
       "                      [-1.1022e-03, -6.3099e-04, -7.0274e-03,  ...,  3.2784e-03,\n",
       "                        1.2046e-02, -6.4660e-03],\n",
       "                      [-3.8424e-03, -7.7052e-03,  1.0153e-02,  ..., -6.8431e-04,\n",
       "                       -3.5021e-03,  2.7638e-03],\n",
       "                      ...,\n",
       "                      [ 5.7108e-04, -2.3094e-03,  3.5360e-03,  ...,  1.7454e-03,\n",
       "                       -2.6458e-03, -9.5012e-03],\n",
       "                      [ 5.4334e-03,  1.5826e-03,  9.0219e-05,  ..., -9.1541e-04,\n",
       "                       -7.5399e-04, -3.9756e-03],\n",
       "                      [-3.7548e-03,  1.6471e-03,  2.4763e-03,  ..., -1.7112e-03,\n",
       "                       -1.0687e-03, -3.5235e-03]])),\n",
       "             ('transformer.h.5.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_f.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.ln_f.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-0.0034, -0.0187, -0.0299,  ...,  0.0290, -0.0106,  0.0040],\n",
       "                      [ 0.0177, -0.0137, -0.0025,  ...,  0.0308,  0.0007, -0.0168],\n",
       "                      [-0.0055, -0.0008, -0.0139,  ..., -0.0303,  0.0064, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0141, -0.0004,  0.0128,  ...,  0.0113, -0.0113,  0.0003],\n",
       "                      [-0.0067,  0.0219,  0.0267,  ...,  0.0151, -0.0265,  0.0033],\n",
       "                      [-0.0253,  0.0030,  0.0068,  ..., -0.0190, -0.0058,  0.0044]]))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Child(hf_cfg)\n",
    "test.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "ids = torch.Tensor(tokenizer.encode_ordinary(\"hello world this is a test.\")).unsqueeze(dim=0).to(dtype=torch.long)\n",
    "labels = torch.Tensor(tokenizer.encode_ordinary(\"and the words are the labels.\")).unsqueeze(dim=0).to(dtype=torch.long)\n",
    "out = test(ids, labels=labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_labels = test(ids, labels=None, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_labels.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one extra word appended to input sequence\n",
    "out.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50304 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "config = GPTConfig(block_size=256, \n",
    "                   vocab_size=50304, \n",
    "                   n_layer=6, \n",
    "                   n_head=6, \n",
    "                   n_embd=384, \n",
    "                   dropout=0.2, \n",
    "                   bias=True, \n",
    "                   flash=False, \n",
    "                   transformer_active_func='ReLU', \n",
    "                   norm_layer='BatchNorm', \n",
    "                   single_output=False, \n",
    "                   use_weight_tying=True, \n",
    "                   custom_ln=False)\n",
    "gpt_qt = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, loss = gpt_qt(ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50304])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[-0.0480, -0.0163, -0.0034,  ...,  0.0231,  0.0399,  0.0322],\n",
       "                      [-0.0097, -0.0217, -0.0156,  ..., -0.0078,  0.0002, -0.0066],\n",
       "                      [ 0.0230, -0.0403,  0.0179,  ...,  0.0164,  0.0006, -0.0172],\n",
       "                      ...,\n",
       "                      [-0.0109, -0.0091, -0.0256,  ...,  0.0080, -0.0260,  0.0130],\n",
       "                      [-0.0125,  0.0159,  0.0072,  ..., -0.0025,  0.0050, -0.0154],\n",
       "                      [ 0.0171, -0.0222, -0.0161,  ..., -0.0008,  0.0018, -0.0086]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[ 0.0174, -0.0088, -0.0167,  ..., -0.0200, -0.0163, -0.0277],\n",
       "                      [-0.0096, -0.0254,  0.0014,  ...,  0.0017,  0.0487, -0.0294],\n",
       "                      [ 0.0069,  0.0073,  0.0126,  ...,  0.0032, -0.0227, -0.0187],\n",
       "                      ...,\n",
       "                      [-0.0023, -0.0291, -0.0032,  ...,  0.0274, -0.0156, -0.0070],\n",
       "                      [ 0.0055, -0.0096,  0.0016,  ..., -0.0173, -0.0041, -0.0089],\n",
       "                      [ 0.0360,  0.0023,  0.0097,  ...,  0.0243, -0.0060,  0.0181]])),\n",
       "             ('transformer.layer.0.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.0.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.ln_1.running_mean',\n",
       "              tensor([ 3.3417e-04,  6.4249e-05, -1.7564e-04,  2.5978e-04,  2.5669e-04,\n",
       "                       1.6849e-04, -1.4753e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.0.ln_1.running_var',\n",
       "              tensor([0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.0.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.0.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.0.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.ln_2.running_mean',\n",
       "              tensor([-0.0004, -0.0003, -0.0004, -0.0002, -0.0002, -0.0001, -0.0002,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])),\n",
       "             ('transformer.layer.0.ln_2.running_var',\n",
       "              tensor([0.9993, 0.9988, 0.9991, 0.9983, 0.9990, 0.9987, 0.9983, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.0.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.0.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.0.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0327,  0.0343,  0.0564,  ...,  0.0229, -0.0261,  0.0239],\n",
       "                      [ 0.0284,  0.0331,  0.0526,  ...,  0.0373, -0.0095,  0.0103],\n",
       "                      [-0.0476, -0.0240,  0.0137,  ..., -0.0208,  0.0301, -0.0612],\n",
       "                      ...,\n",
       "                      [ 0.0134, -0.0077,  0.0258,  ...,  0.0385, -0.0429, -0.0619],\n",
       "                      [ 0.0381, -0.0488, -0.0014,  ..., -0.0604, -0.0477,  0.0044],\n",
       "                      [ 0.0415,  0.0069,  0.0520,  ..., -0.0221, -0.0173, -0.0237]])),\n",
       "             ('transformer.layer.0.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.weight',\n",
       "              tensor([[ 2.1368e-02, -2.2444e-02, -2.8786e-02,  ...,  1.8028e-02,\n",
       "                        1.1564e-02,  3.3639e-03],\n",
       "                      [-4.1301e-02, -5.4199e-03, -2.4576e-02,  ...,  1.3486e-02,\n",
       "                        8.5759e-03, -7.5457e-03],\n",
       "                      [-2.9626e-03,  3.9741e-03, -7.6519e-03,  ..., -1.0736e-02,\n",
       "                        2.3561e-03, -2.3734e-03],\n",
       "                      ...,\n",
       "                      [-3.1809e-05, -2.1282e-03,  3.5508e-02,  ..., -4.9362e-03,\n",
       "                       -8.6604e-03,  5.7153e-04],\n",
       "                      [-8.6202e-04, -9.6304e-03, -1.6080e-03,  ...,  1.3915e-02,\n",
       "                       -2.2579e-02, -3.2772e-02],\n",
       "                      [ 4.6639e-02,  4.8244e-02, -1.3898e-02,  ..., -9.7132e-03,\n",
       "                        3.5791e-02,  2.9523e-02]])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.c_proj.weight',\n",
       "              tensor([[-8.8311e-05,  1.6462e-03,  5.8459e-03,  ...,  9.6361e-03,\n",
       "                       -2.6982e-03,  3.1887e-03],\n",
       "                      [ 9.4704e-04, -5.6763e-03,  2.1477e-03,  ...,  4.9899e-04,\n",
       "                       -4.8711e-03, -8.6017e-03],\n",
       "                      [ 1.1691e-02,  5.3716e-03,  1.4181e-03,  ...,  1.2279e-03,\n",
       "                        8.7907e-03, -2.5356e-03],\n",
       "                      ...,\n",
       "                      [ 1.3080e-02,  1.7922e-03,  9.3596e-03,  ..., -8.9694e-04,\n",
       "                       -1.5338e-02,  1.5187e-03],\n",
       "                      [ 5.9816e-03,  2.4333e-03,  2.5251e-03,  ...,  8.4732e-03,\n",
       "                        1.7239e-03, -4.8735e-04],\n",
       "                      [ 3.4273e-03, -9.2383e-03, -1.6062e-03,  ..., -4.3751e-03,\n",
       "                       -8.9931e-03,  4.8042e-04]])),\n",
       "             ('transformer.layer.0.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.c_attn.weight',\n",
       "              tensor([[-0.0118,  0.0227, -0.0423,  ...,  0.0115,  0.0236,  0.0075],\n",
       "                      [ 0.0292,  0.0084, -0.0145,  ...,  0.0029,  0.0230,  0.0192],\n",
       "                      [-0.0137,  0.0051,  0.0246,  ...,  0.0166, -0.0197, -0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0324,  0.0406, -0.0125,  ..., -0.0089, -0.0051,  0.0079],\n",
       "                      [-0.0163, -0.0042,  0.0137,  ..., -0.0037, -0.0065, -0.0092],\n",
       "                      [-0.0274, -0.0067, -0.0224,  ...,  0.0032, -0.0147, -0.0043]])),\n",
       "             ('transformer.layer.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0093,  0.0113, -0.0179,  ..., -0.0357, -0.0128,  0.0239],\n",
       "                      [ 0.0137,  0.0200,  0.0104,  ..., -0.0175, -0.0169,  0.0014],\n",
       "                      [ 0.0439,  0.0074,  0.0100,  ..., -0.0390, -0.0191,  0.0054],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0137,  0.0092,  ...,  0.0397,  0.0182,  0.0173],\n",
       "                      [-0.0237, -0.0023, -0.0200,  ..., -0.0291,  0.0013,  0.0143],\n",
       "                      [ 0.0125,  0.0181, -0.0033,  ...,  0.0211, -0.0030, -0.0077]])),\n",
       "             ('transformer.layer.0.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0005,  0.0124, -0.0022,  ...,  0.0001,  0.0004,  0.0162],\n",
       "                      [ 0.0057, -0.0037,  0.0039,  ...,  0.0041,  0.0058,  0.0061],\n",
       "                      [ 0.0059,  0.0022, -0.0021,  ...,  0.0015, -0.0082,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0090,  0.0022,  ...,  0.0014, -0.0010,  0.0127],\n",
       "                      [-0.0065, -0.0014,  0.0015,  ..., -0.0022,  0.0050, -0.0054],\n",
       "                      [ 0.0042,  0.0085, -0.0024,  ...,  0.0042,  0.0028,  0.0001]])),\n",
       "             ('transformer.layer.0.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.1.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_1.running_mean',\n",
       "              tensor([-2.5211e-04,  1.9818e-04, -6.1746e-05,  8.6410e-05,  6.2564e-04,\n",
       "                       4.9416e-04,  5.8872e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.1.ln_1.running_var',\n",
       "              tensor([1.0044, 0.9998, 1.0009, 1.0008, 1.0014, 1.0036, 0.9994, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.1.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.1.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.1.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_2.running_mean',\n",
       "              tensor([-3.6266e-04, -1.4068e-04,  4.0739e-06, -1.9030e-04, -7.2249e-05,\n",
       "                      -1.3234e-04, -1.6590e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.1.ln_2.running_var',\n",
       "              tensor([1.0010, 1.0008, 1.0004, 1.0007, 1.0006, 1.0006, 1.0006, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.1.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.1.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.1.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0042,  0.0067,  0.0036,  ...,  0.0401, -0.0550,  0.0283],\n",
       "                      [-0.0450,  0.0131,  0.0262,  ...,  0.0413,  0.0182, -0.0394],\n",
       "                      [-0.0393, -0.0571,  0.0320,  ..., -0.0116,  0.0435,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0399,  0.0382,  0.0399,  ...,  0.0246,  0.0235, -0.0016],\n",
       "                      [-0.0271, -0.0486,  0.0069,  ..., -0.0101,  0.0258,  0.0368],\n",
       "                      [-0.0372,  0.0099,  0.0414,  ..., -0.0529,  0.0298,  0.0019]])),\n",
       "             ('transformer.layer.1.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0300, -0.0486, -0.0398,  ...,  0.0125, -0.0130, -0.0273],\n",
       "                      [ 0.0231,  0.0073, -0.0064,  ..., -0.0162,  0.0433,  0.0037],\n",
       "                      [-0.0179, -0.0231, -0.0141,  ...,  0.0115, -0.0200,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0223, -0.0098, -0.0025,  ...,  0.0256,  0.0094,  0.0003],\n",
       "                      [ 0.0054,  0.0143,  0.0203,  ..., -0.0119, -0.0181, -0.0241],\n",
       "                      [ 0.0185,  0.0150, -0.0101,  ...,  0.0087, -0.0183, -0.0031]])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.c_proj.weight',\n",
       "              tensor([[ 4.4580e-03, -2.5867e-03, -2.4178e-03,  ..., -3.9064e-03,\n",
       "                       -1.0177e-03,  5.6169e-04],\n",
       "                      [ 9.0653e-03,  3.4227e-03, -1.2240e-03,  ..., -1.7433e-02,\n",
       "                       -1.2715e-02,  3.7085e-03],\n",
       "                      [-8.5925e-05,  1.2168e-02, -2.4109e-03,  ..., -6.6085e-03,\n",
       "                       -9.7568e-03,  4.3224e-03],\n",
       "                      ...,\n",
       "                      [-1.3515e-02,  3.5075e-03,  2.8530e-04,  ..., -2.6873e-03,\n",
       "                        1.0414e-03,  5.0562e-03],\n",
       "                      [ 7.8588e-04,  1.7284e-03,  6.5941e-03,  ...,  3.1925e-03,\n",
       "                       -6.9360e-03, -1.3075e-02],\n",
       "                      [-9.2083e-03,  5.3821e-03, -1.0619e-02,  ...,  8.0773e-03,\n",
       "                       -1.0429e-02, -7.1923e-03]])),\n",
       "             ('transformer.layer.1.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0280,  0.0090, -0.0085,  ...,  0.0041, -0.0172, -0.0110],\n",
       "                      [ 0.0031, -0.0004, -0.0204,  ...,  0.0195,  0.0061,  0.0273],\n",
       "                      [ 0.0144, -0.0163,  0.0090,  ..., -0.0367,  0.0121, -0.0257],\n",
       "                      ...,\n",
       "                      [ 0.0253, -0.0274,  0.0218,  ...,  0.0171,  0.0104,  0.0388],\n",
       "                      [-0.0268,  0.0104, -0.0334,  ...,  0.0065, -0.0011, -0.0149],\n",
       "                      [-0.0183,  0.0222, -0.0180,  ...,  0.0269, -0.0175,  0.0134]])),\n",
       "             ('transformer.layer.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_fc.weight',\n",
       "              tensor([[-0.0298,  0.0238,  0.0399,  ...,  0.0321,  0.0268, -0.0263],\n",
       "                      [-0.0072, -0.0047,  0.0203,  ...,  0.0150, -0.0259,  0.0284],\n",
       "                      [-0.0248,  0.0131, -0.0172,  ..., -0.0035,  0.0540,  0.0424],\n",
       "                      ...,\n",
       "                      [ 0.0059, -0.0083, -0.0415,  ...,  0.0101,  0.0067,  0.0094],\n",
       "                      [ 0.0467, -0.0055, -0.0013,  ...,  0.0315,  0.0131, -0.0411],\n",
       "                      [-0.0124, -0.0042,  0.0094,  ..., -0.0200,  0.0217, -0.0260]])),\n",
       "             ('transformer.layer.1.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_proj.weight',\n",
       "              tensor([[-0.0010,  0.0043, -0.0047,  ..., -0.0163,  0.0028,  0.0060],\n",
       "                      [ 0.0086,  0.0051,  0.0056,  ...,  0.0022,  0.0052, -0.0079],\n",
       "                      [ 0.0052, -0.0021,  0.0062,  ..., -0.0028, -0.0090, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0016,  0.0117,  ..., -0.0059,  0.0060,  0.0035],\n",
       "                      [ 0.0069,  0.0048, -0.0037,  ..., -0.0073, -0.0083, -0.0095],\n",
       "                      [-0.0119,  0.0066,  0.0090,  ..., -0.0060,  0.0140, -0.0137]])),\n",
       "             ('transformer.layer.1.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.2.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_1.running_mean',\n",
       "              tensor([-6.2409e-04, -1.7642e-04, -8.0269e-04, -1.3574e-05, -1.9546e-04,\n",
       "                      -1.3271e-04,  6.8201e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.2.ln_1.running_var',\n",
       "              tensor([1.0024, 1.0019, 1.0015, 1.0014, 1.0004, 1.0010, 1.0031, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.2.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.2.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.2.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_2.running_mean',\n",
       "              tensor([-1.5982e-04,  1.5704e-04, -2.7822e-05, -2.3739e-04, -1.8987e-04,\n",
       "                      -9.8857e-05, -1.7925e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.2.ln_2.running_var',\n",
       "              tensor([1.0004, 1.0008, 1.0004, 1.0003, 1.0011, 1.0004, 1.0011, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.2.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.2.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.2.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0280,  0.0288, -0.0137,  ..., -0.0568,  0.0052,  0.0175],\n",
       "                      [-0.0511,  0.0403, -0.0347,  ..., -0.0531,  0.0129, -0.0051],\n",
       "                      [-0.0304,  0.0528,  0.0500,  ..., -0.0569,  0.0508, -0.0595],\n",
       "                      ...,\n",
       "                      [ 0.0072,  0.0170, -0.0242,  ...,  0.0179, -0.0411,  0.0358],\n",
       "                      [ 0.0251, -0.0500, -0.0565,  ..., -0.0108,  0.0080,  0.0332],\n",
       "                      [ 0.0139,  0.0485,  0.0387,  ...,  0.0458, -0.0067,  0.0022]])),\n",
       "             ('transformer.layer.2.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0011, -0.0009,  0.0147,  ...,  0.0019, -0.0018, -0.0227],\n",
       "                      [-0.0169,  0.0020,  0.0301,  ...,  0.0180,  0.0083,  0.0201],\n",
       "                      [-0.0306,  0.0140, -0.0068,  ...,  0.0108,  0.0201, -0.0155],\n",
       "                      ...,\n",
       "                      [-0.0144,  0.0132,  0.0021,  ..., -0.0264,  0.0014, -0.0027],\n",
       "                      [ 0.0346, -0.0126,  0.0287,  ...,  0.0112,  0.0516, -0.0136],\n",
       "                      [-0.0137,  0.0032, -0.0233,  ..., -0.0073,  0.0307, -0.0182]])),\n",
       "             ('transformer.layer.2.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.c_proj.weight',\n",
       "              tensor([[-0.0021, -0.0029, -0.0014,  ..., -0.0051,  0.0055,  0.0099],\n",
       "                      [-0.0029, -0.0024, -0.0089,  ...,  0.0077, -0.0039,  0.0070],\n",
       "                      [-0.0036, -0.0054, -0.0069,  ...,  0.0112,  0.0108,  0.0085],\n",
       "                      ...,\n",
       "                      [-0.0091, -0.0038,  0.0045,  ...,  0.0042,  0.0034,  0.0002],\n",
       "                      [-0.0147, -0.0010, -0.0075,  ..., -0.0149,  0.0038, -0.0090],\n",
       "                      [ 0.0048,  0.0035,  0.0193,  ...,  0.0072, -0.0047,  0.0067]])),\n",
       "             ('transformer.layer.2.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.c_attn.weight',\n",
       "              tensor([[-0.0125,  0.0097,  0.0008,  ...,  0.0282,  0.0103,  0.0067],\n",
       "                      [-0.0013,  0.0204, -0.0152,  ..., -0.0013,  0.0063, -0.0268],\n",
       "                      [ 0.0238, -0.0226, -0.0007,  ...,  0.0134,  0.0055,  0.0358],\n",
       "                      ...,\n",
       "                      [ 0.0145,  0.0171,  0.0464,  ...,  0.0094, -0.0182,  0.0050],\n",
       "                      [-0.0097, -0.0013,  0.0193,  ...,  0.0087,  0.0102,  0.0118],\n",
       "                      [ 0.0136,  0.0063, -0.0082,  ..., -0.0067,  0.0136, -0.0067]])),\n",
       "             ('transformer.layer.2.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.mlp.c_fc.weight',\n",
       "              tensor([[ 1.0232e-02, -2.7240e-02,  7.1662e-03,  ..., -1.7358e-03,\n",
       "                        1.5643e-02, -9.1390e-03],\n",
       "                      [ 1.0988e-02, -3.3004e-02,  7.6915e-03,  ..., -1.6483e-02,\n",
       "                       -2.1697e-02, -3.6452e-02],\n",
       "                      [-1.7921e-05,  1.0917e-02, -8.4004e-03,  ..., -4.0824e-02,\n",
       "                        4.8517e-03,  4.0391e-02],\n",
       "                      ...,\n",
       "                      [-1.4068e-02,  3.7376e-03,  1.9370e-03,  ..., -1.9469e-02,\n",
       "                        3.3798e-02,  4.5801e-03],\n",
       "                      [ 4.0186e-03,  4.5541e-03,  2.2524e-02,  ...,  1.3113e-02,\n",
       "                        2.9036e-02,  1.6006e-02],\n",
       "                      [ 1.0697e-02,  4.2475e-02,  1.6489e-02,  ...,  1.7569e-02,\n",
       "                       -5.5972e-03, -1.9042e-02]])),\n",
       "             ('transformer.layer.2.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.mlp.c_proj.weight',\n",
       "              tensor([[-0.0049, -0.0085, -0.0072,  ..., -0.0013,  0.0046,  0.0025],\n",
       "                      [ 0.0050,  0.0042, -0.0096,  ..., -0.0112, -0.0046,  0.0026],\n",
       "                      [ 0.0045, -0.0121,  0.0038,  ..., -0.0086,  0.0028, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0116,  0.0028,  ..., -0.0032, -0.0044,  0.0009],\n",
       "                      [ 0.0085, -0.0063, -0.0022,  ...,  0.0024,  0.0072,  0.0031],\n",
       "                      [-0.0045, -0.0088, -0.0091,  ..., -0.0094, -0.0008, -0.0040]])),\n",
       "             ('transformer.layer.2.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.3.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_1.running_mean',\n",
       "              tensor([ 3.2829e-04,  7.9684e-05, -3.9067e-04,  3.0088e-04,  1.1539e-03,\n",
       "                       7.9151e-04,  1.5546e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.3.ln_1.running_var',\n",
       "              tensor([1.0032, 1.0001, 0.9981, 1.0005, 1.0018, 1.0012, 1.0020, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.3.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.3.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.3.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_2.running_mean',\n",
       "              tensor([ 5.1885e-04, -1.7353e-05,  6.4138e-05,  4.6098e-05, -2.4954e-05,\n",
       "                       9.7934e-05,  6.0665e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.3.ln_2.running_var',\n",
       "              tensor([1.0016, 0.9996, 1.0002, 1.0009, 1.0013, 1.0005, 1.0008, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.3.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.3.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.3.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0389,  0.0547, -0.0046,  ...,  0.0490,  0.0367,  0.0450],\n",
       "                      [ 0.0443,  0.0137, -0.0537,  ...,  0.0608,  0.0118,  0.0162],\n",
       "                      [ 0.0591, -0.0548,  0.0054,  ..., -0.0152,  0.0218,  0.0065],\n",
       "                      ...,\n",
       "                      [-0.0008, -0.0357, -0.0606,  ...,  0.0283, -0.0586, -0.0312],\n",
       "                      [ 0.0575,  0.0403,  0.0562,  ...,  0.0497,  0.0530,  0.0142],\n",
       "                      [-0.0390, -0.0097, -0.0130,  ..., -0.0046,  0.0108,  0.0455]])),\n",
       "             ('transformer.layer.3.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0132,  0.0290,  0.0266,  ...,  0.0073, -0.0051, -0.0174],\n",
       "                      [-0.0094,  0.0216,  0.0131,  ...,  0.0116,  0.0095,  0.0058],\n",
       "                      [-0.0235, -0.0242, -0.0092,  ...,  0.0021,  0.0518,  0.0051],\n",
       "                      ...,\n",
       "                      [-0.0250,  0.0070,  0.0064,  ...,  0.0292,  0.0006,  0.0115],\n",
       "                      [ 0.0176,  0.0137,  0.0110,  ...,  0.0090,  0.0021,  0.0092],\n",
       "                      [-0.0235, -0.0317,  0.0083,  ..., -0.0108,  0.0442, -0.0281]])),\n",
       "             ('transformer.layer.3.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.c_proj.weight',\n",
       "              tensor([[ 0.0021, -0.0033, -0.0059,  ...,  0.0064,  0.0006,  0.0078],\n",
       "                      [-0.0030, -0.0047, -0.0054,  ...,  0.0047, -0.0081, -0.0058],\n",
       "                      [ 0.0034, -0.0056, -0.0048,  ..., -0.0025,  0.0039,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0068, -0.0001,  ...,  0.0034, -0.0058, -0.0063],\n",
       "                      [-0.0027, -0.0015, -0.0072,  ..., -0.0043,  0.0033,  0.0024],\n",
       "                      [-0.0027, -0.0006, -0.0066,  ...,  0.0054, -0.0063, -0.0006]])),\n",
       "             ('transformer.layer.3.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.c_attn.weight',\n",
       "              tensor([[-0.0051, -0.0094,  0.0362,  ...,  0.0102, -0.0018, -0.0106],\n",
       "                      [-0.0062, -0.0011,  0.0008,  ...,  0.0213,  0.0162, -0.0060],\n",
       "                      [-0.0139,  0.0381, -0.0222,  ..., -0.0003,  0.0173,  0.0157],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0160, -0.0207,  ..., -0.0105, -0.0099, -0.0205],\n",
       "                      [ 0.0029,  0.0304,  0.0013,  ...,  0.0479, -0.0108,  0.0265],\n",
       "                      [-0.0042, -0.0067, -0.0162,  ..., -0.0108, -0.0162, -0.0116]])),\n",
       "             ('transformer.layer.3.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0037,  0.0058,  0.0034,  ...,  0.0187,  0.0023, -0.0295],\n",
       "                      [ 0.0090,  0.0105, -0.0267,  ...,  0.0088,  0.0205,  0.0167],\n",
       "                      [ 0.0158, -0.0439, -0.0019,  ...,  0.0045, -0.0308,  0.0172],\n",
       "                      ...,\n",
       "                      [ 0.0096, -0.0316, -0.0298,  ..., -0.0002, -0.0119,  0.0110],\n",
       "                      [-0.0121,  0.0056,  0.0060,  ..., -0.0046, -0.0116,  0.0021],\n",
       "                      [-0.0257,  0.0124,  0.0161,  ...,  0.0255, -0.0344,  0.0059]])),\n",
       "             ('transformer.layer.3.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0119, -0.0018, -0.0029,  ..., -0.0068,  0.0053, -0.0110],\n",
       "                      [-0.0005, -0.0049,  0.0059,  ...,  0.0024, -0.0062, -0.0051],\n",
       "                      [-0.0056, -0.0045,  0.0056,  ...,  0.0018, -0.0039,  0.0032],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0023,  0.0012,  ...,  0.0036, -0.0039, -0.0066],\n",
       "                      [-0.0110,  0.0023, -0.0014,  ...,  0.0039,  0.0045,  0.0019],\n",
       "                      [ 0.0022,  0.0035, -0.0093,  ..., -0.0049,  0.0031,  0.0131]])),\n",
       "             ('transformer.layer.3.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.4.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_1.running_mean',\n",
       "              tensor([ 3.2635e-04, -2.8256e-04,  1.0005e-04, -5.3350e-04, -2.8570e-04,\n",
       "                       3.2666e-05,  1.6110e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.4.ln_1.running_var',\n",
       "              tensor([1.0018, 1.0015, 1.0011, 1.0029, 1.0002, 1.0034, 1.0009, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.4.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.4.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.4.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_2.running_mean',\n",
       "              tensor([ 2.7777e-04, -7.5782e-06,  4.6470e-04,  1.7742e-04,  1.7507e-04,\n",
       "                       2.4828e-04,  3.0388e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.4.ln_2.running_var',\n",
       "              tensor([1.0014, 1.0008, 1.0011, 1.0003, 1.0001, 1.0001, 1.0002, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.4.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.4.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.4.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0288,  0.0167, -0.0056,  ..., -0.0444, -0.0450, -0.0176],\n",
       "                      [-0.0128, -0.0411, -0.0231,  ...,  0.0228,  0.0208,  0.0347],\n",
       "                      [ 0.0295, -0.0454, -0.0495,  ...,  0.0355,  0.0312,  0.0217],\n",
       "                      ...,\n",
       "                      [-0.0353,  0.0235, -0.0578,  ...,  0.0458,  0.0509, -0.0581],\n",
       "                      [ 0.0560, -0.0616, -0.0558,  ..., -0.0080,  0.0006, -0.0027],\n",
       "                      [ 0.0542, -0.0512,  0.0507,  ..., -0.0287,  0.0274,  0.0274]])),\n",
       "             ('transformer.layer.4.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0025, -0.0269,  0.0087,  ...,  0.0062,  0.0023, -0.0022],\n",
       "                      [-0.0067,  0.0123,  0.0026,  ...,  0.0013,  0.0171,  0.0008],\n",
       "                      [-0.0402, -0.0033,  0.0219,  ...,  0.0130, -0.0149,  0.0257],\n",
       "                      ...,\n",
       "                      [ 0.0193, -0.0064,  0.0281,  ...,  0.0200, -0.0122, -0.0006],\n",
       "                      [ 0.0288, -0.0130, -0.0002,  ..., -0.0119,  0.0167, -0.0388],\n",
       "                      [ 0.0075, -0.0271, -0.0371,  ...,  0.0276,  0.0036,  0.0315]])),\n",
       "             ('transformer.layer.4.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.c_proj.weight',\n",
       "              tensor([[-0.0038,  0.0054,  0.0035,  ...,  0.0027,  0.0002, -0.0078],\n",
       "                      [ 0.0041, -0.0138, -0.0007,  ...,  0.0011,  0.0094, -0.0085],\n",
       "                      [ 0.0023, -0.0053,  0.0041,  ...,  0.0074,  0.0061,  0.0066],\n",
       "                      ...,\n",
       "                      [-0.0043,  0.0004, -0.0061,  ..., -0.0106,  0.0015,  0.0085],\n",
       "                      [-0.0002,  0.0074, -0.0002,  ..., -0.0001,  0.0027,  0.0038],\n",
       "                      [-0.0015, -0.0053, -0.0002,  ...,  0.0065, -0.0018,  0.0041]])),\n",
       "             ('transformer.layer.4.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.c_attn.weight',\n",
       "              tensor([[-2.4835e-02,  1.1955e-02, -6.7136e-02,  ..., -2.4684e-03,\n",
       "                        7.0792e-04, -3.5418e-03],\n",
       "                      [-7.7227e-03,  2.9037e-02,  1.4865e-02,  ...,  2.2977e-03,\n",
       "                        8.7832e-03,  1.9092e-02],\n",
       "                      [ 1.6047e-02,  1.5633e-02, -3.0294e-02,  ..., -2.1802e-02,\n",
       "                        2.0193e-02, -1.7915e-02],\n",
       "                      ...,\n",
       "                      [ 8.3266e-03, -5.5766e-03, -3.4184e-04,  ..., -2.6712e-02,\n",
       "                        1.0628e-02, -7.3286e-03],\n",
       "                      [ 8.0172e-05,  2.5236e-02, -2.4027e-02,  ...,  6.8065e-03,\n",
       "                       -2.4374e-02, -2.3047e-02],\n",
       "                      [ 3.2499e-02,  6.9854e-02, -8.6880e-03,  ..., -2.6728e-02,\n",
       "                        3.2441e-02,  2.2233e-03]])),\n",
       "             ('transformer.layer.4.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0036,  0.0041,  0.0183,  ...,  0.0494, -0.0374,  0.0044],\n",
       "                      [ 0.0034,  0.0164, -0.0052,  ...,  0.0031, -0.0143,  0.0188],\n",
       "                      [ 0.0069,  0.0424, -0.0017,  ..., -0.0451, -0.0181,  0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0040, -0.0087,  0.0132,  ..., -0.0065, -0.0051,  0.0145],\n",
       "                      [ 0.0207,  0.0211,  0.0350,  ...,  0.0248,  0.0468, -0.0027],\n",
       "                      [-0.0557,  0.0222,  0.0033,  ..., -0.0054, -0.0149,  0.0013]])),\n",
       "             ('transformer.layer.4.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0074, -0.0085, -0.0010,  ..., -0.0004,  0.0031, -0.0042],\n",
       "                      [-0.0058, -0.0042, -0.0045,  ..., -0.0058, -0.0037,  0.0018],\n",
       "                      [-0.0002,  0.0104,  0.0128,  ..., -0.0037, -0.0017,  0.0053],\n",
       "                      ...,\n",
       "                      [ 0.0042, -0.0072, -0.0086,  ...,  0.0043,  0.0024, -0.0052],\n",
       "                      [-0.0003, -0.0025, -0.0014,  ..., -0.0001,  0.0011, -0.0049],\n",
       "                      [-0.0029,  0.0064, -0.0024,  ...,  0.0004, -0.0012, -0.0014]])),\n",
       "             ('transformer.layer.4.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.5.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_1.running_mean',\n",
       "              tensor([ 0.0003, -0.0008,  0.0007,  0.0001, -0.0007, -0.0008,  0.0003,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])),\n",
       "             ('transformer.layer.5.ln_1.running_var',\n",
       "              tensor([1.0029, 1.0017, 1.0012, 1.0003, 1.0005, 1.0004, 1.0017, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.5.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.5.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.5.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_2.running_mean',\n",
       "              tensor([ 4.5155e-06,  8.9975e-05, -1.1019e-04,  7.0603e-05, -2.5189e-05,\n",
       "                       1.0571e-04, -2.2604e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.5.ln_2.running_var',\n",
       "              tensor([1.0013, 1.0013, 1.0002, 1.0012, 1.0012, 1.0010, 1.0002, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.5.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.5.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.5.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0552, -0.0202,  0.0418,  ...,  0.0427,  0.0432,  0.0091],\n",
       "                      [-0.0469, -0.0605, -0.0385,  ...,  0.0140,  0.0039, -0.0097],\n",
       "                      [ 0.0084,  0.0558,  0.0508,  ..., -0.0349,  0.0123, -0.0434],\n",
       "                      ...,\n",
       "                      [ 0.0624, -0.0111, -0.0322,  ...,  0.0138, -0.0269, -0.0398],\n",
       "                      [ 0.0615,  0.0210, -0.0127,  ..., -0.0515, -0.0250,  0.0516],\n",
       "                      [ 0.0623, -0.0553, -0.0314,  ...,  0.0050, -0.0415,  0.0302]])),\n",
       "             ('transformer.layer.5.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0180, -0.0069,  0.0089,  ...,  0.0063,  0.0368,  0.0064],\n",
       "                      [-0.0074, -0.0292, -0.0286,  ...,  0.0039, -0.0064, -0.0240],\n",
       "                      [ 0.0066, -0.0291,  0.0019,  ...,  0.0124, -0.0039,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0042, -0.0409, -0.0125,  ..., -0.0162, -0.0184,  0.0189],\n",
       "                      [ 0.0309,  0.0147,  0.0079,  ...,  0.0212, -0.0107,  0.0009],\n",
       "                      [ 0.0326, -0.0485,  0.0142,  ..., -0.0021, -0.0056, -0.0245]])),\n",
       "             ('transformer.layer.5.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.c_proj.weight',\n",
       "              tensor([[-0.0057,  0.0021,  0.0007,  ...,  0.0046, -0.0018,  0.0094],\n",
       "                      [ 0.0014, -0.0021,  0.0039,  ..., -0.0113, -0.0012,  0.0037],\n",
       "                      [ 0.0072, -0.0049, -0.0042,  ..., -0.0209,  0.0017, -0.0101],\n",
       "                      ...,\n",
       "                      [ 0.0070, -0.0042, -0.0073,  ...,  0.0070,  0.0155,  0.0055],\n",
       "                      [ 0.0035,  0.0048, -0.0021,  ...,  0.0034, -0.0064, -0.0069],\n",
       "                      [-0.0011,  0.0077,  0.0017,  ..., -0.0101,  0.0052,  0.0053]])),\n",
       "             ('transformer.layer.5.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.c_attn.weight',\n",
       "              tensor([[ 0.0086,  0.0317, -0.0269,  ...,  0.0051, -0.0214,  0.0003],\n",
       "                      [ 0.0209,  0.0240,  0.0088,  ...,  0.0183,  0.0033, -0.0197],\n",
       "                      [-0.0232,  0.0034, -0.0039,  ..., -0.0017, -0.0101,  0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0107, -0.0164,  0.0018,  ...,  0.0171, -0.0490, -0.0148],\n",
       "                      [ 0.0103,  0.0081,  0.0278,  ...,  0.0082,  0.0091, -0.0013],\n",
       "                      [-0.0006, -0.0294,  0.0118,  ..., -0.0003, -0.0514, -0.0054]])),\n",
       "             ('transformer.layer.5.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0098, -0.0088, -0.0083,  ..., -0.0403,  0.0086,  0.0004],\n",
       "                      [ 0.0138, -0.0084, -0.0006,  ...,  0.0082, -0.0178,  0.0213],\n",
       "                      [-0.0442, -0.0084,  0.0196,  ..., -0.0098, -0.0343, -0.0152],\n",
       "                      ...,\n",
       "                      [-0.0279, -0.0112,  0.0004,  ...,  0.0158, -0.0050,  0.0031],\n",
       "                      [ 0.0146, -0.0206, -0.0212,  ..., -0.0042, -0.0131, -0.0034],\n",
       "                      [-0.0178,  0.0145, -0.0050,  ..., -0.0256, -0.0048,  0.0042]])),\n",
       "             ('transformer.layer.5.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0066,  0.0081, -0.0019,  ..., -0.0005, -0.0044, -0.0004],\n",
       "                      [-0.0058,  0.0005, -0.0085,  ...,  0.0035,  0.0108,  0.0028],\n",
       "                      [ 0.0018, -0.0017,  0.0020,  ...,  0.0001, -0.0069,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0010,  0.0100, -0.0038,  ..., -0.0043,  0.0062,  0.0075],\n",
       "                      [ 0.0003, -0.0051, -0.0006,  ...,  0.0066,  0.0115,  0.0081],\n",
       "                      [-0.0044,  0.0136, -0.0055,  ...,  0.0048, -0.0028,  0.0094]])),\n",
       "             ('transformer.layer.5.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_out.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.ln_out.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_out.running_mean',\n",
       "              tensor([-3.1858e-04, -1.3067e-04, -3.4808e-04,  3.2735e-05, -5.1440e-04,\n",
       "                      -1.3033e-03, -2.0179e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.ln_out.running_var',\n",
       "              tensor([1.0022, 1.0021, 1.0015, 1.0017, 1.0011, 1.0021, 1.0015, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.ln_out.num_batches_tracked', tensor(2)),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[-0.0178,  0.0210,  0.0373,  ...,  0.0198, -0.0191,  0.0009],\n",
       "                      [ 0.0117, -0.0071, -0.0389,  ..., -0.0104, -0.0184, -0.0197],\n",
       "                      [-0.0363,  0.0041, -0.0026,  ..., -0.0074, -0.0342, -0.0105],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0309, -0.0007,  ...,  0.0158,  0.0122,  0.0084],\n",
       "                      [-0.0108, -0.0058, -0.0057,  ...,  0.0121, -0.0143, -0.0220],\n",
       "                      [ 0.0136, -0.0058,  0.0052,  ...,  0.0251, -0.0047,  0.0360]]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_qt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9495eafaba47f881f96c38ca5cf0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_mask = model(ids)\n",
    "with_mask = model(ids, attention_mask=torch.ones(ids.size()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mask.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mask.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.37.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward pass of ONNX models with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-03-12 12:59:49.139317627 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.attn.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139560873 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.mlp.c_fc.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139573813 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.mlp.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139585473 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139598593 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139610803 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139623073 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139634742 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139646122 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139659972 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139671552 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139682982 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.attn.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139694461 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.mlp.c_fc.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139707011 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.mlp.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139718641 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139730371 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139741941 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139754841 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139766430 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139777940 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139789610 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139801160 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139812450 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139829960 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139841369 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139854139 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139865949 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::Add_402 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139878049 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_407 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139889909 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_430 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139903238 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_431 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Load model from /home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx failed:Fatal error: onnx.brevitas:Quant(-1) is not a registered function/op",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m h0 \u001b[38;5;241m=\u001b[39m example_input\n\u001b[1;32m      6\u001b[0m c0 \u001b[38;5;241m=\u001b[39m example_input\n\u001b[0;32m----> 7\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/onnxruntime-1.17.1-py3.10-linux-x86_64.egg/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/onnxruntime-1.17.1-py3.10-linux-x86_64.egg/onnxruntime/capi/onnxruntime_inference_collection.py:472\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[0;32m--> 472\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Load model from /home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx failed:Fatal error: onnx.brevitas:Quant(-1) is not a registered function/op"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "example_input = np.random.randn(8, 3, 3).astype(np.float32)\n",
    "h0 = example_input\n",
    "c0 = example_input\n",
    "ort_session = ort.InferenceSession(\"/home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import qonnx\n",
    "from qonnx.core.onnx_exec import execute_onnx\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "# maybe only do this when it is required, for this howiever is always the case\n",
    "from onnx.shape_inference import infer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "File not found: qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      2\u001b[0m infered_shapes \u001b[38;5;241m=\u001b[39m infer_shapes(model\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelWrapper(infered_shapes)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qonnx-0.3.0-py3.10.egg/qonnx/core/modelwrapper.py:61\u001b[0m, in \u001b[0;36mModelWrapper.__init__\u001b[0;34m(self, onnx_model_proto, make_deepcopy, fix_float64, fix_missing_initializer_valueinfo)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a ModelWrapper instance.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03monnx_model_proto can be either a ModelProto instance, or a string\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mwith the path to a stored .onnx file on disk, or serialized bytes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03minitializers that are missing theirs.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(onnx_model_proto), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_proto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_proto \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(onnx_model_proto)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: File not found: qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper('qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx')    \n",
    "infered_shapes = infer_shapes(model.model)\n",
    "model = ModelWrapper(infered_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "File not found: onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_local \u001b[38;5;241m=\u001b[39m \u001b[43mModelWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      2\u001b[0m infered_shapes \u001b[38;5;241m=\u001b[39m infer_shapes(model_local\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m model_local \u001b[38;5;241m=\u001b[39m ModelWrapper(infered_shapes)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qonnx-0.3.0-py3.10.egg/qonnx/core/modelwrapper.py:61\u001b[0m, in \u001b[0;36mModelWrapper.__init__\u001b[0;34m(self, onnx_model_proto, make_deepcopy, fix_float64, fix_missing_initializer_valueinfo)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a ModelWrapper instance.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03monnx_model_proto can be either a ModelProto instance, or a string\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mwith the path to a stored .onnx file on disk, or serialized bytes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03minitializers that are missing theirs.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(onnx_model_proto), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_proto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_proto \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(onnx_model_proto)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: File not found: onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx"
     ]
    }
   ],
   "source": [
    "model_local = ModelWrapper('onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx')    \n",
    "infered_shapes = infer_shapes(model_local.model)\n",
    "model_local = ModelWrapper(infered_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_local.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"offsets\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.wpe.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.in_proj_bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.out_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.out_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.in_proj_bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.out_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.out_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_480\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_498\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_521\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_526\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_527\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_542\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_560\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_583\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_588\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_589\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_594\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"output\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_402\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_407\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_430\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_431\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper('qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx')    \n",
    "infered_shapes = infer_shapes(model.model)\n",
    "model = ModelWrapper(infered_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dim_value: 1\n",
       ", dim_value: 64\n",
       "]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.input[0].type.tensor_type.shape.dim[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_checkpoint = torch.load(\"/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_wikitext_2024-03-14_10:36:21__epoch:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"quantized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[ 0.6698, -0.0029,  0.2328,  ..., -1.2054,  0.7732,  0.2316],\n",
       "                      [ 0.0911, -0.1520, -0.7457,  ...,  1.8444, -0.9675, -0.9812],\n",
       "                      [-0.0984,  0.6329,  0.2754,  ..., -0.1391,  2.0210,  0.7870],\n",
       "                      ...,\n",
       "                      [-0.9953, -0.0249, -0.7582,  ...,  0.1705,  0.7253, -0.0414],\n",
       "                      [-2.0492,  2.4153, -0.2697,  ...,  0.5925, -1.0206, -0.3261],\n",
       "                      [ 0.7156, -0.3761,  0.4326,  ...,  0.1460,  0.1682,  0.8418]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[-0.0486, -0.5337,  1.3270,  ..., -0.0831,  1.0253,  0.8613],\n",
       "                      [-0.4642, -0.4871,  0.6089,  ..., -0.3660, -0.6049,  0.8867],\n",
       "                      [-1.4405,  1.7084, -0.1334,  ...,  1.6913,  1.5892, -0.9947],\n",
       "                      ...,\n",
       "                      [-0.2837,  0.1455, -0.1076,  ...,  0.7258,  0.2620,  0.2455],\n",
       "                      [-0.1508, -3.0918, -0.1600,  ..., -1.3561,  0.2651,  0.6371],\n",
       "                      [ 1.2966, -0.6439,  1.2848,  ...,  1.0981,  1.1735,  0.3455]])),\n",
       "             ('transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.1572)),\n",
       "             ('transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(5.9887)),\n",
       "             ('transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.2637)),\n",
       "             ('transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.6196)),\n",
       "             ('transformer.layer.0.attn.bias',\n",
       "              tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('transformer.layer.0.attn.mha.q_proj.weight',\n",
       "              tensor([[-0.0353, -0.0486, -0.0943,  ..., -0.0095, -0.0193, -0.0703],\n",
       "                      [-0.0410, -0.1019,  0.0073,  ...,  0.0352,  0.0982, -0.0390],\n",
       "                      [ 0.0166,  0.0541, -0.1053,  ..., -0.0907, -0.0621, -0.0965],\n",
       "                      ...,\n",
       "                      [ 0.0250, -0.0172,  0.0713,  ..., -0.0990, -0.0499,  0.0022],\n",
       "                      [-0.0779, -0.0164, -0.0013,  ...,  0.0542, -0.1028, -0.0861],\n",
       "                      [ 0.0231, -0.0997,  0.0967,  ..., -0.0261,  0.0255, -0.0447]])),\n",
       "             ('transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.k_proj.weight',\n",
       "              tensor([[ 0.0275, -0.0980, -0.0098,  ..., -0.0963, -0.0482, -0.0581],\n",
       "                      [ 0.0344, -0.0248,  0.0696,  ..., -0.0318, -0.0561, -0.1058],\n",
       "                      [-0.0317, -0.0666,  0.0526,  ...,  0.0559,  0.0522, -0.1081],\n",
       "                      ...,\n",
       "                      [-0.0652,  0.0517,  0.0046,  ..., -0.0033, -0.0121,  0.0610],\n",
       "                      [-0.0806,  0.0302,  0.0080,  ...,  0.0048, -0.0848, -0.0531],\n",
       "                      [-0.0925,  0.1031, -0.0675,  ..., -0.0274, -0.0781, -0.0959]])),\n",
       "             ('transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.v_proj.weight',\n",
       "              tensor([[-0.0694, -0.0180, -0.0642,  ...,  0.0686,  0.0647,  0.0616],\n",
       "                      [ 0.1048,  0.0642,  0.0051,  ..., -0.0601,  0.0435,  0.1044],\n",
       "                      [-0.0006, -0.0121,  0.0265,  ..., -0.0450, -0.0708,  0.0399],\n",
       "                      ...,\n",
       "                      [-0.0965, -0.0945, -0.0519,  ...,  0.0976, -0.0706, -0.0664],\n",
       "                      [-0.0814,  0.0656,  0.0814,  ...,  0.0848, -0.0545,  0.0496],\n",
       "                      [ 0.0422,  0.0063, -0.0545,  ...,  0.0334, -0.0487,  0.0121]])),\n",
       "             ('transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0464, -0.0413, -0.0219,  ...,  0.0377,  0.0037,  0.0251],\n",
       "                      [-0.0533,  0.0077,  0.0039,  ..., -0.0615, -0.0360, -0.0285],\n",
       "                      [ 0.0482,  0.0056, -0.0186,  ..., -0.0167, -0.0321,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0234, -0.0076, -0.0600,  ..., -0.0073,  0.0325,  0.0442],\n",
       "                      [ 0.0251, -0.0013,  0.0266,  ..., -0.0392,  0.0052, -0.0356],\n",
       "                      [ 0.0610,  0.0068, -0.0042,  ..., -0.0568, -0.0237,  0.0556]])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(3.3341)),\n",
       "             ('transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(1.1101)),\n",
       "             ('transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(0.3764)),\n",
       "             ('transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2667)),\n",
       "             ('transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2057)),\n",
       "             ('transformer.layer.0.attn.c_proj.weight',\n",
       "              tensor([[ 0.0054,  0.0063,  0.0084,  ...,  0.0132, -0.0037, -0.0191],\n",
       "                      [ 0.0158,  0.0194,  0.0109,  ...,  0.0016, -0.0023, -0.0190],\n",
       "                      [-0.0081, -0.0212, -0.0049,  ..., -0.0082, -0.0038,  0.0013],\n",
       "                      ...,\n",
       "                      [-0.0116, -0.0017, -0.0120,  ...,  0.0028,  0.0094,  0.0177],\n",
       "                      [ 0.0013,  0.0196, -0.0094,  ..., -0.0033, -0.0069,  0.0123],\n",
       "                      [-0.0035, -0.0057, -0.0065,  ..., -0.0120, -0.0158, -0.0079]])),\n",
       "             ('transformer.layer.0.attn.c_proj.bias',\n",
       "              tensor([-3.1958e-03, -2.7938e-03,  4.2348e-03,  6.9851e-04, -4.7405e-04,\n",
       "                      -3.9807e-03, -1.1138e-04, -7.5384e-03, -1.4541e-03, -3.8137e-04,\n",
       "                       4.1511e-03, -5.1171e-04,  2.0046e-03,  9.8140e-03, -1.2610e-03,\n",
       "                       8.6387e-03,  6.5137e-03, -9.7377e-04, -4.7080e-04,  5.3193e-03,\n",
       "                      -3.8207e-03, -2.0345e-03,  3.0142e-03, -2.9232e-04, -5.3041e-03,\n",
       "                       2.0776e-03, -2.1777e-03,  2.4488e-03,  7.2095e-04, -6.2109e-04,\n",
       "                       3.1470e-03, -4.7284e-03,  3.2198e-03, -1.0769e-03, -7.4084e-04,\n",
       "                      -7.5731e-03,  2.5854e-03, -1.8355e-03,  1.9806e-03, -8.9894e-06,\n",
       "                      -5.5757e-03, -3.5957e-03, -1.1352e-03, -2.5207e-03,  6.1156e-03,\n",
       "                      -3.0437e-03, -1.9161e-03, -4.1745e-03, -3.6994e-03,  3.0737e-03,\n",
       "                       2.1440e-03,  1.4744e-03, -6.1409e-04, -2.4526e-03,  7.9957e-03,\n",
       "                       7.2305e-04,  5.4262e-03, -4.7437e-03, -9.0757e-04, -5.3126e-03,\n",
       "                      -4.1813e-03, -3.3698e-03,  5.1947e-04,  8.6721e-03, -3.9753e-04,\n",
       "                       1.6587e-03,  5.4407e-04, -3.6200e-03,  1.0749e-03, -1.1938e-03,\n",
       "                       2.4786e-03, -2.8015e-04,  9.7212e-04, -1.7183e-03, -7.3215e-03,\n",
       "                      -3.7554e-04, -3.9826e-03,  1.8572e-03,  6.4269e-04, -9.3058e-04,\n",
       "                       1.1599e-03, -1.2502e-03,  7.7241e-03,  3.8738e-03,  2.1847e-03,\n",
       "                       4.5545e-03,  7.0674e-04, -4.5284e-04,  2.8594e-03,  6.1084e-03,\n",
       "                       3.5313e-03,  4.5137e-04, -1.8374e-03, -3.9396e-04, -1.8007e-03,\n",
       "                       4.1256e-03, -4.0719e-03, -6.5673e-04, -3.0188e-03, -1.6565e-03,\n",
       "                       1.3387e-03,  8.1945e-03, -1.5822e-03,  3.3044e-03,  9.1487e-04,\n",
       "                      -5.1642e-03, -1.5749e-03, -2.8556e-03,  2.7934e-03, -2.8519e-03,\n",
       "                      -7.0998e-04, -5.9756e-03, -2.8088e-03, -5.3180e-03,  1.5826e-03,\n",
       "                      -2.3094e-03, -7.0455e-04,  1.1676e-02,  4.8946e-03, -3.4066e-03,\n",
       "                      -1.4694e-03,  2.7768e-03,  1.0805e-03,  2.5481e-03, -1.0141e-03,\n",
       "                       6.6496e-03, -7.5653e-05, -5.5602e-04,  2.5933e-03, -5.2210e-03,\n",
       "                      -2.4554e-03,  4.6777e-03, -2.7543e-03,  2.3738e-03,  1.1568e-03,\n",
       "                      -1.6824e-03,  9.0484e-05, -1.9676e-03, -2.5284e-04,  8.3513e-03,\n",
       "                       3.0736e-03, -2.4595e-03,  2.0877e-03, -3.8507e-03,  2.9708e-04,\n",
       "                       3.0518e-03, -2.9975e-03,  4.5871e-03, -2.2610e-03, -3.6620e-03,\n",
       "                      -3.2961e-03,  5.5710e-03, -1.7611e-03, -3.3764e-03, -6.1051e-04,\n",
       "                      -3.0379e-03, -1.3521e-03, -4.5735e-03,  2.2358e-03, -5.7713e-04,\n",
       "                      -2.4444e-03, -4.4614e-03,  4.0799e-04,  2.2649e-03, -9.7010e-04,\n",
       "                       1.2003e-03, -4.2445e-04,  3.6652e-03, -4.1346e-03, -4.8639e-03,\n",
       "                      -3.4428e-03, -2.3577e-04,  1.4184e-03,  4.5522e-03,  4.0986e-03,\n",
       "                      -4.8559e-04, -2.8743e-03,  2.1758e-03,  1.8189e-04, -2.6337e-03,\n",
       "                      -2.7351e-03,  7.4681e-04, -1.3457e-03,  2.4508e-03, -3.9711e-03,\n",
       "                      -9.8269e-04,  1.2864e-03, -1.7708e-03,  7.2904e-04,  2.2170e-03,\n",
       "                      -4.3392e-03, -1.4486e-03,  3.6404e-03,  2.7842e-03, -3.5246e-03,\n",
       "                       3.7262e-03,  3.3332e-03, -2.5613e-03,  2.4462e-03,  4.0126e-04,\n",
       "                      -2.9041e-03, -1.6210e-03, -2.9520e-04,  4.9840e-03, -1.2443e-03,\n",
       "                       3.6681e-03, -3.7540e-03, -3.2127e-04, -2.9366e-03,  5.5271e-03,\n",
       "                      -4.6220e-04,  3.7287e-05, -1.4603e-03,  3.1543e-03, -2.2252e-03,\n",
       "                       2.0338e-03, -3.1750e-03, -3.4455e-03, -2.9756e-03,  4.4175e-04,\n",
       "                       2.0203e-03,  3.7114e-04, -6.8246e-04, -1.7589e-03,  3.1209e-03,\n",
       "                      -5.3027e-03, -1.9076e-03, -2.8565e-03,  4.9105e-04,  2.4106e-03,\n",
       "                      -2.1508e-03,  1.3770e-03, -2.6839e-03, -1.3049e-03, -5.9493e-04,\n",
       "                       4.8399e-03, -4.0349e-03, -1.1218e-03,  1.0545e-03,  2.8495e-03,\n",
       "                       1.7935e-03,  2.6863e-03,  1.1054e-04, -8.5209e-04,  2.7937e-03,\n",
       "                       9.3768e-03, -4.6535e-03, -1.8112e-03, -2.8445e-03,  3.9698e-03,\n",
       "                       3.3034e-03, -5.9716e-04,  5.1642e-03,  6.4905e-03,  4.2956e-03,\n",
       "                      -6.7447e-04])),\n",
       "             ('transformer.layer.0.attn.c_attn.weight',\n",
       "              tensor([[-0.0237,  0.0118,  0.0182,  ..., -0.0220, -0.0080,  0.0096],\n",
       "                      [-0.0032, -0.0402, -0.0430,  ..., -0.0288, -0.0002, -0.0157],\n",
       "                      [ 0.0035, -0.0334, -0.0126,  ..., -0.0387, -0.0046,  0.0054],\n",
       "                      ...,\n",
       "                      [-0.0193, -0.0057, -0.0154,  ...,  0.0353, -0.0240,  0.0698],\n",
       "                      [-0.0259, -0.0031,  0.0203,  ...,  0.0257, -0.0133,  0.0197],\n",
       "                      [ 0.0049,  0.0175, -0.0023,  ...,  0.0252, -0.0217,  0.0100]])),\n",
       "             ('transformer.layer.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0454, -0.0613, -0.0267,  ...,  0.0355, -0.0575,  0.0129],\n",
       "                      [ 0.0580, -0.0514, -0.0221,  ...,  0.0337, -0.0567,  0.0515],\n",
       "                      [-0.0287, -0.0080, -0.0489,  ..., -0.0184,  0.0468, -0.0448],\n",
       "                      ...,\n",
       "                      [-0.0503, -0.0550, -0.0385,  ...,  0.0122,  0.0277,  0.0185],\n",
       "                      [-0.0028, -0.0253, -0.0323,  ...,  0.0527,  0.0582,  0.0172],\n",
       "                      [ 0.0035, -0.0061, -0.0270,  ...,  0.0276, -0.0205, -0.0449]])),\n",
       "             ('transformer.layer.0.mlp.c_fc.bias',\n",
       "              tensor([-0.0563, -0.0550, -0.0205,  ...,  0.0098, -0.0153, -0.0202])),\n",
       "             ('transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5672)),\n",
       "             ('transformer.layer.0.mlp.c_proj.weight',\n",
       "              tensor([[ 6.5457e-05,  1.2368e-02,  2.4754e-02,  ...,  1.8065e-02,\n",
       "                       -1.1650e-02, -1.5157e-02],\n",
       "                      [ 3.5525e-03,  2.0345e-02,  6.8030e-03,  ...,  1.3600e-02,\n",
       "                        2.0385e-03,  8.7703e-03],\n",
       "                      [-1.8472e-03, -3.0831e-03, -1.6212e-03,  ..., -2.8142e-03,\n",
       "                        2.7566e-02,  1.9313e-02],\n",
       "                      ...,\n",
       "                      [ 9.2666e-03, -3.1960e-03, -2.4263e-02,  ..., -4.1364e-03,\n",
       "                       -1.8560e-02, -2.1037e-02],\n",
       "                      [ 7.8176e-03, -1.1376e-02, -2.8996e-03,  ...,  1.3448e-02,\n",
       "                       -6.5596e-03, -6.7725e-03],\n",
       "                      [-1.3990e-02, -1.9434e-02, -3.0217e-02,  ...,  2.6123e-02,\n",
       "                       -2.9926e-02,  1.8299e-02]])),\n",
       "             ('transformer.layer.0.mlp.c_proj.bias',\n",
       "              tensor([ 2.5560e-02, -3.6254e-03,  2.1335e-02, -5.9091e-03,  4.8277e-03,\n",
       "                      -2.8918e-02,  1.7541e-04, -1.0784e-02, -3.0900e-02,  3.1523e-03,\n",
       "                       2.2230e-02,  1.9318e-02,  1.9482e-02,  2.3477e-02, -1.7252e-02,\n",
       "                      -2.9714e-02, -2.1485e-02,  2.1345e-02, -6.1715e-03,  1.3268e-02,\n",
       "                      -1.2446e-02, -1.6207e-03,  3.8003e-03,  1.3590e-02, -8.2977e-03,\n",
       "                      -2.1442e-04,  8.8348e-03, -2.2914e-02, -1.5780e-02,  5.2340e-03,\n",
       "                      -2.8551e-02, -1.6331e-02,  1.2741e-02, -1.9215e-02,  1.2352e-02,\n",
       "                       2.0115e-02,  1.3922e-02, -2.7000e-02, -1.2176e-02,  1.7833e-04,\n",
       "                      -2.9944e-02, -2.8255e-02,  5.4421e-03, -5.5175e-03,  1.6391e-02,\n",
       "                       7.5466e-03,  8.2707e-03, -2.7863e-02,  7.6405e-03, -3.0874e-02,\n",
       "                       5.7807e-03, -2.0139e-02, -1.4726e-02, -7.2625e-03,  7.4324e-03,\n",
       "                      -1.2648e-03, -2.1311e-02,  1.4025e-02,  1.2524e-02, -5.9381e-03,\n",
       "                       7.8724e-03, -1.5685e-02, -2.4763e-02, -2.9692e-02,  1.0313e-02,\n",
       "                       1.5932e-02,  8.0940e-03,  2.0063e-03, -2.6461e-02,  2.2808e-03,\n",
       "                      -9.6476e-03,  1.8935e-02, -1.8500e-02, -7.4442e-03,  2.4223e-02,\n",
       "                       1.9091e-02,  1.2080e-02, -2.3178e-02, -1.3817e-02,  1.2120e-02,\n",
       "                       1.3733e-02, -1.9721e-02,  3.4679e-04,  1.4133e-02,  1.0491e-02,\n",
       "                       4.2477e-03,  6.9141e-03, -1.3758e-03,  2.3635e-02,  3.5476e-03,\n",
       "                      -2.9612e-02,  1.6437e-02,  1.2660e-02,  1.1188e-02,  2.7275e-02,\n",
       "                      -2.5706e-02,  2.5976e-02, -1.9942e-02, -2.1187e-02, -3.2883e-03,\n",
       "                      -9.7723e-03,  7.2397e-05, -1.3205e-02,  1.1411e-02, -2.4637e-02,\n",
       "                      -9.2218e-03, -2.6060e-02,  7.1590e-03,  1.5343e-02, -1.9596e-02,\n",
       "                       2.8486e-03, -8.7011e-03,  1.0588e-02,  2.4273e-02,  2.9334e-02,\n",
       "                       1.8043e-02, -9.3513e-03,  1.5989e-02,  2.9546e-02,  1.7701e-02,\n",
       "                       2.8502e-02,  1.6575e-02, -2.2003e-02,  1.8809e-02,  1.7890e-02,\n",
       "                       1.5180e-02, -1.5251e-02,  1.1484e-02,  2.1115e-02, -1.4904e-02,\n",
       "                      -3.0148e-03,  1.0726e-02, -1.0897e-02, -8.2914e-03, -2.5162e-03,\n",
       "                       2.4354e-02, -1.7783e-02,  1.2528e-02,  1.4450e-02, -1.0135e-02,\n",
       "                       2.0730e-03,  2.2086e-02, -1.5699e-02, -1.7384e-02,  2.5103e-02,\n",
       "                      -1.7455e-02, -1.5156e-02, -1.8509e-02,  1.7692e-02,  7.7175e-03,\n",
       "                       2.9849e-02, -4.6452e-03,  2.3786e-02,  4.6916e-03,  1.2176e-03,\n",
       "                      -1.8175e-02,  2.3829e-02,  2.0415e-02, -1.2483e-03, -2.5004e-02,\n",
       "                      -1.8008e-02, -7.0233e-03,  3.2495e-03, -1.5493e-02, -1.0343e-03,\n",
       "                      -2.5326e-02,  4.0800e-03,  1.0004e-02,  3.1022e-02,  1.0129e-02,\n",
       "                       1.4329e-02,  2.5901e-02, -2.5397e-02, -5.4523e-04,  1.0502e-02,\n",
       "                      -2.2719e-02,  2.6889e-02,  9.6757e-03,  5.7213e-04,  1.7956e-02,\n",
       "                       3.5283e-03, -2.7490e-02,  5.7966e-03, -1.3353e-02, -2.7805e-02,\n",
       "                      -1.8380e-02,  2.0327e-02, -2.1421e-02,  2.7386e-02,  2.0206e-02,\n",
       "                       2.1745e-02,  3.1120e-02, -1.2887e-02,  1.8633e-02,  6.9381e-03,\n",
       "                       1.3052e-02, -2.7839e-02, -2.4401e-02,  2.6906e-02,  1.5014e-02,\n",
       "                       5.2837e-03, -1.2443e-02,  6.0333e-03, -1.6458e-02,  2.9455e-02,\n",
       "                       5.3619e-03,  1.6728e-02, -2.0348e-02,  2.0520e-02,  1.8688e-02,\n",
       "                       3.0824e-02,  4.0416e-03,  1.8692e-02,  6.2782e-03,  1.6105e-02,\n",
       "                      -2.4185e-02, -1.9630e-02,  2.6541e-02,  1.3325e-02, -1.9297e-02,\n",
       "                       1.8631e-02,  2.3606e-02,  1.5237e-02, -1.1671e-02,  1.1380e-02,\n",
       "                      -8.6316e-03, -2.8438e-02,  1.2729e-02,  6.3229e-03, -2.4172e-02,\n",
       "                       1.8103e-02, -2.1087e-02, -1.2268e-02, -3.1685e-04, -2.5833e-02,\n",
       "                       2.5406e-02, -1.9458e-02, -2.1070e-02,  6.4020e-03, -7.5140e-03,\n",
       "                       3.0642e-02, -5.8846e-03,  1.8289e-02, -2.1852e-02,  2.1584e-02,\n",
       "                      -2.5861e-02, -2.1236e-03,  7.7794e-03, -2.8326e-04,  2.5003e-02,\n",
       "                      -2.5294e-02, -1.2434e-02,  2.6409e-02,  8.4354e-03, -3.6013e-03,\n",
       "                      -2.0866e-02])),\n",
       "             ('transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4906)),\n",
       "             ('transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5593)),\n",
       "             ('transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4906)),\n",
       "             ('transformer.layer.0.ln_1.bn.weight',\n",
       "              tensor([0.9931, 0.9964, 0.9996, 1.0089, 0.9846, 0.9893, 0.9935, 1.0041, 0.9947,\n",
       "                      0.9868, 0.9935, 1.0011, 0.9954, 0.9945, 0.9981, 0.9823, 0.9943, 0.9905,\n",
       "                      1.0093, 0.9953, 1.0007, 0.9961, 0.9982, 1.0007, 0.9957, 0.9958, 0.9934,\n",
       "                      1.0037, 0.9926, 0.9979, 0.9937, 0.9859, 0.9978, 0.9925, 1.0059, 0.9872,\n",
       "                      0.9991, 0.9992, 0.9887, 0.9935, 0.9950, 0.9894, 0.9937, 0.9980, 0.9960,\n",
       "                      1.0071, 0.9849, 1.0003, 1.0057, 1.0005, 0.9998, 0.9969, 1.0061, 0.9861,\n",
       "                      0.9965, 0.9929, 0.9949, 1.0012, 0.9946, 0.9863, 0.9972, 0.9977, 0.9952,\n",
       "                      1.0003, 0.9967, 0.9910, 1.0025, 0.9942, 1.0001, 0.9992, 0.9872, 1.0018,\n",
       "                      1.0033, 1.0022, 0.9917, 1.0020, 1.0041, 0.9822, 0.9917, 0.9992, 1.0023,\n",
       "                      1.0033, 0.9977, 0.9922, 0.9940, 1.0029, 0.9909, 0.9966, 0.9934, 0.9997,\n",
       "                      1.0022, 0.9961, 0.9876, 0.9942, 0.9997, 0.9958, 0.9957, 0.9874, 0.9925,\n",
       "                      0.9927, 0.9954, 1.0008, 0.9965, 0.9964, 0.9888, 0.9865, 1.0005, 0.9952,\n",
       "                      0.9991, 0.9934, 0.9925, 0.9914, 0.9993, 0.9969, 1.0064, 0.9983, 1.0027,\n",
       "                      0.9896, 0.9994, 0.9823, 0.9940, 0.9944, 0.9917, 0.9996, 0.9988, 0.9996,\n",
       "                      0.9940, 0.9971, 0.9902, 0.9962, 1.0002, 0.9980, 0.9979, 0.9946, 0.9991,\n",
       "                      0.9926, 0.9905, 0.9983, 0.9967, 0.9802, 0.9931, 0.9935, 0.9981, 0.9934,\n",
       "                      0.9946, 0.9999, 0.9920, 0.9984, 0.9955, 0.9990, 0.9793, 1.0057, 0.9948,\n",
       "                      1.0071, 0.9934, 0.9839, 0.9964, 0.9729, 0.9916, 0.9941, 0.9915, 0.9957,\n",
       "                      0.9937, 0.9998, 0.9968, 0.9970, 0.9811, 0.9944, 0.9993, 0.9956, 0.9942,\n",
       "                      0.9990, 1.0001, 1.0033, 0.9995, 0.9908, 0.9855, 1.0001, 0.9945, 0.9962,\n",
       "                      0.9928, 0.9983, 0.9875, 0.9871, 0.9992, 0.9883, 1.0063, 0.9916, 0.9920,\n",
       "                      0.9912, 0.9965, 0.9915, 0.9952, 1.0028, 0.9948, 0.9945, 0.9943, 0.9994,\n",
       "                      1.0002, 1.0039, 0.9967, 1.0038, 1.0025, 1.0036, 0.9988, 1.0007, 0.9897,\n",
       "                      0.9949, 0.9980, 0.9977, 1.0015, 0.9998, 0.9968, 0.9986, 0.9988, 0.9827,\n",
       "                      1.0026, 0.9923, 0.9972, 0.9920, 0.9867, 0.9944, 0.9912, 0.9886, 0.9922,\n",
       "                      0.9883, 0.9926, 1.0009, 1.0035, 0.9835, 1.0175, 1.0001, 0.9956, 0.9975,\n",
       "                      0.9970, 0.9998, 0.9859, 0.9906, 0.9966, 0.9906, 0.9996, 0.9933, 0.9942,\n",
       "                      0.9976, 0.9945, 0.9972, 1.0051, 1.0059, 0.9902, 0.9909, 1.0032, 0.9979,\n",
       "                      0.9900, 1.0000, 0.9957, 0.9918])),\n",
       "             ('transformer.layer.0.ln_1.bn.bias',\n",
       "              tensor([ 3.1101e-04,  3.5736e-03, -1.9809e-04,  2.6513e-03,  9.1775e-03,\n",
       "                      -2.4963e-03,  2.3886e-03,  5.9799e-04,  6.1797e-03,  2.2492e-03,\n",
       "                      -1.9250e-04,  2.9204e-03,  7.4876e-03, -7.0627e-03, -3.7569e-03,\n",
       "                      -8.6532e-04, -4.2497e-03,  1.3495e-03, -4.0327e-04, -8.8196e-04,\n",
       "                       5.5398e-03, -1.0220e-02,  4.6685e-04,  1.7534e-03, -2.9860e-03,\n",
       "                      -6.8689e-03, -1.5521e-03, -2.6503e-03, -6.0754e-03, -3.9257e-03,\n",
       "                       1.5364e-02,  5.2607e-03, -6.0376e-04, -2.1966e-03,  2.7285e-04,\n",
       "                      -2.8595e-03, -6.5942e-04, -4.3354e-04,  1.1670e-02, -1.0760e-02,\n",
       "                       2.1742e-03,  1.6508e-03,  9.5419e-03,  3.2297e-03, -2.5104e-03,\n",
       "                       4.5822e-03,  7.8498e-03,  1.0039e-02,  7.0227e-03,  6.1845e-03,\n",
       "                       1.3724e-02,  3.6781e-03,  7.1948e-03, -1.0618e-03, -1.2964e-03,\n",
       "                       2.8245e-03,  1.0357e-03,  9.5720e-03, -5.4346e-03, -1.0623e-03,\n",
       "                      -3.9675e-03, -1.7652e-03,  8.9791e-03,  4.5481e-03,  3.2018e-03,\n",
       "                      -9.3412e-03,  2.0188e-03, -2.9314e-03, -2.2752e-03,  3.5520e-03,\n",
       "                      -3.3860e-03,  5.0325e-04, -1.7496e-03, -8.2058e-03, -1.5648e-03,\n",
       "                       3.5578e-03, -8.6766e-03, -9.6731e-03,  1.3405e-04, -4.0559e-03,\n",
       "                      -1.3148e-02,  4.3693e-03, -3.8709e-03, -2.1100e-03,  3.6705e-03,\n",
       "                       4.2317e-04, -5.4165e-03,  1.0699e-02, -1.2027e-02, -5.2371e-03,\n",
       "                       3.3795e-03, -9.5000e-03,  4.9961e-03, -5.1793e-03, -4.5933e-03,\n",
       "                       3.2173e-03,  1.0090e-03,  7.6554e-03,  3.7186e-03,  1.8136e-03,\n",
       "                      -7.0848e-03,  5.4481e-03, -3.7139e-03, -8.7814e-03, -7.1024e-03,\n",
       "                      -2.8528e-03, -5.4810e-03,  9.5472e-04,  7.5642e-04, -7.5182e-03,\n",
       "                      -1.9049e-03,  9.2364e-04,  3.3527e-03, -2.8002e-03,  5.2739e-03,\n",
       "                       2.6143e-03, -4.4881e-03, -3.5840e-03, -2.9885e-03,  1.1135e-02,\n",
       "                       3.9166e-03, -6.2252e-03, -4.4337e-03,  2.0772e-03,  1.9236e-03,\n",
       "                       6.8115e-04,  1.2582e-02,  7.9471e-03, -1.2734e-03,  4.6074e-03,\n",
       "                       7.1962e-03,  5.8031e-03, -3.8361e-03,  1.7625e-03,  7.3836e-03,\n",
       "                      -8.2421e-03,  1.3143e-03,  1.3548e-02, -4.6842e-03,  6.5980e-03,\n",
       "                       1.2809e-03,  1.9124e-03, -3.2874e-04,  1.5460e-03, -2.9076e-03,\n",
       "                       5.7855e-03, -7.2487e-03, -3.9049e-03,  2.0109e-03,  8.4380e-03,\n",
       "                       7.5462e-03,  4.4747e-03,  5.5726e-03, -5.7470e-05, -6.4852e-03,\n",
       "                       4.5246e-03, -1.1324e-03, -3.1481e-02,  1.8122e-03,  1.2636e-03,\n",
       "                      -2.7692e-03, -1.1063e-02,  7.7906e-03,  3.4274e-03, -2.8712e-03,\n",
       "                       1.6354e-03,  1.8837e-04,  3.1335e-04,  6.0853e-03,  9.7286e-03,\n",
       "                      -1.1463e-03,  7.6757e-03,  5.2650e-03,  3.4889e-03, -5.5663e-04,\n",
       "                      -1.5003e-03,  2.4190e-03,  4.7878e-04, -1.1356e-02, -4.7840e-04,\n",
       "                       2.4219e-03,  4.7471e-03, -6.9031e-03, -6.0700e-04, -7.4873e-03,\n",
       "                       9.9954e-04,  4.1532e-03,  8.8810e-03,  1.6333e-03, -3.2158e-03,\n",
       "                       8.2802e-03,  1.5118e-03, -2.9279e-03,  3.8373e-03,  1.9230e-03,\n",
       "                       8.7103e-04,  2.1916e-03, -6.3128e-03,  5.2371e-03,  4.4822e-03,\n",
       "                       5.3361e-03,  4.0997e-03, -7.7335e-03,  7.7373e-04,  3.2307e-04,\n",
       "                      -7.1923e-04,  1.0442e-02, -7.5178e-03, -2.8811e-03,  2.0657e-03,\n",
       "                      -8.4339e-03,  5.2320e-03, -4.0905e-04,  1.0169e-03,  7.9803e-03,\n",
       "                       1.5938e-03, -1.6011e-03,  6.0652e-03, -5.2725e-04,  1.9728e-03,\n",
       "                       9.0869e-04, -2.1465e-03, -1.5243e-04, -6.3015e-03, -1.0420e-03,\n",
       "                      -2.2566e-03,  3.5020e-03, -3.5536e-04, -4.9905e-04,  3.7193e-05,\n",
       "                      -2.6920e-02, -3.9275e-03,  1.2735e-03,  3.4936e-03,  5.7107e-03,\n",
       "                      -1.7339e-03, -3.1639e-04,  4.4286e-03,  3.1310e-03,  4.1988e-05,\n",
       "                       4.1158e-03, -1.5461e-03,  4.9999e-04, -1.4872e-03,  2.2923e-04,\n",
       "                       5.5016e-03,  6.0403e-03, -1.8667e-04, -3.1868e-03, -1.2550e-03,\n",
       "                      -1.9593e-03, -2.7976e-03, -1.7939e-03,  7.7737e-03,  2.8359e-03,\n",
       "                      -5.9998e-03])),\n",
       "             ('transformer.layer.0.ln_1.bn.running_mean',\n",
       "              tensor([ 8.9613e-03, -1.2370e-01,  1.2264e-02, -3.6761e-02, -1.2689e-01,\n",
       "                       5.6001e-02,  6.5052e-02,  1.2855e-01, -8.5032e-02,  1.4266e-02,\n",
       "                       6.1345e-02,  1.2547e-01, -1.5445e-02,  3.7252e-01, -1.9607e-01,\n",
       "                      -2.1439e-01,  3.2442e-02, -1.4839e-01,  3.6336e-02,  2.4684e-01,\n",
       "                      -4.8439e-02,  2.1209e-01, -1.8866e-02, -1.6488e-01, -2.7249e-02,\n",
       "                       3.2804e-02, -2.6093e-02,  1.3427e-01, -8.9339e-02,  9.2865e-02,\n",
       "                      -7.5938e-02, -9.5080e-02,  2.4438e-02, -1.6972e-01, -2.4107e-01,\n",
       "                      -4.3595e-02,  1.7633e-01,  3.2695e-02,  3.6585e-03,  1.0136e-01,\n",
       "                      -1.7988e-01, -3.1662e-01,  1.6895e-02, -1.4918e-01, -9.8692e-02,\n",
       "                      -1.1284e-01, -4.4821e-02,  3.6330e-03, -1.4188e-01,  1.0072e-01,\n",
       "                      -1.7571e-01, -8.0654e-02, -1.2633e-01, -2.3966e-02,  7.7860e-03,\n",
       "                       7.1017e-02,  1.3281e-01, -1.2771e-01, -1.2919e-01, -3.2759e-01,\n",
       "                      -2.1682e-01, -1.2878e-01, -1.8552e-01, -2.4980e-01, -7.1415e-02,\n",
       "                       2.4791e-01, -9.0915e-02,  2.2548e-02,  5.9753e-02,  9.4027e-02,\n",
       "                      -4.0220e-02,  1.8115e-01,  5.6399e-02,  4.2331e-02, -6.6840e-02,\n",
       "                      -2.9072e-02, -3.3451e-01, -6.9308e-02, -3.6482e-02,  2.5665e-02,\n",
       "                      -2.5645e-01,  9.5390e-02, -3.5530e-02,  8.6358e-02,  1.2303e-01,\n",
       "                       1.8283e-01, -3.4372e-01, -9.1104e-02, -1.1298e-02, -6.3889e-02,\n",
       "                       6.3522e-02,  2.2962e-01, -7.2341e-02, -4.6083e-02,  1.0854e-01,\n",
       "                      -3.5479e-01,  4.6228e-02,  1.2360e-01,  2.8492e-01, -1.0071e-01,\n",
       "                       7.4886e-02, -4.0985e-02, -7.5252e-03, -9.5699e-02,  1.0469e-01,\n",
       "                       1.2558e-01,  8.2541e-02,  4.9260e-02,  1.0244e-01,  2.6517e-02,\n",
       "                       9.8550e-02,  1.4047e-02, -1.6093e-02, -1.0230e-01, -1.1157e-01,\n",
       "                      -1.1022e-01,  2.6249e-01, -9.7288e-02, -1.1720e-01,  9.1511e-02,\n",
       "                      -4.3909e-02, -2.4690e-01, -2.9092e-01, -1.4486e-01,  1.4933e-01,\n",
       "                      -8.4626e-02, -2.1428e-01, -1.5468e-01, -1.5313e-02,  2.4612e-02,\n",
       "                      -6.0043e-02, -1.0013e-01,  9.2507e-02, -6.6692e-03,  7.0078e-02,\n",
       "                       6.3382e-02,  2.1417e-02,  4.2641e-02, -2.0347e-01, -1.0468e-01,\n",
       "                      -1.7609e-01,  8.5733e-02, -3.3848e-02,  1.2050e-02, -2.6971e-01,\n",
       "                       9.6207e-02, -1.6236e-01,  2.4704e-01, -5.5348e-03, -2.1625e-01,\n",
       "                      -2.2096e-01,  7.0111e-02,  3.1566e-01, -4.7322e-02,  4.3996e-02,\n",
       "                      -4.5475e-02, -5.8858e-02,  1.5261e-01, -1.1475e-01, -3.5395e-03,\n",
       "                       1.8488e-01, -1.8400e-01, -5.6036e-02, -3.0918e-01, -1.7152e-02,\n",
       "                       3.5248e-02,  1.2123e-01, -3.5578e-02,  2.1033e-02,  7.4461e-02,\n",
       "                       2.4327e-02, -9.2737e-02, -2.1670e-01, -3.3930e-02,  9.7354e-02,\n",
       "                      -1.8130e-01,  1.5200e-01,  9.3920e-03,  5.3919e-02,  9.6113e-02,\n",
       "                      -3.3220e-05, -2.1484e-02, -6.2922e-02, -9.7986e-02,  2.3778e-01,\n",
       "                      -6.2653e-02,  4.7574e-02, -1.6614e-01, -5.0956e-02, -1.7901e-01,\n",
       "                      -2.0242e-01,  1.6595e-01,  8.4326e-03, -2.0296e-02,  3.3846e-02,\n",
       "                      -1.0240e-01,  2.2613e-01,  7.4510e-02,  1.8720e-02,  1.2926e-01,\n",
       "                      -6.9275e-03,  1.7067e-01,  8.2102e-02, -4.9122e-04,  1.2157e-01,\n",
       "                       1.0372e-01, -1.9239e-01, -2.7830e-01,  1.9367e-01,  1.2532e-01,\n",
       "                      -7.9966e-02, -1.0177e-01,  6.2981e-02,  9.1120e-04,  8.9377e-02,\n",
       "                      -3.2817e-01,  4.8527e-02,  6.6968e-02,  3.9556e-02,  5.4926e-02,\n",
       "                       1.3325e-01,  1.0052e-02, -2.0190e-01,  3.1939e-01,  8.0386e-03,\n",
       "                      -8.2851e-02, -1.0221e-01,  7.1491e-02, -1.2475e-01,  1.3005e-01,\n",
       "                      -2.4310e-01,  1.3214e-01,  1.7610e-01,  8.8242e-02, -4.4540e-01,\n",
       "                      -1.3117e-01,  1.1320e-01,  1.4869e-02, -2.1633e-01,  7.6324e-02,\n",
       "                      -4.5106e-02, -4.1815e-02,  7.4547e-02,  5.6021e-02, -9.8935e-02,\n",
       "                       2.1980e-01,  1.0372e-01,  9.8856e-02, -2.0176e-01, -1.2405e-01,\n",
       "                       1.5523e-01,  3.4723e-02, -1.2795e-01,  1.2103e-01,  6.1370e-02,\n",
       "                       3.0158e-01])),\n",
       "             ('transformer.layer.0.ln_1.bn.running_var',\n",
       "              tensor([1.8630, 2.7696, 2.0784, 2.2451, 2.5418, 2.1541, 2.0901, 1.9175, 2.4395,\n",
       "                      2.6734, 1.9732, 2.1172, 1.9562, 2.3357, 2.2713, 2.4075, 2.1341, 2.1796,\n",
       "                      2.1201, 2.2759, 2.3647, 2.1635, 2.1766, 2.3060, 1.9830, 2.4331, 1.9320,\n",
       "                      2.3622, 2.1974, 2.4310, 2.3769, 2.2353, 1.8425, 2.3998, 2.8219, 2.0226,\n",
       "                      2.5734, 2.3142, 2.6012, 2.2048, 2.1489, 2.9597, 2.1346, 2.2536, 2.0015,\n",
       "                      2.5245, 2.4192, 1.9835, 2.2949, 1.8772, 2.4173, 2.4063, 2.0481, 1.9125,\n",
       "                      2.0900, 2.2612, 2.2144, 2.1282, 2.1192, 2.7872, 2.5928, 1.9631, 1.8723,\n",
       "                      2.5845, 2.0100, 2.5560, 1.9949, 2.2161, 2.3845, 2.1990, 1.8597, 2.0815,\n",
       "                      2.6053, 2.0464, 1.8208, 1.8994, 2.1686, 2.3446, 2.2001, 2.0110, 2.1137,\n",
       "                      2.1000, 2.5426, 1.9756, 2.1436, 2.0523, 3.0309, 2.1023, 2.1708, 2.0207,\n",
       "                      2.0158, 2.5429, 2.1344, 2.2058, 2.3356, 2.3804, 2.3058, 2.0681, 2.1153,\n",
       "                      2.1283, 2.0516, 2.6495, 2.3169, 1.9235, 1.9665, 2.4516, 2.5483, 2.2798,\n",
       "                      2.1599, 2.0456, 1.9137, 2.2804, 2.3421, 2.3374, 2.6425, 2.1271, 1.7549,\n",
       "                      2.0471, 1.8564, 2.3790, 2.2713, 2.2933, 2.1576, 2.0121, 2.3519, 2.1794,\n",
       "                      2.0212, 2.0229, 2.1946, 1.8762, 2.0886, 2.0122, 2.1047, 2.1856, 2.1119,\n",
       "                      2.2480, 2.2446, 2.2901, 2.1857, 1.9344, 2.1660, 2.2110, 2.1276, 2.6364,\n",
       "                      2.0116, 2.2875, 2.1258, 2.1786, 1.8827, 2.1022, 2.0931, 2.1815, 2.2863,\n",
       "                      2.1692, 2.0579, 1.8030, 2.0218, 2.5830, 2.2119, 2.1736, 2.3562, 1.9443,\n",
       "                      2.1095, 2.2936, 2.0598, 2.4091, 2.2420, 2.4007, 2.2670, 2.5419, 2.4926,\n",
       "                      2.6086, 2.0493, 2.3237, 1.9104, 2.2876, 2.2403, 2.2264, 2.2652, 2.0055,\n",
       "                      2.1918, 2.1658, 2.3181, 1.9615, 2.1852, 2.5133, 1.7870, 1.8803, 2.1345,\n",
       "                      1.9570, 2.3325, 2.0299, 1.9512, 2.1836, 2.2559, 2.1994, 1.8379, 2.4424,\n",
       "                      2.1583, 2.1971, 2.0699, 2.0430, 2.1637, 2.1366, 1.9638, 2.1158, 2.5285,\n",
       "                      2.1851, 2.5068, 2.0872, 2.3871, 2.1194, 2.3151, 2.2502, 2.1883, 2.2507,\n",
       "                      2.0095, 2.6236, 2.1232, 1.8058, 2.7215, 2.4070, 2.2099, 2.3653, 2.0759,\n",
       "                      2.3096, 1.9526, 2.2820, 2.5435, 2.3583, 2.0978, 2.1407, 2.4108, 2.0108,\n",
       "                      2.3646, 1.8780, 2.0884, 2.0445, 2.2944, 1.9166, 1.8874, 2.0484, 2.0168,\n",
       "                      2.1996, 2.3855, 2.0565, 2.3479, 1.9634, 2.2511, 2.2176, 2.0768, 2.1958,\n",
       "                      2.2830, 2.1369, 2.0713, 1.9631])),\n",
       "             ('transformer.layer.0.ln_1.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2659)),\n",
       "             ('transformer.layer.0.ln_2.bn.weight',\n",
       "              tensor([1.0036, 1.0194, 0.9903, 1.0046, 0.9668, 0.9915, 1.0093, 1.0029, 0.9824,\n",
       "                      1.0044, 0.9963, 0.9974, 0.9948, 1.0061, 0.9953, 0.9817, 0.9904, 0.9620,\n",
       "                      0.9871, 0.9723, 0.9851, 0.9824, 0.9792, 0.9925, 1.0087, 1.0253, 0.9839,\n",
       "                      1.0104, 0.9893, 0.9797, 0.9759, 0.9899, 0.9942, 0.9953, 0.9906, 1.0039,\n",
       "                      1.0147, 1.0075, 0.9724, 1.0022, 1.0003, 1.0028, 0.9894, 1.0075, 0.9792,\n",
       "                      0.9887, 0.9926, 1.0121, 0.9879, 0.9824, 1.0104, 1.0076, 0.9739, 0.9821,\n",
       "                      0.9873, 1.0014, 0.9782, 1.0091, 1.0031, 0.9699, 1.0057, 0.9779, 0.9746,\n",
       "                      0.9930, 1.0021, 1.0102, 1.0050, 0.9886, 0.9952, 0.9755, 0.9964, 0.9978,\n",
       "                      0.9852, 0.9986, 0.9861, 1.0213, 0.9749, 0.9961, 0.9969, 0.9917, 0.9977,\n",
       "                      1.0033, 0.9935, 0.9988, 0.9936, 1.0080, 1.0145, 0.9967, 0.9927, 0.9761,\n",
       "                      1.0034, 0.9985, 0.9931, 0.9958, 0.9815, 0.9761, 0.9829, 0.9686, 0.9747,\n",
       "                      0.9854, 0.9794, 0.9897, 1.0121, 0.9647, 0.9761, 1.0046, 0.9853, 0.9969,\n",
       "                      1.0020, 0.9873, 0.9756, 1.0028, 0.9975, 0.9906, 0.9858, 0.9702, 0.9857,\n",
       "                      1.0014, 0.9788, 0.9787, 0.9958, 0.9722, 1.0035, 0.9924, 0.9909, 0.9818,\n",
       "                      0.9825, 1.0154, 0.9793, 1.0082, 0.9834, 1.0029, 0.9846, 0.9994, 0.9830,\n",
       "                      0.9739, 0.9945, 1.0070, 0.9884, 0.9916, 0.9890, 1.0065, 0.9699, 1.0047,\n",
       "                      0.9863, 0.9880, 0.9966, 0.9886, 0.9855, 0.9816, 0.9939, 0.9954, 0.9801,\n",
       "                      0.9811, 1.0019, 1.0037, 0.9901, 1.0134, 0.9840, 0.9893, 0.9925, 0.9750,\n",
       "                      0.9898, 0.9976, 0.9942, 0.9918, 1.0000, 0.9881, 0.9812, 0.9798, 1.0026,\n",
       "                      0.9917, 0.9749, 0.9886, 1.0010, 0.9948, 0.9940, 0.9802, 1.0027, 1.0013,\n",
       "                      0.9873, 0.9914, 0.9821, 0.9932, 1.0190, 0.9907, 0.9944, 0.9961, 0.9908,\n",
       "                      1.0087, 0.9860, 0.9839, 1.0102, 0.9978, 0.9881, 0.9786, 1.0140, 0.9887,\n",
       "                      1.0010, 0.9869, 0.9964, 1.0066, 1.0043, 1.0009, 0.9901, 0.9813, 0.9933,\n",
       "                      0.9950, 0.9918, 1.0021, 0.9979, 0.9838, 1.0092, 0.9905, 0.9613, 0.9999,\n",
       "                      0.9750, 0.9813, 1.0036, 0.9744, 0.9962, 1.0067, 0.9962, 1.0004, 0.9862,\n",
       "                      0.9949, 0.9870, 0.9972, 1.0093, 0.9818, 1.0126, 1.0003, 0.9891, 1.0036,\n",
       "                      1.0111, 0.9960, 1.0039, 0.9942, 0.9822, 0.9877, 1.0042, 0.9809, 0.9979,\n",
       "                      0.9892, 0.9790, 0.9715, 0.9898, 0.9928, 0.9913, 0.9893, 0.9911, 0.9781,\n",
       "                      0.9989, 0.9921, 0.9956, 0.9974])),\n",
       "             ('transformer.layer.0.ln_2.bn.bias',\n",
       "              tensor([-1.1617e-03,  1.5654e-02, -1.8790e-02,  6.1449e-03, -8.6869e-03,\n",
       "                      -1.0334e-02,  8.9562e-03, -1.6320e-02, -1.0743e-02,  1.0246e-02,\n",
       "                      -3.1414e-02,  6.9664e-03,  1.3626e-02,  4.3714e-04, -1.7708e-03,\n",
       "                      -9.5165e-03,  8.9959e-03, -9.7104e-03,  1.3981e-02,  3.5272e-03,\n",
       "                      -2.4233e-02,  3.2937e-03, -1.7318e-02,  3.4604e-05,  1.0131e-02,\n",
       "                       6.7072e-04, -1.0355e-02,  1.3371e-02,  1.7957e-02,  6.2535e-03,\n",
       "                       4.6281e-03,  1.8063e-02,  2.3879e-03,  1.2372e-03,  8.1688e-03,\n",
       "                      -6.8264e-04, -4.9611e-03,  9.2970e-03, -1.0648e-04, -2.6237e-03,\n",
       "                       9.2607e-03,  1.1736e-02,  5.2010e-03,  1.5031e-02, -1.4019e-02,\n",
       "                       1.5601e-03,  2.2779e-02,  8.7433e-03, -2.3392e-02, -5.1128e-03,\n",
       "                       2.0698e-02,  1.4696e-02,  1.8549e-02, -7.3791e-04,  1.1616e-02,\n",
       "                      -3.3466e-03, -1.0966e-02, -2.2087e-02,  2.5562e-02, -5.7451e-03,\n",
       "                       2.2835e-02,  2.9015e-02,  8.1164e-03, -8.2396e-03,  9.1784e-04,\n",
       "                       1.7896e-02, -1.7235e-02,  4.7802e-03,  2.5887e-03, -1.1818e-03,\n",
       "                      -5.8181e-03, -1.4006e-03, -1.5991e-02,  2.2670e-02, -8.7560e-03,\n",
       "                      -1.1325e-02,  1.5039e-02,  3.5482e-03, -1.1875e-02,  2.2314e-02,\n",
       "                      -1.0241e-02, -9.0580e-04,  2.1325e-03,  2.5347e-02,  2.3930e-03,\n",
       "                      -1.7677e-02, -3.2906e-03,  5.5757e-03,  1.0787e-02,  2.9402e-03,\n",
       "                      -1.0092e-02, -1.6945e-02,  1.4501e-02,  4.1303e-03,  1.1130e-02,\n",
       "                       5.4631e-03,  1.5103e-02,  1.0329e-03,  1.8381e-02, -3.9449e-03,\n",
       "                       2.3707e-04,  3.6188e-03, -1.4835e-02,  1.0678e-02,  2.5156e-02,\n",
       "                      -8.0874e-03,  3.5044e-03, -5.0502e-04,  4.1528e-03,  1.5032e-02,\n",
       "                      -5.1855e-03, -5.4355e-03, -2.8857e-03,  7.9513e-03, -1.5374e-02,\n",
       "                      -1.7870e-02, -1.3349e-02,  1.7038e-02, -1.4965e-02, -1.1407e-04,\n",
       "                       1.6526e-02, -1.0807e-02,  3.1279e-03, -1.7861e-02, -1.4402e-02,\n",
       "                       9.9527e-03, -8.1853e-03,  2.6262e-04, -2.0511e-02, -9.5721e-03,\n",
       "                       7.3772e-03,  2.5280e-03, -5.8308e-03, -1.9561e-03,  2.2649e-03,\n",
       "                      -2.0045e-02, -3.7415e-03, -7.8910e-03, -9.0600e-03, -1.0289e-02,\n",
       "                      -8.8055e-04, -6.7048e-04,  1.3263e-02,  2.8746e-04, -1.0058e-03,\n",
       "                       1.2260e-02,  4.9638e-03,  2.6292e-03, -5.1895e-03,  4.8054e-03,\n",
       "                      -1.0795e-02,  4.3272e-03, -3.9734e-03,  1.7559e-03, -5.7020e-03,\n",
       "                       2.2606e-03, -5.9245e-03,  3.8286e-03,  1.5825e-02,  1.9344e-02,\n",
       "                       2.3315e-03, -1.0266e-03,  4.5507e-03, -1.0265e-02, -2.0932e-02,\n",
       "                      -1.5586e-02,  7.2955e-03,  3.9704e-03,  2.4837e-02,  1.6357e-02,\n",
       "                      -1.5650e-02,  6.4151e-03, -2.1876e-02,  1.2612e-02, -5.3367e-03,\n",
       "                      -9.6919e-03, -8.7552e-03,  1.0627e-03, -1.6313e-02, -1.4360e-02,\n",
       "                      -5.0603e-04,  8.6458e-03,  3.1626e-03,  7.5228e-03, -1.5497e-02,\n",
       "                       2.4973e-02,  9.2009e-04,  8.8796e-03, -3.8995e-03, -2.5552e-03,\n",
       "                      -2.3560e-02,  5.5258e-03, -1.3867e-02, -1.4173e-02,  7.4257e-03,\n",
       "                      -1.9307e-02, -1.7087e-02,  2.6576e-03, -1.0954e-02, -3.7312e-03,\n",
       "                       3.0616e-03, -2.0454e-02,  1.6245e-02, -1.7996e-02,  5.9428e-03,\n",
       "                       1.1906e-02,  2.0609e-02, -1.5677e-02,  1.0083e-02, -1.7679e-02,\n",
       "                       2.7291e-02,  1.7250e-02, -1.9806e-02, -4.0750e-03,  3.2714e-03,\n",
       "                      -2.0266e-05, -4.3149e-03, -1.8603e-03,  1.3511e-02,  5.4139e-03,\n",
       "                       2.1898e-02,  9.1348e-03,  1.1247e-02, -8.3234e-03,  4.7891e-03,\n",
       "                      -6.7161e-03, -7.2063e-03,  1.6437e-02, -1.4907e-03, -1.5479e-02,\n",
       "                       2.2923e-02,  4.6226e-03, -1.6084e-02,  1.5069e-02, -9.0403e-03,\n",
       "                       1.7145e-02, -1.4515e-02, -8.0070e-03, -8.0038e-03, -1.4850e-04,\n",
       "                       1.0565e-02,  6.4835e-03, -8.3489e-03,  1.1790e-02,  4.8108e-03,\n",
       "                       1.6109e-02,  4.4754e-04,  1.4808e-02,  1.3281e-03, -4.8641e-03,\n",
       "                      -1.3839e-02,  6.0345e-03,  1.4542e-02,  1.0951e-02, -5.6220e-03,\n",
       "                       3.6378e-03])),\n",
       "             ('transformer.layer.0.ln_2.bn.running_mean',\n",
       "              tensor([-5.0900e-03, -1.0721e-02,  3.8086e-02,  8.2083e-03,  4.0791e-03,\n",
       "                      -1.5757e-02, -1.5869e-03, -4.0722e-03,  1.9033e-02,  6.0358e-03,\n",
       "                       1.6505e-05, -4.9093e-03, -1.2209e-02, -8.7771e-03, -1.4959e-02,\n",
       "                       2.4640e-02, -5.0177e-03,  2.0662e-02,  6.9389e-03, -4.2177e-03,\n",
       "                       2.8362e-03, -8.8493e-03, -3.2864e-03, -3.4676e-03,  8.8807e-03,\n",
       "                      -1.1969e-02,  2.0179e-02,  4.4348e-04, -1.0332e-02, -2.6240e-02,\n",
       "                      -5.4242e-04, -1.5830e-02, -2.7030e-03,  6.7843e-03, -1.9277e-02,\n",
       "                      -3.0499e-02, -6.8424e-04,  3.6796e-03,  4.2282e-02, -2.0701e-02,\n",
       "                      -2.0995e-02, -2.6358e-03,  8.7215e-04,  1.3189e-02,  4.0794e-04,\n",
       "                      -3.1295e-03, -8.1328e-03,  4.5888e-03, -1.1396e-03, -1.0827e-02,\n",
       "                       1.6416e-02,  2.8933e-02,  5.0613e-03,  4.7441e-03,  2.0560e-02,\n",
       "                       7.4190e-03, -7.0605e-03,  2.5724e-02, -2.5804e-03, -1.5834e-02,\n",
       "                      -2.1643e-02, -2.0849e-02, -1.0131e-02,  1.2788e-02,  3.5538e-03,\n",
       "                       1.3842e-03,  9.7056e-03, -1.2455e-02, -2.8297e-03,  1.0676e-02,\n",
       "                       1.5990e-02, -1.9289e-02,  3.8453e-03, -1.2578e-02, -2.4527e-02,\n",
       "                       4.0436e-03, -1.2882e-02,  1.5468e-03,  6.1483e-03, -1.3916e-02,\n",
       "                      -3.4099e-03, -2.1674e-03, -2.4861e-03,  1.8135e-02, -2.8915e-03,\n",
       "                       8.7743e-03,  4.7296e-03, -8.3798e-03, -2.7891e-02, -8.3316e-03,\n",
       "                      -3.3166e-03, -3.2707e-02,  2.6956e-02,  8.5065e-04, -2.0160e-02,\n",
       "                      -9.9133e-03,  4.3786e-04,  1.3307e-02,  4.7673e-03,  3.5659e-03,\n",
       "                       1.9744e-03,  2.4958e-02, -2.6892e-02, -1.2225e-03,  8.3819e-03,\n",
       "                      -5.2288e-02, -1.2676e-02, -1.0421e-02, -1.5855e-03,  8.2935e-03,\n",
       "                       5.7946e-03,  1.5647e-03,  3.9233e-03, -1.1129e-02,  1.0203e-02,\n",
       "                       1.4875e-02, -6.6866e-03, -4.7482e-04,  1.7492e-02, -1.1491e-02,\n",
       "                      -3.1674e-03,  3.1042e-03, -8.4997e-03,  2.2395e-02, -5.1092e-03,\n",
       "                       1.1958e-02, -5.9185e-03,  1.8376e-02,  3.7141e-03, -2.4477e-02,\n",
       "                      -2.3261e-03,  2.7153e-02, -1.7070e-02,  3.2139e-03,  9.8418e-03,\n",
       "                       3.7204e-03, -4.8453e-03,  2.2284e-02,  1.9756e-02,  8.5836e-03,\n",
       "                       1.2711e-02, -3.4019e-04,  1.4558e-02,  4.2319e-03, -1.2487e-03,\n",
       "                       1.2874e-02, -2.9393e-02, -4.4419e-03,  9.6483e-04,  1.7746e-04,\n",
       "                       7.5834e-03,  3.2333e-03, -2.5262e-02, -7.8071e-03, -1.0614e-02,\n",
       "                       5.2585e-03,  1.5115e-02, -2.7218e-02, -1.4371e-03, -1.6751e-02,\n",
       "                      -1.6378e-02, -9.2631e-03,  1.7552e-02,  1.4014e-02, -1.5652e-02,\n",
       "                      -6.2528e-03,  1.9234e-04, -8.2318e-03, -1.2588e-02,  1.1316e-02,\n",
       "                      -1.7938e-02, -3.2783e-03,  2.6472e-02,  2.5226e-02, -3.6355e-03,\n",
       "                       9.7574e-03, -9.1546e-03,  1.2679e-02, -1.3290e-02, -1.8622e-02,\n",
       "                      -2.4983e-02,  2.2928e-02,  2.4071e-03,  1.9825e-03, -2.5308e-02,\n",
       "                       6.0616e-03,  1.5534e-02,  3.6195e-03, -2.6011e-02,  1.3159e-02,\n",
       "                      -7.7617e-03,  8.4955e-03,  2.3088e-02,  1.9208e-03, -3.7350e-03,\n",
       "                       4.9570e-03,  1.1132e-02, -4.8787e-03,  1.2850e-02,  3.1416e-03,\n",
       "                      -6.0059e-03, -9.9623e-03,  1.2791e-02,  1.0964e-02,  1.5833e-02,\n",
       "                      -1.9202e-02,  3.7600e-03, -2.6611e-02, -1.2429e-02,  5.2836e-03,\n",
       "                      -4.5105e-03,  6.8247e-03,  1.8158e-02,  2.7214e-02,  1.6454e-02,\n",
       "                       5.6127e-03, -2.0812e-03, -8.1859e-03, -7.0998e-03,  1.2417e-02,\n",
       "                       1.5451e-02, -1.0423e-02,  1.0884e-02,  7.4714e-03, -1.2460e-02,\n",
       "                      -2.2207e-02, -1.8058e-02, -5.1364e-03, -5.5807e-04,  1.5648e-02,\n",
       "                      -2.0213e-02, -5.0812e-03, -1.2590e-02,  6.0696e-03,  2.8794e-02,\n",
       "                       1.9313e-02, -3.0259e-04, -4.1645e-03,  8.9691e-05,  1.3176e-02,\n",
       "                       5.5752e-03,  3.9541e-05, -3.1730e-02, -6.4912e-03,  1.3219e-02,\n",
       "                       2.2198e-02,  2.8339e-03, -1.5230e-02,  2.6949e-02, -8.7384e-04,\n",
       "                      -3.5156e-03,  1.0783e-03,  1.0803e-02,  2.4274e-02, -2.0325e-03,\n",
       "                       4.9847e-03])),\n",
       "             ('transformer.layer.0.ln_2.bn.running_var',\n",
       "              tensor([0.9524, 0.9480, 0.9745, 0.9642, 0.9369, 0.9231, 0.9267, 0.9414, 0.9541,\n",
       "                      0.9510, 0.9204, 0.9540, 0.9181, 0.9392, 0.9381, 0.9102, 0.9504, 0.9420,\n",
       "                      0.9889, 0.9660, 0.9528, 0.9543, 0.9452, 0.9614, 0.9597, 0.9580, 0.9392,\n",
       "                      0.9682, 0.9393, 0.9807, 0.9612, 0.9404, 0.9574, 0.9452, 0.9588, 0.9421,\n",
       "                      0.9745, 0.9390, 0.9084, 0.9597, 0.9367, 0.9417, 0.9363, 0.9601, 0.9470,\n",
       "                      0.9735, 0.9327, 0.9330, 0.9393, 0.9588, 0.9669, 0.9387, 0.9560, 0.9040,\n",
       "                      0.9693, 0.9274, 0.9404, 0.9456, 0.9145, 0.9405, 0.9656, 0.9415, 0.9318,\n",
       "                      0.9719, 0.9575, 0.9563, 0.9611, 0.9525, 0.9602, 0.9615, 0.9159, 0.9605,\n",
       "                      0.9601, 0.9575, 0.9496, 0.9590, 0.9775, 0.9133, 0.9524, 0.9759, 0.9509,\n",
       "                      0.9684, 0.9610, 0.9416, 0.9586, 0.9711, 0.9317, 0.9646, 0.9555, 0.9489,\n",
       "                      0.9690, 0.9560, 0.9298, 0.9554, 0.9633, 0.9430, 0.9642, 0.9184, 0.9188,\n",
       "                      0.9321, 0.9481, 0.9684, 0.9634, 0.9305, 0.9310, 0.9331, 0.9668, 0.9491,\n",
       "                      0.9537, 0.9424, 0.9204, 0.9325, 0.9622, 0.9564, 0.9721, 0.9327, 0.9599,\n",
       "                      0.9385, 0.9464, 0.9295, 0.9523, 0.9453, 0.9523, 0.9564, 0.9578, 0.9558,\n",
       "                      0.9303, 0.9417, 0.9537, 0.9346, 0.9565, 0.9404, 0.9460, 0.9533, 0.9515,\n",
       "                      0.9413, 0.9485, 0.9590, 0.9346, 0.9152, 0.9464, 0.9497, 0.9769, 0.9398,\n",
       "                      0.9290, 0.9486, 0.9314, 0.9638, 0.9504, 0.9477, 0.9260, 0.9332, 0.9631,\n",
       "                      0.9498, 0.9417, 0.9281, 0.9672, 0.8978, 0.9366, 0.9354, 0.9520, 0.9321,\n",
       "                      0.9478, 0.9342, 0.9473, 0.9590, 0.9379, 0.9532, 0.9730, 0.9348, 0.9544,\n",
       "                      0.9536, 0.9308, 0.9410, 0.9689, 0.9251, 0.9312, 0.9551, 0.9487, 0.9641,\n",
       "                      0.9274, 0.9621, 0.9406, 0.9437, 0.9577, 0.9400, 0.9696, 0.9418, 0.9210,\n",
       "                      0.9086, 0.9626, 0.9421, 0.9564, 0.9679, 0.9514, 0.9489, 0.9381, 0.9375,\n",
       "                      0.9354, 0.9574, 0.9486, 0.9772, 0.9653, 0.9412, 0.9534, 0.9635, 0.9123,\n",
       "                      0.9405, 0.9450, 0.9425, 0.9596, 0.9580, 0.9584, 0.9431, 0.9548, 0.9343,\n",
       "                      0.9396, 0.9576, 0.9459, 0.9407, 0.9378, 0.9256, 0.9456, 0.9437, 0.9296,\n",
       "                      0.9246, 0.9437, 0.9707, 0.9426, 0.9025, 0.9976, 0.9719, 0.9458, 0.9411,\n",
       "                      0.9624, 0.9400, 0.9297, 0.9328, 0.9500, 0.9241, 0.9477, 0.9466, 0.9427,\n",
       "                      0.9615, 0.9483, 0.9376, 0.9634, 0.9532, 0.9119, 0.9369, 0.9760, 0.9560,\n",
       "                      0.9477, 0.9404, 0.9738, 0.9779])),\n",
       "             ('transformer.layer.0.ln_2.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5780)),\n",
       "             ('transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.2201)),\n",
       "             ('transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.6164)),\n",
       "             ('transformer.layer.1.attn.bias',\n",
       "              tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('transformer.layer.1.attn.mha.q_proj.weight',\n",
       "              tensor([[-0.0289,  0.0976,  0.0667,  ..., -0.0131,  0.0932,  0.0024],\n",
       "                      [-0.0678, -0.0605,  0.0941,  ...,  0.0241,  0.0261,  0.0976],\n",
       "                      [ 0.0276, -0.0368, -0.0275,  ...,  0.0095, -0.1039, -0.0963],\n",
       "                      ...,\n",
       "                      [-0.0174, -0.0388,  0.0762,  ..., -0.0933, -0.0762,  0.0182],\n",
       "                      [-0.1064, -0.1046,  0.0996,  ...,  0.0059,  0.0950,  0.0947],\n",
       "                      [ 0.0433, -0.0998,  0.0985,  ..., -0.0557, -0.0753, -0.0908]])),\n",
       "             ('transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.k_proj.weight',\n",
       "              tensor([[ 0.0670,  0.0631, -0.0680,  ...,  0.0421,  0.0778,  0.0756],\n",
       "                      [-0.0360, -0.0908,  0.0439,  ...,  0.0365,  0.0324,  0.0242],\n",
       "                      [ 0.0767,  0.0078,  0.0632,  ...,  0.0169, -0.0303,  0.0467],\n",
       "                      ...,\n",
       "                      [-0.0232,  0.0720,  0.0345,  ...,  0.0828, -0.0528, -0.0473],\n",
       "                      [ 0.0859,  0.1058,  0.0129,  ...,  0.0084, -0.0406, -0.0608],\n",
       "                      [-0.1061,  0.0643, -0.0219,  ...,  0.0850, -0.0083,  0.0746]])),\n",
       "             ('transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.v_proj.weight',\n",
       "              tensor([[ 0.0770, -0.0444,  0.0392,  ...,  0.0213, -0.0336, -0.1049],\n",
       "                      [-0.0791, -0.0693, -0.0372,  ..., -0.0016, -0.0555, -0.0367],\n",
       "                      [ 0.0394,  0.0130, -0.0104,  ..., -0.0837,  0.0782,  0.0857],\n",
       "                      ...,\n",
       "                      [-0.0449,  0.0637, -0.0511,  ...,  0.0733, -0.0521, -0.0959],\n",
       "                      [ 0.0212, -0.0945, -0.0851,  ..., -0.0861,  0.1063, -0.0324],\n",
       "                      [ 0.0266,  0.0173,  0.0705,  ..., -0.0137,  0.0776,  0.0054]])),\n",
       "             ('transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0383, -0.0247,  0.0034,  ..., -0.0250, -0.0135,  0.0593],\n",
       "                      [ 0.0116,  0.0308,  0.0596,  ..., -0.0100, -0.0307, -0.0493],\n",
       "                      [ 0.0376, -0.0257, -0.0307,  ...,  0.0383,  0.0414, -0.0497],\n",
       "                      ...,\n",
       "                      [-0.0564, -0.0499,  0.0457,  ...,  0.0512, -0.0417,  0.0239],\n",
       "                      [ 0.0428,  0.0313,  0.0073,  ...,  0.0434,  0.0338, -0.0295],\n",
       "                      [-0.0285,  0.0077, -0.0428,  ...,  0.0328,  0.0573,  0.0241]])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(3.1205)),\n",
       "             ('transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(1.1111)),\n",
       "             ('transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(0.3889)),\n",
       "             ('transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2181)),\n",
       "             ('transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.3162)),\n",
       "             ('transformer.layer.1.attn.c_proj.weight',\n",
       "              tensor([[-0.0121,  0.0034, -0.0166,  ...,  0.0043,  0.0106, -0.0059],\n",
       "                      [ 0.0002,  0.0207,  0.0165,  ...,  0.0044,  0.0283, -0.0115],\n",
       "                      [-0.0110,  0.0047,  0.0163,  ..., -0.0086, -0.0136, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0152, -0.0017, -0.0024,  ...,  0.0169,  0.0018, -0.0160],\n",
       "                      [-0.0023,  0.0208,  0.0079,  ..., -0.0002, -0.0090, -0.0032],\n",
       "                      [ 0.0041, -0.0305, -0.0199,  ...,  0.0094, -0.0021,  0.0034]])),\n",
       "             ('transformer.layer.1.attn.c_proj.bias',\n",
       "              tensor([-1.0001e-02,  8.5324e-04, -1.0371e-03, -3.6230e-04,  3.3133e-03,\n",
       "                      -1.2187e-04, -4.2462e-03, -9.1936e-04,  6.8946e-04,  2.1431e-03,\n",
       "                      -3.1210e-03, -3.5645e-03, -2.9548e-03, -3.4646e-03,  5.6597e-03,\n",
       "                       6.4772e-04, -1.2795e-04,  4.6478e-03,  2.6459e-03, -5.5229e-03,\n",
       "                       3.3030e-03,  1.3496e-03, -2.6890e-03, -2.0859e-03, -6.0766e-04,\n",
       "                       1.7153e-03, -1.9855e-03, -2.5495e-03,  1.7874e-03, -6.7082e-03,\n",
       "                      -5.6755e-04, -2.3912e-03, -4.3582e-03,  2.7330e-03, -1.6397e-03,\n",
       "                      -1.0015e-04, -8.4450e-04,  7.5479e-04,  3.6866e-03,  2.5952e-03,\n",
       "                       9.9290e-04,  6.2599e-04, -3.0715e-03,  7.0385e-03, -4.3600e-03,\n",
       "                      -1.6166e-03, -2.7170e-03,  3.1233e-03,  5.6342e-04, -1.8623e-03,\n",
       "                       4.1094e-03, -4.2669e-03, -3.2268e-03,  3.1488e-04, -5.0921e-03,\n",
       "                       1.7216e-03, -1.8930e-03,  1.0187e-03,  1.2571e-03, -1.0238e-03,\n",
       "                       3.8939e-03, -7.8471e-04,  6.8841e-04,  2.7645e-03,  9.7573e-04,\n",
       "                       2.8124e-03,  2.1228e-03, -3.2062e-03,  6.6098e-04,  1.6510e-03,\n",
       "                      -4.8358e-04, -8.3054e-04,  1.7340e-03, -3.5693e-03,  1.0891e-03,\n",
       "                       5.2370e-03, -6.6089e-04,  1.1281e-02, -2.5373e-04,  1.2816e-03,\n",
       "                       3.8372e-03, -4.4826e-03,  2.1975e-03,  2.7572e-03,  1.1562e-03,\n",
       "                       1.9154e-03, -1.4028e-03, -6.4799e-03, -5.4713e-03,  4.6193e-04,\n",
       "                       6.2959e-03, -3.5250e-03,  5.6006e-03, -3.1833e-03,  4.2460e-03,\n",
       "                       1.4790e-04, -2.2666e-03,  4.5003e-03, -1.7849e-03,  5.7741e-04,\n",
       "                       7.4971e-03, -4.0401e-03, -1.4316e-03,  1.8538e-04,  2.5096e-03,\n",
       "                      -2.8483e-03, -1.6946e-03, -7.0243e-04,  1.9334e-03, -1.4484e-03,\n",
       "                       4.3673e-03,  6.0470e-03, -1.4517e-03, -1.4012e-03,  2.1070e-03,\n",
       "                      -7.7036e-03,  6.3211e-03, -1.0998e-03, -9.5744e-04,  3.0299e-03,\n",
       "                       1.5667e-03, -1.3419e-03, -5.3833e-03, -5.6620e-04,  9.3516e-03,\n",
       "                       4.8909e-04, -3.6963e-03, -6.9707e-03,  2.1502e-03,  1.1926e-03,\n",
       "                      -1.8200e-03,  2.2529e-03, -2.4530e-03, -3.2041e-03, -2.5172e-03,\n",
       "                       2.1932e-04,  3.1836e-05,  1.5012e-03, -2.8934e-03, -1.2549e-03,\n",
       "                       7.0520e-03,  2.5215e-03,  2.1365e-03,  1.1457e-03,  8.0462e-04,\n",
       "                       2.3927e-03,  3.9076e-03,  1.3535e-03,  2.2825e-03,  4.7756e-03,\n",
       "                       1.4805e-03, -3.4977e-03, -9.9157e-04,  2.6944e-03,  5.5162e-04,\n",
       "                      -3.5070e-03,  1.9986e-03, -3.0507e-03,  2.2575e-03, -3.5908e-03,\n",
       "                       5.0501e-04, -4.6861e-04, -1.1597e-03,  3.1375e-03, -2.1633e-03,\n",
       "                       4.5996e-03, -4.3433e-03,  2.1133e-03, -1.3463e-03,  3.3768e-03,\n",
       "                       5.5218e-03, -4.8867e-04, -1.8089e-03, -8.6995e-04,  3.8831e-03,\n",
       "                      -4.5628e-03,  1.2847e-03, -8.5883e-04,  6.4863e-03, -3.0359e-03,\n",
       "                       2.5516e-03, -5.4719e-03, -3.0461e-03, -9.3430e-04,  1.1867e-03,\n",
       "                      -5.6178e-04, -4.1363e-03,  5.5099e-03,  1.5559e-04, -1.8867e-03,\n",
       "                      -4.8013e-04,  2.1434e-03,  1.7304e-03, -4.8338e-03,  1.3595e-03,\n",
       "                      -5.6351e-03, -3.2486e-03, -1.7714e-03,  3.6017e-03, -8.0431e-04,\n",
       "                      -1.6796e-03,  4.6677e-03,  2.4440e-03,  1.0739e-03, -5.1304e-03,\n",
       "                      -4.2639e-03, -2.7939e-03,  2.9703e-03,  9.2800e-03,  2.2361e-04,\n",
       "                       4.5265e-03,  2.5850e-03, -1.0793e-03, -1.3141e-05,  4.1272e-03,\n",
       "                      -4.2361e-03,  4.8523e-05, -1.1420e-03,  2.8526e-03,  4.5000e-03,\n",
       "                       1.9379e-03,  2.6035e-03,  1.2161e-03, -2.0494e-03,  1.3908e-03,\n",
       "                       3.3852e-03, -4.9066e-03, -1.0401e-03, -2.3964e-04,  8.2049e-03,\n",
       "                       2.0331e-03, -1.8505e-03, -1.7263e-03,  1.6367e-03, -5.3243e-03,\n",
       "                       2.1911e-03, -1.8749e-05, -4.0067e-04,  3.0792e-03,  7.8981e-04,\n",
       "                      -6.1128e-03, -6.4850e-05,  1.0346e-03,  6.0894e-04, -1.6089e-03,\n",
       "                      -2.2540e-03,  1.1035e-03, -2.5577e-03, -1.4807e-03,  5.8240e-03,\n",
       "                       3.8623e-03,  1.0280e-03,  3.6149e-03,  4.3118e-03,  4.4890e-03,\n",
       "                       4.6817e-03])),\n",
       "             ('transformer.layer.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0296,  0.0019,  0.0275,  ..., -0.0367,  0.0327,  0.0085],\n",
       "                      [ 0.0132,  0.0106,  0.0004,  ..., -0.0132, -0.0035,  0.0032],\n",
       "                      [-0.0170,  0.0133,  0.0263,  ...,  0.0311,  0.0103,  0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0189, -0.0202,  0.0250,  ..., -0.0290,  0.0136, -0.0242],\n",
       "                      [ 0.0051,  0.0136, -0.0009,  ..., -0.0116,  0.0404,  0.0048],\n",
       "                      [-0.0009,  0.0096,  0.0206,  ..., -0.0171, -0.0136, -0.0004]])),\n",
       "             ('transformer.layer.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_fc.weight',\n",
       "              tensor([[-0.0058,  0.0039,  0.0383,  ..., -0.0456,  0.0403, -0.0299],\n",
       "                      [-0.0213,  0.0152,  0.0204,  ...,  0.0470, -0.0493,  0.0346],\n",
       "                      [-0.0433,  0.0593, -0.0227,  ...,  0.0539,  0.0619, -0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0066,  0.0602, -0.0372,  ..., -0.0466,  0.0298, -0.0297],\n",
       "                      [-0.0296, -0.0246,  0.0449,  ...,  0.0440,  0.0335, -0.0466],\n",
       "                      [ 0.0471,  0.0025,  0.0513,  ..., -0.0563,  0.0324, -0.0364]])),\n",
       "             ('transformer.layer.1.mlp.c_fc.bias',\n",
       "              tensor([ 0.0063,  0.0311, -0.0120,  ...,  0.0166,  0.0098, -0.0140])),\n",
       "             ('transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4474)),\n",
       "             ('transformer.layer.1.mlp.c_proj.weight',\n",
       "              tensor([[ 1.3174e-02, -2.1647e-02,  3.5113e-03,  ...,  7.6342e-05,\n",
       "                        8.9009e-03, -1.6953e-02],\n",
       "                      [ 1.1414e-02, -1.1850e-02, -7.3429e-03,  ..., -2.6661e-02,\n",
       "                        1.8389e-02,  6.3918e-03],\n",
       "                      [ 5.1879e-03, -2.9540e-02,  9.4862e-03,  ...,  1.7773e-05,\n",
       "                        1.8000e-02,  9.6450e-03],\n",
       "                      ...,\n",
       "                      [-2.0446e-02,  2.4317e-02,  2.8374e-02,  ..., -2.2716e-02,\n",
       "                       -6.1415e-03,  3.1544e-03],\n",
       "                      [-1.5467e-02,  1.9042e-02,  2.5861e-02,  ...,  2.3330e-03,\n",
       "                       -5.8655e-03,  2.2571e-02],\n",
       "                      [-2.2951e-02,  4.9628e-03,  7.5520e-03,  ...,  2.2974e-02,\n",
       "                        1.7007e-02,  7.2556e-03]])),\n",
       "             ('transformer.layer.1.mlp.c_proj.bias',\n",
       "              tensor([-0.0203,  0.0033, -0.0026,  0.0247, -0.0087, -0.0038, -0.0253, -0.0213,\n",
       "                      -0.0306,  0.0258,  0.0227,  0.0158,  0.0239, -0.0068,  0.0224,  0.0016,\n",
       "                      -0.0198, -0.0067,  0.0080,  0.0026,  0.0126, -0.0081, -0.0180,  0.0100,\n",
       "                       0.0013, -0.0307, -0.0031, -0.0113,  0.0093,  0.0020,  0.0086, -0.0160,\n",
       "                       0.0026,  0.0231, -0.0236,  0.0087, -0.0304, -0.0234, -0.0012,  0.0067,\n",
       "                       0.0145,  0.0221, -0.0309, -0.0225, -0.0281, -0.0106,  0.0173,  0.0129,\n",
       "                       0.0017, -0.0208,  0.0049,  0.0303,  0.0083,  0.0230, -0.0145, -0.0226,\n",
       "                       0.0265, -0.0144, -0.0127,  0.0077,  0.0188,  0.0069, -0.0065, -0.0138,\n",
       "                      -0.0014,  0.0152,  0.0252, -0.0112, -0.0011, -0.0217,  0.0068, -0.0118,\n",
       "                      -0.0071, -0.0150,  0.0170, -0.0167, -0.0134,  0.0001,  0.0169,  0.0284,\n",
       "                       0.0143, -0.0269,  0.0250,  0.0108,  0.0197, -0.0126, -0.0119,  0.0230,\n",
       "                       0.0117,  0.0290,  0.0259, -0.0186,  0.0024, -0.0116,  0.0062,  0.0243,\n",
       "                      -0.0308,  0.0046, -0.0167, -0.0041, -0.0089,  0.0032,  0.0204,  0.0188,\n",
       "                      -0.0165,  0.0219, -0.0044, -0.0301, -0.0226,  0.0079,  0.0158, -0.0265,\n",
       "                       0.0122, -0.0195,  0.0179,  0.0069, -0.0273,  0.0051, -0.0088, -0.0080,\n",
       "                       0.0200, -0.0057, -0.0045, -0.0067, -0.0165,  0.0264,  0.0277, -0.0157,\n",
       "                       0.0066, -0.0221, -0.0218, -0.0252,  0.0178,  0.0267,  0.0227, -0.0050,\n",
       "                      -0.0223, -0.0117,  0.0125,  0.0284, -0.0298, -0.0170, -0.0188, -0.0078,\n",
       "                      -0.0084, -0.0159, -0.0006,  0.0287, -0.0157,  0.0039, -0.0260, -0.0095,\n",
       "                      -0.0052,  0.0206, -0.0014,  0.0123, -0.0103, -0.0253, -0.0295,  0.0218,\n",
       "                       0.0171,  0.0034, -0.0256, -0.0031,  0.0214, -0.0184, -0.0264, -0.0168,\n",
       "                       0.0038, -0.0141, -0.0172, -0.0132, -0.0135,  0.0124,  0.0045, -0.0302,\n",
       "                       0.0056,  0.0130,  0.0113,  0.0204, -0.0084, -0.0086, -0.0048, -0.0040,\n",
       "                      -0.0082,  0.0211, -0.0151,  0.0118,  0.0244,  0.0295, -0.0292, -0.0251,\n",
       "                       0.0092, -0.0117,  0.0219,  0.0050,  0.0057, -0.0171,  0.0158,  0.0151,\n",
       "                      -0.0103, -0.0057,  0.0191,  0.0191, -0.0224,  0.0108,  0.0227,  0.0293,\n",
       "                      -0.0054,  0.0266, -0.0182,  0.0258,  0.0008,  0.0176,  0.0038,  0.0281,\n",
       "                      -0.0247,  0.0310,  0.0082, -0.0075,  0.0054, -0.0289, -0.0208,  0.0090,\n",
       "                       0.0030,  0.0151, -0.0029, -0.0150,  0.0161, -0.0237,  0.0169,  0.0027,\n",
       "                      -0.0119, -0.0142,  0.0015,  0.0200,  0.0107,  0.0082, -0.0165,  0.0166,\n",
       "                       0.0119, -0.0306,  0.0076,  0.0051,  0.0123, -0.0080,  0.0052, -0.0057,\n",
       "                       0.0302, -0.0263, -0.0028,  0.0247, -0.0093,  0.0036,  0.0219, -0.0136])),\n",
       "             ('transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4092)),\n",
       "             ('transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5107)),\n",
       "             ('transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4092)),\n",
       "             ('transformer.layer.1.ln_1.bn.weight',\n",
       "              tensor([0.9953, 0.9932, 0.9971, 1.0079, 0.9953, 0.9981, 0.9995, 0.9931, 0.9836,\n",
       "                      0.9842, 0.9947, 0.9938, 0.9917, 0.9951, 0.9944, 0.9910, 0.9929, 0.9975,\n",
       "                      0.9935, 1.0084, 0.9960, 0.9917, 0.9960, 0.9947, 0.9932, 0.9956, 1.0002,\n",
       "                      0.9976, 0.9886, 0.9934, 1.0020, 0.9934, 1.0021, 0.9986, 1.0133, 0.9871,\n",
       "                      0.9969, 0.9965, 0.9939, 0.9887, 0.9960, 1.0091, 0.9890, 0.9865, 0.9997,\n",
       "                      1.0014, 0.9993, 0.9897, 1.0014, 0.9966, 0.9872, 1.0013, 0.9941, 0.9872,\n",
       "                      0.9921, 1.0012, 0.9935, 0.9968, 0.9854, 1.0029, 0.9942, 0.9881, 0.9891,\n",
       "                      0.9974, 0.9974, 1.0017, 1.0034, 1.0035, 0.9920, 0.9993, 0.9938, 0.9974,\n",
       "                      1.0005, 1.0010, 0.9961, 0.9963, 0.9994, 0.9768, 0.9938, 1.0028, 0.9967,\n",
       "                      0.9886, 0.9906, 0.9976, 0.9811, 0.9994, 1.0018, 0.9920, 0.9948, 0.9963,\n",
       "                      0.9997, 0.9997, 1.0014, 1.0060, 1.0010, 1.0047, 1.0035, 0.9964, 0.9901,\n",
       "                      0.9854, 0.9958, 0.9922, 1.0070, 0.9946, 0.9877, 1.0087, 0.9858, 1.0002,\n",
       "                      1.0004, 0.9863, 1.0058, 1.0011, 0.9839, 0.9933, 1.0121, 0.9948, 1.0038,\n",
       "                      1.0012, 1.0009, 0.9809, 0.9970, 0.9901, 0.9920, 1.0030, 0.9907, 1.0038,\n",
       "                      0.9949, 0.9970, 0.9884, 0.9918, 0.9939, 0.9959, 0.9922, 0.9875, 0.9960,\n",
       "                      1.0034, 1.0041, 0.9867, 0.9978, 0.9917, 0.9972, 0.9843, 0.9936, 0.9979,\n",
       "                      0.9920, 0.9946, 0.9826, 0.9917, 0.9935, 0.9915, 0.9922, 0.9902, 0.9978,\n",
       "                      0.9868, 0.9886, 0.9908, 1.0002, 0.9890, 0.9904, 0.9992, 0.9971, 0.9788,\n",
       "                      1.0005, 0.9975, 0.9959, 0.9952, 0.9939, 1.0068, 0.9967, 0.9937, 0.9878,\n",
       "                      0.9989, 0.9945, 0.9936, 1.0015, 0.9944, 0.9865, 0.9948, 0.9938, 0.9932,\n",
       "                      0.9998, 1.0011, 1.0017, 0.9924, 0.9962, 0.9965, 0.9976, 0.9984, 0.9953,\n",
       "                      0.9954, 0.9939, 0.9933, 0.9999, 0.9954, 1.0084, 0.9995, 0.9997, 1.0018,\n",
       "                      0.9978, 0.9983, 0.9962, 1.0098, 1.0011, 0.9939, 0.9929, 0.9970, 0.9897,\n",
       "                      0.9959, 1.0044, 1.0011, 0.9889, 1.0035, 0.9832, 0.9904, 0.9997, 0.9959,\n",
       "                      1.0028, 0.9988, 0.9927, 1.0058, 1.0059, 1.0045, 1.0068, 0.9885, 0.9949,\n",
       "                      0.9968, 0.9945, 0.9976, 0.9946, 0.9958, 0.9965, 1.0070, 0.9971, 0.9986,\n",
       "                      0.9945, 0.9939, 1.0013, 0.9985, 0.9964, 0.9972, 1.0188, 0.9984, 0.9891,\n",
       "                      0.9987, 0.9978, 0.9934, 0.9992, 0.9953, 0.9908, 0.9905, 1.0006, 0.9931,\n",
       "                      0.9987, 1.0025, 1.0015, 1.0029])),\n",
       "             ('transformer.layer.1.ln_1.bn.bias',\n",
       "              tensor([-5.8221e-03,  5.1567e-03, -2.7539e-03, -3.2386e-03, -1.4047e-03,\n",
       "                       2.1491e-03,  7.0566e-03, -7.2472e-04,  7.1937e-03, -3.3103e-04,\n",
       "                       7.6884e-03, -5.6532e-03,  4.5102e-03, -2.5344e-03,  4.6091e-03,\n",
       "                       5.8198e-03, -4.4172e-03,  6.7577e-04,  3.1598e-03, -6.0722e-03,\n",
       "                       2.3039e-03, -2.0422e-03, -2.6043e-03,  2.3514e-04, -1.3696e-04,\n",
       "                      -2.7391e-03,  3.0222e-04, -7.9647e-03, -2.0969e-03, -8.2105e-04,\n",
       "                       1.3519e-02,  9.0652e-03, -2.3756e-03, -5.7700e-03,  7.1253e-03,\n",
       "                      -6.1457e-03,  1.3592e-03,  4.0590e-03,  1.9296e-02, -8.8499e-03,\n",
       "                      -2.7389e-03,  1.0754e-02, -3.8546e-04, -7.5427e-03,  1.0755e-02,\n",
       "                       9.2125e-04, -8.2215e-04, -4.6560e-04, -8.4366e-03,  4.2312e-03,\n",
       "                       7.4299e-03, -2.8996e-03, -1.4430e-02, -1.5712e-02,  4.3866e-04,\n",
       "                       6.7002e-03, -2.7704e-03, -3.0450e-03, -7.3768e-03,  1.7011e-03,\n",
       "                      -5.2680e-03, -1.1853e-03, -4.2184e-03, -4.4225e-03,  4.4505e-03,\n",
       "                      -9.1620e-03, -5.2125e-03, -5.6020e-03,  8.8070e-03, -6.4308e-03,\n",
       "                       3.6090e-03, -7.4969e-03,  3.8329e-04, -7.5339e-03, -1.2599e-02,\n",
       "                       1.1228e-02, -1.8359e-03, -1.0769e-02, -6.0324e-03,  1.0948e-02,\n",
       "                      -3.9661e-03,  2.8852e-03, -6.9461e-03,  4.5364e-03, -2.0028e-03,\n",
       "                      -3.2919e-03, -5.0685e-03,  3.4899e-05, -2.6441e-03, -9.3618e-03,\n",
       "                      -4.0739e-03, -9.2865e-03,  2.1343e-03,  6.3682e-03,  1.0055e-02,\n",
       "                      -7.2832e-03, -1.8698e-04, -1.3970e-03,  1.0466e-02, -4.9834e-03,\n",
       "                      -7.1049e-04,  1.9894e-03, -2.7675e-03,  5.0164e-03,  5.1143e-03,\n",
       "                      -8.2889e-03,  9.7298e-03, -2.8950e-04,  1.2949e-03, -3.2998e-04,\n",
       "                       1.2702e-03, -1.8586e-03,  2.2046e-03,  7.5215e-03,  1.9129e-03,\n",
       "                       6.1702e-04, -8.9103e-04,  2.4675e-03, -6.3069e-03,  1.7767e-03,\n",
       "                      -6.6770e-03, -3.2735e-03, -5.1627e-03, -1.1403e-03, -6.3716e-03,\n",
       "                       2.0715e-03,  8.9919e-03,  2.4345e-03, -1.2673e-03,  3.7260e-03,\n",
       "                      -7.0699e-03,  3.8565e-05, -4.2306e-05,  9.5679e-03, -1.0547e-02,\n",
       "                       1.9003e-03, -3.3257e-03,  1.3666e-02,  2.6138e-04, -4.1423e-03,\n",
       "                      -6.5063e-03, -3.4874e-03, -3.7485e-03,  1.0733e-03,  4.7532e-04,\n",
       "                       1.0517e-02, -7.4096e-03, -3.5391e-03,  6.0124e-04,  3.1639e-03,\n",
       "                      -6.1817e-04,  8.0520e-03, -4.7760e-03,  4.6868e-03, -3.4434e-03,\n",
       "                      -4.0857e-04,  5.5039e-05, -1.4874e-02,  1.6638e-02,  5.6338e-03,\n",
       "                      -5.4811e-03, -1.3302e-02,  9.2119e-03,  2.2444e-03,  6.9384e-04,\n",
       "                      -1.2393e-02,  3.9307e-03,  1.3712e-03,  6.2569e-03,  5.7022e-03,\n",
       "                      -1.1866e-02,  2.6987e-04, -2.9723e-03,  4.6817e-04, -5.7003e-03,\n",
       "                       4.4396e-03, -8.1133e-04,  1.2583e-02, -1.0043e-02, -5.6614e-03,\n",
       "                       3.9674e-03,  5.8616e-03,  4.2365e-03,  1.1961e-02,  1.8009e-05,\n",
       "                       1.0455e-03, -4.7964e-03,  8.5359e-03,  1.0323e-02,  1.8004e-04,\n",
       "                      -1.7126e-03,  3.7596e-03,  3.9523e-03,  6.2968e-03, -1.8048e-03,\n",
       "                       1.2506e-03, -5.6832e-03, -4.9432e-03, -6.7194e-03,  5.4706e-03,\n",
       "                       9.6134e-03,  3.5158e-03, -9.5797e-03, -1.3620e-02, -4.0934e-04,\n",
       "                       3.6817e-03,  1.3185e-02, -3.0881e-03, -5.7888e-03, -1.2137e-04,\n",
       "                      -6.4653e-04, -6.1570e-03, -1.1290e-03, -8.3489e-04,  7.9154e-03,\n",
       "                       4.0959e-03, -3.2487e-03,  2.6119e-03,  2.4061e-03,  4.6800e-03,\n",
       "                      -1.8684e-03,  8.0169e-03, -5.1568e-04, -5.0112e-03,  5.9902e-03,\n",
       "                      -2.5044e-03,  1.4882e-02,  6.8808e-03, -1.2065e-03, -6.5927e-03,\n",
       "                      -1.7777e-02,  1.9434e-03,  6.0327e-03, -2.3244e-03,  6.9356e-03,\n",
       "                      -5.0458e-04,  6.5660e-03, -7.7588e-03,  8.1408e-03,  1.4749e-03,\n",
       "                       7.7715e-03, -3.8637e-03,  7.4098e-03, -2.4731e-03, -1.5271e-03,\n",
       "                      -5.2852e-03, -1.0320e-02, -3.9096e-03,  1.0299e-04, -7.2018e-03,\n",
       "                      -3.8862e-03, -3.0657e-03, -3.3569e-03, -1.0884e-03,  1.5932e-03,\n",
       "                       1.8049e-03])),\n",
       "             ('transformer.layer.1.ln_1.bn.running_mean',\n",
       "              tensor([ 1.1139e-02,  1.5441e-01,  1.1539e-01,  1.0086e-01,  1.0197e-01,\n",
       "                       2.2128e-02, -3.2519e-02,  6.2103e-02, -1.3045e-01, -6.4599e-02,\n",
       "                      -7.3170e-02, -5.1458e-03, -1.3689e-01,  7.8431e-02, -5.1795e-02,\n",
       "                       4.6568e-02,  5.8648e-02, -1.3398e-01, -3.9924e-05, -3.2148e-02,\n",
       "                      -1.3576e-01,  1.8537e-01,  1.1509e-01,  2.2635e-02,  7.0844e-02,\n",
       "                      -4.4114e-02,  3.5481e-02, -1.7169e-01,  1.2571e-01, -6.7973e-02,\n",
       "                      -7.8461e-02,  8.5584e-02, -5.9387e-02,  4.3660e-02,  2.6600e-01,\n",
       "                      -1.2397e-01, -5.2547e-03,  4.4324e-02, -1.7845e-01, -4.4629e-03,\n",
       "                      -3.9899e-02,  2.1679e-02,  1.1464e-01,  6.7306e-02, -9.9234e-02,\n",
       "                       4.4555e-02, -1.1312e-02, -2.3754e-01,  4.3854e-03, -1.2301e-01,\n",
       "                       5.2956e-02, -2.3322e-02,  1.2563e-01,  1.1484e-01, -1.7586e-01,\n",
       "                       9.1538e-02, -1.8795e-01, -1.2293e-01, -5.1974e-02,  8.3926e-02,\n",
       "                      -8.7769e-02,  1.2748e-01,  3.1639e-02, -1.3973e-01,  2.7462e-02,\n",
       "                      -1.6644e-01,  4.4277e-02,  1.6907e-02,  1.6516e-01,  7.4672e-02,\n",
       "                       9.7025e-02, -1.9486e-01,  2.0828e-02,  1.6407e-01, -2.1148e-01,\n",
       "                       2.2964e-02,  7.8262e-02, -2.8969e-01,  8.4015e-02,  2.6156e-01,\n",
       "                      -1.2613e-01, -1.0260e-01, -1.2713e-01, -1.5781e-02,  1.4045e-01,\n",
       "                      -2.9894e-01, -2.5795e-02,  1.9799e-01,  2.1032e-01,  3.9289e-03,\n",
       "                       5.3842e-02, -2.9281e-01, -6.1684e-02, -1.1645e-01, -6.0875e-02,\n",
       "                      -3.3407e-02,  9.5081e-02,  5.6504e-02,  1.0790e-01,  1.0986e-01,\n",
       "                      -2.1840e-01,  1.8539e-02, -1.4655e-01,  1.3299e-01,  8.7966e-02,\n",
       "                       5.1052e-02,  2.6262e-02,  1.3373e-01, -3.6845e-02,  4.6378e-02,\n",
       "                       1.8143e-02,  1.3235e-01, -9.9589e-03, -6.1850e-03, -1.3508e-01,\n",
       "                      -1.8862e-01,  3.4351e-02, -2.0028e-01,  3.9759e-02, -7.9982e-02,\n",
       "                      -2.5930e-01,  6.1060e-02,  1.5390e-01,  3.1436e-02,  5.1514e-03,\n",
       "                      -5.5080e-02,  1.5296e-01, -1.7800e-01,  5.4793e-02, -2.1589e-01,\n",
       "                      -1.0736e-01, -6.0281e-03, -2.7824e-02,  2.3527e-01,  6.2133e-02,\n",
       "                      -8.3617e-02, -1.2886e-01, -1.9083e-01,  8.5297e-02, -9.8991e-02,\n",
       "                      -1.0497e-01,  6.5014e-02, -8.6640e-02, -3.5754e-01, -4.0042e-02,\n",
       "                       1.5787e-02, -2.3844e-02,  2.6347e-02, -7.5449e-02,  3.6007e-02,\n",
       "                      -2.4754e-01,  1.3986e-02, -7.2623e-02, -3.2258e-02, -1.4196e-01,\n",
       "                      -4.4362e-02,  9.7508e-03,  4.6639e-02, -3.1924e-02,  5.5722e-02,\n",
       "                      -2.6200e-01,  2.5941e-01, -2.3457e-01,  6.0053e-02,  1.4134e-01,\n",
       "                       4.7252e-02, -1.8339e-01, -1.3771e-01, -2.1559e-01,  9.1178e-02,\n",
       "                      -1.6983e-01,  1.0473e-01,  9.7824e-02,  2.6898e-02, -7.4671e-02,\n",
       "                       3.2569e-02,  9.9439e-02, -4.5330e-02, -1.0039e-01,  3.8105e-02,\n",
       "                      -1.1793e-01, -3.8735e-02,  1.5331e-01,  2.1662e-02,  4.2178e-02,\n",
       "                      -1.1204e-01, -8.0245e-02, -1.2954e-01, -1.6914e-01,  8.1506e-02,\n",
       "                      -1.3791e-01,  3.2912e-01, -1.5844e-01,  5.2776e-02, -8.9005e-03,\n",
       "                      -7.9340e-02, -4.1346e-01,  3.3298e-03, -9.8208e-02,  9.2478e-02,\n",
       "                       1.2854e-01, -1.3172e-01, -1.1384e-01,  1.3338e-01,  1.9197e-01,\n",
       "                      -2.1196e-01,  4.5352e-02,  1.0272e-01, -7.9552e-02,  7.0065e-02,\n",
       "                       2.2445e-02,  8.6010e-03, -2.0632e-02, -1.7339e-01, -2.1702e-01,\n",
       "                      -2.5469e-01,  7.9613e-02, -6.6653e-02, -4.2330e-04,  1.7982e-02,\n",
       "                       2.2468e-01,  1.5981e-01, -1.0608e-01, -2.1494e-01,  1.4774e-01,\n",
       "                       1.7087e-01,  5.2804e-02, -5.2291e-02, -4.7880e-02,  3.7800e-01,\n",
       "                      -4.0258e-02, -3.6016e-02, -9.1592e-02,  4.2890e-02,  8.6158e-02,\n",
       "                      -1.0563e-01, -2.9807e-01, -4.1502e-02, -4.3571e-02,  5.2386e-02,\n",
       "                       6.4171e-03,  6.2174e-02, -1.6760e-01,  2.3647e-01,  2.3520e-02,\n",
       "                      -2.7284e-01, -7.6285e-02,  2.4832e-02, -1.5880e-01, -1.8341e-01,\n",
       "                       5.7653e-02,  4.1724e-02, -2.7600e-02, -9.4982e-02, -2.7544e-01,\n",
       "                      -5.9760e-02])),\n",
       "             ('transformer.layer.1.ln_1.bn.running_var',\n",
       "              tensor([1.1033, 1.0432, 1.0183, 1.0500, 0.9855, 1.0931, 1.0439, 1.0350, 1.0518,\n",
       "                      1.0544, 1.0641, 1.0645, 1.0392, 1.0054, 1.0231, 1.0044, 1.0192, 0.9599,\n",
       "                      1.0473, 1.0104, 0.9968, 1.0258, 1.0443, 1.0178, 1.0672, 1.1176, 1.0329,\n",
       "                      1.0873, 1.0577, 1.0530, 1.0039, 1.0278, 1.0293, 1.0099, 1.0441, 1.0627,\n",
       "                      1.0239, 1.0314, 0.9695, 1.0625, 1.0818, 1.0460, 1.0204, 1.0429, 1.0220,\n",
       "                      1.0164, 1.0350, 1.0767, 1.0520, 1.0461, 1.0272, 1.0502, 0.9929, 1.0492,\n",
       "                      1.0361, 1.0371, 1.0138, 1.0513, 1.0876, 0.9735, 1.0299, 0.9701, 1.0592,\n",
       "                      1.0017, 1.0320, 1.0372, 0.9741, 1.0133, 1.0668, 0.9822, 1.0476, 0.9450,\n",
       "                      0.9798, 1.0441, 0.9788, 1.1448, 0.9832, 1.0937, 1.0427, 1.0188, 1.0787,\n",
       "                      1.0354, 1.0573, 1.0409, 1.0294, 1.0981, 1.1061, 1.0575, 1.0609, 1.0710,\n",
       "                      1.0707, 1.0341, 1.0641, 1.0417, 1.0230, 0.9654, 0.9841, 0.9771, 1.0003,\n",
       "                      1.0244, 1.0036, 1.0452, 1.0454, 0.9848, 0.9974, 1.1427, 0.9959, 0.9853,\n",
       "                      1.0678, 0.9970, 1.0179, 1.0616, 1.0373, 1.0306, 1.0441, 0.9299, 1.0579,\n",
       "                      1.0239, 1.0620, 0.9936, 1.0690, 0.9638, 1.0729, 1.0719, 1.0439, 0.9687,\n",
       "                      1.0046, 1.1030, 1.0135, 1.0096, 0.9952, 1.0575, 1.0608, 1.0719, 0.9842,\n",
       "                      0.9927, 0.9772, 1.0270, 1.0925, 0.9793, 1.0268, 1.0366, 0.9834, 1.0327,\n",
       "                      1.0388, 0.9900, 1.0105, 1.0517, 1.0497, 1.0206, 1.0536, 1.0243, 1.0068,\n",
       "                      1.0358, 1.0978, 1.0729, 1.0664, 1.0748, 1.0141, 1.0272, 1.0366, 0.9710,\n",
       "                      1.0201, 1.0659, 1.0192, 1.0368, 1.0496, 1.0301, 0.9891, 0.9521, 1.0530,\n",
       "                      0.9831, 0.9514, 1.0495, 1.0363, 1.0566, 1.0154, 1.0215, 1.0709, 1.0324,\n",
       "                      1.0471, 1.0398, 1.0214, 1.0146, 1.1199, 1.0366, 1.0296, 1.0396, 1.0525,\n",
       "                      1.0182, 1.0364, 1.0133, 1.0839, 1.0291, 1.0432, 1.0077, 1.0752, 1.0203,\n",
       "                      1.0220, 1.0641, 1.0535, 1.0942, 1.0811, 1.0717, 1.0118, 1.0736, 1.0451,\n",
       "                      1.0070, 0.9879, 1.0629, 1.0582, 0.9886, 1.0738, 1.0105, 0.9976, 1.0374,\n",
       "                      1.0130, 1.0024, 1.1088, 1.0263, 0.9969, 1.0849, 1.0101, 1.0430, 1.0199,\n",
       "                      1.0436, 1.0303, 1.0576, 1.0494, 1.0157, 1.0423, 1.0492, 1.0377, 1.0853,\n",
       "                      1.0643, 1.0718, 1.0914, 1.0004, 1.0318, 1.0022, 1.0329, 0.9753, 1.0275,\n",
       "                      1.0359, 0.9929, 0.9399, 1.0037, 1.0580, 1.0311, 1.0435, 1.0287, 1.0051,\n",
       "                      1.0385, 1.0428, 1.0632, 1.0259])),\n",
       "             ('transformer.layer.1.ln_1.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9571)),\n",
       "             ('transformer.layer.1.ln_2.bn.weight',\n",
       "              tensor([0.9831, 1.0079, 1.0054, 0.9961, 1.0219, 0.9974, 0.9965, 0.9988, 1.0022,\n",
       "                      0.9969, 0.9748, 0.9880, 0.9796, 0.9950, 1.0020, 0.9958, 0.9953, 0.9800,\n",
       "                      0.9917, 0.9957, 0.9950, 1.0026, 0.9891, 0.9901, 1.0093, 0.9957, 0.9909,\n",
       "                      1.0078, 0.9992, 1.0023, 0.9896, 1.0086, 0.9895, 0.9783, 0.9703, 0.9986,\n",
       "                      0.9989, 1.0226, 0.9950, 1.0076, 0.9941, 0.9928, 0.9742, 0.9934, 0.9953,\n",
       "                      0.9915, 0.9860, 0.9958, 1.0101, 0.9899, 1.0053, 0.9970, 0.9896, 1.0014,\n",
       "                      0.9983, 0.9677, 0.9818, 0.9887, 0.9839, 0.9920, 1.0060, 0.9750, 0.9835,\n",
       "                      0.9916, 0.9750, 1.0108, 0.9988, 0.9874, 0.9782, 1.0152, 0.9805, 0.9864,\n",
       "                      0.9892, 0.9947, 0.9761, 1.0020, 0.9885, 0.9667, 0.9924, 1.0001, 0.9770,\n",
       "                      0.9956, 0.9998, 1.0158, 0.9911, 0.9854, 0.9901, 0.9829, 1.0041, 1.0120,\n",
       "                      0.9811, 0.9989, 1.0080, 0.9922, 1.0077, 0.9867, 0.9968, 0.9811, 1.0070,\n",
       "                      0.9955, 0.9771, 1.0112, 0.9920, 1.0053, 0.9876, 1.0030, 1.0151, 0.9948,\n",
       "                      0.9674, 0.9946, 1.0019, 0.9759, 0.9872, 0.9893, 0.9974, 0.9946, 0.9858,\n",
       "                      0.9860, 0.9886, 0.9968, 0.9902, 0.9883, 0.9659, 0.9846, 0.9814, 0.9808,\n",
       "                      0.9886, 0.9850, 0.9824, 0.9845, 0.9859, 1.0141, 0.9733, 0.9876, 0.9920,\n",
       "                      0.9949, 0.9952, 1.0184, 0.9934, 0.9684, 1.0093, 1.0035, 0.9852, 0.9783,\n",
       "                      0.9836, 0.9887, 0.9958, 0.9815, 0.9965, 0.9884, 0.9741, 0.9836, 1.0124,\n",
       "                      0.9779, 0.9963, 1.0127, 1.0056, 0.9954, 0.9948, 0.9766, 1.0048, 0.9928,\n",
       "                      0.9938, 1.0110, 0.9843, 0.9822, 0.9910, 0.9909, 0.9759, 0.9716, 1.0075,\n",
       "                      0.9870, 0.9887, 0.9977, 0.9949, 0.9852, 0.9749, 0.9874, 0.9857, 1.0015,\n",
       "                      0.9860, 0.9869, 1.0038, 1.0065, 1.0047, 0.9825, 0.9787, 0.9935, 0.9988,\n",
       "                      0.9974, 1.0004, 0.9787, 0.9689, 0.9696, 0.9813, 1.0047, 1.0058, 1.0073,\n",
       "                      0.9890, 1.0005, 0.9837, 1.0025, 0.9964, 0.9877, 1.0060, 1.0121, 0.9818,\n",
       "                      0.9579, 0.9970, 1.0190, 0.9763, 1.0005, 0.9952, 1.0000, 0.9996, 0.9836,\n",
       "                      1.0126, 0.9999, 0.9875, 0.9900, 0.9964, 1.0033, 0.9892, 0.9759, 0.9907,\n",
       "                      0.9822, 0.9948, 0.9778, 0.9799, 0.9828, 0.9693, 1.0185, 1.0125, 0.9905,\n",
       "                      0.9993, 0.9903, 0.9887, 0.9933, 0.9999, 0.9985, 0.9938, 0.9882, 0.9912,\n",
       "                      0.9966, 0.9874, 0.9870, 0.9788, 0.9902, 1.0050, 0.9749, 0.9955, 0.9911,\n",
       "                      0.9913, 0.9864, 0.9788, 1.0012])),\n",
       "             ('transformer.layer.1.ln_2.bn.bias',\n",
       "              tensor([ 5.0810e-04,  2.2260e-03,  2.0139e-02,  1.4007e-02, -1.7009e-02,\n",
       "                       1.4115e-02, -2.3324e-02, -1.2341e-03, -1.2072e-02, -2.0208e-03,\n",
       "                      -1.8786e-02, -3.0154e-03,  3.9364e-03, -3.3066e-03,  1.9050e-02,\n",
       "                      -6.3591e-04,  1.8423e-02,  1.8148e-02,  1.0736e-02,  3.2033e-03,\n",
       "                       4.8249e-03, -8.6844e-03, -9.3156e-03, -9.7410e-03, -1.6282e-02,\n",
       "                      -3.0445e-04, -7.9906e-03,  8.2209e-03, -2.1252e-03, -2.4963e-02,\n",
       "                      -9.3795e-03,  1.6345e-02,  2.2916e-02,  1.9191e-03,  5.3739e-03,\n",
       "                       8.4440e-04, -8.8147e-03, -2.4676e-02,  1.2385e-02, -4.8957e-03,\n",
       "                      -1.1943e-02, -9.0304e-03,  6.3513e-03,  5.4888e-03,  1.1922e-03,\n",
       "                       6.7514e-03,  2.9962e-04, -2.9280e-02, -7.5892e-03,  4.0167e-03,\n",
       "                      -5.4013e-03, -7.1883e-03, -2.1465e-03, -9.5972e-03,  2.8820e-03,\n",
       "                      -6.5771e-03,  1.3990e-03, -1.4911e-02, -1.1819e-02,  6.5806e-03,\n",
       "                       8.5870e-03, -1.4183e-02,  1.0823e-02,  4.9194e-03, -8.6388e-03,\n",
       "                      -3.3424e-03,  3.7726e-06,  4.1957e-03, -4.3190e-03, -2.3190e-03,\n",
       "                      -5.3815e-03, -5.8048e-03,  3.2512e-03,  2.2481e-02,  5.2318e-03,\n",
       "                      -4.0507e-03, -1.5391e-03, -2.0346e-03, -1.4961e-02,  1.6682e-02,\n",
       "                      -9.1186e-03,  8.2513e-03,  2.1799e-02, -9.8468e-03,  5.1663e-05,\n",
       "                       8.1228e-05, -1.1013e-02, -1.7367e-02,  4.6571e-03, -1.3915e-02,\n",
       "                      -2.3020e-03,  2.0314e-02,  9.7351e-03,  2.3743e-02, -1.3724e-02,\n",
       "                       9.7753e-04, -2.8619e-02, -1.0550e-02,  2.2225e-02, -4.1173e-03,\n",
       "                       1.6873e-02, -2.0226e-02, -2.0354e-03,  3.4337e-03, -2.2665e-03,\n",
       "                       7.2060e-03, -3.3002e-03,  1.0266e-02,  4.7099e-03,  2.4212e-02,\n",
       "                      -1.6816e-03,  8.1377e-04,  3.3085e-03,  2.4109e-03, -1.1460e-02,\n",
       "                       2.1251e-02, -1.3491e-03, -1.1962e-02,  3.0592e-03, -1.4343e-02,\n",
       "                       5.1494e-03, -7.3220e-03, -7.0191e-03, -1.3196e-02, -6.9530e-04,\n",
       "                      -1.7020e-02, -2.5450e-02, -1.3397e-02,  9.5504e-03,  8.2364e-03,\n",
       "                       8.6899e-03,  8.8057e-03,  1.3925e-02,  1.6805e-02,  2.1568e-04,\n",
       "                       8.7147e-03, -1.5728e-03, -1.1679e-04,  1.5903e-02, -7.1933e-03,\n",
       "                      -1.2309e-02,  8.7611e-03, -4.4405e-03, -3.6146e-03, -3.8659e-03,\n",
       "                      -9.8101e-03,  1.0793e-04,  3.3641e-03,  5.9730e-03,  1.4035e-02,\n",
       "                       1.3719e-02, -9.8242e-03, -2.7027e-03, -3.9151e-03, -1.1985e-02,\n",
       "                      -4.8718e-03,  7.9249e-03, -2.8365e-03, -2.5403e-03, -9.2564e-03,\n",
       "                       1.9051e-02, -8.1137e-03,  3.1790e-03,  2.2123e-02,  2.0604e-02,\n",
       "                       8.7327e-03,  1.7366e-02,  1.5353e-02, -2.2964e-02,  4.2339e-03,\n",
       "                      -6.1526e-03,  2.0417e-03, -1.4667e-03, -7.8034e-03,  5.0231e-03,\n",
       "                       3.9734e-03, -2.0449e-02, -3.1907e-03, -1.9084e-02, -6.5902e-03,\n",
       "                       7.0325e-03, -3.4281e-03, -4.5021e-04,  1.1948e-02, -8.5247e-03,\n",
       "                       1.5528e-02,  5.2197e-03, -1.9829e-02, -7.3630e-04,  1.8906e-02,\n",
       "                      -5.3263e-03,  7.6433e-03, -6.3328e-03,  1.0419e-02, -3.1822e-02,\n",
       "                      -1.0802e-02,  1.4463e-02, -9.5410e-03, -3.1390e-03, -1.0768e-02,\n",
       "                       1.6894e-02, -1.8262e-02,  3.7256e-03,  1.3838e-02, -3.4028e-03,\n",
       "                       1.2367e-02, -1.8607e-04, -4.1150e-03, -7.6958e-04,  1.4816e-02,\n",
       "                      -3.4654e-03, -2.9064e-03, -4.5450e-03, -4.1109e-03, -8.3334e-03,\n",
       "                       2.0326e-02,  1.2961e-02,  4.9955e-03, -7.9388e-03, -1.9305e-03,\n",
       "                      -1.2993e-02, -1.3283e-03,  9.7526e-03, -5.5189e-03,  3.0794e-03,\n",
       "                      -4.6258e-03,  1.2484e-02, -9.9925e-03, -4.8786e-03,  2.3004e-02,\n",
       "                       4.2997e-03, -9.9293e-03, -7.5440e-03, -1.1227e-03, -3.2404e-03,\n",
       "                       3.1280e-03,  2.8269e-02,  1.3784e-02,  1.4942e-02, -5.6776e-03,\n",
       "                      -1.2616e-02, -1.3355e-02, -3.0417e-02, -9.7531e-04,  4.8611e-03,\n",
       "                      -4.3179e-03,  8.5396e-04,  2.3420e-02,  4.8037e-03,  6.5845e-03,\n",
       "                      -1.4697e-02, -1.8788e-02,  1.1566e-03,  1.4785e-02, -8.8301e-03,\n",
       "                       5.9381e-03])),\n",
       "             ('transformer.layer.1.ln_2.bn.running_mean',\n",
       "              tensor([-1.9021e-02, -1.9254e-02, -3.0956e-04, -5.2507e-03,  6.1926e-03,\n",
       "                      -1.0408e-02, -8.9222e-03,  1.6712e-02,  3.0598e-03, -8.3229e-05,\n",
       "                      -2.7902e-02, -1.4596e-02,  7.3436e-03, -1.4639e-02,  1.3739e-02,\n",
       "                       1.1970e-02, -1.7694e-02,  2.5341e-02,  1.6107e-02, -2.6036e-02,\n",
       "                       1.2762e-02,  1.6440e-02, -1.5854e-02,  3.9164e-03, -7.3323e-04,\n",
       "                      -1.2801e-02,  6.2168e-03, -2.1561e-02, -9.2425e-03, -2.3547e-02,\n",
       "                       1.5407e-02,  5.9060e-04,  1.1714e-02,  1.0355e-03, -5.4140e-03,\n",
       "                      -1.0986e-02,  3.2414e-04,  2.0229e-02,  2.9609e-02, -1.3866e-02,\n",
       "                      -1.6150e-02,  1.5443e-03, -1.9695e-02, -1.4983e-02,  6.5565e-03,\n",
       "                       1.4927e-02, -1.8320e-02,  1.4951e-03, -1.5021e-02, -3.8744e-03,\n",
       "                       2.0430e-02,  1.5196e-02, -3.0891e-02, -1.5315e-02, -1.3729e-02,\n",
       "                       6.9245e-03, -1.3314e-02, -1.0494e-02,  6.5611e-03, -6.9887e-03,\n",
       "                       4.2155e-03,  1.0500e-02, -8.4994e-03, -1.2193e-02,  1.0530e-02,\n",
       "                       5.9812e-03,  5.3299e-03, -5.0148e-03,  1.2544e-02,  6.7684e-04,\n",
       "                       1.8312e-02, -2.7306e-02,  2.5535e-03, -1.6624e-02, -1.3025e-02,\n",
       "                       6.9766e-03, -1.1135e-02, -3.3762e-03, -1.1720e-02,  7.4370e-03,\n",
       "                       3.4904e-03, -3.7858e-03, -1.1269e-02,  9.6979e-03,  8.6640e-03,\n",
       "                      -6.8590e-03,  9.7572e-04, -1.7568e-02, -1.2438e-02, -3.6310e-03,\n",
       "                      -8.3306e-03, -3.6405e-02,  2.1746e-02,  1.2477e-03,  8.7972e-03,\n",
       "                      -5.9339e-04, -8.6709e-03, -3.3926e-04,  2.4160e-02,  1.3363e-04,\n",
       "                      -4.2162e-03, -3.2987e-03, -8.9551e-03, -1.8979e-03,  2.0890e-02,\n",
       "                      -8.0663e-03,  2.5025e-02,  5.1623e-03, -1.1619e-02,  1.5582e-02,\n",
       "                       2.0806e-02, -4.5018e-03,  5.6075e-03,  1.2597e-02,  7.1517e-03,\n",
       "                      -1.6939e-02,  8.2110e-03, -1.4567e-02, -8.2460e-03, -8.5253e-03,\n",
       "                      -1.1154e-02, -1.4337e-02, -1.0535e-02, -6.4409e-03,  1.0007e-02,\n",
       "                       6.2339e-03, -1.7154e-03, -1.2003e-02,  8.3916e-03,  7.0523e-03,\n",
       "                      -1.5099e-02,  1.1364e-02, -7.4600e-03, -5.2077e-03, -1.7737e-02,\n",
       "                       2.2117e-02, -3.1385e-03,  1.7640e-02,  8.0655e-03, -2.6088e-02,\n",
       "                       5.4053e-03,  1.2795e-02,  1.9379e-03,  6.5554e-03, -7.8491e-03,\n",
       "                       3.8630e-03, -1.5895e-02, -2.8392e-02,  2.1838e-02,  1.4942e-02,\n",
       "                       1.0721e-02, -5.2960e-03, -2.2822e-02, -5.4426e-03, -5.0739e-03,\n",
       "                      -8.2867e-04,  1.1658e-02, -1.9404e-02,  9.0003e-03, -8.1483e-03,\n",
       "                      -5.2467e-03, -6.7552e-03, -7.6710e-03,  1.6222e-02, -6.8017e-03,\n",
       "                      -4.9373e-03, -1.6385e-02,  2.5699e-02,  4.3059e-03, -3.1656e-03,\n",
       "                       1.0242e-02, -2.7279e-02, -2.4342e-03, -7.1079e-04,  4.2255e-03,\n",
       "                       1.3536e-02, -5.2789e-03,  5.1763e-03, -3.7729e-03, -8.1752e-03,\n",
       "                      -2.5513e-03, -1.0004e-02,  1.0159e-02,  6.1186e-03, -1.1485e-02,\n",
       "                       1.6314e-02,  4.4675e-03,  1.5057e-02,  8.3205e-03, -7.5839e-04,\n",
       "                      -4.5638e-03, -2.2232e-03,  1.6166e-02, -1.2078e-02,  3.7097e-03,\n",
       "                      -1.3655e-02, -3.1096e-02, -1.0156e-02, -9.8921e-03,  7.8636e-04,\n",
       "                      -7.1055e-03,  9.3466e-04, -1.8699e-02, -2.1860e-02,  1.6112e-02,\n",
       "                      -9.8161e-03, -1.1986e-03, -1.2667e-02, -2.2781e-03,  4.6060e-04,\n",
       "                       2.1649e-02,  6.7031e-03,  1.5357e-02, -6.5635e-03,  3.8374e-02,\n",
       "                       3.5005e-03,  6.6233e-04, -7.4751e-03,  5.8409e-03, -6.5037e-04,\n",
       "                       6.1062e-03, -4.1679e-03, -8.4528e-03, -6.8942e-03,  3.0859e-03,\n",
       "                       1.5446e-02,  1.7444e-02,  8.1855e-03, -9.0888e-03,  1.0062e-02,\n",
       "                      -5.9940e-03,  5.0823e-03,  5.4972e-03, -5.9466e-04,  1.1431e-02,\n",
       "                       1.1935e-02,  2.8431e-02, -1.6094e-02,  2.5942e-02,  2.1838e-02,\n",
       "                      -1.5457e-02,  1.3336e-03, -1.9556e-02, -3.8828e-03, -3.7749e-03,\n",
       "                      -1.6257e-02,  2.0440e-03, -2.1535e-02,  9.8596e-03,  1.0084e-02,\n",
       "                      -1.6165e-02, -9.0303e-03, -7.9722e-03,  1.2207e-02,  1.1675e-03,\n",
       "                       1.6462e-02])),\n",
       "             ('transformer.layer.1.ln_2.bn.running_var',\n",
       "              tensor([0.9857, 0.9734, 0.9868, 1.0032, 0.9843, 0.9848, 0.9754, 0.9659, 0.9696,\n",
       "                      0.9697, 0.9733, 0.9767, 0.9654, 0.9795, 0.9753, 0.9727, 0.9827, 0.9733,\n",
       "                      0.9796, 1.0122, 0.9786, 0.9692, 0.9767, 0.9780, 0.9735, 0.9897, 0.9924,\n",
       "                      0.9929, 0.9659, 0.9800, 0.9989, 0.9835, 0.9972, 0.9910, 1.0051, 0.9697,\n",
       "                      0.9912, 0.9794, 0.9642, 0.9789, 0.9913, 1.0098, 0.9715, 0.9669, 0.9851,\n",
       "                      0.9929, 0.9909, 0.9602, 0.9745, 0.9856, 0.9708, 0.9952, 0.9821, 0.9627,\n",
       "                      0.9823, 0.9859, 0.9792, 0.9828, 0.9555, 0.9971, 0.9930, 0.9668, 0.9653,\n",
       "                      0.9857, 0.9856, 0.9921, 0.9906, 0.9968, 0.9798, 0.9824, 0.9664, 0.9871,\n",
       "                      0.9914, 0.9932, 0.9841, 0.9746, 0.9969, 0.9435, 0.9683, 1.0013, 0.9910,\n",
       "                      0.9657, 0.9771, 0.9729, 0.9644, 0.9932, 0.9969, 0.9785, 0.9841, 0.9827,\n",
       "                      0.9913, 1.0006, 0.9867, 0.9989, 0.9910, 0.9998, 0.9972, 0.9775, 0.9561,\n",
       "                      0.9567, 0.9879, 0.9714, 1.0111, 0.9767, 0.9676, 1.0051, 0.9640, 0.9892,\n",
       "                      0.9939, 0.9685, 0.9915, 0.9878, 0.9640, 0.9802, 1.0161, 0.9549, 0.9923,\n",
       "                      0.9976, 0.9922, 0.9503, 0.9966, 0.9745, 0.9768, 0.9923, 0.9787, 0.9979,\n",
       "                      0.9707, 0.9702, 0.9746, 0.9759, 0.9792, 0.9848, 0.9811, 0.9613, 0.9918,\n",
       "                      0.9969, 0.9960, 0.9699, 0.9805, 0.9745, 0.9879, 0.9658, 0.9805, 0.9841,\n",
       "                      0.9786, 0.9718, 0.9532, 0.9724, 0.9756, 0.9844, 0.9699, 0.9668, 0.9938,\n",
       "                      0.9696, 0.9726, 0.9694, 1.0007, 0.9604, 0.9616, 0.9801, 0.9792, 0.9446,\n",
       "                      0.9889, 0.9754, 0.9748, 0.9850, 0.9730, 1.0038, 0.9982, 0.9691, 0.9790,\n",
       "                      0.9880, 0.9726, 0.9747, 1.0049, 0.9809, 0.9759, 0.9756, 0.9807, 0.9866,\n",
       "                      0.9910, 0.9896, 1.0003, 0.9776, 0.9887, 0.9831, 0.9873, 0.9937, 0.9679,\n",
       "                      0.9642, 0.9866, 0.9733, 0.9906, 0.9869, 0.9946, 0.9901, 0.9919, 0.9895,\n",
       "                      0.9832, 0.9865, 0.9779, 1.0158, 0.9960, 0.9688, 0.9758, 0.9943, 0.9656,\n",
       "                      0.9892, 0.9963, 0.9855, 0.9727, 0.9983, 0.9642, 0.9682, 0.9804, 0.9794,\n",
       "                      0.9844, 0.9871, 0.9743, 1.0132, 0.9948, 0.9875, 1.0067, 0.9745, 0.9773,\n",
       "                      0.9720, 0.9851, 0.9912, 0.9707, 0.9801, 0.9883, 1.0123, 0.9770, 0.9822,\n",
       "                      0.9772, 0.9794, 0.9920, 0.9823, 0.9864, 0.9752, 1.0170, 0.9897, 0.9735,\n",
       "                      0.9933, 0.9880, 0.9744, 0.9864, 0.9718, 0.9652, 0.9748, 1.0104, 0.9867,\n",
       "                      0.9812, 0.9929, 0.9977, 1.0020])),\n",
       "             ('transformer.layer.1.ln_2.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4514)),\n",
       "             ('transformer.ln_out.bn.weight',\n",
       "              tensor([1.0098, 1.0182, 1.0206, 0.9940, 1.0315, 1.0193, 1.0004, 1.0023, 1.0057,\n",
       "                      0.9918, 1.0130, 0.9933, 1.0149, 1.0294, 0.9949, 1.0068, 1.0120, 1.0224,\n",
       "                      1.0030, 1.0115, 0.9876, 1.0168, 1.0086, 1.0177, 1.0216, 1.0171, 1.0006,\n",
       "                      1.0164, 1.0143, 1.0215, 1.0114, 0.9880, 0.9893, 1.0057, 1.0144, 0.9998,\n",
       "                      1.0189, 1.0160, 0.9874, 1.0189, 0.9929, 1.0300, 1.0104, 1.0037, 1.0004,\n",
       "                      1.0024, 1.0053, 1.0110, 1.0136, 1.0256, 1.0079, 1.0065, 1.0299, 1.0078,\n",
       "                      1.0012, 0.9975, 0.9710, 1.0101, 1.0126, 1.0157, 1.0074, 1.0027, 1.0170,\n",
       "                      1.0229, 1.0147, 1.0192, 1.0030, 1.0072, 1.0302, 1.0167, 1.0174, 1.0123,\n",
       "                      1.0002, 1.0119, 1.0056, 1.0248, 1.0043, 1.0070, 0.9948, 0.9980, 1.0049,\n",
       "                      0.9948, 0.9829, 0.9788, 1.0201, 1.0257, 1.0108, 0.9941, 1.0030, 1.0316,\n",
       "                      0.9967, 1.0079, 1.0311, 1.0181, 1.0181, 1.0116, 0.9900, 1.0156, 0.9960,\n",
       "                      0.9906, 1.0092, 0.9983, 1.0271, 1.0082, 1.0130, 1.0217, 1.0167, 1.0176,\n",
       "                      0.9991, 1.0043, 1.0212, 1.0026, 1.0277, 0.9908, 1.0168, 1.0010, 1.0118,\n",
       "                      0.9952, 1.0050, 1.0103, 1.0050, 1.0244, 1.0176, 0.9882, 0.9866, 1.0051,\n",
       "                      1.0036, 1.0082, 1.0149, 1.0202, 1.0042, 1.0413, 1.0166, 1.0226, 1.0058,\n",
       "                      1.0240, 0.9976, 1.0226, 1.0226, 1.0170, 1.0164, 1.0039, 1.0253, 1.0155,\n",
       "                      0.9835, 0.9922, 1.0211, 1.0205, 0.9855, 1.0257, 0.9913, 1.0290, 1.0055,\n",
       "                      1.0079, 1.0054, 1.0004, 0.9977, 1.0273, 1.0101, 1.0184, 1.0115, 1.0320,\n",
       "                      1.0169, 1.0317, 1.0170, 1.0118, 1.0101, 1.0018, 0.9991, 1.0060, 1.0218,\n",
       "                      1.0239, 1.0106, 1.0193, 1.0137, 0.9872, 0.9885, 1.0219, 1.0399, 0.9963,\n",
       "                      1.0011, 0.9980, 1.0137, 0.9815, 1.0146, 1.0338, 1.0161, 1.0183, 1.0066,\n",
       "                      1.0055, 0.9909, 0.9987, 1.0081, 1.0107, 0.9875, 1.0136, 1.0337, 1.0043,\n",
       "                      1.0020, 0.9884, 1.0179, 1.0238, 1.0131, 1.0110, 1.0024, 1.0192, 1.0095,\n",
       "                      1.0121, 1.0183, 1.0073, 0.9964, 0.9865, 1.0182, 1.0095, 0.9935, 1.0159,\n",
       "                      1.0196, 0.9940, 1.0054, 1.0104, 1.0103, 1.0225, 1.0099, 1.0282, 0.9893,\n",
       "                      0.9953, 1.0268, 0.9931, 1.0150, 0.9976, 0.9982, 1.0164, 1.0189, 1.0129,\n",
       "                      1.0222, 0.9824, 1.0187, 1.0157, 1.0350, 1.0114, 1.0163, 1.0136, 1.0236,\n",
       "                      1.0001, 1.0041, 1.0137, 1.0097, 1.0074, 1.0075, 1.0170, 1.0172, 0.9880,\n",
       "                      1.0179, 0.9931, 1.0029, 1.0216])),\n",
       "             ('transformer.ln_out.bn.bias',\n",
       "              tensor([ 0.0330,  0.0654,  0.0652,  0.0756,  0.0675,  0.0731,  0.0707,  0.0718,\n",
       "                       0.0746,  0.0751, -0.0680,  0.0748, -0.0683, -0.0693, -0.0662,  0.0733,\n",
       "                      -0.0735, -0.0425,  0.0659, -0.0752,  0.0604,  0.0705, -0.0752, -0.0696,\n",
       "                       0.0505,  0.0651,  0.0608, -0.0721,  0.0683,  0.0671, -0.0751,  0.0733,\n",
       "                       0.0683, -0.0739,  0.0532, -0.0737,  0.0448, -0.0695, -0.0712, -0.0538,\n",
       "                      -0.0666,  0.0757,  0.0750,  0.0731, -0.0688,  0.0650,  0.0709, -0.0397,\n",
       "                       0.0696, -0.0719,  0.0564,  0.0693,  0.0719,  0.0752,  0.0521, -0.0629,\n",
       "                       0.0738,  0.0584,  0.0762, -0.0709, -0.0551, -0.0720,  0.0490, -0.0655,\n",
       "                       0.0526, -0.0558, -0.0724,  0.0747,  0.0628,  0.0675, -0.0693,  0.0616,\n",
       "                       0.0486, -0.0532,  0.0708,  0.0676, -0.0645, -0.0670,  0.0740,  0.0729,\n",
       "                      -0.0592,  0.0711, -0.0745,  0.0725,  0.0494, -0.0753,  0.0739,  0.0396,\n",
       "                      -0.0474,  0.0629,  0.0699, -0.0526,  0.0759,  0.0648,  0.0728, -0.0759,\n",
       "                       0.0728,  0.0699, -0.0589,  0.0699,  0.0758,  0.0709, -0.0755, -0.0739,\n",
       "                      -0.0736, -0.0734, -0.0738,  0.0521, -0.0742, -0.0746,  0.0556, -0.0658,\n",
       "                       0.0699,  0.0730,  0.0560,  0.0733, -0.0560, -0.0692,  0.0716, -0.0656,\n",
       "                      -0.0706,  0.0749, -0.0486,  0.0729,  0.0605, -0.0438, -0.0613,  0.0477,\n",
       "                      -0.0640, -0.0629,  0.0621, -0.0405, -0.0751, -0.0540,  0.0733, -0.0527,\n",
       "                       0.0613, -0.0743,  0.0698,  0.0614,  0.0695, -0.0366, -0.0685,  0.0655,\n",
       "                      -0.0618, -0.0722, -0.0742,  0.0588,  0.0753,  0.0724, -0.0699,  0.0694,\n",
       "                      -0.0749,  0.0669, -0.0711,  0.0755,  0.0709,  0.0723, -0.0599,  0.0701,\n",
       "                       0.0191,  0.0713,  0.0323,  0.0683, -0.0753,  0.0727, -0.0710, -0.0393,\n",
       "                      -0.0240, -0.0694, -0.0456, -0.0534,  0.0637, -0.0730, -0.0724, -0.0710,\n",
       "                       0.0626,  0.0657,  0.0714, -0.0712, -0.0735,  0.0676,  0.0615, -0.0748,\n",
       "                      -0.0603,  0.0575,  0.0534, -0.0685, -0.0753,  0.0715,  0.0334, -0.0625,\n",
       "                       0.0605,  0.0704,  0.0688, -0.0628,  0.0473, -0.0706, -0.0571, -0.0680,\n",
       "                       0.0693, -0.0536,  0.0706,  0.0752,  0.0653, -0.0696, -0.0596, -0.0588,\n",
       "                       0.0684, -0.0736,  0.0707,  0.0701, -0.0735,  0.0670,  0.0702, -0.0584,\n",
       "                      -0.0736, -0.0754, -0.0749,  0.0719, -0.0690, -0.0675, -0.0703,  0.0743,\n",
       "                      -0.0708,  0.0710, -0.0639,  0.0701,  0.0751,  0.0714,  0.0719, -0.0704,\n",
       "                      -0.0401,  0.0741,  0.0501, -0.0580, -0.0751, -0.0735, -0.0518, -0.0737,\n",
       "                       0.0697,  0.0647, -0.0670, -0.0647, -0.0621,  0.0690,  0.0568, -0.0734,\n",
       "                       0.0744, -0.0694,  0.0640, -0.0629,  0.0475, -0.0728,  0.0697, -0.0714])),\n",
       "             ('transformer.ln_out.bn.running_mean',\n",
       "              tensor([-5.8141e-02,  1.3079e-01, -2.1400e-02,  4.9686e-03, -1.0193e-01,\n",
       "                       2.4564e-02, -7.5131e-02,  1.3508e-01,  7.0382e-02,  2.4447e-01,\n",
       "                      -2.2274e-03, -6.0076e-02,  5.6936e-02,  1.6127e-01,  3.1789e-01,\n",
       "                       2.3420e-01,  4.2342e-02, -4.8172e-02, -6.9289e-02, -3.1723e-02,\n",
       "                      -2.2066e-01, -8.1715e-02, -2.5716e-01,  1.5230e-01,  1.7414e-01,\n",
       "                       2.1539e-01, -1.5150e-01, -7.6097e-02, -1.9538e-02, -5.4798e-02,\n",
       "                       6.3223e-02,  2.0898e-01, -1.4062e-02, -7.6384e-02,  2.9279e-02,\n",
       "                       5.0781e-02, -1.6929e-01,  2.7221e-01,  1.7878e-02,  3.6105e-03,\n",
       "                      -1.7082e-01, -7.1669e-02,  1.8795e-01,  6.2661e-02, -1.0697e-01,\n",
       "                      -1.8645e-01, -1.4992e-01,  8.2736e-02,  7.7978e-02, -1.6668e-01,\n",
       "                      -2.7592e-02,  5.3022e-02,  9.3130e-02,  4.6861e-02,  5.0822e-02,\n",
       "                      -1.0599e-01,  5.1424e-02, -1.3053e-02,  3.0266e-01, -3.9892e-02,\n",
       "                      -1.9838e-01, -1.3706e-01, -3.2121e-02,  1.0754e-01,  9.4593e-02,\n",
       "                      -1.0040e-01, -8.7830e-02, -2.4346e-01, -3.7514e-02,  1.3650e-01,\n",
       "                       1.7066e-02,  8.8375e-02, -1.3163e-03, -8.2109e-02, -2.8969e-01,\n",
       "                       4.2483e-02, -5.7037e-02,  2.7972e-02, -3.0250e-01, -2.4733e-01,\n",
       "                       1.3485e-01, -9.5632e-02,  1.1636e-01, -1.5129e-01,  3.2581e-01,\n",
       "                      -1.4147e-01,  1.4585e-01, -1.3530e-01,  8.9173e-02,  7.4837e-03,\n",
       "                       1.9528e-01, -3.0883e-02, -2.0584e-02,  1.2603e-01, -1.9749e-02,\n",
       "                       1.3061e-01,  2.0388e-01,  6.2487e-02, -1.5527e-01, -7.3609e-04,\n",
       "                      -9.3891e-03,  2.0884e-02, -1.5609e-01, -4.6201e-02,  3.3214e-01,\n",
       "                       8.3825e-02,  7.2540e-02, -3.3934e-02, -3.5572e-02, -6.2375e-02,\n",
       "                       3.8312e-02, -1.3194e-01,  1.7720e-01, -3.4696e-04, -1.3001e-01,\n",
       "                      -1.9460e-01,  1.2290e-01,  1.6266e-01,  1.0195e-01,  1.2091e-02,\n",
       "                       2.7424e-02, -6.8262e-02, -9.8489e-02, -1.6596e-01, -2.1163e-01,\n",
       "                      -4.9602e-02,  1.3223e-01,  5.6779e-02,  6.5867e-02,  2.0605e-01,\n",
       "                       1.2081e-01,  2.8241e-01, -2.0585e-01, -1.3696e-01, -1.1372e-01,\n",
       "                       3.4088e-02, -4.5516e-02, -3.7179e-02,  1.4639e-01, -3.9199e-02,\n",
       "                      -2.6581e-02, -2.4225e-01,  1.8658e-01,  2.2573e-01, -1.7433e-01,\n",
       "                       1.7373e-01,  2.0363e-01, -1.1421e-01, -1.0053e-01,  1.5971e-01,\n",
       "                      -2.3361e-02, -1.0206e-01, -3.2853e-01, -3.3045e-02, -7.2461e-02,\n",
       "                       1.0288e-01, -5.9134e-02, -9.3943e-02,  1.1036e-01, -6.8434e-02,\n",
       "                       7.8905e-02, -9.1916e-02, -1.4464e-01,  2.3768e-01,  1.1836e-01,\n",
       "                      -6.0382e-02,  4.3319e-02,  1.0165e-01, -2.3457e-01, -7.9387e-02,\n",
       "                       3.3591e-01,  9.3516e-03,  2.4645e-01,  9.3470e-02,  3.7699e-02,\n",
       "                       1.0106e-01, -1.0120e-01,  3.8802e-02, -1.8633e-02, -1.4278e-01,\n",
       "                      -7.9285e-02, -1.1819e-01, -1.1813e-01, -3.0546e-02, -1.3863e-01,\n",
       "                       1.6456e-01,  2.1189e-01, -2.1941e-01, -1.7039e-01,  1.8242e-01,\n",
       "                      -5.1359e-02,  2.2068e-01,  9.5329e-02, -2.1995e-02,  3.1592e-02,\n",
       "                       1.2317e-01,  8.0677e-02, -2.2732e-01,  2.8156e-02, -1.8660e-01,\n",
       "                      -2.3038e-01, -3.3026e-02,  2.2210e-01, -1.0486e-01, -1.0387e-01,\n",
       "                       1.2544e-01,  1.6322e-01, -1.5510e-02, -1.4063e-01, -1.2700e-02,\n",
       "                       8.4921e-02,  1.8081e-02, -1.1513e-01,  5.9176e-02, -3.0183e-01,\n",
       "                      -8.6616e-02, -1.6011e-01, -7.0512e-02, -5.0375e-02,  2.5973e-01,\n",
       "                       2.1508e-01, -4.7449e-03,  1.1025e-02,  5.0393e-02, -1.0840e-02,\n",
       "                      -5.5376e-02, -3.9630e-02, -1.8986e-01,  3.4925e-01, -1.8029e-01,\n",
       "                       4.4797e-02,  1.9314e-01, -8.9562e-02,  1.3119e-01,  2.1669e-01,\n",
       "                      -4.2978e-02,  1.8271e-02,  6.0693e-02,  5.0944e-02,  6.1720e-02,\n",
       "                      -1.3678e-01, -5.2286e-02,  6.9372e-02, -1.7248e-01, -8.9332e-02,\n",
       "                       4.9449e-02,  4.9949e-02,  2.6415e-01, -5.8301e-02,  9.1511e-02,\n",
       "                       3.0951e-01,  1.1935e-01,  5.5150e-02,  2.3039e-01, -7.3223e-02,\n",
       "                      -1.0753e-01])),\n",
       "             ('transformer.ln_out.bn.running_var',\n",
       "              tensor([0.9969, 1.0467, 1.0465, 1.0654, 1.0545, 1.0892, 1.0856, 1.0478, 1.0312,\n",
       "                      1.0162, 1.0004, 1.0295, 1.0486, 1.0980, 1.0818, 1.0225, 1.0284, 0.9922,\n",
       "                      1.0822, 1.0137, 1.0299, 1.0202, 1.0466, 1.0210, 1.0818, 1.0014, 0.9589,\n",
       "                      1.0730, 1.0639, 1.1062, 1.0362, 1.0269, 1.0055, 1.0400, 0.9799, 1.0768,\n",
       "                      1.0543, 1.0697, 1.0811, 1.0434, 1.0187, 0.9911, 1.0428, 1.0479, 0.9924,\n",
       "                      1.0044, 1.0620, 1.0827, 1.0550, 1.0243, 1.0035, 1.0683, 1.0615, 1.0242,\n",
       "                      1.0879, 0.9367, 1.0786, 1.0521, 0.9815, 1.0367, 1.0704, 1.0031, 1.0126,\n",
       "                      1.0288, 1.0331, 1.1320, 1.0429, 1.0356, 1.0192, 1.0793, 1.0459, 0.9690,\n",
       "                      1.0143, 1.0810, 1.0111, 1.0727, 0.9373, 0.9825, 1.0224, 1.0877, 1.0015,\n",
       "                      1.0800, 1.1019, 1.0943, 1.0470, 1.0198, 1.0126, 0.9919, 1.0553, 1.0873,\n",
       "                      0.9989, 1.0761, 1.0752, 1.0212, 1.0382, 1.0394, 1.0096, 1.0131, 0.9870,\n",
       "                      1.0654, 1.0145, 1.0793, 1.0067, 1.0213, 1.0847, 1.1082, 1.1286, 1.0252,\n",
       "                      0.9953, 1.0613, 1.0191, 1.0053, 1.0397, 1.0859, 1.0029, 1.0888, 1.0446,\n",
       "                      1.0220, 1.0107, 1.0663, 1.0220, 1.0388, 0.9750, 1.0733, 0.9956, 1.0061,\n",
       "                      1.0360, 1.0505, 1.0497, 0.9533, 1.0502, 1.0774, 1.0269, 0.9849, 1.0073,\n",
       "                      1.0246, 1.0756, 1.0644, 1.0312, 0.9985, 1.0837, 1.0771, 1.0211, 0.9838,\n",
       "                      0.9782, 1.0497, 1.0451, 1.0669, 1.0650, 1.0352, 1.0158, 1.0037, 1.1058,\n",
       "                      1.0264, 1.0702, 1.0836, 1.1206, 1.0433, 1.0114, 0.9476, 1.0889, 1.0146,\n",
       "                      1.0243, 1.0046, 0.9884, 1.0500, 1.0404, 1.0060, 1.0365, 1.0114, 1.1147,\n",
       "                      1.0352, 1.0491, 1.0699, 1.0487, 1.0182, 1.0374, 1.0910, 1.0382, 1.0816,\n",
       "                      0.9940, 0.9976, 1.0391, 1.0654, 1.0333, 1.0092, 1.0135, 1.0358, 1.0536,\n",
       "                      1.0609, 1.0580, 1.0203, 0.9887, 1.0132, 0.9878, 1.0918, 1.0612, 1.0295,\n",
       "                      1.0554, 1.0260, 1.0038, 1.0099, 1.1072, 1.1019, 1.0079, 1.0642, 1.0379,\n",
       "                      1.0148, 1.1003, 1.1166, 1.0295, 1.0274, 1.0525, 1.0162, 1.0531, 1.0604,\n",
       "                      1.0649, 1.0421, 1.0173, 1.0357, 1.0720, 1.0892, 1.0170, 0.9748, 1.0283,\n",
       "                      0.9910, 1.0571, 0.9770, 0.9610, 1.0118, 0.9224, 1.0593, 1.0740, 0.9613,\n",
       "                      1.0454, 0.9824, 1.0161, 1.0442, 1.1050, 0.9971, 1.0708, 1.0707, 1.0677,\n",
       "                      1.0247, 1.0695, 1.0039, 1.0344, 1.0423, 1.0319, 0.9838, 1.0555, 1.0578,\n",
       "                      1.0390, 1.0595, 1.0095, 1.1293])),\n",
       "             ('transformer.ln_out.bn.num_batches_tracked', tensor(501)),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[-0.0106, -0.0437, -0.0375,  ...,  0.0632, -0.0366,  0.0532],\n",
       "                      [-0.0259, -0.0383, -0.0318,  ...,  0.0783, -0.0497,  0.0691],\n",
       "                      [-0.0085, -0.0504, -0.0363,  ...,  0.0654, -0.0368,  0.0471],\n",
       "                      ...,\n",
       "                      [-0.0043, -0.0436, -0.0466,  ...,  0.0608, -0.0356,  0.0632],\n",
       "                      [-0.0011, -0.0477, -0.0482,  ...,  0.0582, -0.0489,  0.0669],\n",
       "                      [-0.0216, -0.0454, -0.0616,  ...,  0.0540, -0.0662,  0.0498]]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quantize': True, 'model': {'cls': 'gpt', 'throw_errors_on_duplicate': False, 'layers': {\"transformer.r'w[tp]e'\": {'quantize': True, 'layer_type': 'Embedding', 'quantizers': {'weight': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}}}, 'transformer.dropout': {'quantize': True, 'layer_type': 'Dropout'}, 'transformer.emb_add': {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.residual1\": {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': None}}, \"transformer.layer.r'[0-9]+'.residual2\": {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': None}}, \"transformer.layer.r'[0-9]+'.r'ln_[0-9].id'\": {'quantize': True, 'layer_type': 'Identity', 'quantizers': {'act': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}, 'args': {'return_quant_tensor': True}}, \"transformer.layer.r'[0-9]+'.mlp.r'c_.+'\": {'quantize': True, 'layer_type': 'Linear', 'quantizers': {'weight': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'bias': {'default_quantizer': 'Int8Bias', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.mlp.active\": {'quantize': True, 'layer_type': 'ReLU', 'quantizers': {'act': {'default_quantizer': 'Uint8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.mlp.dropout\": {'quantize': True, 'layer_type': 'Dropout'}, \"transformer.layer.r'[0-9]+'.attn.r'.+dropout'\": {'quantize': True, 'layer_type': 'Dropout'}, \"transformer.layer.r'[0-9]+'.attn.mha\": {'quantize': True, 'layer_type': 'MultiheadAttention', 'quantizers': {'in_proj_input_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'in_proj_weight_quant': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'in_proj_bias_quant': {'default_quantizer': 'Int32Bias', 'args': {'bit_width': 8}}, 'attn_output_weights_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'q_scaled_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'k_transposed_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'v_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_input_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_weight_quant': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_bias_quant': {'default_quantizer': 'Int32Bias', 'args': {'bit_width': 8}}}, 'args': {'packed_in_proj': False, 'batch_first': True}}}, 'dtype': '${data.dtype}'}, 'kind': 'qat', 'type': 'BrevitasQuantizer'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"quant_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT\n",
    "gpt = GPT(quant_checkpoint[\"model_cfg\"][\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.layer.0.attn.mha.in_proj_weight\", \"transformer.layer.0.attn.mha.in_proj_bias\", \"transformer.layer.0.attn.mha.out_proj.bias\", \"transformer.layer.1.attn.mha.in_proj_weight\", \"transformer.layer.1.attn.mha.in_proj_bias\", \"transformer.layer.1.attn.mha.out_proj.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_proj.weight\", \"transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_proj.weight\", \"transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_proj.weight\", \"transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_proj.weight\", \"transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_proj.weight\", \"transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_proj.weight\", \"transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#doesnt work, maybe because gpt_quant is unquantized currently\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_checkpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.layer.0.attn.mha.in_proj_weight\", \"transformer.layer.0.attn.mha.in_proj_bias\", \"transformer.layer.0.attn.mha.out_proj.bias\", \"transformer.layer.1.attn.mha.in_proj_weight\", \"transformer.layer.1.attn.mha.in_proj_bias\", \"transformer.layer.1.attn.mha.out_proj.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_proj.weight\", \"transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_proj.weight\", \"transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_proj.weight\", \"transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_proj.weight\", \"transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_proj.weight\", \"transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_proj.weight\", \"transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\". "
     ]
    }
   ],
   "source": [
    "#doesnt work, maybe because gpt_quant is unquantized currently\n",
    "gpt.load_state_dict(quant_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "setting quantizer output for layer transformer.layer.0.residual1 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.1.residual1 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.0.residual2 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.1.residual2 to None as config is left empty\n",
      "Using unsigned quantizer for Uint8ActPerTensorFloat at brevitas.quant.scaled_int\n",
      "Using unsigned quantizer for Uint8ActPerTensorFloat at brevitas.quant.scaled_int\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization import get_quantizer\n",
    "quantizer, model_cfg = get_quantizer(quant_checkpoint[\"quant_cfg\"], gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelQuantConfig(cls='gpt', layers={'transformer.wte': LayerQuantConfig(layer=Embedding(50257, 256), quantize=True, layer_type='Embedding', name='transformer.wte', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.wpe': LayerQuantConfig(layer=Embedding(128, 256), quantize=True, layer_type='Embedding', name='transformer.wpe', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.dropout', quantizers={}, replace_later=False, args={}), 'transformer.emb_add': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.emb_add', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.residual1': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.0.residual1', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.1.residual1': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.1.residual1', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.0.residual2': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.0.residual2', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.1.residual2': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.1.residual2', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.0.ln_1.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.0.ln_1.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.0.ln_2.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.0.ln_2.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.1.ln_1.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.1.ln_1.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.1.ln_2.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.1.ln_2.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.0.mlp.c_fc': LayerQuantConfig(layer=Linear(in_features=256, out_features=1024, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.0.mlp.c_fc', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.c_proj': LayerQuantConfig(layer=Linear(in_features=1024, out_features=256, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.0.mlp.c_proj', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.c_fc': LayerQuantConfig(layer=Linear(in_features=256, out_features=1024, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.1.mlp.c_fc', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.c_proj': LayerQuantConfig(layer=Linear(in_features=1024, out_features=256, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.1.mlp.c_proj', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.active': LayerQuantConfig(layer=ReLU(), quantize=True, layer_type='ReLU', name='transformer.layer.0.mlp.active', quantizers={'act': ActQuant(default_quantizer='Uint8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.active': LayerQuantConfig(layer=ReLU(), quantize=True, layer_type='ReLU', name='transformer.layer.1.mlp.active', quantizers={'act': ActQuant(default_quantizer='Uint8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.mlp.dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.mlp.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.mlp.dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.resid_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.attn.resid_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.attn_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.attn.attn_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.attn.resid_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.attn.resid_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.attn.attn_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.attn.attn_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.mha': LayerQuantConfig(layer=MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "), quantize=True, layer_type='MultiheadAttention', name='transformer.layer.0.attn.mha', quantizers={'in_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'attn_output_weights_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'q_scaled_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'k_transposed_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'v_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'packed_in_proj': False, 'batch_first': True}), 'transformer.layer.1.attn.mha': LayerQuantConfig(layer=MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "), quantize=True, layer_type='MultiheadAttention', name='transformer.layer.1.attn.mha', quantizers={'in_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'attn_output_weights_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'q_scaled_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'k_transposed_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'v_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'packed_in_proj': False, 'batch_first': True})}, dtype='float32', model=GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 256)\n",
       "    (wpe): Embedding(128, 256)\n",
       "    (emb_add): EltwiseAdd()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (residual1): EltwiseAdd()\n",
       "        (residual2): EltwiseAdd()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (active): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (identity): Identity()\n",
       "        (ln_1): BatchNormTranspose(\n",
       "          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (id): Identity()\n",
       "        )\n",
       "        (ln_2): BatchNormTranspose(\n",
       "          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (id): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_out): BatchNormTranspose(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (id): Identity()\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=256, out_features=50257, bias=False)\n",
       "), quantized=False, throw_errors_on_duplicate=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_gpt, other_layers = quantizer.get_quantized_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_gpt.load_state_dict(quant_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_bn = qnn.BatchNorm1dToQuantScaleBias(64)\n",
    "from qtransform.quantization.quant_bn import QuantBatchnorm1d, CustomBatchNorm1d, replace_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singleton from https://refactoring.guru/design-patterns/singleton/python/example#example-0--main-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton works, both variables contain the same instance.\n"
     ]
    }
   ],
   "source": [
    "class SingletonMeta(type):\n",
    "    \"\"\"\n",
    "    The Singleton class can be implemented in different ways in Python. Some\n",
    "    possible methods include: base class, decorator, metaclass. We will use the\n",
    "    metaclass because it is best suited for this purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        if cls not in cls._instances:\n",
    "            instance = super().__call__(*args, **kwargs)\n",
    "            cls._instances[cls] = instance\n",
    "        return cls._instances[cls]\n",
    "\n",
    "\n",
    "class Singleton(metaclass=SingletonMeta):\n",
    "    \n",
    "    value: int\n",
    "    def some_business_logic(self):\n",
    "        \"\"\"\n",
    "        Finally, any singleton should define some business logic, which can be\n",
    "        executed on its instance.\n",
    "        \"\"\"\n",
    "        print(self.value)\n",
    "\n",
    "s1 = Singleton()\n",
    "s2 = Singleton()\n",
    "\n",
    "if id(s1) == id(s2):\n",
    "    print(\"Singleton works, both variables contain the same instance.\")\n",
    "else:\n",
    "    print(\"Singleton failed, variables contain different instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alright\n"
     ]
    }
   ],
   "source": [
    "s1.some_business_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer singleton requires config to initialize tokenizer\n",
    "\n",
    "class TokenizerSingletonMeta(type):\n",
    "    \"\"\"\n",
    "    The Singleton class can be implemented in different ways in Python. Some\n",
    "    possible methods include: base class, decorator, metaclass. We will use the\n",
    "    metaclass because it is best suited for this purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        if cls not in cls._instances:\n",
    "            instance = super().__call__(*args, **kwargs)\n",
    "            cls._instances[cls] = instance\n",
    "        return cls._instances[cls]\n",
    "\n",
    "\n",
    "class TokenizerSingleton(metaclass=TokenizerSingletonMeta):\n",
    "    def some_business_logic(self):\n",
    "        \"\"\"\n",
    "        Finally, any singleton should define some business logic, which can be\n",
    "        executed on its instance.\n",
    "        \"\"\"\n",
    "        print(\"alright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    _instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        raise RuntimeError('Call instance() instead')\n",
    "\n",
    "    @classmethod\n",
    "    def instance(cls):\n",
    "        if cls._instance is None:\n",
    "            print('Creating new instance')\n",
    "            cls._instance = cls.__new__(cls)\n",
    "            # Put any initialization here.\n",
    "        return cls._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Logger._instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property meta_file omited in config. Assuming default: \"meta.pkl\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from tiktoken import get_encoding\n",
    "from qtransform.dataset.files import MemmapDataset\n",
    "from qtransform.tokenizer import TikTokenizer\n",
    "\n",
    "tokenizer = TikTokenizer({\n",
    "    \"wrapper\": \"TikTokenizer\",\n",
    "    \"encoding\": \"gpt2\",\n",
    "    \"module\": \"tiktoken\",\n",
    "    \"name\": \"gpt2\"})\n",
    "block_size = 128\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length', max_length=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = MemmapDataset(\"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/train-tiny_shakespeare-float32.bin\", dtype=np.float32, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TikTokenizer' object has no attribute 'pad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TikTokenizer' object has no attribute 'pad'"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(data_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow/cache-gpt2-128-tokenized-train.arrow.py or any data file in the same directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wikitext_hf \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wikitext_hf\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1514\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1516\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow/cache-gpt2-128-tokenized-train.arrow.py or any data file in the same directory."
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "wikitext_hf = load_dataset(\"arrow\" , data_files = {\"/home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow\")\n",
    "wikitext_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetRunType(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    EVAL = \"eval\"\n",
    "    BENCH = \"bench\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetSplits:\n",
    "    \"\"\"\n",
    "        Dataclass containing the datasets for training, eval, testing, benchmark along with the name of the dataset.\n",
    "        After construction, a simple type check is done with the __post_init__ hook.\n",
    "    \"\"\"\n",
    "    DatasetRunType.TRAIN: Dataset = None\n",
    "    DatasetRunType.EVAL: Dataset = None\n",
    "    DatasetRunType.BENCH: Dataset = None\n",
    "\n",
    "    # make class subscritable aka: self['train'] works\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "    \n",
    "    def __setitem__(self, index, item):\n",
    "        setattr(self, index, item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
