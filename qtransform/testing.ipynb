{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod, ABC\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\"\n",
    "\n",
    "class Foo(ABC):\n",
    "    def __init__(self, encoding: str):\n",
    "        self.metadata: Metadata = Metadata(encoding)\n",
    "\n",
    "\n",
    "    def load_metadata():\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def test(self, file: str):\n",
    "        file += \"   padding\"\n",
    "\n",
    "class Bar(Foo):\n",
    "    def __init__(self, encoding: str):\n",
    "        super().__init__()\n",
    "        self.metadata: BarMetadata\n",
    "\n",
    "    def test(self, file: str):\n",
    "        super().test(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarMetadata(encoding='gpt2', other='Bruh')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BarMetadata(encoding=\"gpt2\", other=\"ok\")\n",
    "import dataclasses\n",
    "dataclasses.replace(test, **{\"other\": \"Bruh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.BarMetadata() argument after ** must be a mapping, not Metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test: Metadata \u001b[39m=\u001b[39m Metadata(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test: BarMetadata \u001b[39m=\u001b[39m BarMetadata(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest, other\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.BarMetadata() argument after ** must be a mapping, not Metadata"
     ]
    }
   ],
   "source": [
    "test: Metadata = Metadata(\"gpt2\")\n",
    "test: BarMetadata = BarMetadata(**test, other=\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = test\n",
    "params = set(inspect.signature(Metadata.__init__).parameters.keys()) - set(['self'])\n",
    "{x:getattr(obj, x) for x in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, \n",
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# padding does not get appended to the parameter as it is a seperate function\n",
    "Bar().test(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,c,e = 4, 5,6\n",
    "tensor_3d = torch.arange(b*c*e).reshape(b,c,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17],\n",
       "         [ 18,  19,  20,  21,  22,  23],\n",
       "         [ 24,  25,  26,  27,  28,  29]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47],\n",
       "         [ 48,  49,  50,  51,  52,  53],\n",
       "         [ 54,  55,  56,  57,  58,  59]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77],\n",
       "         [ 78,  79,  80,  81,  82,  83],\n",
       "         [ 84,  85,  86,  87,  88,  89]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107],\n",
       "         [108, 109, 110, 111, 112, 113],\n",
       "         [114, 115, 116, 117, 118, 119]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch has 5 rows, only want 3 \n",
    "index = torch.tile(torch.arange(3).reshape(3,1), (b,1,e))\n",
    "#you only consider the first batch\n",
    "torch.gather(tensor_3d, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 0,  1,  2,  3,  4]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [30, 31, 32, 33, 34]],\n",
       "\n",
       "        [[60, 61, 62, 63, 64],\n",
       "         [60, 61, 62, 63, 64]],\n",
       "\n",
       "        [[90, 91, 92, 93, 94],\n",
       "         [90, 91, 92, 93, 94]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#objective: retrieve first rows of tensor_3d -> if we specify dim=1, we collapse along the rows (we perform indexing for each row)\n",
    "#b,c,e = 4,5,6\n",
    "#i always want the first row -> specify by row, dim=1\n",
    "#how do i reduce the amount of rows if the index tensor has to be of the same dimension?\n",
    "#dimension has to be the same but not the shape\n",
    "#torch.zeros(4,1,6) gets the first row of the tensor, but it is problematic if i want multiple rows as i \n",
    "#then use the same index (0) while having the output shape that i want\n",
    "#solution: arange\n",
    "#index=torch.zeros(4,1,6) -> if we use 5 instead of 6, each row has 5 columns\n",
    "#meaning: we need a row containing the same index \n",
    "tensor_3d.gather(dim=1, index=torch.zeros(4,2,6, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ]\n",
    "   ])\n",
    "#size is: 3, 4, 3. if you collapse in the first dimension (dim=0), the result tensor becomes of size 4,3. if you collapse it in the second dimension, you get a tensor of size 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)\n",
    "#in transformers, we usually have tensors of shape b,c,e (batch_size, context, embedding_dimension).\n",
    "#if we specify dim=0, we perform the operation along the entire batch, in dim=1 along the context and in dim=2 along the embedding dimension.\n",
    "#if we were to sum the tensors together, sum(dim=1) will yield the sum of the embeddings of each word.\n",
    "#think of it as squishing a dimension together so that it is of size 1, meaning that we have to squeeze in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchnorm performed with padding\n",
    "[-0.9041, -1.0757,  0.5234, -0.0512,  0.6242, -0.1665, -1.0276,\n",
    "           1.6814, -0.9740, -1.8245, -0.3201,  0.7656,  0.2983,  0.1673,\n",
    "           0.2048,  2.0786],\n",
    "[-0.8093,  0.6853, -1.1480, -1.3283,  1.0316,  1.3167, -0.2717,\n",
    "          -1.2307,  1.8153, -0.4639,  1.2193, -0.7859, -0.2366, -0.7097,\n",
    "           1.0948, -0.1788],\n",
    "[ 0.8650, -1.5553,  0.7642, -1.4681,  0.5856, -1.0539,  0.5444,\n",
    "           0.3433, -1.5597,  0.4344,  1.2369, -0.9978, -0.2923,  1.5946,\n",
    "          -0.1032,  0.6620],\n",
    "#without padding\n",
    "[-0.9041, -1.0757,  0.5234, -0.0512,  0.6242, -0.1665, -1.0276,\n",
    "           1.6814, -0.9740, -1.8245, -0.3201,  0.7656,  0.2983,  0.1673,\n",
    "           0.2048,  2.0786],\n",
    "[-0.8093,  0.6853, -1.1480, -1.3283,  1.0316,  1.3167, -0.2717,\n",
    "          -1.2307,  1.8153, -0.4639,  1.2193, -0.7859, -0.2366, -0.7097,\n",
    "           1.0948, -0.1788],\n",
    "[ 0.8650, -1.5553,  0.7642, -1.4681,  0.5856, -1.0539,  0.5444,\n",
    "           0.3433, -1.5597,  0.4344,  1.2369, -0.9978, -0.2923,  1.5946,\n",
    "          -0.1032,  0.6620]\n",
    "#-> padding does not artificially lower mean in 3d inputs, in 2d inputs they are lowered though\n",
    "#as normalization is performed along the embeddings\n",
    "#larger batches will calculate the mean of the words of multiple sentences\n",
    "tensor([[[-0.5822,  1.3051,  0.5337, -0.3419, -0.5721,  1.6196, -0.0168,\n",
    "           0.5965,  0.3864, -1.1112, -1.4737,  1.6359, -0.6583, -1.1196,\n",
    "          -0.3974,  0.9116],\n",
    "         [ 1.2417,  1.5342, -1.2204,  0.7292,  0.8772, -1.2491, -1.8027,\n",
    "           0.5878, -0.2417, -1.0617, -0.0714, -1.5258,  0.6894, -0.6110,\n",
    "           1.4409,  0.8721],\n",
    "         [ 1.2212,  1.5189,  0.8535,  0.5534,  1.3774, -0.5945, -0.4424,\n",
    "          -0.3580,  0.1088, -1.7387,  0.1289, -0.2090,  0.1482, -1.7862,\n",
    "          -1.1194,  0.3945],\n",
    "         [ 0.0922,  1.5306, -0.4437,  0.8035, -0.0281, -0.0908,  0.4842,\n",
    "           1.1845, -1.6692,  0.9497,  1.3172, -0.6635, -0.4553,  0.3977,\n",
    "          -1.3318,  0.6270],\n",
    "         [ 0.0024,  0.5799,  1.7395, -1.0177,  0.6052, -1.6885,  0.0204,\n",
    "           0.1526,  1.2536, -1.1935,  1.7002,  0.2616, -1.5268, -0.8731,\n",
    "          -1.0120, -0.7853],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000]],\n",
    "\n",
    "        [[ 1.1908,  1.0052, -0.8598,  1.0370,  1.3233, -1.2132, -0.3867,\n",
    "          -0.7785, -1.3937, -0.0684,  1.4887, -0.2831,  1.5336,  1.6622,\n",
    "           1.3477, -0.2572],\n",
    "         [-0.1492, -0.6825,  0.4371,  1.0500,  1.9715,  0.0609,  1.1558,\n",
    "          -1.1479,  0.6885,  1.6560, -0.3192,  1.6064, -0.3162, -0.9805,\n",
    "          -1.9670,  1.6830],\n",
    "         [ 1.2578,  0.2911, -1.3203,  0.3955, -0.0091,  0.9071,  0.6828,\n",
    "          -1.6579, -0.4551,  0.9677,  0.6999, -0.1291,  0.3606, -1.3197,\n",
    "          -0.9213,  0.7788],\n",
    "         [ 0.3692, -0.9861, -1.7729,  0.2044,  1.0779, -1.5686, -0.0076,\n",
    "          -1.0156, -0.1939,  0.0496,  0.3885, -1.6978,  0.0456,  1.6219,\n",
    "           1.2923, -0.0506],\n",
    "         [ 0.6666, -0.2983,  0.0922, -1.0471,  1.2768,  1.1265,  1.0946,\n",
    "           1.4914,  0.7205, -0.4837, -0.1287, -0.0737,  0.7687, -0.5311,\n",
    "           1.4159, -1.6555],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000]],\n",
    "\n",
    "        [[-1.5902, -1.0678,  1.8357,  0.0030, -1.1217, -1.3159, -0.6406,\n",
    "          -0.1677,  1.0418, -0.7997,  0.0218, -0.5721, -0.9313, -0.4731,\n",
    "          -0.4762,  0.1906],\n",
    "         [-0.9226, -0.6351, -0.2052, -0.8969, -0.4510, -0.4629, -0.9043,\n",
    "           0.7516,  0.4243, -1.1716,  0.5445, -0.0452, -1.0431,  0.0052,\n",
    "          -0.2020,  0.2791],\n",
    "         [ 0.8080, -1.1421, -1.2642,  0.5870, -0.5482,  0.1853,  1.5595,\n",
    "          -0.2276, -0.6175,  1.0407,  1.5567,  0.9807, -1.3606,  1.1637,\n",
    "          -1.8081, -1.4988],\n",
    "         [-1.1202, -1.4404,  1.2313,  0.7874,  0.2202,  0.5783,  1.5642,\n",
    "          -1.4807, -1.0937,  0.7240,  0.6155,  0.2237, -1.6044,  0.9075,\n",
    "          -1.3431,  0.7700],\n",
    "         [ 0.4308, -0.4005,  0.5008,  1.2831,  0.5751,  0.3695, -0.7355,\n",
    "          -1.5796,  1.5674, -1.8740, -1.0547,  0.6519, -0.7017, -0.7984,\n",
    "           0.0731, -0.9610],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
    "           0.0000,  0.0000]]], grad_fn=<NativeBatchNormBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(torch.nn.BatchNorm1d):\n",
    "    \"\"\" BatchNorm but with an optional bias and padding to support variable input length. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, bias, mask: int=0, *args, **kwargs): #arg names need to be identical to torch argnames for quantization support\n",
    "        self.num_features = num_features\n",
    "        self.mask = mask\n",
    "        super().__init__(num_features, *args, **kwargs)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        #dirty workaround to avoid runtimeerrors by adding a padding if the input is smaller than the feature length\n",
    "        #padding does not artificially lower mean as normalization is performed along the word embeddings\n",
    "        n,c,l = input.size()\n",
    "        if c < self.num_features:\n",
    "            #input tensor should always be three dimensional\n",
    "            input = torch.cat((input, torch.full((n, self.num_features - c, l), self.mask)), dim=1)\n",
    "        input = super().forward(input, *args, **kwargs)\n",
    "        #remove padding \n",
    "        index = torch.tile(torch.arange(c).reshape(c,1), (n,1,l))\n",
    "        return torch.gather(input=input, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,c,e = 12,64,256\n",
    "batch = torch.rand(b,4,e)\n",
    "bn = BatchNorm(c, bias=False, mask = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_norm = bn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = torch.nn.BatchNorm1d(4)(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_norm == norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Meta():\n",
    "    num_tokens: int = 0\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta(3).num_tokens = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#test if huggingface datasets can be created from text files\n",
    "import datasets\n",
    "\n",
    "BASEDIR = '/home/mabot004/.qtransform/datasets/files/shakespeare/untokenized/'\n",
    "#number of rows depends on the amount of files\n",
    "files = [os.path.join(BASEDIR, 'shakespeare.txt'), os.path.join(BASEDIR, 'shakespeare_2.txt')]\n",
    "#does the same as huggingface mapping but now with files\n",
    "def gen_text():\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as file:\n",
    "            yield {\"text\": file.read()}\n",
    "\n",
    "#chunk size from config, default 100\n",
    "def chunk_examples(examples):\n",
    "                #splits the text of each row into chunks of length chunk_length. currently it is only used\n",
    "                #for character tokenization to avoid feeding large samples to the tokenizer\n",
    "    chunk_length = 100\n",
    "                #perform tokenization on a handful of characters at a time\n",
    "                #from: https://huggingface.co/docs/datasets/process#split-long-examples            \n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in examples[\"text\"]:\n",
    "        new_chunks = [sentence[i:i + chunk_length] for i in range(0, len(sentence), chunk_length)]\n",
    "        chunks.extend(new_chunks)\n",
    "    return {\"chunks\": chunks}\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2 examples [00:00, 32.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "shakespeare = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 20.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok: 100%|██████████| 100/100 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "msg = 'ok'\n",
    "for i in tqdm(range(100), desc=f'{msg}'):\n",
    "    msg = str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten_tomatoes[\"train\"].shard(num_shards=1000, index=0, contiguous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten_tomatoes.select_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_tomatoes = datasets.load_dataset('rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error occurs because the splits have more than one feature and this function changes the amount of samples in each split of one feature without changint the other\n",
    "#so: 5 samples, 2 features. after mapping: text has 10 samples, other feature still has 5 features\n",
    "#from: https://github.com/huggingface/datasets/issues/1817#issuecomment-774066254\n",
    "rt_chunks = datasets.concatenate_datasets(rotten_tomatoes.select_columns(\"text\").map(chunk_examples, batched=True, remove_columns = \"text\").values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 17458\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the dataset from chunks: 100%|██████████| 17458/17458 [00:00<00:00, 42685.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rt_chunks = rt_chunks.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 17458/17458 [00:00<00:00, 437870.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rt_chunks.save_to_disk('/home/mabot004/custom_hf_datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 13952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash eve'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(rt_chunks[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "shakespeare = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "shakespeare = shakespeare.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.concatenate(shakespeare[:3][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memmap(memmap, start, end, data):\n",
    "    memmap[start:end] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 163 examples [00:00, 31912.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def gen_text():\n",
    "    for i in range(163):\n",
    "        yield {\"text\": i}\n",
    "\n",
    "test_threading = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 163\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.rename_column(\"text\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.shard(num_shards=30, index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "num_threads = 3 #os.cpu_count // 2\n",
    "batch_size = 30\n",
    "num_samples = len(test_threading)\n",
    "# 163 // 30 shards\n",
    "# -> 3 threads, each having a batch size of 30 samples\n",
    "# dataset has 163 samples -> each thread should have around 50-60 samples max\n",
    "# -> divide samples of dataset with num_threads\n",
    "# -> each thread should have the entire dataset as an arg, but split differently\n",
    "# range of splitting should be specified as an arg in thread -> index arg in parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why should you use multithreading? the writing process is I/O based\n",
    "#if anything, the amount of write requests increases with the amount of threads\n",
    "memmap = np.memmap('test', mode='w+', shape=(163,), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [x for x in range(163)]\n",
    "def write(memmap, index):\n",
    "    memmap[] = data[]\n",
    "\n",
    "#threads = [threading.Thread() for x in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if memmap file can be removed when catching sigint\n",
    "import signal\n",
    "\n",
    "def delete_memmap():\n",
    "    pass\n",
    "\n",
    "signal.signal(signal.SIGINT, delete_memmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: 'abcd'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    int(\"abcd\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def function(self):\n",
    "        def other():\n",
    "            print(self.a)\n",
    "        other()\n",
    "\n",
    "Foo().function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(self.tokenized_file):\n",
    "    hf_tokenized_dir = join(tokenized_dir, huggingface_tokens_dir)\n",
    "    if not exists(hf_tokenized_dir):\n",
    "        #perform tokenization\n",
    "        pass\n",
    "        dataset_splits = None\n",
    "    else:\n",
    "        dataset_splits = load_dataset(hf_tokenized_dir)\n",
    "    #perform memmap writing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#no high memory usage as memmap values are lazily loaded, only overhead is the pages (around 5MB per memmap )\n",
    "memmap = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap2 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap3 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap4 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from logging import getLogger\n",
    "import os\n",
    "log = getLogger('memmapDataset')\n",
    "class MemmapDataset(Dataset):\n",
    "    \n",
    "    #TODO: is dtype necessary? since each file only contains the ids of tokens, not the actual embeddings\n",
    "    def __init__(self, token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0):\n",
    "        \"\"\"\n",
    "            Creates a dataset which loads a numpy array from a file. \n",
    "            Slices of the dataset can be retrieved with the start and end parameters. \n",
    "            They specify the starting and ending range of the dataset in percent.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not isinstance(start, float) or start < 0.0 or start >= 1.0:\n",
    "            log.error(f'Invalid starting range for dataset ({start})')\n",
    "            raise KeyError()\n",
    "        if not isinstance(end, float) or end <= 0.0 or end > 1.0:\n",
    "            log.error(f'Invalid ending range for dataset ({end})')\n",
    "            raise KeyError()\n",
    "        if not isinstance(dtype, np.dtype): #np.dtype(\"dtype\") or np.<dtype> e.g.: np.dtype(\"float32\") or np.float32\n",
    "            try:\n",
    "                dtype = np.dtype(dtype) #not an instance (np.float32)\n",
    "            except TypeError as e:\n",
    "                log.error(e)\n",
    "                raise TypeError\n",
    "        self.block_size = block_size\n",
    "        if self.block_size <= 0:\n",
    "            log.error(f'Block size of 0 is invalid.')\n",
    "            raise ValueError()\n",
    "        log.info(f\"Attempting to retrieve tokenized dataset under \\\"{token_file}\\\"\")\n",
    "        self.token_file = token_file\n",
    "        self.dtype = dtype\n",
    "        #the method of retrieving the byte size is somewhat inspired from the stackoverflow article\n",
    "        #https://stackoverflow.com/questions/19599864/easy-way-of-getting-number-of-bits-from-a-numpy-type\n",
    "        self.bytes = self.dtype.itemsize\n",
    "        amnt_tokens = os.path.getsize(self.token_file) / self.bytes\n",
    "        if amnt_tokens % 1 != 0.0:\n",
    "            log.error(f'The amount of tokens is supposed to be a whole number, but it is {amnt_tokens}. Maybe due to a wrong datatype?')\n",
    "            raise ValueError()\n",
    "        offset = int(amnt_tokens * start)\n",
    "        #rounding to the nearest multiplicative of datatype to make sure not to read half a token too much\n",
    "        offset -= offset % self.bytes\n",
    "        log.debug(f'Offset is {offset}, start is {start}, end is {end}')\n",
    "        #skip the first start * amnt_tokens and the last amnt_tokens * end items\n",
    "        log.debug(f'Tokenized file has {amnt_tokens} tokens of datatype: {dtype}. Attempting to start at token: {offset}')\n",
    "        self.data = np.memmap(self.token_file, dtype=self.dtype, mode='r', offset=offset)[:int(amnt_tokens * end)]\n",
    "        if len(self.data) < self.block_size:\n",
    "            log.error(f'Loaded data has less tokens than block size {self.block_size} for starting range {start} and ending range {end}. Maybe check size of splits?')\n",
    "            raise ValueError()\n",
    "        log.info(f'Loaded data has {len(self.data)} tokens.')\n",
    "        #log.debug(f'all unique tokens in dataset: {set(self.data)}, length: {len(set(self.data))}')\n",
    "        self.length = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    #the dataloader works with batch sizes, dataset only works with indices\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "            Returns inputs and labels. Called when iterating with e.g. a dataloader.\n",
    "        \"\"\"\n",
    "        if index < 0:\n",
    "            index += len(self)\n",
    "        #lower index to make sure that block_size elements are always retrieved\n",
    "        if index + self.block_size > len(self) - 1:\n",
    "            index = self.length - self.block_size - 2\n",
    "        offset = index + self.block_size\n",
    "        #fixed dtype as torch embeddings need int64 tensor to work\n",
    "        inputs: torch.Tensor = torch.from_numpy(self.data[index:offset].astype(np.int64))\n",
    "        #labels are always the following word for each word within the context\n",
    "        labels : torch.Tensor = torch.from_numpy(self.data[index +1:offset+1].astype(np.int64))\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709600997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0\n",
    "memmap_ds = MemmapDataset(\n",
    "    token_file='/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin',\n",
    "    dtype=np.float32,\n",
    "    block_size=64,\n",
    "    start=0.0,\n",
    "    end=0.3\n",
    ")\n",
    "len(memmap_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(memmap_ds, batch_size=12, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[13924,    12,   559,    12, 35784,    11, 25051,   357, 18474,     8,\n",
       "           1377, 45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,\n",
       "            379,  1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,\n",
       "            257,  2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074,\n",
       "          23724,   262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,\n",
       "             13,   198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,\n",
       "            298,  2986, 33708, 42095],\n",
       "         [   12,   559,    12, 35784,    11, 25051,   357, 18474,     8,  1377,\n",
       "          45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,\n",
       "           1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,\n",
       "           2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,\n",
       "            262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,\n",
       "            198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,\n",
       "           2986, 33708, 42095,   355],\n",
       "         [  559,    12, 35784,    11, 25051,   357, 18474,     8,  1377, 45591,\n",
       "           4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,  1204,\n",
       "             11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,  2214,\n",
       "           4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,   262,\n",
       "           1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,   198,\n",
       "            198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,  2986,\n",
       "          33708, 42095,   355,   262],\n",
       "         [   12, 35784,    11, 25051,   357, 18474,     8,  1377, 45591,  4970,\n",
       "             11,  1319, 44556,   287,  2356,   290, 44787,   379,  1204,    11,\n",
       "           7342,  7519,   290, 20669,  2513,  1497,   422,   257,  2214,  4436,\n",
       "           3217,  1755,   706,   257, 21402,  3315,  1074, 23724,   262,  1989,\n",
       "             11,  2282,   340,   373,  5213,   546,  2324,    13,   198,   198,\n",
       "            464,  2551,  1364,  8100,  5953,  8366, 34428,   298,  2986, 33708,\n",
       "          42095,   355,   262,   691],\n",
       "         [35784,    11, 25051,   357, 18474,     8,  1377, 45591,  4970,    11,\n",
       "           1319, 44556,   287,  2356,   290, 44787,   379,  1204,    11,  7342,\n",
       "           7519,   290, 20669,  2513,  1497,   422,   257,  2214,  4436,  3217,\n",
       "           1755,   706,   257, 21402,  3315,  1074, 23724,   262,  1989,    11,\n",
       "           2282,   340,   373,  5213,   546,  2324,    13,   198,   198,   464,\n",
       "           2551,  1364,  8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,\n",
       "            355,   262,   691,  6253],\n",
       "         [   11, 25051,   357, 18474,     8,  1377, 45591,  4970,    11,  1319,\n",
       "          44556,   287,  2356,   290, 44787,   379,  1204,    11,  7342,  7519,\n",
       "            290, 20669,  2513,  1497,   422,   257,  2214,  4436,  3217,  1755,\n",
       "            706,   257, 21402,  3315,  1074, 23724,   262,  1989,    11,  2282,\n",
       "            340,   373,  5213,   546,  2324,    13,   198,   198,   464,  2551,\n",
       "           1364,  8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,   355,\n",
       "            262,   691,  6253,   379],\n",
       "         [25051,   357, 18474,     8,  1377, 45591,  4970,    11,  1319, 44556,\n",
       "            287,  2356,   290, 44787,   379,  1204,    11,  7342,  7519,   290,\n",
       "          20669,  2513,  1497,   422,   257,  2214,  4436,  3217,  1755,   706,\n",
       "            257, 21402,  3315,  1074, 23724,   262,  1989,    11,  2282,   340,\n",
       "            373,  5213,   546,  2324,    13,   198,   198,   464,  2551,  1364,\n",
       "           8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,   355,   262,\n",
       "            691,  6253,   379,   262],\n",
       "         [  357, 18474,     8,  1377, 45591,  4970,    11,  1319, 44556,   287,\n",
       "           2356,   290, 44787,   379,  1204,    11,  7342,  7519,   290, 20669,\n",
       "           2513,  1497,   422,   257,  2214,  4436,  3217,  1755,   706,   257,\n",
       "          21402,  3315,  1074, 23724,   262,  1989,    11,  2282,   340,   373,\n",
       "           5213,   546,  2324,    13,   198,   198,   464,  2551,  1364,  8100,\n",
       "           5953,  8366, 34428,   298,  2986, 33708, 42095,   355,   262,   691,\n",
       "           6253,   379,   262,  4436],\n",
       "         [18474,     8,  1377, 45591,  4970,    11,  1319, 44556,   287,  2356,\n",
       "            290, 44787,   379,  1204,    11,  7342,  7519,   290, 20669,  2513,\n",
       "           1497,   422,   257,  2214,  4436,  3217,  1755,   706,   257, 21402,\n",
       "           3315,  1074, 23724,   262,  1989,    11,  2282,   340,   373,  5213,\n",
       "            546,  2324,    13,   198,   198,   464,  2551,  1364,  8100,  5953,\n",
       "           8366, 34428,   298,  2986, 33708, 42095,   355,   262,   691,  6253,\n",
       "            379,   262,  4436,   284],\n",
       "         [    8,  1377, 45591,  4970,    11,  1319, 44556,   287,  2356,   290,\n",
       "          44787,   379,  1204,    11,  7342,  7519,   290, 20669,  2513,  1497,\n",
       "            422,   257,  2214,  4436,  3217,  1755,   706,   257, 21402,  3315,\n",
       "           1074, 23724,   262,  1989,    11,  2282,   340,   373,  5213,   546,\n",
       "           2324,    13,   198,   198,   464,  2551,  1364,  8100,  5953,  8366,\n",
       "          34428,   298,  2986, 33708, 42095,   355,   262,   691,  6253,   379,\n",
       "            262,  4436,   284,   651],\n",
       "         [ 1377, 45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,\n",
       "            379,  1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,\n",
       "            257,  2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074,\n",
       "          23724,   262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,\n",
       "             13,   198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,\n",
       "            298,  2986, 33708, 42095,   355,   262,   691,  6253,   379,   262,\n",
       "           4436,   284,   651,   262],\n",
       "         [45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,\n",
       "           1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,\n",
       "           2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,\n",
       "            262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,\n",
       "            198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,\n",
       "           2986, 33708, 42095,   355,   262,   691,  6253,   379,   262,  4436,\n",
       "            284,   651,   262,  3871]]),\n",
       " tensor([[   12,   559,    12, 35784,    11, 25051,   357, 18474,     8,  1377,\n",
       "          45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,\n",
       "           1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,\n",
       "           2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,\n",
       "            262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,\n",
       "            198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,\n",
       "           2986, 33708, 42095,   355],\n",
       "         [  559,    12, 35784,    11, 25051,   357, 18474,     8,  1377, 45591,\n",
       "           4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,  1204,\n",
       "             11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,  2214,\n",
       "           4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,   262,\n",
       "           1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,   198,\n",
       "            198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,  2986,\n",
       "          33708, 42095,   355,   262],\n",
       "         [   12, 35784,    11, 25051,   357, 18474,     8,  1377, 45591,  4970,\n",
       "             11,  1319, 44556,   287,  2356,   290, 44787,   379,  1204,    11,\n",
       "           7342,  7519,   290, 20669,  2513,  1497,   422,   257,  2214,  4436,\n",
       "           3217,  1755,   706,   257, 21402,  3315,  1074, 23724,   262,  1989,\n",
       "             11,  2282,   340,   373,  5213,   546,  2324,    13,   198,   198,\n",
       "            464,  2551,  1364,  8100,  5953,  8366, 34428,   298,  2986, 33708,\n",
       "          42095,   355,   262,   691],\n",
       "         [35784,    11, 25051,   357, 18474,     8,  1377, 45591,  4970,    11,\n",
       "           1319, 44556,   287,  2356,   290, 44787,   379,  1204,    11,  7342,\n",
       "           7519,   290, 20669,  2513,  1497,   422,   257,  2214,  4436,  3217,\n",
       "           1755,   706,   257, 21402,  3315,  1074, 23724,   262,  1989,    11,\n",
       "           2282,   340,   373,  5213,   546,  2324,    13,   198,   198,   464,\n",
       "           2551,  1364,  8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,\n",
       "            355,   262,   691,  6253],\n",
       "         [   11, 25051,   357, 18474,     8,  1377, 45591,  4970,    11,  1319,\n",
       "          44556,   287,  2356,   290, 44787,   379,  1204,    11,  7342,  7519,\n",
       "            290, 20669,  2513,  1497,   422,   257,  2214,  4436,  3217,  1755,\n",
       "            706,   257, 21402,  3315,  1074, 23724,   262,  1989,    11,  2282,\n",
       "            340,   373,  5213,   546,  2324,    13,   198,   198,   464,  2551,\n",
       "           1364,  8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,   355,\n",
       "            262,   691,  6253,   379],\n",
       "         [25051,   357, 18474,     8,  1377, 45591,  4970,    11,  1319, 44556,\n",
       "            287,  2356,   290, 44787,   379,  1204,    11,  7342,  7519,   290,\n",
       "          20669,  2513,  1497,   422,   257,  2214,  4436,  3217,  1755,   706,\n",
       "            257, 21402,  3315,  1074, 23724,   262,  1989,    11,  2282,   340,\n",
       "            373,  5213,   546,  2324,    13,   198,   198,   464,  2551,  1364,\n",
       "           8100,  5953,  8366, 34428,   298,  2986, 33708, 42095,   355,   262,\n",
       "            691,  6253,   379,   262],\n",
       "         [  357, 18474,     8,  1377, 45591,  4970,    11,  1319, 44556,   287,\n",
       "           2356,   290, 44787,   379,  1204,    11,  7342,  7519,   290, 20669,\n",
       "           2513,  1497,   422,   257,  2214,  4436,  3217,  1755,   706,   257,\n",
       "          21402,  3315,  1074, 23724,   262,  1989,    11,  2282,   340,   373,\n",
       "           5213,   546,  2324,    13,   198,   198,   464,  2551,  1364,  8100,\n",
       "           5953,  8366, 34428,   298,  2986, 33708, 42095,   355,   262,   691,\n",
       "           6253,   379,   262,  4436],\n",
       "         [18474,     8,  1377, 45591,  4970,    11,  1319, 44556,   287,  2356,\n",
       "            290, 44787,   379,  1204,    11,  7342,  7519,   290, 20669,  2513,\n",
       "           1497,   422,   257,  2214,  4436,  3217,  1755,   706,   257, 21402,\n",
       "           3315,  1074, 23724,   262,  1989,    11,  2282,   340,   373,  5213,\n",
       "            546,  2324,    13,   198,   198,   464,  2551,  1364,  8100,  5953,\n",
       "           8366, 34428,   298,  2986, 33708, 42095,   355,   262,   691,  6253,\n",
       "            379,   262,  4436,   284],\n",
       "         [    8,  1377, 45591,  4970,    11,  1319, 44556,   287,  2356,   290,\n",
       "          44787,   379,  1204,    11,  7342,  7519,   290, 20669,  2513,  1497,\n",
       "            422,   257,  2214,  4436,  3217,  1755,   706,   257, 21402,  3315,\n",
       "           1074, 23724,   262,  1989,    11,  2282,   340,   373,  5213,   546,\n",
       "           2324,    13,   198,   198,   464,  2551,  1364,  8100,  5953,  8366,\n",
       "          34428,   298,  2986, 33708, 42095,   355,   262,   691,  6253,   379,\n",
       "            262,  4436,   284,   651],\n",
       "         [ 1377, 45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,\n",
       "            379,  1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,\n",
       "            257,  2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074,\n",
       "          23724,   262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,\n",
       "             13,   198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,\n",
       "            298,  2986, 33708, 42095,   355,   262,   691,  6253,   379,   262,\n",
       "           4436,   284,   651,   262],\n",
       "         [45591,  4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,\n",
       "           1204,    11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,\n",
       "           2214,  4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,\n",
       "            262,  1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,\n",
       "            198,   198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,\n",
       "           2986, 33708, 42095,   355,   262,   691,  6253,   379,   262,  4436,\n",
       "            284,   651,   262,  3871],\n",
       "         [ 4970,    11,  1319, 44556,   287,  2356,   290, 44787,   379,  1204,\n",
       "             11,  7342,  7519,   290, 20669,  2513,  1497,   422,   257,  2214,\n",
       "           4436,  3217,  1755,   706,   257, 21402,  3315,  1074, 23724,   262,\n",
       "           1989,    11,  2282,   340,   373,  5213,   546,  2324,    13,   198,\n",
       "            198,   464,  2551,  1364,  8100,  5953,  8366, 34428,   298,  2986,\n",
       "          33708, 42095,   355,   262,   691,  6253,   379,   262,  4436,   284,\n",
       "            651,   262,  3871,   832]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    input, labels = data\n",
    "    print(f'{input.size()}, {labels.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 324.37 MB\n"
     ]
    }
   ],
   "source": [
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 1it [00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 2it [00:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 3it [00:01,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 4it [00:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 5it [00:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 6it [00:03,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 7it [00:03,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 8it [00:04,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 9it [00:04,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test progress bar and other stdout stuff: 10it [00:05,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "#test progress bar and other stdout stuff\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i, data in tqdm(enumerate(range(10)), desc='test progress bar and other stdout stuff'):\n",
    "    print(data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[155, 152, 196,  34],\n",
       "         [ 61,  80,   4, 114]],\n",
       "\n",
       "        [[179, 106, 184, 132],\n",
       "         [ 79, 105,  36,   6]],\n",
       "\n",
       "        [[165,  82,  48,  48],\n",
       "         [  4, 107, 119, 187]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 2 #2 words\n",
    "prompt = torch.randint(200, (3, c, 4))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(c).reshape((c,1)).repeat((3,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tile(torch.arange(c).reshape((c,1)), (3,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#testing batchnorm quant\n",
    "#https://github.com/Xilinx/brevitas/issues/542\n",
    "#https://github.com/Xilinx/brevitas/issues/363\n",
    "import torch\n",
    "import brevitas.nn as qnn\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if quantized layers having return_quant_tensor set to True are compatible with torch operations \n",
    "quant_tensor_linear = qnn.QuantLinear(1,1,True,return_quant_tensor=True)\n",
    "quant_tensor_linear(torch.Tensor(8,1)) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug loading quantized checkpoint\n",
    "CHECKPOINT = '/home/mabot004/eki-transformer-dev/qtransform/outputs/models/GPT_2024-01-17_08:30:49__epoch:1'\n",
    "#doesnt work since qtransform.dataset cannot be found\n",
    "#but module info about tokenizers is not saved in checkpoint, only their names\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.layer.0.attn.attn_mask',\n",
       " 'transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.attn.attn_mask',\n",
       " 'transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if info about quant params are even saved within checkpoint\n",
    "import re\n",
    "keys = checkpoint[\"model_state_dict\"].keys()\n",
    "#quant param that exists within checkpoint: \n",
    "#transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value \n",
    "weights_and_biases = list(filter(lambda x: re.search(r'.+\\.(weight|bias)$', x), keys))\n",
    "def find(x):\n",
    "    if not re.search(r'.+\\.(weight|bias)$', x):\n",
    "        return x\n",
    "other_keys = list(filter(find, keys))\n",
    "len(keys) == len(weights_and_biases) # not only weights and biases in state dict\n",
    "#only scaling_impl is saved in state dict\n",
    "#no multiheadattention though\n",
    "#in gpt quant config, every single layer has a quantizer (most commonly Int8WeightPerTensorFloat)\n",
    "#that quantizer has ScalingImplType STATS\n",
    "#the layers with scaling_impl had an activation quantizer named Int8ActPerTensorFloat\n",
    "#it had the ScalingImplType PARAMETER_FROM_STATS\n",
    "other_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if scaling_impl params exist within model\n",
    "test_mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=256)\n",
    "#simulate some learning steps for param\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value = torch.nn.Parameter(torch.tensor(3.1415))\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant import scaled_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[0.9874]])), ('bias', tensor([-0.8623]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test which brevitas layers return qparams in state_dict\n",
    "qnn.QuantLinear(1,1,True,input_quant=scaled_int.Int8ActPerTensorFloat).state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnn.QuantIdentity(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnn.QuantReLU(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason, qparams are saved within this model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.active  = nn.ReLU() #or GELU\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.c_fc(x)\n",
    "        x = self.active(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "from dataclasses import dataclass\n",
    "class Config():\n",
    "    n_embd: int = 256\n",
    "    dropout: bool = True\n",
    "    bias: bool = True\n",
    "\n",
    "mlp = MLP(Config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['c_fc.weight', 'c_fc.bias', 'c_proj.weight', 'c_proj.bias'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_mha.state_dict(), 'mha.chpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_proj.weight', 'in_proj.bias', 'out_proj.weight', 'out_proj.bias'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v_quant etc. not appearing within state_dict\n",
    "test_mha.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 5), match='allo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?!hallo|welt).*$', \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if storing checkpoints of quantized models even is working\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.ModuleDict(dict(\n",
    "            wte = qnn.QuantEmbedding(32, 128),\n",
    "            pos = qnn.QuantEmbedding(16, 128),\n",
    "            logic = nn.ModuleDict(dict(\n",
    "                layer1 = qnn.QuantLinear(128, 16, True),\n",
    "                layer2 = qnn.QuantLinear(16,1, True))\n",
    "            )\n",
    "        ))\n",
    "    def forward(self, x):\n",
    "        embd = self.network.wte(x)\n",
    "        b,t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "        pos = self.network.pos(pos)\n",
    "        output = embd + pos\n",
    "        for name, layer in self.network.logic.items():\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3944],\n",
       "         [ 0.2287],\n",
       "         [-0.5937],\n",
       "         [-0.8445],\n",
       "         [ 0.4049],\n",
       "         [-0.1961],\n",
       "         [ 0.2558],\n",
       "         [ 0.5325],\n",
       "         [-0.2270],\n",
       "         [ 0.0485],\n",
       "         [-0.5637],\n",
       "         [ 0.1862],\n",
       "         [ 0.7595],\n",
       "         [-0.2511],\n",
       "         [ 0.1841],\n",
       "         [-0.3207]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(torch.randint(32, (1,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#doesnt work, Quantizer cannot be found in brevitas.inject\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#why are they being searched for in inject if they are in brevitas.quant.scaled_int\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m'\u001b[39;49m\u001b[39mquantized_test\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:831\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    829\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    830\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 831\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    832\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    833\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed"
     ]
    }
   ],
   "source": [
    "#doesnt work, Quantizer cannot be found in brevitas.inject\n",
    "#why are they being searched for in inject if they are in brevitas.quant.scaled_int\n",
    "torch.save(model, 'quantized_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test class attributes\n",
    "class DeviceSingleton:\n",
    "    \"\"\"\n",
    "        Boilerplate class which contains the device used for the entire process. The reason why a class is created is in order to monitor when changes\n",
    "        to the device are going to occur by using the property decorator. Currently, changes will be allowed and logged.\n",
    "    \"\"\"\n",
    "    _device = None\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def device():\n",
    "        return _device\n",
    "\n",
    "    @device.setter\n",
    "    def device(new_device):\n",
    "        match new_device:\n",
    "            case 'cuda':\n",
    "                new_device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "            case 'gpu':\n",
    "                new_device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "            case 'mps':\n",
    "                new_device = 'mps' if backends.mps.is_available() else 'cpu'\n",
    "            case 'cpu':\n",
    "                new_device = 'cpu'\n",
    "            case _:\n",
    "                print(f'Device {new_device} not recognized. Using default: CPU')\n",
    "                new_device = 'cpu'\n",
    "        _device = device(new_device)\n",
    "        print(f'Using device: {new_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeviceSingleton.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton = DeviceSingleton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DeviceSingleton.device() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m singleton\u001b[39m.\u001b[39;49mdevice\n",
      "\u001b[0;31mTypeError\u001b[0m: DeviceSingleton.device() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "singleton.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeviceSingleton.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.49767026\n"
     ]
    }
   ],
   "source": [
    "def fuse_conv_and_bn(conv, bn):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv1d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) )\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\tfusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )\n",
    "\t#\n",
    "\t# we're done\n",
    "\treturn fusedconv\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = (16, 64, 256)\n",
    "x = torch.randn(16, 64, 256)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=(256,256)),\n",
    "    torch.nn.BatchNorm1d(64)\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy.load_state_dict(cv1.state_dict())\n",
    "bn1 = torch.nn.BatchNorm1d(5)\n",
    "qnn.utils.merge_bn(cv1, bn1)\n",
    "input = torch.Tensor(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 is cv1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9793e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9794e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is the same without batchnorm, why?\n",
    "output = cv1_copy(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272],\n",
       "        [ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bn1(input)\n",
    "cv1_copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(30, (3,5,20)).to(dtype=torch.float32) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5470e+00,  1.5270e-01, -2.5464e-01,  2.9082e-01,  1.2001e+00,\n",
       "           6.7265e-01,  7.3291e-01, -1.8250e-01, -1.2105e+00, -1.0728e+00,\n",
       "           3.1590e-01,  2.5334e+00, -1.3411e-01,  2.5901e+00,  6.9120e-02,\n",
       "          -5.1725e-01,  1.9493e-01,  2.2703e+00],\n",
       "         [-3.4492e-01, -9.6099e-01, -9.2788e-01, -5.6099e-01, -2.0823e+00,\n",
       "           8.8297e-01,  4.6034e-01,  9.3609e-01,  1.8312e+00, -8.3214e-01,\n",
       "          -1.0253e+00, -1.3361e+00, -1.3721e+00,  4.9575e-01, -6.1378e-01,\n",
       "           3.7313e-01, -1.6607e+00, -7.7247e-01],\n",
       "         [ 1.8919e+00,  8.4047e-01,  5.7258e-01,  3.1601e-01,  1.4367e-01,\n",
       "          -4.8590e-01,  7.8809e-01,  5.6231e-01, -9.9302e-01, -5.6623e-01,\n",
       "          -2.1978e-01, -8.2209e-01, -2.8324e-02,  9.5371e-01,  9.6952e-02,\n",
       "          -6.7169e-01, -8.7423e-02, -4.8495e-02],\n",
       "         [-2.3595e+00, -4.5535e-01,  1.1663e+00,  1.6639e+00,  5.8315e-01,\n",
       "          -4.0086e-01, -4.8103e-01, -2.0247e+00,  4.6665e-01,  2.1141e-01,\n",
       "           2.7884e-02,  1.7325e+00,  2.3958e+00,  7.5227e-01,  6.7972e-02,\n",
       "           5.4808e-01,  1.2512e+00,  8.5656e-01],\n",
       "         [-6.4959e-01,  6.6814e-01,  4.8005e-01, -2.0198e+00,  4.6459e-01,\n",
       "          -8.1610e-01, -2.8203e-01,  3.0454e-01,  1.0447e+00,  4.4882e-02,\n",
       "          -9.1393e-01,  1.3076e+00, -2.0745e+00, -3.1045e-01,  6.2427e-01,\n",
       "           5.3256e-01,  4.6315e-01, -1.1006e+00]],\n",
       "\n",
       "        [[ 1.9405e-01,  9.4450e-01, -2.1945e-01, -1.0464e-01, -2.1462e+00,\n",
       "          -1.8074e-01, -5.1549e-01,  2.3760e-01, -4.5039e-01,  2.9090e-02,\n",
       "          -2.0757e-01,  1.0382e+00, -1.5610e-01,  5.3232e-01, -9.5934e-01,\n",
       "           2.4156e-01,  1.1666e+00, -3.6109e-01],\n",
       "         [ 1.9643e+00, -3.5404e-02,  2.8985e-01,  1.2767e+00,  1.5938e+00,\n",
       "           2.8329e-01,  1.3378e-01, -5.0787e-01,  1.2574e+00, -6.1935e-01,\n",
       "          -8.2427e-01,  4.4821e-01, -2.4590e-01, -8.7280e-01,  1.4246e+00,\n",
       "           8.9188e-02,  1.6821e-01, -2.1289e+00],\n",
       "         [-6.4067e-02, -3.6310e-01, -5.1032e-01,  4.1864e-01,  5.1348e-01,\n",
       "          -2.7373e+00,  2.9017e-01,  2.4135e+00, -7.8918e-01, -1.9854e-01,\n",
       "           2.0203e+00, -3.6310e-01, -5.2306e-01,  8.9957e-01, -1.4630e+00,\n",
       "           1.2373e-02, -1.2238e+00,  9.9299e-01],\n",
       "         [ 9.1042e-01,  1.4595e-01, -1.4738e+00, -1.5985e+00, -1.7431e+00,\n",
       "           3.1330e-02,  4.3972e-01, -7.1936e-01,  5.5935e-01,  3.6048e-01,\n",
       "           6.7992e-01,  1.2452e+00, -1.1293e+00, -6.2311e-02,  7.0498e-01,\n",
       "           1.4889e+00,  8.2691e-02, -1.0297e+00],\n",
       "         [-2.0543e-01,  5.3759e-01, -9.8586e-01,  2.0672e-01,  6.3649e-01,\n",
       "           2.3649e+00, -1.7379e+00, -6.4168e-01,  1.0911e+00,  3.2361e-01,\n",
       "          -2.0871e+00,  1.4637e+00, -1.4250e+00, -1.5364e-01,  4.2754e-01,\n",
       "          -4.6797e-01,  1.4504e+00, -5.4709e-01]],\n",
       "\n",
       "        [[-8.9680e-02, -1.4670e+00,  1.2023e+00,  6.6737e-01, -1.5057e+00,\n",
       "          -4.8822e-01,  2.4024e-01, -1.5994e+00,  1.7873e+00, -5.9907e-01,\n",
       "          -8.3309e-01,  5.0197e-01, -7.8162e-01,  1.8217e-01, -4.5251e-02,\n",
       "          -1.7203e+00, -8.5721e-02, -5.5332e-01],\n",
       "         [ 1.1663e+00, -1.0131e+00,  8.3614e-02,  6.0756e-01,  1.2145e+00,\n",
       "           1.1872e+00,  1.1804e-01, -2.2524e-01,  3.7509e-01, -2.6787e-01,\n",
       "           4.7772e-01,  2.2624e-01, -1.5147e-01,  6.2887e-01,  1.1935e-01,\n",
       "           1.5600e+00, -8.1634e-02, -2.2099e+00],\n",
       "         [ 5.1029e-01, -1.4743e+00,  9.7388e-01,  3.3264e-01,  2.0862e+00,\n",
       "          -1.2319e+00, -2.1481e+00,  7.9234e-01, -7.6193e-01,  1.6490e-01,\n",
       "           8.2065e-01,  6.9478e-04,  4.6818e-01, -1.6127e+00, -4.3211e-01,\n",
       "           4.9885e-02, -7.4565e-01,  6.3875e-01],\n",
       "         [-4.3844e-01,  2.0107e-01, -7.5047e-03,  1.8040e-01, -1.4647e+00,\n",
       "           7.5665e-01,  1.1131e+00, -6.5673e-01, -2.1358e-01, -8.7125e-01,\n",
       "          -5.8282e-01,  1.0587e-01, -1.6796e+00, -2.1013e-01, -6.5641e-01,\n",
       "          -8.1833e-01, -2.7934e-01,  6.2668e-01],\n",
       "         [ 1.6464e+00,  4.3653e-01, -1.2452e+00, -5.8335e-02, -3.4749e-01,\n",
       "           4.8509e-01,  3.7072e-01, -3.7842e-01,  3.8690e-01,  8.0157e-01,\n",
       "          -8.9019e-01, -1.9134e-02, -1.5724e-01,  2.2794e-01, -6.8663e-01,\n",
       "           8.4329e-01,  2.0301e+00, -1.4638e+00]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.nn.BatchNorm1d(5)(qnn.QuantConv1d(5,5,kernel_size=3)(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5805,  1.6874,  1.4232,  0.4953,  0.3392,  1.6921,  0.4289,\n",
       "           0.6056,  1.4143, -0.3349,  0.7054,  1.2688,  0.7755,  0.8200,\n",
       "           0.8781,  1.1979,  0.5057,  0.3852],\n",
       "         [-0.6557,  0.1111,  0.1168,  0.1569, -0.1044, -0.7017, -0.4960,\n",
       "          -0.0382, -0.2186,  0.1632,  0.3991, -0.2319, -0.1412, -1.0671,\n",
       "          -0.2227, -0.4349,  0.0598, -0.0862],\n",
       "         [-1.8093, -1.3581, -1.4218, -1.9729, -2.4638, -1.5891, -2.7596,\n",
       "          -1.0707, -0.9489, -1.2964, -0.4576, -2.2601, -2.3817, -2.9702,\n",
       "          -2.1696, -1.4183, -1.6867, -1.5476],\n",
       "         [ 0.0087,  0.9669,  0.2861, -0.0195,  1.7805, -0.1940,  1.5801,\n",
       "           0.4090,  0.1231, -0.3474,  0.4677,  0.5724,  0.8208,  1.0850,\n",
       "           1.3689,  0.1611,  0.5700, -0.0545],\n",
       "         [-1.7327, -0.0812, -0.3896, -1.2272, -0.7673, -1.1279, -1.3454,\n",
       "          -0.1006,  0.1854, -0.1570, -0.0988, -0.2189, -0.5590, -1.7284,\n",
       "          -0.0418, -0.0416,  0.0663, -0.0783]],\n",
       "\n",
       "        [[ 1.6639,  0.7003,  1.2008,  0.9851,  0.8164,  1.4518,  1.4647,\n",
       "          -0.1788,  2.0859,  0.5120,  0.8852,  0.7612,  0.6098,  0.6913,\n",
       "           0.9692,  0.6834,  1.3616,  0.9310],\n",
       "         [-0.8825, -0.3148, -0.2431, -0.5726, -0.2170, -0.3808, -0.1197,\n",
       "          -0.2470, -0.1361, -0.0082,  0.8303, -0.6241,  0.2627,  0.3186,\n",
       "          -0.6388, -0.2168, -0.9436,  0.2259],\n",
       "         [-1.2239, -2.1880, -1.9660, -2.3704, -0.7485, -0.4421, -1.5090,\n",
       "          -2.1500, -0.3124, -1.8201, -1.7527, -2.3098, -1.7470, -2.1069,\n",
       "          -1.3791, -1.5358, -2.2366, -2.0583],\n",
       "         [ 0.6760,  1.1154,  0.6760,  1.2122,  0.3283,  0.1619, -0.1343,\n",
       "           0.7418,  0.9900,  0.0270,  0.7077,  0.9982,  0.1400,  1.7570,\n",
       "          -0.0549,  1.0549,  0.4207,  0.7455],\n",
       "         [-1.1979, -1.6723, -1.1881, -1.0570, -0.1860,  0.5241, -1.5507,\n",
       "          -0.7745, -0.1433, -0.6879, -0.4526, -0.6777, -1.1168, -0.2085,\n",
       "          -0.0175, -0.5790, -1.0503, -0.2322]],\n",
       "\n",
       "        [[ 1.5133,  1.3983,  1.5074,  1.0723,  0.7788,  0.7588,  0.6164,\n",
       "           1.0027,  1.2222,  1.5164,  1.4553,  0.7161,  0.7796,  1.6876,\n",
       "           0.4413,  2.1108,  1.6400,  0.9259],\n",
       "         [-0.9756,  0.0696, -0.2441, -0.4958,  0.0807,  0.5572,  0.0222,\n",
       "          -0.4592, -0.5887, -0.3081, -0.0168, -0.1240,  0.1828, -0.1130,\n",
       "          -0.5912, -1.0130, -0.6067,  0.7823],\n",
       "         [-1.9029, -1.1918, -1.7631, -2.7777, -1.8128, -0.9612, -1.9680,\n",
       "          -2.2855, -1.8218, -1.8851, -1.3642, -2.3375, -1.5278, -1.6818,\n",
       "          -2.4809, -1.1818, -1.5407, -1.7131],\n",
       "         [ 0.4436,  0.8576,  0.8286,  1.5920,  0.6813,  1.2457,  0.7904,\n",
       "           0.3165,  0.7779,  1.4541, -0.0767,  1.1432,  0.8329,  0.7933,\n",
       "           0.9158,  1.3503, -0.0775,  0.6725],\n",
       "         [-0.9279, -0.1795, -1.8110, -1.0357, -0.6544,  0.1664, -0.4564,\n",
       "          -0.7024, -0.6100, -1.0599, -1.0029, -1.0985, -0.9784, -0.8995,\n",
       "          -1.0086, -0.7555, -0.4560, -0.3629]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function brevitas.nn.utils.merge_bn(layer, bn, output_channel_dim=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv1d and batchnorm1d merge\n",
    "\n",
    "qnn.quant_layer.merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9917, 0.4984, 0.6176, 0.5039, 0.8158, 0.8521, 0.0155, 0.1858,\n",
       "          0.8048],\n",
       "         [0.1621, 0.4298, 0.3947, 0.5427, 0.8238, 0.9419, 0.7478, 0.4333,\n",
       "          0.0647],\n",
       "         [0.0897, 0.2927, 0.9780, 0.6710, 0.0377, 0.8199, 0.1301, 0.8592,\n",
       "          0.8216],\n",
       "         [0.2074, 0.6790, 0.2042, 0.7838, 0.5414, 0.5088, 0.8481, 0.2490,\n",
       "          0.1760],\n",
       "         [0.0197, 0.6737, 0.1897, 0.2794, 0.4024, 0.3306, 0.8610, 0.8641,\n",
       "          0.6871],\n",
       "         [0.7651, 0.4413, 0.9831, 0.4328, 0.2344, 0.0799, 0.4901, 0.1151,\n",
       "          0.9380]],\n",
       "\n",
       "        [[0.4503, 0.5180, 0.3012, 0.7354, 0.2637, 0.9073, 0.9226, 0.7925,\n",
       "          0.0674],\n",
       "         [0.9067, 0.1654, 0.9186, 0.1072, 0.0438, 0.4049, 0.1374, 0.3990,\n",
       "          0.6381],\n",
       "         [0.3767, 0.8549, 0.5588, 0.2489, 0.2599, 0.6461, 0.5800, 0.1559,\n",
       "          0.0832],\n",
       "         [0.9381, 0.2192, 0.7259, 0.7615, 0.1411, 0.1472, 0.9268, 0.6733,\n",
       "          0.9049],\n",
       "         [0.1468, 0.8668, 0.3151, 0.5401, 0.4347, 0.5541, 0.0995, 0.6333,\n",
       "          0.1252],\n",
       "         [0.4964, 0.4591, 0.3443, 0.2972, 0.6705, 0.2664, 0.4867, 0.5302,\n",
       "          0.6139]],\n",
       "\n",
       "        [[0.3803, 0.7252, 0.5246, 0.4232, 0.7195, 0.7118, 0.8266, 0.7990,\n",
       "          0.3631],\n",
       "         [0.1904, 0.0903, 0.8097, 0.7286, 0.2548, 0.3355, 0.7833, 0.9820,\n",
       "          0.1257],\n",
       "         [0.6124, 0.5454, 0.4477, 0.6442, 0.5862, 0.4324, 0.3639, 0.6780,\n",
       "          0.3984],\n",
       "         [0.4506, 0.1190, 0.4191, 0.3696, 0.6122, 0.7606, 0.1218, 0.3204,\n",
       "          0.6428],\n",
       "         [0.0765, 0.5835, 0.6852, 0.1305, 0.3852, 0.2372, 0.3551, 0.0528,\n",
       "          0.6343],\n",
       "         [0.0852, 0.0078, 0.7411, 0.6070, 0.8316, 0.9041, 0.3005, 0.5442,\n",
       "          0.2225]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand((3,6,9))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5546, -0.3177,  0.1346, -0.2969,  0.8870,  1.0246, -2.1506,\n",
       "          -1.5043,  0.8454],\n",
       "         [-0.9784, -0.1143, -0.2279,  0.2499,  1.1573,  1.5384,  0.9118,\n",
       "          -0.1030, -1.2930],\n",
       "         [-1.5225, -0.7464,  1.8741,  0.7002, -1.7215,  1.2696, -1.3680,\n",
       "           1.4198,  1.2762],\n",
       "         [-1.0571,  0.6570, -1.0687,  1.0382,  0.1571,  0.0385,  1.2718,\n",
       "          -0.9059, -1.1712],\n",
       "         [-1.5085,  0.9969, -0.8575, -0.5138, -0.0426, -0.3174,  1.7148,\n",
       "           1.7266,  1.0484],\n",
       "         [ 1.0773, -0.1349,  1.8936, -0.1669, -0.9097, -1.4882,  0.0477,\n",
       "          -1.3565,  1.7249]],\n",
       "\n",
       "        [[-0.5003, -0.2434, -1.0663,  0.5820, -1.2088,  1.2342,  1.2923,\n",
       "           0.7987, -1.9539],\n",
       "         [ 1.4249, -0.9680,  1.4631, -1.1558, -1.3605, -0.1948, -1.0582,\n",
       "          -0.2139,  0.5578],\n",
       "         [-0.4253,  1.4034,  0.2712, -0.9140, -0.8717,  0.6049,  0.3522,\n",
       "          -1.2693, -1.5475],\n",
       "         [ 1.5987, -1.0141,  0.8276,  0.9569, -1.2980, -1.2758,  1.5576,\n",
       "           0.6364,  1.4780],\n",
       "         [-1.0216,  1.7369, -0.3769,  0.4850,  0.0815,  0.5390, -1.2028,\n",
       "           0.8424, -1.1045],\n",
       "         [ 0.0714, -0.0682, -0.4982, -0.6744,  0.7233, -0.7897,  0.0349,\n",
       "           0.1978,  0.5111]],\n",
       "\n",
       "        [[-0.7660,  0.5431, -0.2185, -0.6032,  0.5214,  0.4922,  0.9280,\n",
       "           0.8232, -0.8312],\n",
       "         [-0.8871, -1.2103,  1.1116,  0.8499, -0.6794, -0.4189,  1.0267,\n",
       "           1.6678, -1.0960],\n",
       "         [ 0.4760,  0.2200, -0.1538,  0.5978,  0.3758, -0.2121, -0.4741,\n",
       "           0.7270, -0.3422],\n",
       "         [-0.1729, -1.3784, -0.2876, -0.4673,  0.4144,  0.9536, -1.3683,\n",
       "          -0.6463,  0.5256],\n",
       "         [-1.2911,  0.6514,  1.0409, -1.0840, -0.1082, -0.6752, -0.2235,\n",
       "          -1.3819,  0.8459],\n",
       "         [-1.4683, -1.7582,  0.9877,  0.4856,  1.3264,  1.5978, -0.6620,\n",
       "           0.2501, -0.9544]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized values along second dimension, meaning: along sentences\n",
    "#are \n",
    "torch.nn.BatchNorm1d(6)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor retains size, batchnorm essentially is a linear transformation to shift values to have a mean of 0 and a standard deviation of 1\n",
    "torch.nn.BatchNorm1d(10)(torch.Tensor(3,10,16)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = qnn.QuantIdentity()\n",
    "tensor = torch.Tensor(12,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1617e+35,  3.0907e-41, -1.5597e+37,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.1351e-43,  0.0000e+00],\n",
      "         [-1.5597e+37,  3.0907e-41, -3.0176e+34,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test if quantidentity is a simple wrapper around a tensor that does nothing\n",
    "#if so, it could be useful for merging with batchnorm\n",
    "tensor = torch.Tensor(2,3,4)\n",
    "print(tensor)\n",
    "print(\"\\n\" + 30* \"-\" + \"\\n\")\n",
    "print(qnn.QuantIdentity()(tensor).isclose(tensor).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = identity(tensor)\n",
    "output.size == tensor.size\n",
    "output == tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
