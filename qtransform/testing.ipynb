{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General testing notebook for qtransform and quantization\n",
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from logging import getLogger\n",
    "import os\n",
    "from omegaconf import DictConfig\n",
    "from brevitas import nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with dataclasses and python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod, ABC\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\"\n",
    "\n",
    "class Foo(ABC):\n",
    "    def __init__(self, encoding: str):\n",
    "        self.metadata: Metadata = Metadata(encoding)\n",
    "\n",
    "\n",
    "    def load_metadata():\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def test(self, file: str):\n",
    "        file += \"   padding\"\n",
    "\n",
    "class Bar(Foo):\n",
    "    def __init__(self, encoding: str):\n",
    "        super().__init__()\n",
    "        self.metadata: BarMetadata\n",
    "\n",
    "    def test(self, file: str):\n",
    "        super().test(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Foo(a=1, b=2, c=0.0, d=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if some keys can be omited within a dict if they are default params within the dataclass\n",
    "@dataclass\n",
    "class Foo():\n",
    "    a: float = 0.0\n",
    "    b: float = 0.0\n",
    "    c: float = 0.0\n",
    "    d: float = 0.0\n",
    "    \n",
    "args = {\"a\": 1, \"b\": 2}\n",
    "\n",
    "Foo(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarMetadata(encoding='gpt2', other='Bruh')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BarMetadata(encoding=\"gpt2\", other=\"ok\")\n",
    "import dataclasses\n",
    "dataclasses.replace(test, **{\"other\": \"Bruh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.BarMetadata() argument after ** must be a mapping, not Metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test: Metadata \u001b[39m=\u001b[39m Metadata(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test: BarMetadata \u001b[39m=\u001b[39m BarMetadata(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest, other\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.BarMetadata() argument after ** must be a mapping, not Metadata"
     ]
    }
   ],
   "source": [
    "test: Metadata = Metadata(\"gpt2\")\n",
    "test: BarMetadata = BarMetadata(**test, other=\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = test\n",
    "params = set(inspect.signature(Metadata.__init__).parameters.keys()) - set(['self'])\n",
    "{x:getattr(obj, x) for x in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, \n",
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#test if inner functions can access member attributes\n",
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def function(self):\n",
    "        def other():\n",
    "            print(self.a)\n",
    "        other()\n",
    "\n",
    "Foo().function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# padding does not get appended to the parameter as it is a seperate function\n",
    "Bar().test(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tests with torch framework to gain familiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b,c,e = 4, 5,6\n",
    "tensor_3d = torch.arange(b*c*e).reshape(b,c,e)\n",
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch has 5 rows, only want 3 \n",
    "index = torch.tile(torch.arange(3).reshape(3,1), (b,1,e))\n",
    "#you only consider the first batch\n",
    "torch.gather(tensor_3d, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 0,  1,  2,  3,  4]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [30, 31, 32, 33, 34]],\n",
       "\n",
       "        [[60, 61, 62, 63, 64],\n",
       "         [60, 61, 62, 63, 64]],\n",
       "\n",
       "        [[90, 91, 92, 93, 94],\n",
       "         [90, 91, 92, 93, 94]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#objective: retrieve first rows of tensor_3d -> if we specify dim=1, we collapse along the rows (we perform indexing for each row)\n",
    "#b,c,e = 4,5,6\n",
    "#i always want the first row -> specify by row, dim=1\n",
    "#how do i reduce the amount of rows if the index tensor has to be of the same dimension?\n",
    "#dimension has to be the same but not the shape\n",
    "#torch.zeros(4,1,6) gets the first row of the tensor, but it is problematic if i want multiple rows as i \n",
    "#then use the same index (0) while having the output shape that i want\n",
    "#solution: arange\n",
    "#index=torch.zeros(4,1,6) -> if we use 5 instead of 6, each row has 5 columns\n",
    "#meaning: we need a row containing the same index \n",
    "tensor_3d.gather(dim=1, index=torch.zeros(4,2,6, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ]\n",
    "   ])\n",
    "#size is: 3, 4, 3. if you collapse in the first dimension (dim=0), the result tensor becomes of size 4,3. if you collapse it in the second dimension, you get a tensor of size 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)\n",
    "#in transformers, we usually have tensors of shape b,c,e (batch_size, context, embedding_dimension).\n",
    "#if we specify dim=0, we perform the operation along the entire batch, in dim=1 along the context and in dim=2 along the embedding dimension.\n",
    "#if we were to sum the tensors together, sum(dim=1) will yield the sum of the embeddings of each word.\n",
    "#think of it as squishing a dimension together so that it is of size 1, meaning that we have to squeeze in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "#test if torch.tile and tensor.repeat are the same\n",
    "c = 2 #simulate two words\n",
    "a = torch.arange(c).reshape((c,1)).repeat((3,1,4))\n",
    "b = torch.tile(torch.arange(c).reshape((c,1)), (3,1,4))\n",
    "print(a.equal(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 7],\n",
       "        [23]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"experiments with torch.gather\"\n",
    "M = torch.tensor([[1,2,3], [4,7,18], [19,9,23]])\n",
    "#if there is more than one value inside of the last dimension, continue along current index\n",
    "#meaning at dim=1:\n",
    "#[1,1,1] -> 2,7,9\n",
    "#[0,0,0] -> 1,4,19\n",
    "#increments along the current dimension\n",
    "#at new row, reset counter ->\n",
    "#[1] -> 2\n",
    "#[1] -> 2\n",
    "indexes = torch.tensor([1,1,2]).view(-1,1) \n",
    "\n",
    "dimension = 0 #2d, meaning dim=0 along rows, dim=1 along columns\n",
    "out = M.gather(dimension ,indexes) #dim=0: , dim=1: tensor([[ 2],[ 7],[23]])\n",
    "M.gather(1, torch.Tensor([[1],[1],[2]]).to(dtype=torch.long)) #counter along the current dimension for the dimension of index\n",
    "#M.gather(1, torch.tensor([[0,0,0],[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BatchNorm with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are: same\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding\n",
    "\"test if padding does not lower values\"\n",
    "#first word of each batch -> gather by column\n",
    "#result tensor: (3, 1, 64)\n",
    "#retrieving an index from the dimension increases the counter along index of said dimension by one\n",
    "#e.g. indexing 0 twice will retrieve two different values\n",
    "FEATURES = 16\n",
    "EMBEDDINGS = 64\n",
    "BATCH_SIZE = 3\n",
    "bn = torch.nn.BatchNorm1d(FEATURES)\n",
    "#get first word embeddings of three batches\n",
    "embedding_layer = torch.nn.Embedding(FEATURES, EMBEDDINGS)\n",
    "batch = embedding_layer(torch.randint(16, (BATCH_SIZE, FEATURES)))\n",
    "index = torch.arange(1).repeat(BATCH_SIZE,1,EMBEDDINGS).to(dtype=torch.long)\n",
    "embd_first_word = torch.gather(batch, index=index, dim=1)\n",
    "padding_bn = BatchNormWithPadding(FEATURES,bias=True)\n",
    "norm_padding = padding_bn(embd_first_word)\n",
    "norm = bn(batch)\n",
    "#check if values are the same\n",
    "print(f'Values are: {\"same\" if torch.gather(norm, index=index, dim=1).equal(norm_padding) else \"different\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#test if huggingface datasets can be created from text files\n",
    "import datasets\n",
    "\n",
    "BASEDIR = '/home/mabot004/.qtransform/datasets/files/tiny_shakespeare/raw'\n",
    "#number of rows depends on the amount of files\n",
    "files = [os.path.join(BASEDIR, 'data.txt')]\n",
    "#does the same as huggingface mapping but now with files\n",
    "def gen_text():\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as file:\n",
    "            yield {\"text\": file.read()}\n",
    "\n",
    "#chunk size from config, default 100\n",
    "def chunk_examples(examples):\n",
    "                #splits the text of each row into chunks of length chunk_length. currently it is only used\n",
    "                #for character tokenization to avoid feeding large samples to the tokenizer\n",
    "    chunk_length = 100\n",
    "                #perform tokenization on a handful of characters at a time\n",
    "                #from: https://huggingface.co/docs/datasets/process#split-long-examples            \n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in examples[\"text\"]:\n",
    "        new_chunks = [sentence[i:i + chunk_length] for i in range(0, len(sentence), chunk_length)]\n",
    "        chunks.extend(new_chunks)\n",
    "    return {\"chunks\": chunks}\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    CHUNK_LENGTH = 100\n",
    "    for sentence in examples[\"text\"]:\n",
    "        chunks += [sentence[i:i + CHUNK_LENGTH] for i in range(0, len(sentence), CHUNK_LENGTH)]\n",
    "    return {\"chunks\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09760c91ca94125a042cb91b256764f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "#chunks = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "#rotten_tomatoes = datasets.load_dataset('rotten_tomatoes')#\n",
    "#rotten_tomatoes[\"train\"].shard(num_shards=1000, index=0, contiguous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 11154\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked = shakespeare.map(chunk_examples, batched=True, remove_columns=shakespeare.column_names)\n",
    "chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok: 100%|██████████| 100/100 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# status bar like huggingface dataset map process\n",
    "from tqdm import tqdm\n",
    "msg = 'ok'\n",
    "for i in tqdm(range(100), desc=f'{msg}'):\n",
    "    msg = str(i)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i, data in tqdm(enumerate(range(10)), desc='test progress bar and other stdout stuff'):\n",
    "    print(data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 13952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['chunks'],\n",
      "        num_rows: 1745\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e85d125997141e9aff299774dc53747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd5be2fac8a4e98af374c8771baa1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29862032dbf1442fa5b3ee6e43f03da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash eve'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#error occurs because the splits have more than one feature and this function changes the amount of samples in each split of one feature without changint the other\n",
    "#so: 5 samples, 2 features. after mapping: text has 10 samples, other feature still has 5 features\n",
    "#from: https://github.com/huggingface/datasets/issues/1817#issuecomment-774066254\n",
    "rt_chunks = rotten_tomatoes.select_columns(\"text\").map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "print(rt_chunks)\n",
    "#tokenize\n",
    "rt_chunks = rt_chunks.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")\n",
    "rt_chunks.save_to_disk('/home/mabot004/custom_hf_datasets/')\n",
    "\"test if tokenizing is correct\"\n",
    "tokenizer.decode(rt_chunks[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d1411c88164121bfb64ff217c471df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170153f358c9444c95ba1d014ca57337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1fd3b4fac54f56ac9c6bcc2e6f90ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "block_size = 64\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "rt_grouped = rt_chunks.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "owt = datasets.load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 3429\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 434\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(rt_grouped[\"train\"], Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_hf = DataLoader(rt_grouped[\"train\"], batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1392248133.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[58], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(len(data[\"input_ids\"])\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader_hf))\n",
    "print(len(data[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "shakespeare = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "shakespeare = shakespeare.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.concatenate(shakespeare[:3][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memmap(memmap, start, end, data):\n",
    "    memmap[start:end] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test generating huggingface datasets from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 163 examples [00:00, 31912.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def gen_text():\n",
    "    for i in range(163):\n",
    "        yield {\"text\": i}\n",
    "\n",
    "test_threading = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 163\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.rename_column(\"text\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.shard(num_shards=30, index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "num_threads = 3 #os.cpu_count // 2\n",
    "batch_size = 30\n",
    "num_samples = len(test_threading)\n",
    "# 163 // 30 shards\n",
    "# -> 3 threads, each having a batch size of 30 samples\n",
    "# dataset has 163 samples -> each thread should have around 50-60 samples max\n",
    "# -> divide samples of dataset with num_threads\n",
    "# -> each thread should have the entire dataset as an arg, but split differently\n",
    "# range of splitting should be specified as an arg in thread -> index arg in parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why should you use multithreading? the writing process is I/O based\n",
    "#if anything, the amount of write requests increases with the amount of threads\n",
    "memmap = np.memmap('test', mode='w+', shape=(163,), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: 'abcd'\n"
     ]
    }
   ],
   "source": [
    "#playing around with error messages\n",
    "try:\n",
    "    int(\"abcd\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test memory usage in worst case scenarios\n",
    "\"\"\"\n",
    "\n",
    "#no high memory usage as memmap values are lazily loaded, only overhead is the pages (around 5MB per memmap )\n",
    "memmap = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap2 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap3 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap4 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "import psutil\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709600997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform.dataset import MemmapDataset\n",
    "#token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0\n",
    "memmap_ds = MemmapDataset(\n",
    "    token_file='/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin',\n",
    "    dtype=np.float32,\n",
    "    block_size=64,\n",
    "    start=0.0,\n",
    "    end=0.3\n",
    ")\n",
    "len(memmap_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(memmap_ds, batch_size=12, num_workers=8)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    input, labels = data\n",
    "    print(f'{input.size()}, {labels.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing batchnorm quant\n",
    "#https://github.com/Xilinx/brevitas/issues/542\n",
    "#https://github.com/Xilinx/brevitas/issues/363\n",
    "#test merge_bn from qtransform\n",
    "from qtransform.model.modules import CausalSelfAttention\n",
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding, MLP\n",
    "from qtransform.model.gpt import GPTConfig\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import utils as qutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from brevitas.quant import scaled_int\n",
    "#simulate values from embedding, skip positional encoding\n",
    "wte = torch.nn.Embedding(16,64)\n",
    "tokens = torch.randint(16, (3,16))\n",
    "embeddings = wte(tokens)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if quantized layers having return_quant_tensor set to True are compatible with torch operations \n",
    "quant_tensor_linear = qnn.QuantLinear(1,1,True,return_quant_tensor=True)\n",
    "quant_tensor_linear(torch.Tensor(8,1)) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug loading quantized checkpoint\n",
    "CHECKPOINT = '/home/mabot004/eki-transformer-dev/shakespeare_owt_benchmarking/owt/GPT_2024-03-01_11:22:53__epoch:9'\n",
    "#doesnt work since qtransform.dataset cannot be found\n",
    "#but module info about tokenizers is not saved in checkpoint, only their names\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': True, 'block_size': 384, 'vocab_size': 50256, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNorm', 'flash': False}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.layer.0.attn.attn_mask',\n",
       " 'transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.attn.attn_mask',\n",
       " 'transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if info about quant params are even saved within checkpoint\n",
    "import re\n",
    "keys = checkpoint[\"model_state_dict\"].keys()\n",
    "#quant param that exists within checkpoint: \n",
    "#transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value \n",
    "weights_and_biases = list(filter(lambda x: re.search(r'.+\\.(weight|bias)$', x), keys))\n",
    "def find(x):\n",
    "    if not re.search(r'.+\\.(weight|bias)$', x):\n",
    "        return x\n",
    "other_keys = list(filter(find, keys))\n",
    "len(keys) == len(weights_and_biases) # not only weights and biases in state dict\n",
    "#only scaling_impl is saved in state dict\n",
    "#no multiheadattention though\n",
    "#in gpt quant config, every single layer has a quantizer (most commonly Int8WeightPerTensorFloat)\n",
    "#that quantizer has ScalingImplType STATS\n",
    "#the layers with scaling_impl had an activation quantizer named Int8ActPerTensorFloat\n",
    "#it had the ScalingImplType PARAMETER_FROM_STATS\n",
    "other_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6414)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if qparam is not one \n",
    "checkpoint[\"model_state_dict\"][\"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(3.1415, requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if scaling_impl params exist within model\n",
    "test_mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=256)\n",
    "#simulate some learning steps for param\n",
    "print(test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value = torch.nn.Parameter(torch.tensor(3.1415))\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_mha.state_dict(), 'mha.chpt')\n",
    "#v_quant etc. not appearing within state_dict\n",
    "test_mha.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[0.9874]])), ('bias', tensor([-0.8623]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if brevitas layers relevant for Transformers return qparams in state_dict\n",
    "print(qnn.QuantLinear(1,1,True,input_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantIdentity(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantReLU(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 5), match='allo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?!hallo|welt).*$', \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if storing checkpoints of quantized models even is working\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.ModuleDict(dict(\n",
    "            wte = qnn.QuantEmbedding(32, 128),\n",
    "            pos = qnn.QuantEmbedding(16, 128),\n",
    "            logic = nn.ModuleDict(dict(\n",
    "                layer1 = qnn.QuantLinear(128, 16, True),\n",
    "                layer2 = qnn.QuantLinear(16,1, True))\n",
    "            )\n",
    "        ))\n",
    "    def forward(self, x):\n",
    "        embd = self.network.wte(x)\n",
    "        b,t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "        pos = self.network.pos(pos)\n",
    "        output = embd + pos\n",
    "        for name, layer in self.network.logic.items():\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3944],\n",
       "         [ 0.2287],\n",
       "         [-0.5937],\n",
       "         [-0.8445],\n",
       "         [ 0.4049],\n",
       "         [-0.1961],\n",
       "         [ 0.2558],\n",
       "         [ 0.5325],\n",
       "         [-0.2270],\n",
       "         [ 0.0485],\n",
       "         [-0.5637],\n",
       "         [ 0.1862],\n",
       "         [ 0.7595],\n",
       "         [-0.2511],\n",
       "         [ 0.1841],\n",
       "         [-0.3207]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(torch.randint(32, (1,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#doesnt work, Quantizer cannot be found in brevitas.inject\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#why are they being searched for in inject if they are in brevitas.quant.scaled_int\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m'\u001b[39;49m\u001b[39mquantized_test\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:831\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    829\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    830\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 831\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    832\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    833\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed"
     ]
    }
   ],
   "source": [
    "#doesnt work, Quantizer cannot be found in brevitas.inject\n",
    "#why are they being searched for in inject if they are in brevitas.quant.scaled_int\n",
    "torch.save(model, 'quantized_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform import DeviceSingleton\n",
    "#check if value from class is set in object\n",
    "DeviceSingleton.device = 'cuda'\n",
    "singleton = DeviceSingleton()\n",
    "singleton.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Batchnorm and Conv merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.49767026\n"
     ]
    }
   ],
   "source": [
    "#from: \n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv1d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) )\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\tfusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )\n",
    "\t#\n",
    "\t# we're done\n",
    "\treturn fusedconv\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = (16, 64, 256)\n",
    "x = torch.randn(16, 64, 256)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=(256,256)),\n",
    "    torch.nn.BatchNorm1d(64)\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy.load_state_dict(cv1.state_dict())\n",
    "bn1 = torch.nn.BatchNorm1d(5)\n",
    "qnn.utils.merge_bn(cv1, bn1)\n",
    "input = torch.Tensor(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 is cv1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9793e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9794e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is the same without batchnorm, why?\n",
    "output = cv1_copy(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272],\n",
       "        [ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bn1(input)\n",
    "cv1_copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(30, (3,5,20)).to(dtype=torch.float32) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5470e+00,  1.5270e-01, -2.5464e-01,  2.9082e-01,  1.2001e+00,\n",
       "           6.7265e-01,  7.3291e-01, -1.8250e-01, -1.2105e+00, -1.0728e+00,\n",
       "           3.1590e-01,  2.5334e+00, -1.3411e-01,  2.5901e+00,  6.9120e-02,\n",
       "          -5.1725e-01,  1.9493e-01,  2.2703e+00],\n",
       "         [-3.4492e-01, -9.6099e-01, -9.2788e-01, -5.6099e-01, -2.0823e+00,\n",
       "           8.8297e-01,  4.6034e-01,  9.3609e-01,  1.8312e+00, -8.3214e-01,\n",
       "          -1.0253e+00, -1.3361e+00, -1.3721e+00,  4.9575e-01, -6.1378e-01,\n",
       "           3.7313e-01, -1.6607e+00, -7.7247e-01],\n",
       "         [ 1.8919e+00,  8.4047e-01,  5.7258e-01,  3.1601e-01,  1.4367e-01,\n",
       "          -4.8590e-01,  7.8809e-01,  5.6231e-01, -9.9302e-01, -5.6623e-01,\n",
       "          -2.1978e-01, -8.2209e-01, -2.8324e-02,  9.5371e-01,  9.6952e-02,\n",
       "          -6.7169e-01, -8.7423e-02, -4.8495e-02],\n",
       "         [-2.3595e+00, -4.5535e-01,  1.1663e+00,  1.6639e+00,  5.8315e-01,\n",
       "          -4.0086e-01, -4.8103e-01, -2.0247e+00,  4.6665e-01,  2.1141e-01,\n",
       "           2.7884e-02,  1.7325e+00,  2.3958e+00,  7.5227e-01,  6.7972e-02,\n",
       "           5.4808e-01,  1.2512e+00,  8.5656e-01],\n",
       "         [-6.4959e-01,  6.6814e-01,  4.8005e-01, -2.0198e+00,  4.6459e-01,\n",
       "          -8.1610e-01, -2.8203e-01,  3.0454e-01,  1.0447e+00,  4.4882e-02,\n",
       "          -9.1393e-01,  1.3076e+00, -2.0745e+00, -3.1045e-01,  6.2427e-01,\n",
       "           5.3256e-01,  4.6315e-01, -1.1006e+00]],\n",
       "\n",
       "        [[ 1.9405e-01,  9.4450e-01, -2.1945e-01, -1.0464e-01, -2.1462e+00,\n",
       "          -1.8074e-01, -5.1549e-01,  2.3760e-01, -4.5039e-01,  2.9090e-02,\n",
       "          -2.0757e-01,  1.0382e+00, -1.5610e-01,  5.3232e-01, -9.5934e-01,\n",
       "           2.4156e-01,  1.1666e+00, -3.6109e-01],\n",
       "         [ 1.9643e+00, -3.5404e-02,  2.8985e-01,  1.2767e+00,  1.5938e+00,\n",
       "           2.8329e-01,  1.3378e-01, -5.0787e-01,  1.2574e+00, -6.1935e-01,\n",
       "          -8.2427e-01,  4.4821e-01, -2.4590e-01, -8.7280e-01,  1.4246e+00,\n",
       "           8.9188e-02,  1.6821e-01, -2.1289e+00],\n",
       "         [-6.4067e-02, -3.6310e-01, -5.1032e-01,  4.1864e-01,  5.1348e-01,\n",
       "          -2.7373e+00,  2.9017e-01,  2.4135e+00, -7.8918e-01, -1.9854e-01,\n",
       "           2.0203e+00, -3.6310e-01, -5.2306e-01,  8.9957e-01, -1.4630e+00,\n",
       "           1.2373e-02, -1.2238e+00,  9.9299e-01],\n",
       "         [ 9.1042e-01,  1.4595e-01, -1.4738e+00, -1.5985e+00, -1.7431e+00,\n",
       "           3.1330e-02,  4.3972e-01, -7.1936e-01,  5.5935e-01,  3.6048e-01,\n",
       "           6.7992e-01,  1.2452e+00, -1.1293e+00, -6.2311e-02,  7.0498e-01,\n",
       "           1.4889e+00,  8.2691e-02, -1.0297e+00],\n",
       "         [-2.0543e-01,  5.3759e-01, -9.8586e-01,  2.0672e-01,  6.3649e-01,\n",
       "           2.3649e+00, -1.7379e+00, -6.4168e-01,  1.0911e+00,  3.2361e-01,\n",
       "          -2.0871e+00,  1.4637e+00, -1.4250e+00, -1.5364e-01,  4.2754e-01,\n",
       "          -4.6797e-01,  1.4504e+00, -5.4709e-01]],\n",
       "\n",
       "        [[-8.9680e-02, -1.4670e+00,  1.2023e+00,  6.6737e-01, -1.5057e+00,\n",
       "          -4.8822e-01,  2.4024e-01, -1.5994e+00,  1.7873e+00, -5.9907e-01,\n",
       "          -8.3309e-01,  5.0197e-01, -7.8162e-01,  1.8217e-01, -4.5251e-02,\n",
       "          -1.7203e+00, -8.5721e-02, -5.5332e-01],\n",
       "         [ 1.1663e+00, -1.0131e+00,  8.3614e-02,  6.0756e-01,  1.2145e+00,\n",
       "           1.1872e+00,  1.1804e-01, -2.2524e-01,  3.7509e-01, -2.6787e-01,\n",
       "           4.7772e-01,  2.2624e-01, -1.5147e-01,  6.2887e-01,  1.1935e-01,\n",
       "           1.5600e+00, -8.1634e-02, -2.2099e+00],\n",
       "         [ 5.1029e-01, -1.4743e+00,  9.7388e-01,  3.3264e-01,  2.0862e+00,\n",
       "          -1.2319e+00, -2.1481e+00,  7.9234e-01, -7.6193e-01,  1.6490e-01,\n",
       "           8.2065e-01,  6.9478e-04,  4.6818e-01, -1.6127e+00, -4.3211e-01,\n",
       "           4.9885e-02, -7.4565e-01,  6.3875e-01],\n",
       "         [-4.3844e-01,  2.0107e-01, -7.5047e-03,  1.8040e-01, -1.4647e+00,\n",
       "           7.5665e-01,  1.1131e+00, -6.5673e-01, -2.1358e-01, -8.7125e-01,\n",
       "          -5.8282e-01,  1.0587e-01, -1.6796e+00, -2.1013e-01, -6.5641e-01,\n",
       "          -8.1833e-01, -2.7934e-01,  6.2668e-01],\n",
       "         [ 1.6464e+00,  4.3653e-01, -1.2452e+00, -5.8335e-02, -3.4749e-01,\n",
       "           4.8509e-01,  3.7072e-01, -3.7842e-01,  3.8690e-01,  8.0157e-01,\n",
       "          -8.9019e-01, -1.9134e-02, -1.5724e-01,  2.2794e-01, -6.8663e-01,\n",
       "           8.4329e-01,  2.0301e+00, -1.4638e+00]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.nn.BatchNorm1d(5)(qnn.QuantConv1d(5,5,kernel_size=3)(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5805,  1.6874,  1.4232,  0.4953,  0.3392,  1.6921,  0.4289,\n",
       "           0.6056,  1.4143, -0.3349,  0.7054,  1.2688,  0.7755,  0.8200,\n",
       "           0.8781,  1.1979,  0.5057,  0.3852],\n",
       "         [-0.6557,  0.1111,  0.1168,  0.1569, -0.1044, -0.7017, -0.4960,\n",
       "          -0.0382, -0.2186,  0.1632,  0.3991, -0.2319, -0.1412, -1.0671,\n",
       "          -0.2227, -0.4349,  0.0598, -0.0862],\n",
       "         [-1.8093, -1.3581, -1.4218, -1.9729, -2.4638, -1.5891, -2.7596,\n",
       "          -1.0707, -0.9489, -1.2964, -0.4576, -2.2601, -2.3817, -2.9702,\n",
       "          -2.1696, -1.4183, -1.6867, -1.5476],\n",
       "         [ 0.0087,  0.9669,  0.2861, -0.0195,  1.7805, -0.1940,  1.5801,\n",
       "           0.4090,  0.1231, -0.3474,  0.4677,  0.5724,  0.8208,  1.0850,\n",
       "           1.3689,  0.1611,  0.5700, -0.0545],\n",
       "         [-1.7327, -0.0812, -0.3896, -1.2272, -0.7673, -1.1279, -1.3454,\n",
       "          -0.1006,  0.1854, -0.1570, -0.0988, -0.2189, -0.5590, -1.7284,\n",
       "          -0.0418, -0.0416,  0.0663, -0.0783]],\n",
       "\n",
       "        [[ 1.6639,  0.7003,  1.2008,  0.9851,  0.8164,  1.4518,  1.4647,\n",
       "          -0.1788,  2.0859,  0.5120,  0.8852,  0.7612,  0.6098,  0.6913,\n",
       "           0.9692,  0.6834,  1.3616,  0.9310],\n",
       "         [-0.8825, -0.3148, -0.2431, -0.5726, -0.2170, -0.3808, -0.1197,\n",
       "          -0.2470, -0.1361, -0.0082,  0.8303, -0.6241,  0.2627,  0.3186,\n",
       "          -0.6388, -0.2168, -0.9436,  0.2259],\n",
       "         [-1.2239, -2.1880, -1.9660, -2.3704, -0.7485, -0.4421, -1.5090,\n",
       "          -2.1500, -0.3124, -1.8201, -1.7527, -2.3098, -1.7470, -2.1069,\n",
       "          -1.3791, -1.5358, -2.2366, -2.0583],\n",
       "         [ 0.6760,  1.1154,  0.6760,  1.2122,  0.3283,  0.1619, -0.1343,\n",
       "           0.7418,  0.9900,  0.0270,  0.7077,  0.9982,  0.1400,  1.7570,\n",
       "          -0.0549,  1.0549,  0.4207,  0.7455],\n",
       "         [-1.1979, -1.6723, -1.1881, -1.0570, -0.1860,  0.5241, -1.5507,\n",
       "          -0.7745, -0.1433, -0.6879, -0.4526, -0.6777, -1.1168, -0.2085,\n",
       "          -0.0175, -0.5790, -1.0503, -0.2322]],\n",
       "\n",
       "        [[ 1.5133,  1.3983,  1.5074,  1.0723,  0.7788,  0.7588,  0.6164,\n",
       "           1.0027,  1.2222,  1.5164,  1.4553,  0.7161,  0.7796,  1.6876,\n",
       "           0.4413,  2.1108,  1.6400,  0.9259],\n",
       "         [-0.9756,  0.0696, -0.2441, -0.4958,  0.0807,  0.5572,  0.0222,\n",
       "          -0.4592, -0.5887, -0.3081, -0.0168, -0.1240,  0.1828, -0.1130,\n",
       "          -0.5912, -1.0130, -0.6067,  0.7823],\n",
       "         [-1.9029, -1.1918, -1.7631, -2.7777, -1.8128, -0.9612, -1.9680,\n",
       "          -2.2855, -1.8218, -1.8851, -1.3642, -2.3375, -1.5278, -1.6818,\n",
       "          -2.4809, -1.1818, -1.5407, -1.7131],\n",
       "         [ 0.4436,  0.8576,  0.8286,  1.5920,  0.6813,  1.2457,  0.7904,\n",
       "           0.3165,  0.7779,  1.4541, -0.0767,  1.1432,  0.8329,  0.7933,\n",
       "           0.9158,  1.3503, -0.0775,  0.6725],\n",
       "         [-0.9279, -0.1795, -1.8110, -1.0357, -0.6544,  0.1664, -0.4564,\n",
       "          -0.7024, -0.6100, -1.0599, -1.0029, -1.0985, -0.9784, -0.8995,\n",
       "          -1.0086, -0.7555, -0.4560, -0.3629]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if QuantMHA is learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug QuantMultiheadAttention and merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_bn_mha(layer, bn, output_channel_dim=0):\n",
    "    \n",
    "    #retrieve learnable parameters from batchnorm (scale + bias)\n",
    "    mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "        bn_mean=bn.running_mean,\n",
    "        bn_var=bn.running_var,\n",
    "        bn_eps=bn.eps,\n",
    "        bn_weight=bn.weight.data.clone(),\n",
    "        bn_bias=bn.bias.data.clone())\n",
    "    #out_proj is QuantLinear(in_features=embd_dim, out_features=embd_dim)\n",
    "    out_ch_weight_shape = qutils.compute_channel_view_shape(layer.weight, output_channel_dim)\n",
    "    #apply batchnorm during after forward pass of layer, before returning result\n",
    "    \n",
    "    #!!\n",
    "    layer.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "    #!!\n",
    "    \n",
    "    if layer.out_proj.bias is not None:\n",
    "        out_ch_bias_shape = qutils.compute_channel_view_shape(layer.out_proj.bias, channel_dim=0)\n",
    "        layer.out_proj.bias.data.mul_(mul_factor.view(out_ch_bias_shape))\n",
    "        layer.out_proj.bias.data.add_(add_factor.view(out_ch_bias_shape))\n",
    "    else:\n",
    "        layer.out_proj.bias = nn.Parameter(add_factor)\n",
    "    if (hasattr(layer, 'out_proj_weight_quant') and\n",
    "            isinstance(layer.out_proj_weight_quant, WeightQuantProxyFromInjector)):\n",
    "        layer.out_proj_weight_quant.init_tensor_quant()\n",
    "    if (hasattr(layer, 'out_proj_bias_quant') and isinstance(layer.out_proj_bias_quant, BiasQuantProxyFromInjector)):\n",
    "        layer.out_proj_bias_quant.init_tensor_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_linear = torch.nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "test_linear.weight = torch.nn.parameter.Parameter(torch.ones((embed_dim, embed_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         [ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         ...,\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         [ -8.1817,  -8.1817,  -8.1817,  ...,  -8.1817,  -8.1817,  -8.1817],\n",
       "         [  1.8297,   1.8297,   1.8297,  ...,   1.8297,   1.8297,   1.8297]],\n",
       "\n",
       "        [[-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670],\n",
       "         [ -3.0146,  -3.0146,  -3.0146,  ...,  -3.0146,  -3.0146,  -3.0146],\n",
       "         [  2.3900,   2.3900,   2.3900,  ...,   2.3900,   2.3900,   2.3900],\n",
       "         ...,\n",
       "         [  9.5870,   9.5870,   9.5870,  ...,   9.5870,   9.5870,   9.5870],\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [ -5.1709,  -5.1709,  -5.1709,  ...,  -5.1709,  -5.1709,  -5.1709]],\n",
       "\n",
       "        [[ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [ -0.0878,  -0.0878,  -0.0878,  ...,  -0.0878,  -0.0878,  -0.0878],\n",
       "         [ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         ...,\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [  4.6854,   4.6854,   4.6854,  ...,   4.6854,   4.6854,   4.6854],\n",
       "         [-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670]]],\n",
       "       grad_fn=<UnsafeViewBackward0>, names=('L', 'N', None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "heads = 2\n",
    "embed_dim = 64\n",
    "context = 16\n",
    "quant_mha = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "#pass the same input tensor to merged and unmerged mha + batchnorm and compare results\n",
    "quant_mha_merged = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "from brevitas import config\n",
    "#qparams not imported from state dict\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "quant_mha_merged.load_state_dict(quant_mha.state_dict())\n",
    "#feature length not critical\n",
    "bn = torch.nn.BatchNorm1d(context)\n",
    "#test if quantmha works\n",
    "assert quant_mha(embeddings, embeddings, embeddings)[0].size() == embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLinear(\n",
       "  in_features=64, out_features=64, bias=True\n",
       "  (input_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "      (activation_impl): Identity()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "          (stats_input_view_shape_impl): OverTensorView()\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsPercentile()\n",
       "          )\n",
       "          (restrict_scaling): _RestrictValue(\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (clamp_scaling): _ClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "          )\n",
       "          (restrict_inplace_preprocess): Identity()\n",
       "          (restrict_preprocess): Identity()\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (weight_quant): WeightQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): RescalingIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClampSte()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (scaling_impl): StatsFromParameterScaling(\n",
       "        (parameter_list_stats): _ParameterListStats(\n",
       "          (first_tracked_param): _ViewParameterWrapper(\n",
       "            (view_shape_impl): OverTensorView()\n",
       "          )\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsMax()\n",
       "          )\n",
       "        )\n",
       "        (stats_scaling_impl): _StatsScaling(\n",
       "          (affine_rescaling): Identity()\n",
       "          (restrict_clamp_scaling): _RestrictClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (restrict_scaling_pre): Identity()\n",
       "        )\n",
       "      )\n",
       "      (int_scaling_impl): IntScaling()\n",
       "      (zero_point_impl): ZeroZeroPoint(\n",
       "        (zero_point): StatelessBuffer()\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bias_quant): BiasQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): PrescaledRestrictIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClamp()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "      (zero_point): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_mha.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0926,  0.1662,  0.0606,  ...,  0.0211,  0.0027,  0.6270],\n",
       "          [-0.4505,  0.2013,  0.0885,  ..., -0.1597, -0.3558,  0.4523],\n",
       "          [-0.3425, -0.0519,  0.0775,  ..., -0.1996, -0.2760, -0.2787],\n",
       "          ...,\n",
       "          [-0.0925,  0.2803, -0.1011,  ...,  0.2038, -0.0426,  0.4598],\n",
       "          [ 0.1152,  0.0099, -0.0458,  ...,  0.0279, -0.2678, -0.3036],\n",
       "          [ 0.0462,  0.4055, -0.4288,  ...,  1.0150,  0.1507, -0.0658]],\n",
       " \n",
       "         [[-0.1684,  0.2740, -0.0567,  ...,  0.1945, -0.1586,  0.4784],\n",
       "          [-0.2414,  0.2263,  0.0706,  ..., -0.2693, -0.4626,  0.4440],\n",
       "          [-0.3649,  0.0976, -0.0294,  ..., -0.0971, -0.1957, -0.1036],\n",
       "          ...,\n",
       "          [-0.0461,  0.1246, -0.1975,  ...,  0.0336,  0.2553,  0.3853],\n",
       "          [ 0.0460,  0.0541, -0.0979,  ..., -0.0303, -0.3825, -0.3351],\n",
       "          [-0.0157,  0.2822, -0.4208,  ...,  0.5638,  0.3149,  0.0858]],\n",
       " \n",
       "         [[-0.0181,  0.1725, -0.0047,  ...,  0.1487,  0.1976,  0.5633],\n",
       "          [-0.1927,  0.1515, -0.0845,  ..., -0.2242, -0.4402,  0.5122],\n",
       "          [-0.1058, -0.1061,  0.0588,  ..., -0.2105, -0.3280, -0.3857],\n",
       "          ...,\n",
       "          [-0.0667,  0.1150, -0.3250,  ..., -0.0771,  0.1500,  0.4351],\n",
       "          [ 0.0051,  0.1560, -0.1228,  ...,  0.3762, -0.2658,  0.0824],\n",
       "          [ 0.0911,  0.2581, -0.2653,  ...,  0.3346,  0.2105,  0.3753]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.2267, 0.3770, 0.3974],\n",
       "          [0.2486, 0.5163, 0.2363],\n",
       "          [0.2704, 0.3674, 0.3619]],\n",
       " \n",
       "         [[0.4780, 0.2718, 0.2513],\n",
       "          [0.2445, 0.2800, 0.4767],\n",
       "          [0.3401, 0.3811, 0.2773]],\n",
       " \n",
       "         [[0.3292, 0.3783, 0.2923],\n",
       "          [0.2991, 0.5053, 0.1953],\n",
       "          [0.4384, 0.2035, 0.3578]],\n",
       " \n",
       "         [[0.2322, 0.4357, 0.3333],\n",
       "          [0.2759, 0.4958, 0.2281],\n",
       "          [0.2827, 0.2062, 0.5122]],\n",
       " \n",
       "         [[0.3565, 0.4220, 0.2199],\n",
       "          [0.4971, 0.1079, 0.3933],\n",
       "          [0.3510, 0.3333, 0.3155]],\n",
       " \n",
       "         [[0.3292, 0.2636, 0.4070],\n",
       "          [0.2868, 0.3141, 0.3988],\n",
       "          [0.3059, 0.3414, 0.3537]],\n",
       " \n",
       "         [[0.2759, 0.2800, 0.4439],\n",
       "          [0.3701, 0.2035, 0.4248],\n",
       "          [0.2144, 0.4534, 0.3319]],\n",
       " \n",
       "         [[0.3414, 0.4220, 0.2376],\n",
       "          [0.2213, 0.4070, 0.3715],\n",
       "          [0.2581, 0.3974, 0.3442]],\n",
       " \n",
       "         [[0.4644, 0.2827, 0.2540],\n",
       "          [0.4138, 0.0970, 0.4889],\n",
       "          [0.3005, 0.3360, 0.3633]],\n",
       " \n",
       "         [[0.3578, 0.2144, 0.4275],\n",
       "          [0.2295, 0.5545, 0.2172],\n",
       "          [0.3947, 0.1980, 0.4084]],\n",
       " \n",
       "         [[0.3865, 0.2103, 0.4029],\n",
       "          [0.2827, 0.5286, 0.1885],\n",
       "          [0.3906, 0.4398, 0.1694]],\n",
       " \n",
       "         [[0.3169, 0.3169, 0.3674],\n",
       "          [0.3169, 0.3169, 0.3674],\n",
       "          [0.3974, 0.3974, 0.2049]],\n",
       " \n",
       "         [[0.3251, 0.3496, 0.3251],\n",
       "          [0.3428, 0.3155, 0.3428],\n",
       "          [0.3251, 0.3496, 0.3251]],\n",
       " \n",
       "         [[0.4999, 0.2240, 0.2759],\n",
       "          [0.3770, 0.4097, 0.2131],\n",
       "          [0.4439, 0.3797, 0.1762]],\n",
       " \n",
       "         [[0.3578, 0.4275, 0.2144],\n",
       "          [0.3947, 0.4084, 0.1980],\n",
       "          [0.2295, 0.2172, 0.5545]],\n",
       " \n",
       "         [[0.0819, 0.4111, 0.5053],\n",
       "          [0.3046, 0.3387, 0.3565],\n",
       "          [0.3920, 0.3988, 0.2090]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#added a print statement after out_proj to see if the shape is changed afterwards\n",
    "quant_mha(embeddings,embeddings,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test if qparams are the same without copying them. they should have value 1 \n",
    "MISSING_QPARAMS =  [\"in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]\n",
    "for missing_qparam in MISSING_QPARAMS:\n",
    "    #omit \".value\" to get submodule \n",
    "    submodule = missing_qparam[:-len(\".value\")]\n",
    "    assert quant_mha.get_submodule(submodule).value == quant_mha_merged.get_submodule(submodule).value, f'qparam {missing_qparam} is different' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0486e+05, 4.5555e-41, 1.2967e-05],\n",
      "        [3.0866e-41, 4.4842e-44, 0.0000e+00]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(test(tensor))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'"
     ]
    }
   ],
   "source": [
    "import brevitas\n",
    "test = brevitas.core.quant.IntQuant(True, True)\n",
    "tensor = torch.Tensor(2,3)\n",
    "print(tensor)\n",
    "print(test(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if creating a custom activation can simulate batchnorm\n",
    "#### Alternative: Implement custom Quantizer which performs normalization based on a scale and add factor which can be passed in its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "def QuantIdentityWithWeights(qnn.QuantIdentity):\n",
    "    def __init__(self,\n",
    "            act_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        super().__init__(self,\n",
    "            act_quant = act_quant\n",
    "            return_quant_tensor = return_quant_tensor,\n",
    "            **kwargs):\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        return super().forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from brevitas.nn.quant_layer import ActQuantType\n",
    "from torch import Tensor\n",
    "from brevitas.quant.scaled_int import *\n",
    "from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL\n",
    "\n",
    "class QuantCustom(QuantNLAL):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mul_factor: Tensor,\n",
    "            add_factor: Tensor,\n",
    "            act_quant: Optional[ActQuantType] = Uint8ActPerTensorFloat,\n",
    "            input_quant: Optional[ActQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        QuantNLAL.__init__(\n",
    "            self,\n",
    "            act_impl=None,\n",
    "            passthrough_act=True,\n",
    "            input_quant=input_quant,\n",
    "            act_quant=act_quant,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "        self.mul_factor = mul_factor\n",
    "        self.add_factor = add_factor\n",
    "    \n",
    "\n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        input = self.unpack_input(input)\n",
    "        quant_input = self.input_quant(input)\n",
    "        # shortcut execution through the export impl during export\n",
    "        if self.export_mode:\n",
    "            out = self.export_handler(quant_input.value)\n",
    "            self._set_global_is_quant_layer(False)\n",
    "            return out\n",
    "        out = self.act_quant(quant_input)\n",
    "        \n",
    "        out = self.pack_output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL\n",
    "from torch.nn import Module as TorchModule\n",
    "from brevitas.nn.mixin import * #WeightQuantType, BiasQuantType\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat\n",
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "import torch\n",
    "#test if a quantized layer can be implemented which basically scales the values along a tensor and adds a bias, thereby simulating batch normalization\n",
    "class QuantBatchnorm1d(QuantWBIOL, TorchModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
    "            bias_quant: Optional[BiasQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs) -> None:\n",
    "        TorchModule.__init__(self)\n",
    "        if not isinstance(num_features, int) or num_features <= 0:\n",
    "            raise AttributeError()\n",
    "        #do the same as quantidentity\n",
    "        self.weight = torch.ones(num_features)\n",
    "        self.bias = torch.zeros(num_features)\n",
    "        QuantWBIOL.__init__(\n",
    "            self,\n",
    "            weight_quant=weight_quant,\n",
    "            bias_quant=bias_quant,\n",
    "            input_quant=None,\n",
    "            output_quant=None,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "    \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:\n",
    "        return self.forward_impl(input)\n",
    "    \n",
    "    def inner_forward_impl(self, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n",
    "        #inner_forward_impl is apparently the actual forward pass of the layer \n",
    "        return x * quant_weight[:,None] + quant_bias[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test functionality of custom Quantized layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.]), tensor([0., 0., 0., 0.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "#mul_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "#add_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "batch, context, embeds = 2,4,8\n",
    "#should do nothing as weights are 1 and biases are 0\n",
    "custom_quant_layer = QuantBatchnorm1d(num_features=context)\n",
    "print(f'{custom_quant_layer.weight}, {custom_quant_layer.bias}\\n')\n",
    "input = torch.randint(30, (batch, context, embeds))\n",
    "output = custom_quant_layer(input)\n",
    "assert input.equal(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### compare results of custom Batchnorm with regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True, False, False,  True,  True,  True, False,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True, False,  True, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True, False, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False,  True,  True, False,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embeddings = torch.randn(batch, sentence_length, embedding_dim)\n",
    "custom_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "regular_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "#simulate some forward passes to change the mean and standard deviation\n",
    "for _ in range(10):\n",
    "    regular_bn(embeddings)\n",
    "#test if merge_bn works\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=regular_bn.running_mean,\n",
    "    bn_var=regular_bn.running_var,\n",
    "    bn_eps=regular_bn.eps,\n",
    "    bn_weight=regular_bn.weight.data.clone(),\n",
    "    bn_bias=regular_bn.bias.data.clone())\n",
    "assert custom_quant_bn.weight.size() == mul_factor.size()\n",
    "assert custom_quant_bn.bias.size() == add_factor.size()\n",
    "#change weight and bias of quantized batchnorm\n",
    "custom_quant_bn.weight = mul_factor\n",
    "custom_quant_bn.bias = add_factor\n",
    "#test if results are at least somewhat similiar\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_bn.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "qtransform_quant_bn = merge_quant_bn(regular_bn)\n",
    "out_qtransform_quant_bn = qtransform_quant_bn(embeddings)\n",
    "print((out_regular_bn - out_qtransform_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn.utils import merge_bn\n",
    "custom_merged_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "merge_bn(layer=custom_merged_quant_bn, bn = regular_bn)\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy pasted from brevitas.nn.utils\n",
    "def compute_channel_view_shape(tensor: torch.Tensor, channel_dim: int):\n",
    "    #create a list containing ones with length of the tensor dimension (for mha: always length of 3)\n",
    "    broadcast_shape = [1] * len(tensor.size())\n",
    "    #why is that important\n",
    "    broadcast_shape[channel_dim] = -1\n",
    "    return tuple(broadcast_shape)\n",
    "\n",
    "def mul_add_from_bn(bn_mean, bn_var, bn_eps, bn_weight, bn_bias):\n",
    "    denom = torch.sqrt(bn_var + bn_eps)\n",
    "    mul_factor = bn_weight / denom\n",
    "    add_factor = -bn_mean * mul_factor + bn_bias\n",
    "    return mul_factor, add_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layernorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = embeddings.rename(None)\n",
    "for i in range(10):\n",
    "    layernorm(tensor)\n",
    "    bn(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerNorm' object has no attribute 'running_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test if mul_add_from_bn works for layernorm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layernorm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embed_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m ln_mul, ln_add \u001b[38;5;241m=\u001b[39m qutils\u001b[38;5;241m.\u001b[39mmul_add_from_bn(\n\u001b[0;32m----> 4\u001b[0m     bn_mean\u001b[38;5;241m=\u001b[39m\u001b[43mlayernorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m,\n\u001b[1;32m      5\u001b[0m     bn_var\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mrunning_var,\n\u001b[1;32m      6\u001b[0m     bn_eps\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m      7\u001b[0m     bn_weight\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m      8\u001b[0m     bn_bias\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerNorm' object has no attribute 'running_mean'"
     ]
    }
   ],
   "source": [
    "# test if mul_add_from_bn works for layernorm\n",
    "layernorm = torch.nn.LayerNorm(embed_dim, bias=True)\n",
    "ln_mul, ln_add = qutils.mul_add_from_bn(\n",
    "    bn_mean=layernorm.running_mean,\n",
    "    bn_var=layernorm.running_var,\n",
    "    bn_eps=layernorm.eps,\n",
    "    bn_weight=layernorm.weight.data.clone(),\n",
    "    bn_bias=layernorm.bias.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, -1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basically a list of ones with a -1 at index channel_dim\n",
    "compute_channel_view_shape(torch.Tensor(3,3,3,3,3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quant_mha_merged.out_proj.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (-1, 1), mul_factor view: torch.Size([16, 1])\n",
      "quant_mha_merged.out_proj.weight.data shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#disassemble functionality of merge_bn_mha\n",
    "#one dimensional tensor to scale all values of a batch with a corresponding scalar\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=bn.running_mean,\n",
    "    bn_var=bn.running_var,\n",
    "    bn_eps=bn.eps,\n",
    "    bn_weight=bn.weight.data.clone(),\n",
    "    bn_bias=bn.bias.data.clone())\n",
    "assert mul_factor.size() == add_factor.size()\n",
    "assert mul_factor.size()[0] == add_factor.size()[0] == context\n",
    "output_channel_dim = 0\n",
    "#out_proj is QuantLinear -> 2d Tensor, [-1, 1]\n",
    "#meaning: reverse shape of mul_factor tensor\n",
    "#currently: [1,context] now: [context, 1]\n",
    "out_ch_weight_shape = qutils.compute_channel_view_shape(quant_mha_merged.out_proj.weight, output_channel_dim)\n",
    "#out_proj is a linear layer applying a scaling factor to quantize outputs -> inputs: n_embd, outputs: n_embd\n",
    "#batchnorm applied normalization along second dimension, linear layer along third\n",
    "#-> could work if batchnorm normalizes along embeddings\n",
    "assert mul_factor.view(out_ch_weight_shape).size() != quant_mha_merged.out_proj.weight.data.size()\n",
    "print(f'shape: {out_ch_weight_shape}, mul_factor view: {mul_factor.view(out_ch_weight_shape).size()}')\n",
    "print(f'quant_mha_merged.out_proj.weight.data shape: {quant_mha_merged.out_proj.weight.data.size()}')\n",
    "#quant_mha_merged.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "#merge params of bn in quant_mha_merged\n",
    "#merge_bn_mha(quant_mha_merged, bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_factor: torch.Size([16]), add_factor: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#each value of mul_factor is the scale with which the embedding of one word is multiplied with\n",
    "#in total: context amount of words\n",
    "#linear layer calculates the sum of each embedding of a word with a weight\n",
    "#problem: weights are the same for every word\n",
    "print(f'mul_factor: {mul_factor.size()}, add_factor: {add_factor.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qutils.merge_bn(qnn.QuantLinear(context, context, True), bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <p>Rows: Context, Columns: Embeddings</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Row</th>\n",
       "            <th>Embedding 1</th>\n",
       "            <th>Embedding 2</th>\n",
       "            <th>Embedding 3</th>\n",
       "            <th>Embedding 4</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>-0.6182</td>\n",
       "            <td>0.6397</td>\n",
       "            <td>-0.6141</td>\n",
       "            <td>0.8668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>0.4140</td>\n",
       "            <td>0.1806</td>\n",
       "            <td>-1.1200</td>\n",
       "            <td>-0.3160</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\n",
    "    \"\"\"<body>\n",
    "    <p>Rows: Context, Columns: Embeddings</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Row</th>\n",
    "            <th>Embedding 1</th>\n",
    "            <th>Embedding 2</th>\n",
    "            <th>Embedding 3</th>\n",
    "            <th>Embedding 4</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>-0.6182</td>\n",
    "            <td>0.6397</td>\n",
    "            <td>-0.6141</td>\n",
    "            <td>0.8668</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>0.4140</td>\n",
    "            <td>0.1806</td>\n",
    "            <td>-1.1200</td>\n",
    "            <td>-0.3160</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each row of a prompt needs to be multiplied with the same scalar from mul_factor, the position of the row determines the index of mul_factor\\\n",
    "basically: row 1 * mul_factor[0], row 2 * mul_factor[2] etc.\\\n",
    "remember the bit_width value from quant config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6182,  0.6397, -0.6141,  0.1633,  0.1303,  0.9678, -0.0135, -0.3592,\n",
      "         1.0851,  1.2883, -1.4943, -1.3554, -1.2857,  0.5534, -0.9053, -0.2538,\n",
      "         2.0112,  1.5106, -0.5143,  0.0181, -1.1853, -0.1291,  1.1889, -0.2304,\n",
      "        -0.1677, -1.0456, -0.1630,  0.8798,  0.4793,  1.3267,  0.9272,  0.4181,\n",
      "        -1.6796, -0.2393,  0.7780, -1.7058, -1.1486, -1.5907,  0.9055, -0.6892,\n",
      "        -0.3182,  1.7268,  1.3576, -0.0698,  0.5315, -0.9513,  0.0850, -0.0770,\n",
      "         0.6108, -0.9660, -0.2021, -0.8171, -0.4134, -0.9940, -1.1997,  1.1844,\n",
      "         2.1950, -0.9969, -0.6415,  2.3817, -0.1636,  0.8668, -1.2963,  0.2591],\n",
      "       grad_fn=<SelectBackward0>, names=('E',)), \n",
      "tensor([-0.6476,  0.6723, -0.6434,  0.1723,  0.1378,  1.0165, -0.0132, -0.3759,\n",
      "         1.1397,  1.3528, -1.5670, -1.4212, -1.3480,  0.5817, -0.9490, -0.2653,\n",
      "         2.1113,  1.5861, -0.5386,  0.0200, -1.2427, -0.1344,  1.2486, -0.2408,\n",
      "        -0.1750, -1.0961, -0.1700,  0.9242,  0.5039,  1.3931,  0.9740,  0.4398,\n",
      "        -1.7614, -0.2501,  0.8174, -1.7889, -1.2042, -1.6681,  0.9512, -0.7222,\n",
      "        -0.3328,  1.8130,  1.4256, -0.0722,  0.5587, -0.9972,  0.0902, -0.0798,\n",
      "         0.6419, -1.0126, -0.2110, -0.8563, -0.4327, -1.0420, -1.2578,  1.2439,\n",
      "         2.3043, -1.0450, -0.6721,  2.5002, -0.1707,  0.9105, -1.3592,  0.2729],\n",
      "       grad_fn=<AddBackward0>, names=('E',))\n"
     ]
    }
   ],
   "source": [
    "#perform normalization like so:\n",
    "print(f'{embeddings[0][0]}, \\n{mul_factor[0] * embeddings[0][0] + add_factor[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n",
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_no_merge \u001b[39m=\u001b[39m bn(output_no_merge)\n\u001b[1;32m      4\u001b[0m output_merge \u001b[39m=\u001b[39m quant_mha_merged(q,k,v)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m output_no_merge\u001b[39m.\u001b[39mequal(output_merge)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q,k,v = [embeddings for _ in range(3)]\n",
    "output_no_merge = quant_mha(q,k,v)[0]\n",
    "output_no_merge = bn(output_no_merge)\n",
    "output_merge = quant_mha_merged(q,k,v)[0]\n",
    "assert output_no_merge.equal(output_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test what happens if values are transposed after merging QuantLinear with BatchNorm\n",
    "quant_linear = qnn.QuantLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if transposing inputs changes values for batchnorm with two different feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "transposing does not work",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m out_context \u001b[39m=\u001b[39m bn_context(\u001b[39minput\u001b[39m)\n\u001b[1;32m      6\u001b[0m out_embedding \u001b[39m=\u001b[39m bn_embedding(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39massert\u001b[39;00m out_context\u001b[39m.\u001b[39mequal(out_embedding), \u001b[39m\"\u001b[39m\u001b[39mtransposing does not work\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: transposing does not work"
     ]
    }
   ],
   "source": [
    "bn_context = torch.nn.BatchNorm1d(context)\n",
    "bn_embedding = torch.nn.BatchNorm1d(embed_dim)\n",
    "input = embeddings\n",
    "input = input.rename(None)\n",
    "out_context = bn_context(input)\n",
    "out_embedding = bn_embedding(input.transpose(-1,-2))\n",
    "#after transposing, normalize along embeddings instead of along words\n",
    "assert out_context.equal(out_embedding), \"transposing does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if linear transformation can simulate the functionality of batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9493,  2.7542,  9.5925,  7.8767,  1.7580,  3.1383,  6.2312,  5.3196,\n",
       "          7.8936,  1.0332],\n",
       "        [12.1436, 11.1988, 15.5672, 13.1937,  5.2797,  7.7320, 12.6711,  8.1652,\n",
       "         13.6740,  6.0436],\n",
       "        [ 4.6590,  9.2941,  0.4616, -0.8165,  1.4461,  7.0240, -2.8188,  0.2568,\n",
       "          0.8149,  6.8677]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randint(30, (3,10)).to(dtype=torch.float) * 0.7\n",
    "linear = torch.nn.Linear(3, 3)\n",
    "linear(test.transpose(-1,0)).transpose(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before attention calculation, q,k,v are quantized\n",
    "#shape: n_embd * 3 as q,k,v are the same and of shape n_embd\n",
    "quant_mha_merged.in_proj.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64]), torch.Size([3, 16, 64]), torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "#code copied from forward pass of QuantMultiheadAttention\n",
    "#function is called if in_proj quantization has been set\n",
    "#TODO: find out what it does\n",
    "def chunk(x, num=3, dim=-1):\n",
    "    _len, _bsz, _dim = x.shape\n",
    "    x = x.reshape(_len, _bsz, num, dim)\n",
    "    return x[:, :, 0, :], x[:, :, 1, :], x[:, :, 2, :]\n",
    "assert attn_no_batchfirst.in_proj is not None\n",
    "from brevitas.nn.utils import check_tensors_same_ptr\n",
    "#no idea what it does, it has to be True or else an Exception will be thrown\n",
    "assert check_tensors_same_ptr([embeddings, embeddings, embeddings]) == True\n",
    "torch._C._get_tracing_state()\n",
    "\n",
    "query = embeddings\n",
    "query.rename_('L', 'N', 'E')\n",
    "#no idea why q,k,v are infered from the query and params key and value are still used\n",
    "#this is an issue if no in_proj is specified i think\n",
    "q,k,v = chunk(attn_no_batchfirst.in_proj(query))\n",
    "print(f'{q.size()}, {k.size()}, {v.size()}')\n",
    "#issue with wrong shapes could be that batch size is transposed instead of embedding dimension\n",
    "#q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]), \n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(9).reshape(3,3)\n",
    "#columns become rows, rows become columns\n",
    "print(f'{tensor}, \\n{tensor.transpose(1,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "small_attn_cpy = CausalSelfAttention(GPTConfig(block_size=16, n_embd=64, n_head=2))\n",
    "#if batchnorm and mha are merged together, padding should not be necessary for inference\n",
    "small_attn_cpy.mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=64)\n",
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True #copy state dict does not return brevitas qparams\n",
    "small_attn_cpy.load_state_dict(small_attn.state_dict())\n",
    "#qparams from state dict are set to 1 at first\n",
    "print(small_attn.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "print(small_attn_cpy.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "idea from: https://github.com/Xilinx/brevitas/issues/542#issuecomment-1446338490\n",
    "merge_bn does not delete current batchnorm, meaning that one model has to be initialiized without bn and the parameters from the trained model\n",
    "have to be copied to the model without bn\n",
    "TODO: find more ressource efficient ways\n",
    "\"\"\"\n",
    "#at one step in merge_bn_mha, layer.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape)) is performed\n",
    "#weight is of shape (embd_dim, embd_dim), mul_factor is of (shape features, 1)\n",
    "#meaning that batchnorm probably normalizes along the embeddings instead of each sentence\n",
    "\"bn_alt feature length is 64 (embedding dimension)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     88\u001b[0m out_ch_weight_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mweight, output_channel_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     out_ch_bias_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias, channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#merge_bn_mha appends batchnorm to mha, TODO: prepend it (maybe use input_quant_tensor or something)\n",
    "#problem: merged and unmerged outputs are not the same, possibly since feature length is different\n",
    "no_merge_attn_output = small_attn(embeddings)\n",
    "no_merge_bn_output = bn(no_merge_attn_output)\n",
    "try:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=0)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=1)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=2)\n",
    "merge_attn_output = small_attn(embeddings)\n",
    "assert torch.equal(no_merge_bn_output, merge_attn_output) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mul_:\n",
      "\n",
      "mul_(...) method of torch.Tensor instance\n",
      "    mul_(value) -> Tensor\n",
      "    \n",
      "    In-place version of :meth:`~Tensor.mul`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer.out_proj.weight.data.mul_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a.mul_(tensor) basically is a = a * tensor\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.mul_(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_attn(torch.Tensor(3,16,64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function brevitas.nn.utils.merge_bn(layer, bn, output_channel_dim=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv1d and batchnorm1d merge\n",
    "\n",
    "qnn.quant_layer.merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9917, 0.4984, 0.6176, 0.5039, 0.8158, 0.8521, 0.0155, 0.1858,\n",
       "          0.8048],\n",
       "         [0.1621, 0.4298, 0.3947, 0.5427, 0.8238, 0.9419, 0.7478, 0.4333,\n",
       "          0.0647],\n",
       "         [0.0897, 0.2927, 0.9780, 0.6710, 0.0377, 0.8199, 0.1301, 0.8592,\n",
       "          0.8216],\n",
       "         [0.2074, 0.6790, 0.2042, 0.7838, 0.5414, 0.5088, 0.8481, 0.2490,\n",
       "          0.1760],\n",
       "         [0.0197, 0.6737, 0.1897, 0.2794, 0.4024, 0.3306, 0.8610, 0.8641,\n",
       "          0.6871],\n",
       "         [0.7651, 0.4413, 0.9831, 0.4328, 0.2344, 0.0799, 0.4901, 0.1151,\n",
       "          0.9380]],\n",
       "\n",
       "        [[0.4503, 0.5180, 0.3012, 0.7354, 0.2637, 0.9073, 0.9226, 0.7925,\n",
       "          0.0674],\n",
       "         [0.9067, 0.1654, 0.9186, 0.1072, 0.0438, 0.4049, 0.1374, 0.3990,\n",
       "          0.6381],\n",
       "         [0.3767, 0.8549, 0.5588, 0.2489, 0.2599, 0.6461, 0.5800, 0.1559,\n",
       "          0.0832],\n",
       "         [0.9381, 0.2192, 0.7259, 0.7615, 0.1411, 0.1472, 0.9268, 0.6733,\n",
       "          0.9049],\n",
       "         [0.1468, 0.8668, 0.3151, 0.5401, 0.4347, 0.5541, 0.0995, 0.6333,\n",
       "          0.1252],\n",
       "         [0.4964, 0.4591, 0.3443, 0.2972, 0.6705, 0.2664, 0.4867, 0.5302,\n",
       "          0.6139]],\n",
       "\n",
       "        [[0.3803, 0.7252, 0.5246, 0.4232, 0.7195, 0.7118, 0.8266, 0.7990,\n",
       "          0.3631],\n",
       "         [0.1904, 0.0903, 0.8097, 0.7286, 0.2548, 0.3355, 0.7833, 0.9820,\n",
       "          0.1257],\n",
       "         [0.6124, 0.5454, 0.4477, 0.6442, 0.5862, 0.4324, 0.3639, 0.6780,\n",
       "          0.3984],\n",
       "         [0.4506, 0.1190, 0.4191, 0.3696, 0.6122, 0.7606, 0.1218, 0.3204,\n",
       "          0.6428],\n",
       "         [0.0765, 0.5835, 0.6852, 0.1305, 0.3852, 0.2372, 0.3551, 0.0528,\n",
       "          0.6343],\n",
       "         [0.0852, 0.0078, 0.7411, 0.6070, 0.8316, 0.9041, 0.3005, 0.5442,\n",
       "          0.2225]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand((3,6,9))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5546, -0.3177,  0.1346, -0.2969,  0.8870,  1.0246, -2.1506,\n",
       "          -1.5043,  0.8454],\n",
       "         [-0.9784, -0.1143, -0.2279,  0.2499,  1.1573,  1.5384,  0.9118,\n",
       "          -0.1030, -1.2930],\n",
       "         [-1.5225, -0.7464,  1.8741,  0.7002, -1.7215,  1.2696, -1.3680,\n",
       "           1.4198,  1.2762],\n",
       "         [-1.0571,  0.6570, -1.0687,  1.0382,  0.1571,  0.0385,  1.2718,\n",
       "          -0.9059, -1.1712],\n",
       "         [-1.5085,  0.9969, -0.8575, -0.5138, -0.0426, -0.3174,  1.7148,\n",
       "           1.7266,  1.0484],\n",
       "         [ 1.0773, -0.1349,  1.8936, -0.1669, -0.9097, -1.4882,  0.0477,\n",
       "          -1.3565,  1.7249]],\n",
       "\n",
       "        [[-0.5003, -0.2434, -1.0663,  0.5820, -1.2088,  1.2342,  1.2923,\n",
       "           0.7987, -1.9539],\n",
       "         [ 1.4249, -0.9680,  1.4631, -1.1558, -1.3605, -0.1948, -1.0582,\n",
       "          -0.2139,  0.5578],\n",
       "         [-0.4253,  1.4034,  0.2712, -0.9140, -0.8717,  0.6049,  0.3522,\n",
       "          -1.2693, -1.5475],\n",
       "         [ 1.5987, -1.0141,  0.8276,  0.9569, -1.2980, -1.2758,  1.5576,\n",
       "           0.6364,  1.4780],\n",
       "         [-1.0216,  1.7369, -0.3769,  0.4850,  0.0815,  0.5390, -1.2028,\n",
       "           0.8424, -1.1045],\n",
       "         [ 0.0714, -0.0682, -0.4982, -0.6744,  0.7233, -0.7897,  0.0349,\n",
       "           0.1978,  0.5111]],\n",
       "\n",
       "        [[-0.7660,  0.5431, -0.2185, -0.6032,  0.5214,  0.4922,  0.9280,\n",
       "           0.8232, -0.8312],\n",
       "         [-0.8871, -1.2103,  1.1116,  0.8499, -0.6794, -0.4189,  1.0267,\n",
       "           1.6678, -1.0960],\n",
       "         [ 0.4760,  0.2200, -0.1538,  0.5978,  0.3758, -0.2121, -0.4741,\n",
       "           0.7270, -0.3422],\n",
       "         [-0.1729, -1.3784, -0.2876, -0.4673,  0.4144,  0.9536, -1.3683,\n",
       "          -0.6463,  0.5256],\n",
       "         [-1.2911,  0.6514,  1.0409, -1.0840, -0.1082, -0.6752, -0.2235,\n",
       "          -1.3819,  0.8459],\n",
       "         [-1.4683, -1.7582,  0.9877,  0.4856,  1.3264,  1.5978, -0.6620,\n",
       "           0.2501, -0.9544]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized values along second dimension, meaning: along sentences\n",
    "#are \n",
    "torch.nn.BatchNorm1d(6)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor retains size, batchnorm essentially is a linear transformation to shift values to have a mean of 0 and a standard deviation of 1\n",
    "torch.nn.BatchNorm1d(10)(torch.Tensor(3,10,16)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = qnn.QuantIdentity()\n",
    "tensor = torch.Tensor(12,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1617e+35,  3.0907e-41, -1.5597e+37,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.1351e-43,  0.0000e+00],\n",
      "         [-1.5597e+37,  3.0907e-41, -3.0176e+34,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test if quantidentity is a simple wrapper around a tensor that does nothing\n",
    "#if so, it could be useful for merging with batchnorm\n",
    "tensor = torch.Tensor(2,3,4)\n",
    "print(tensor)\n",
    "print(\"\\n\" + 30* \"-\" + \"\\n\")\n",
    "print(qnn.QuantIdentity()(tensor).isclose(tensor).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = identity(tensor)\n",
    "output.size == tensor.size\n",
    "output == tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test custom batchnorm and merging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2659,  0.6091,  1.4836,  0.9439,  1.2065, -0.9223, -0.5282, -0.8455,\n",
      "         -0.6554, -1.1303],\n",
      "        [-1.3342, -0.6483, -1.0222,  1.5021, -0.2206,  0.6714, -0.7772,  1.1717,\n",
      "          1.8188, -0.7320],\n",
      "        [-0.2430, -2.0209,  0.5107, -1.3762,  0.5122, -0.6174, -0.2816, -0.9368,\n",
      "          0.0110, -0.6685],\n",
      "        [ 1.0439,  0.5637,  0.0682,  0.5662, -0.7288, -0.8823,  1.3416,  0.7040,\n",
      "         -0.4979,  1.0843],\n",
      "        [-0.8410, -0.4107,  1.9964, -0.3908, -1.1501, -1.4927, -1.0785, -0.1600,\n",
      "         -0.2757, -2.0370]], grad_fn=<SelectBackward0>)\n",
      "------------------------------ unquantized: tensor([[-1.2628,  0.6076,  1.4799,  0.9416,  1.2035, -0.9200, -0.5269, -0.8434,\n",
      "         -0.6538, -1.1275],\n",
      "        [-1.3309, -0.6467, -1.0197,  1.4984, -0.2201,  0.6698, -0.7753,  1.1687,\n",
      "          1.8142, -0.7302],\n",
      "        [-0.2424, -2.0158,  0.5095, -1.3728,  0.5109, -0.6158, -0.2809, -0.9344,\n",
      "          0.0109, -0.6668],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8389, -0.4097,  1.9914, -0.3899, -1.1472, -1.4889, -1.0758, -0.1596,\n",
      "         -0.2750, -2.0318]])\n",
      "------------------------------ quantized: tensor([[-1.2582,  0.6056,  1.4749,  0.9384,  1.1995, -0.9166, -0.5249, -0.8402,\n",
      "         -0.6513, -1.1234],\n",
      "        [-1.3332, -0.6480, -1.0215,  1.5007, -0.2206,  0.6707, -0.7768,  1.1705,\n",
      "          1.8171, -0.7316],\n",
      "        [-0.2421, -2.0135,  0.5089, -1.3712,  0.5104, -0.6151, -0.2805, -0.9333,\n",
      "          0.0110, -0.6660],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8408, -0.4106,  1.9961, -0.3907, -1.1498, -1.4923, -1.0783, -0.1599,\n",
      "         -0.2755, -2.0365]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "import torch\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "normal_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "unquantized_custom_bn = CustomBatchNorm1d(sentence_length)\n",
    "#simulate some forward passes, without updating learnable scale and bias\n",
    "for _ in range(100):\n",
    "    normal_bn(embedding)\n",
    "#before merging, do nothing\n",
    "assert embedding.equal(unquantized_custom_bn(embedding))\n",
    "unquantized_custom_bn = replace_bn(normal_bn, unquantized_custom_bn)\n",
    "out_normal_bn = normal_bn(embedding)\n",
    "out_unquantized_custom_bn = unquantized_custom_bn(embedding)\n",
    "print(out_normal_bn[0])\n",
    "print(f'{30*\"-\"} unquantized: {out_unquantized_custom_bn[0]}')\n",
    "quantized_custom_bn = QuantBatchnorm1d(sentence_length)\n",
    "assert embedding.equal(quantized_custom_bn(embedding))\n",
    "#replace\n",
    "quantized_custom_bn = replace_bn(normal_bn, quantized_custom_bn)\n",
    "out_quantized_custom_bn = quantized_custom_bn(embedding)\n",
    "print(f'{30*\"-\"} quantized: {out_quantized_custom_bn[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x TransformerBlock(\n",
      "        (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (mha): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (active): ReLU()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n",
      "ModuleDict(\n",
      "  (wte): Embedding(50304, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (mha): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (active): ReLU()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Embedding(50304, 768)\n",
      "Embedding(1024, 768)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "ModuleList(\n",
      "  (0-11): 12 x TransformerBlock(\n",
      "    (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attn): CausalSelfAttention(\n",
      "      (mha): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (active): ReLU()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=768, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for mn, module in gpt.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4834, -0.1027, -2.2402,  0.4352,  0.2077,  1.0181,  0.6446, -0.6509,\n",
      "          0.9757,  0.8973, -0.7394,  0.0084,  1.1972, -1.3001,  0.8528,  0.6930,\n",
      "          1.2027, -0.3247,  0.2913, -0.3543, -1.0235,  1.0952,  0.9958, -0.4298,\n",
      "          0.6690, -0.7751,  1.2495,  1.1023, -0.6781,  0.4847, -1.4741, -0.9532]])\n",
      "------------------------------\n",
      "tensor([[-0.4549, -0.9629, -2.8159, -0.4967, -0.6939,  0.0086, -0.3151, -1.4382,\n",
      "         -0.0281, -0.0961, -1.5149, -0.8666,  0.1639, -2.0010, -0.1346, -0.2732,\n",
      "          0.1687, -1.1554, -0.6214, -1.1810, -1.7612,  0.0755, -0.0107, -1.2465,\n",
      "         -0.2940, -1.5459,  0.2093,  0.0817, -1.4618, -0.4537, -2.1518, -1.7002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.quantization import quant_bn\n",
    "#one word with 32 embeddings\n",
    "input = torch.randn(1,32)\n",
    "#context is max. of 8 words\n",
    "weight, bias = torch.randn(2, 8)\n",
    "output = quant_bn.custom_bn1d(input, weight, bias)\n",
    "#input and output are the same, why?\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug custom_bn1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_shapes(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Checks if a tensor is of shape [C], [N,C] or [C,N] with N = 1 and C >= 1.\n",
    "    If tensor is of a different shape, a ValueError will be thrown.\n",
    "    The returning tensor will be of shape [C, 1].\n",
    "    \"\"\"\n",
    "    shape_tensor = tensor.size()\n",
    "    if len(shape_tensor) == 1:\n",
    "        tensor = tensor[:,None]\n",
    "    if len(shape_tensor) == 2:\n",
    "        if shape_tensor[0] > 1 and shape_tensor[1] > 1:\n",
    "            raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "        elif shape_tensor[0] == 1 and shape_tensor[1] > 1:\n",
    "            tensor = tensor.transpose(0,1)\n",
    "    elif len(shape_tensor) > 2:\n",
    "        raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def custom_bn1d(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass of custom BatchNorm implementation. It expects a Tensor x of size [N,C] or [N,C,L]\n",
    "    and both a weight and bias Tensor, each of size [C, 1] or of size [1,C] / [C].\n",
    "    Each row/ embedding of a sentence (dimension C) will be multiplied with one value from the index of the corresponding\n",
    "    weight tensor and added with the value of the bias tensor.\n",
    "\n",
    "    Output: tensor of shape [N,C] or [N,C,L], basically of the same size as the input tensor.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor) :\n",
    "        raise TypeError('Input is not a tensor')\n",
    "    elif not isinstance(weight, torch.Tensor):\n",
    "        raise TypeError('Weight is not a tensor')\n",
    "    elif not isinstance(bias, torch.Tensor):\n",
    "        raise TypeError('Bias is not a Tensor')\n",
    "    #make sure that weights and biases are of shape [C,1]\n",
    "    print(weight)\n",
    "    print(10*\"#\")\n",
    "    weight = check_shapes(weight)\n",
    "    bias = check_shapes(bias)\n",
    "    C_x = x.size()[0] if len(x.size()) == 2 else x.size()[1]\n",
    "    print(f'ok: {weight[:C_x]}')\n",
    "    out = x * weight[:C_x] + bias[:C_x]\n",
    "    #only return the first C_x rows of output tensor\n",
    "    return out[:,None:C_x] if len(x.size()) == 3 else out[:C_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "tensor([ 0,  8,  9, 14, 13,  3, 14,  9, 11,  8, 10,  6, 17, 15,  9,  3])\n",
      "tensor([[ 0, 16,  3,  2,  9, 19, 13,  7],\n",
      "        [10,  4,  1, 18, 13,  7,  8,  5],\n",
      "        [17, 19,  3,  5, 12, 19, 18, 11]])\n",
      "------------------------------\n",
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "##########\n",
      "ok: tensor([[ 6],\n",
      "        [10],\n",
      "        [10]])\n",
      "tensor([[  0,  96,  18,  12,  54, 114,  78,  42],\n",
      "        [108,  48,  18, 188, 138,  78,  88,  58],\n",
      "        [179, 199,  39,  59, 129, 199, 189, 119]])\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.randint(20, (16,))\n",
    "bias = torch.randint(20, (16,))\n",
    "input = torch.randint(20, (3,8))\n",
    "print(weight)\n",
    "print(bias)\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "out = custom_bn1d(input,weight,bias)\n",
    "print(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(3,3,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17],\n",
       "        [18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(9,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:,None:2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(input.size())\n",
    "output = custom_bn1d(input, weight, bias)\n",
    "custom_bn1d(torch.randn(3,1,32), weight, bias)\n",
    "output.size()\n",
    "assert output.equal(input * weight[0] + bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.4805e-10, 6.3011e-10, 6.6376e-07, 6.7212e-04, 1.7340e-07,\n",
       "          1.6594e-07, 6.4097e-10, 1.4580e-19, 1.1495e+24, 3.0956e-18,\n",
       "          5.8981e-10, 3.2506e+21, 1.0528e-11, 2.7625e-06, 6.4103e-10,\n",
       "          2.1744e+23, 1.2794e+22, 2.1574e-04, 3.3980e+21, 3.0818e-18,\n",
       "          3.1360e+27, 7.0800e+31, 3.1095e-18, 4.7851e+22, 2.8826e+32,\n",
       "          4.4248e+30, 7.2442e+22, 2.3086e-12, 7.1760e+22, 7.2250e+28,\n",
       "          1.5766e-19, 2.7447e-06]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.8091e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00, 9.1844e-41,\n",
       "          1.1551e-40, 4.5919e-41, 8.2957e-43, 2.9147e-43, 0.0000e+00,\n",
       "          6.7262e-44, 0.0000e+00]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((tensor,torch.Tensor(1,1,32)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MergeBatchNorm class from brevitas.graph.quantize.preprocess_for_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "import torch\n",
    "\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1150\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:140\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb)\u001b[38;5;66;03m# + pos_emb)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:184\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[0;32m--> 184\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    185\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:40\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#dirty workaround to avoid runtimeerrors by adding a padding if the input is smaller than the feature length\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#padding does not artificially lower mean as normalization is performed along the word embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     n,c,l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m#input tensor should always be three dimensional\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m-\u001b[39m c, l)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:437\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:300\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "#initialization of normalization layer dependent on whether batchnorm or layernorm is used\n",
    "#\n",
    "torch.fx.symbolic_trace(gpt, concrete_args= {'idx': torch.Tensor(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tracer' object has no attribute 'unpack_arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getLogger\n\u001b[1;32m      3\u001b[0m log \u001b[38;5;241m=\u001b[39m getLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m other_model \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_for_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixed_point\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MergeBatchNorm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model needs a graph attribute from torch.fx.symbolic_trace\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/quantize.py:272\u001b[0m, in \u001b[0;36mpreprocess_for_quantize\u001b[0;34m(model, trace_model, relu6_to_relu, equalize_iters, equalize_merge_bias, merge_bn, equalize_bias_shrinkage, equalize_scale_computation)\u001b[0m\n\u001b[1;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_model:\n\u001b[0;32m--> 272\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchFunctionalToModule()\u001b[38;5;241m.\u001b[39mapply(model)\n\u001b[1;32m    274\u001b[0m model \u001b[38;5;241m=\u001b[39m DuplicateSharedStatelessModule()\u001b[38;5;241m.\u001b[39mapply(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:133\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    131\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#assert block_size <= self.config.block_size, f\"Cannot forward sequence of length {block_size}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    136\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:53\u001b[0m, in \u001b[0;36m_gen_torch_fn_patches.<locals>.new_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tracers\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m orig_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_arg\u001b[49m(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer\u001b[38;5;241m.\u001b[39munpack_arg(kwargs))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnsetValueException:\n\u001b[1;32m     55\u001b[0m     value \u001b[38;5;241m=\u001b[39m _UNSET\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tracer' object has no attribute 'unpack_arg'"
     ]
    }
   ],
   "source": [
    "#preprocess_for_quantize needs access to a graph representation of the model\n",
    "from logging import getLogger\n",
    "log = getLogger(__name__)\n",
    "other_model = preprocess_for_quantize(gpt)\n",
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "#model needs a graph attribute from torch.fx.symbolic_trace\n",
    "#the purpose of that probably is the same as in https://github.com/pytorch/examples/blob/main/fx/replace_op.py\n",
    "\"\"\"\n",
    "it seems that control flow depending on arguments leads to this error\n",
    "https://discuss.tvm.apache.org/t/torch-fx-symbolic-trace-fails-for-most-encoder-decoder-nlp-models/16004\n",
    "\"\"\"\n",
    "try:\n",
    "    MergeBatchNorm().apply(gpt)\n",
    "except Exception as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'dummy_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;66;03m# is being used with transformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/utils/fx.py:1241\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(model, input_names, disable_check, tracer_cls)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03mPerforms symbolic tracing on the model.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     input_names \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1243\u001b[0m input_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_names)\n\u001b[1;32m   1244\u001b[0m concrete_args \u001b[38;5;241m=\u001b[39m get_concrete_args(model, input_names)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'dummy_inputs'"
     ]
    }
   ],
   "source": [
    "from transformers.utils.fx import symbolic_trace # is being used with transformers\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MinstConv(nn.Module):\n",
    "    def __init__(self, param = 10):\n",
    "        super(MinstConv, self).__init__()\n",
    "        #each model needs nn.module for quantization to work\n",
    "        self.model = nn.ModuleDict(dict(\n",
    "            conv1 = nn.Conv2d(1, 32, 3, 1),\n",
    "            relu1 = nn.ReLU(),\n",
    "            conv2 = nn.Conv2d(32, 64, 3, 1),\n",
    "            relu2 = nn.ReLU(),\n",
    "            maxpool2d = nn.MaxPool2d(kernel_size=2),\n",
    "            dropout1 = nn.Dropout(0.25),\n",
    "            flatten = nn.Flatten(),\n",
    "            fc1 = nn.Linear(9216, 128),\n",
    "            relu3 = nn.ReLU(),\n",
    "            dropout2 = nn.Dropout(0.5),\n",
    "            fc2 = nn.Linear(128, 10)\n",
    "        ))\n",
    "        #check symbolic traceability\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        #no exception\n",
    "        assert self.param > 0\n",
    "        #exception, meaning param checking during forward pass not possible\n",
    "        assert x.size()[-1] > 0\n",
    "        output = x\n",
    "        for layer_name, layer in self.model.items():\n",
    "            output = layer(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinstConv(\n",
       "  (model): Module(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout1): Dropout(p=0.25, inplace=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (relu3): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fx.symbolic_trace(MinstConv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:130\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    128\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot forward sequence of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, block size is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, t, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:495\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:352\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "from brevitas.fx.brevitas_tracer import symbolic_trace\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.fx.proxy.Proxy'>\n",
      "Proxy(size)\n",
      "Proxy(size_1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(l, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m traced \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1109\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1109\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1111\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1112\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m l \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fx as fx\n",
    "\"\"\"\n",
    "using values from param leads to errors\n",
    "\"\"\"\n",
    "try:\n",
    "    fx.symbolic_trace(model)\n",
    "except:\n",
    "    pass\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "def test(x):\n",
    "    print(type(x))\n",
    "    print(x.size(1))\n",
    "    l = x.size(1)\n",
    "    print(l)\n",
    "    return torch.arange(l, dtype=torch.long, device='cuda')\n",
    "traced = symbolic_trace(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' (position 1) must be tuple of ints, but found element of type Proxy at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSimpleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_for_quantize\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m preprocess_for_quantize(model, trace_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1109\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1109\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1111\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1112\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#param checking fails for symbolic tracing, unless modules are leaf modules which are not traced\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#assert isinstance(x,torch.Tensor), 'x is not a tensor'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#assert x.size()[-1] < 10, f'Size of input tensor {x.size()} not compatible with size {self.size}'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#custom modules for operations only necessary when param checking is performed\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mMultiplyModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 20\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mTypeError\u001b[0m: randn(): argument 'size' (position 1) must be tuple of ints, but found element of type Proxy at pos 0"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, size: int = 10):\n",
    "        super().__init__()\n",
    "        #attributes can be param checked during symbolic tracing\n",
    "        self.size = size\n",
    "        self.mul = MultiplyModule()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #param checking fails for symbolic tracing, unless modules are leaf modules which are not traced\n",
    "        #assert isinstance(x,torch.Tensor), 'x is not a tensor'\n",
    "        #assert x.size()[-1] < 10, f'Size of input tensor {x.size()} not compatible with size {self.size}'\n",
    "        #custom modules for operations only necessary when param checking is performed\n",
    "        return self.mul.forward(x)\n",
    "    \n",
    "class MultiplyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        #difficult to process with proxies\n",
    "        size = x.size()\n",
    "        y = torch.randn(size)\n",
    "        y = x + y\n",
    "        return y\n",
    "        \n",
    "model = torch.fx.symbolic_trace(SimpleModel())\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "model = preprocess_for_quantize(model, trace_model = False)\n",
    "#forward pass works for model after preprocess_model_for_quantize, but not for gpt\n",
    "assert model(10) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, size: int = 10):\n",
    "        super().__init__()\n",
    "        #attributes can be param checked during symbolic tracing\n",
    "        self.size = size\n",
    "        self.mul = MultiplyModule()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #param checking fails for symbolic tracing, unless modules are leaf modules which are not traced\n",
    "        #assert isinstance(x,torch.Tensor), 'x is not a tensor'\n",
    "        #assert x.size()[-1] < 10, f'Size of input tensor {x.size()} not compatible with size {self.size}'\n",
    "        #custom modules for operations only necessary when param checking is performed\n",
    "        return self.mul.forward(x)\n",
    "    \n",
    "class MultiplyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        #difficult to process with proxies\n",
    "        size = x.size()\n",
    "        y = torch.randn(size)\n",
    "        y = x + y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' (position 1) must be tuple of ints, but found element of type Proxy at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultiplyModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mMultiplyModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#difficult to process with proxies\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 21\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mTypeError\u001b[0m: randn(): argument 'size' (position 1) must be tuple of ints, but found element of type Proxy at pos 0"
     ]
    }
   ],
   "source": [
    "out = torch.fx.Tracer().trace(MultiplyModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleModel()]\n",
      "[SimpleModel(\n",
      "  (mul): MultiplyModule()\n",
      "), MultiplyModule()]\n"
     ]
    }
   ],
   "source": [
    "#submodules disappear form module list after preprocess_for_quantize\n",
    "print(list(model.modules()))\n",
    "print(list(SimpleModel().modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    size = x.size(1)\n",
      "    arange = self.arange(size);  size = None\n",
      "    getattr_1 = x.device;  x = None\n",
      "    to = arange.to(dtype = torch.int64, device = getattr_1);  arange = getattr_1 = None\n",
      "    return to\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/pytorch/pytorch/issues/51803#issuecomment-1104634592\n",
    "#experiments to make fx graphing work with transformers\n",
    "import torch\n",
    "\n",
    "from torch.fx import Tracer\n",
    "from torch.fx import symbolic_trace\n",
    "from torch.fx.graph_module import GraphModule\n",
    "\n",
    "\n",
    "class CustomedTracer(Tracer):\n",
    "    \"\"\"\n",
    "    ``Tracer`` is the class that implements the symbolic tracing functionality\n",
    "    of ``torch.fx.symbolic_trace``. A call to ``symbolic_trace(m)`` is equivalent\n",
    "    to ``Tracer().trace(m)``.\n",
    "    This Tracer override the ``is_leaf_module`` function to make symbolic trace\n",
    "    right in some cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, customed_leaf_module=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.customed_leaf_module = customed_leaf_module\n",
    "\n",
    "    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name : str) -> bool:\n",
    "        \"\"\"\n",
    "        A method to specify whether a given ``nn.Module`` is a \"leaf\" module.\n",
    "        Leaf modules are the atomic units that appear in\n",
    "        the IR, referenced by ``call_module`` calls. By default,\n",
    "        Modules in the PyTorch standard library namespace (torch.nn)\n",
    "        are leaf modules. All other modules are traced through and\n",
    "        their constituent ops are recorded, unless specified otherwise\n",
    "        via this parameter.\n",
    "        Args:\n",
    "            m (Module): The module being queried about\n",
    "            module_qualified_name (str): The path to root of this module. For example,\n",
    "                if you have a module hierarchy where submodule ``foo`` contains\n",
    "                submodule ``bar``, which contains submodule ``baz``, that module will\n",
    "                appear with the qualified name ``foo.bar.baz`` here.\n",
    "        \"\"\"\n",
    "        if self.customed_leaf_module and isinstance(m, self.customed_leaf_module):\n",
    "            return True\n",
    "        return m.__module__.startswith('torch.nn') and not isinstance(m, torch.nn.Sequential)\n",
    "\n",
    "\n",
    "\n",
    "class ArangeForFx(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.arange(x)\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.arange = ArangeForFx()\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = x.size(1)\n",
    "        return self.arange(l).to(dtype=torch.long, device=x.device)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "#all ops which need a param from a symbolically traced variable (from the function parameter) should be a leaf module\n",
    "#no idea why it fixes it though\n",
    "tracer = CustomedTracer(customed_leaf_module=(ArangeForFx,))\n",
    "graph = tracer.trace(model)\n",
    "#graph = symbolic_trace(model)\n",
    "name = model.__class__.__name__ if isinstance(model, torch.nn.Module) else model.__name__\n",
    "traced = GraphModule(tracer.root, graph, name)\n",
    "\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qtransform.model.gpt import GPT\n",
    "from qtransform.model import ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(ModelArgs(block_size=128, n_layer=2, n_head=4, n_embd=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (int, Proxy, device=Attribute, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1109\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1109\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1111\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1112\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:129\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    123\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# remove this line because it messed with fx.trace, do shape check before hand!\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#TODO: add padding for FINN support\u001b[39;00m\n\u001b[1;32m    133\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (int, Proxy, device=Attribute, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.fx.symbolic_trace(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tracer' object has no attribute 'unpack_arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m gpt\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mquantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_for_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m quantize\u001b[38;5;241m.\u001b[39mquantize(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/graph/quantize.py:276\u001b[0m, in \u001b[0;36mpreprocess_for_quantize\u001b[0;34m(model, trace_model, relu6_to_relu, equalize_iters, equalize_merge_bias, merge_bn, equalize_bias_shrinkage, equalize_scale_computation, channel_splitting_ratio, channel_splitting_split_input, channel_splitting_criterion)\u001b[0m\n\u001b[1;32m    273\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_model:\n\u001b[0;32m--> 276\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchFunctionalToModule()\u001b[38;5;241m.\u001b[39mapply(model)\n\u001b[1;32m    278\u001b[0m model \u001b[38;5;241m=\u001b[39m DuplicateSharedStatelessModule()\u001b[38;5;241m.\u001b[39mapply(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:129\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    123\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# remove this line because it messed with fx.trace, do shape check before hand!\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#TODO: add padding for FINN support\u001b[39;00m\n\u001b[1;32m    133\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:53\u001b[0m, in \u001b[0;36m_gen_torch_fn_patches.<locals>.new_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tracers\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m orig_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_arg\u001b[49m(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer\u001b[38;5;241m.\u001b[39munpack_arg(kwargs))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnsetValueException:\n\u001b[1;32m     55\u001b[0m     value \u001b[38;5;241m=\u001b[39m _UNSET\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tracer' object has no attribute 'unpack_arg'"
     ]
    }
   ],
   "source": [
    "from brevitas.graph import quantize\n",
    "model = gpt\n",
    "model = quantize.preprocess_for_quantize(model)\n",
    "model = quantize.quantize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tracer' object has no attribute 'unpack_arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_for_quantize\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpreprocess_for_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/graph/quantize.py:276\u001b[0m, in \u001b[0;36mpreprocess_for_quantize\u001b[0;34m(model, trace_model, relu6_to_relu, equalize_iters, equalize_merge_bias, merge_bn, equalize_bias_shrinkage, equalize_scale_computation, channel_splitting_ratio, channel_splitting_split_input, channel_splitting_criterion)\u001b[0m\n\u001b[1;32m    273\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_model:\n\u001b[0;32m--> 276\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchFunctionalToModule()\u001b[38;5;241m.\u001b[39mapply(model)\n\u001b[1;32m    278\u001b[0m model \u001b[38;5;241m=\u001b[39m DuplicateSharedStatelessModule()\u001b[38;5;241m.\u001b[39mapply(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:129\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    123\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# remove this line because it messed with fx.trace, do shape check before hand!\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#TODO: add padding for FINN support\u001b[39;00m\n\u001b[1;32m    133\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas-0.10.2-py3.10.egg/brevitas/fx/brevitas_tracer.py:53\u001b[0m, in \u001b[0;36m_gen_torch_fn_patches.<locals>.new_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tracers\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m orig_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_arg\u001b[49m(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer\u001b[38;5;241m.\u001b[39munpack_arg(kwargs))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnsetValueException:\n\u001b[1;32m     55\u001b[0m     value \u001b[38;5;241m=\u001b[39m _UNSET\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tracer' object has no attribute 'unpack_arg'"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "preprocess_for_quantize(gpt, merge_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "# Symbolic tracing frontend - captures the semantics of the module\n",
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('param', Parameter containing:\n",
       "               tensor([[0.1441, 0.3501, 0.0502, 0.8434],\n",
       "                       [0.8379, 0.2308, 0.6766, 0.2203],\n",
       "                       [0.4860, 0.5945, 0.5727, 0.0350]], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('linear',\n",
       "               Linear(in_features=4, out_features=5, bias=True))]),\n",
       " '_graph': <torch.fx.graph.Graph at 0x7fda8d3ce500>,\n",
       " '_code': '\\n\\n\\ndef forward(self, x):\\n    param = self.param\\n    add = x + param;  x = param = None\\n    linear = self.linear(add);  add = None\\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\\n    return clamp\\n    ',\n",
       " '_tracer_cls': torch.fx._symbolic_trace.Tracer,\n",
       " '_tracer_extras': {},\n",
       " 'meta': {}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolic_traced.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.fx.graph.Graph at 0x7fda8d3ce500>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolic_traced.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.model.modules import BatchNorm, LayerNorm\n",
    "#https://pytorch.org/docs/stable/fx.html#leaf-modules\n",
    "#leaf modules are not being traced through\n",
    "#could be problematic as the output of batch/layer norm are being passed into the attention and mlp layer\n",
    "tracer_gpt = CustomedTracer(customed_leaf_module=(BatchNorm,LayerNorm))\n",
    "from qtransform.model.gpt import GPT as qGPT, GPTConfig\n",
    "gpt = qGPT(GPTConfig())\n",
    "tokens = torch.randint(50304, (2, 1024))\n",
    "#assert statements should be nested inside of a custom module\n",
    "graph_gpt = tracer_gpt.trace(gpt, {\"idx\": tokens})\n",
    "name = gpt.__class__.__name__ if isinstance(gpt, torch.nn.Module) else gpt.__name__\n",
    "traced_gpt = GraphModule(tracer_gpt.root, graph_gpt, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preceding layer is merged with batchnorm layer\n",
    "#batchnorm, conv, linear and their quantized versions can be merged\n",
    "#if merging occurs before quantization, how can the learnable parameters from batchnorm be merged?\n",
    "#\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "processed_gpt = preprocess_for_quantize(traced_gpt, trace_model = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=1024, vocab_size=50304, n_layer=12, n_head=12, n_embd=768, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_symbolic_tracing = gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %idx_1 : [num_users=0] = placeholder[target=idx_1]\n",
      "    %targets : [num_users=1] = placeholder[target=targets](default=None)\n",
      "    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]\n",
      "    %transformer_wte : [num_users=1] = call_module[target=transformer.wte](args = (%_tensor_constant0,), kwargs = {})\n",
      "    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]\n",
      "    %transformer_wpe : [num_users=1] = call_module[target=transformer.wpe](args = (%_tensor_constant1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%transformer_wte, %transformer_wpe), kwargs = {})\n",
      "    %transformer_dropout : [num_users=2] = call_module[target=transformer.dropout](args = (%add,), kwargs = {})\n",
      "    %transformer_layer_0_ln_1 : [num_users=1] = call_module[target=transformer.layer.0.ln_1](args = (%transformer_dropout,), kwargs = {})\n",
      "    %transformer_layer_0_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.0.attn.attn_mask]\n",
      "    %transformer_layer_0_attn_mha : [num_users=2] = call_module[target=transformer.layer.0.attn.mha](args = (%transformer_layer_0_ln_1, %transformer_layer_0_ln_1, %transformer_layer_0_ln_1), kwargs = {attn_mask: %transformer_layer_0_attn_attn_mask, need_weights: False})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 0), kwargs = {})\n",
      "    %getitem_1 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 1), kwargs = {})\n",
      "    %add_1 : [num_users=2] = call_function[target=operator.add](args = (%transformer_dropout, %getitem), kwargs = {})\n",
      "    %transformer_layer_0_ln_2 : [num_users=1] = call_module[target=transformer.layer.0.ln_2](args = (%add_1,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_fc](args = (%transformer_layer_0_ln_2,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_active : [num_users=1] = call_module[target=transformer.layer.0.mlp.active](args = (%transformer_layer_0_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_proj](args = (%transformer_layer_0_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.0.mlp.dropout](args = (%transformer_layer_0_mlp_c_proj,), kwargs = {})\n",
      "    %add_2 : [num_users=2] = call_function[target=operator.add](args = (%add_1, %transformer_layer_0_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_1_ln_1 : [num_users=1] = call_module[target=transformer.layer.1.ln_1](args = (%add_2,), kwargs = {})\n",
      "    %transformer_layer_1_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.1.attn.attn_mask]\n",
      "    %transformer_layer_1_attn_mha : [num_users=2] = call_module[target=transformer.layer.1.attn.mha](args = (%transformer_layer_1_ln_1, %transformer_layer_1_ln_1, %transformer_layer_1_ln_1), kwargs = {attn_mask: %transformer_layer_1_attn_attn_mask, need_weights: False})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 0), kwargs = {})\n",
      "    %getitem_3 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 1), kwargs = {})\n",
      "    %add_3 : [num_users=2] = call_function[target=operator.add](args = (%add_2, %getitem_2), kwargs = {})\n",
      "    %transformer_layer_1_ln_2 : [num_users=1] = call_module[target=transformer.layer.1.ln_2](args = (%add_3,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_fc](args = (%transformer_layer_1_ln_2,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_active : [num_users=1] = call_module[target=transformer.layer.1.mlp.active](args = (%transformer_layer_1_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_proj](args = (%transformer_layer_1_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.1.mlp.dropout](args = (%transformer_layer_1_mlp_c_proj,), kwargs = {})\n",
      "    %add_4 : [num_users=2] = call_function[target=operator.add](args = (%add_3, %transformer_layer_1_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_2_ln_1 : [num_users=1] = call_module[target=transformer.layer.2.ln_1](args = (%add_4,), kwargs = {})\n",
      "    %transformer_layer_2_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.2.attn.attn_mask]\n",
      "    %transformer_layer_2_attn_mha : [num_users=2] = call_module[target=transformer.layer.2.attn.mha](args = (%transformer_layer_2_ln_1, %transformer_layer_2_ln_1, %transformer_layer_2_ln_1), kwargs = {attn_mask: %transformer_layer_2_attn_attn_mask, need_weights: False})\n",
      "    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 1), kwargs = {})\n",
      "    %add_5 : [num_users=2] = call_function[target=operator.add](args = (%add_4, %getitem_4), kwargs = {})\n",
      "    %transformer_layer_2_ln_2 : [num_users=1] = call_module[target=transformer.layer.2.ln_2](args = (%add_5,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_fc](args = (%transformer_layer_2_ln_2,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_active : [num_users=1] = call_module[target=transformer.layer.2.mlp.active](args = (%transformer_layer_2_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_proj](args = (%transformer_layer_2_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.2.mlp.dropout](args = (%transformer_layer_2_mlp_c_proj,), kwargs = {})\n",
      "    %add_6 : [num_users=2] = call_function[target=operator.add](args = (%add_5, %transformer_layer_2_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_3_ln_1 : [num_users=1] = call_module[target=transformer.layer.3.ln_1](args = (%add_6,), kwargs = {})\n",
      "    %transformer_layer_3_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.3.attn.attn_mask]\n",
      "    %transformer_layer_3_attn_mha : [num_users=2] = call_module[target=transformer.layer.3.attn.mha](args = (%transformer_layer_3_ln_1, %transformer_layer_3_ln_1, %transformer_layer_3_ln_1), kwargs = {attn_mask: %transformer_layer_3_attn_attn_mask, need_weights: False})\n",
      "    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 0), kwargs = {})\n",
      "    %getitem_7 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 1), kwargs = {})\n",
      "    %add_7 : [num_users=2] = call_function[target=operator.add](args = (%add_6, %getitem_6), kwargs = {})\n",
      "    %transformer_layer_3_ln_2 : [num_users=1] = call_module[target=transformer.layer.3.ln_2](args = (%add_7,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_fc](args = (%transformer_layer_3_ln_2,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_active : [num_users=1] = call_module[target=transformer.layer.3.mlp.active](args = (%transformer_layer_3_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_proj](args = (%transformer_layer_3_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.3.mlp.dropout](args = (%transformer_layer_3_mlp_c_proj,), kwargs = {})\n",
      "    %add_8 : [num_users=2] = call_function[target=operator.add](args = (%add_7, %transformer_layer_3_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_4_ln_1 : [num_users=1] = call_module[target=transformer.layer.4.ln_1](args = (%add_8,), kwargs = {})\n",
      "    %transformer_layer_4_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.4.attn.attn_mask]\n",
      "    %transformer_layer_4_attn_mha : [num_users=2] = call_module[target=transformer.layer.4.attn.mha](args = (%transformer_layer_4_ln_1, %transformer_layer_4_ln_1, %transformer_layer_4_ln_1), kwargs = {attn_mask: %transformer_layer_4_attn_attn_mask, need_weights: False})\n",
      "    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 0), kwargs = {})\n",
      "    %getitem_9 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 1), kwargs = {})\n",
      "    %add_9 : [num_users=2] = call_function[target=operator.add](args = (%add_8, %getitem_8), kwargs = {})\n",
      "    %transformer_layer_4_ln_2 : [num_users=1] = call_module[target=transformer.layer.4.ln_2](args = (%add_9,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_fc](args = (%transformer_layer_4_ln_2,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_active : [num_users=1] = call_module[target=transformer.layer.4.mlp.active](args = (%transformer_layer_4_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_proj](args = (%transformer_layer_4_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.4.mlp.dropout](args = (%transformer_layer_4_mlp_c_proj,), kwargs = {})\n",
      "    %add_10 : [num_users=2] = call_function[target=operator.add](args = (%add_9, %transformer_layer_4_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_5_ln_1 : [num_users=1] = call_module[target=transformer.layer.5.ln_1](args = (%add_10,), kwargs = {})\n",
      "    %transformer_layer_5_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.5.attn.attn_mask]\n",
      "    %transformer_layer_5_attn_mha : [num_users=2] = call_module[target=transformer.layer.5.attn.mha](args = (%transformer_layer_5_ln_1, %transformer_layer_5_ln_1, %transformer_layer_5_ln_1), kwargs = {attn_mask: %transformer_layer_5_attn_attn_mask, need_weights: False})\n",
      "    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 0), kwargs = {})\n",
      "    %getitem_11 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 1), kwargs = {})\n",
      "    %add_11 : [num_users=2] = call_function[target=operator.add](args = (%add_10, %getitem_10), kwargs = {})\n",
      "    %transformer_layer_5_ln_2 : [num_users=1] = call_module[target=transformer.layer.5.ln_2](args = (%add_11,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_fc](args = (%transformer_layer_5_ln_2,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_active : [num_users=1] = call_module[target=transformer.layer.5.mlp.active](args = (%transformer_layer_5_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_proj](args = (%transformer_layer_5_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.5.mlp.dropout](args = (%transformer_layer_5_mlp_c_proj,), kwargs = {})\n",
      "    %add_12 : [num_users=2] = call_function[target=operator.add](args = (%add_11, %transformer_layer_5_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_6_ln_1 : [num_users=1] = call_module[target=transformer.layer.6.ln_1](args = (%add_12,), kwargs = {})\n",
      "    %transformer_layer_6_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.6.attn.attn_mask]\n",
      "    %transformer_layer_6_attn_mha : [num_users=2] = call_module[target=transformer.layer.6.attn.mha](args = (%transformer_layer_6_ln_1, %transformer_layer_6_ln_1, %transformer_layer_6_ln_1), kwargs = {attn_mask: %transformer_layer_6_attn_attn_mask, need_weights: False})\n",
      "    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 0), kwargs = {})\n",
      "    %getitem_13 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 1), kwargs = {})\n",
      "    %add_13 : [num_users=2] = call_function[target=operator.add](args = (%add_12, %getitem_12), kwargs = {})\n",
      "    %transformer_layer_6_ln_2 : [num_users=1] = call_module[target=transformer.layer.6.ln_2](args = (%add_13,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_fc](args = (%transformer_layer_6_ln_2,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_active : [num_users=1] = call_module[target=transformer.layer.6.mlp.active](args = (%transformer_layer_6_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_proj](args = (%transformer_layer_6_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.6.mlp.dropout](args = (%transformer_layer_6_mlp_c_proj,), kwargs = {})\n",
      "    %add_14 : [num_users=2] = call_function[target=operator.add](args = (%add_13, %transformer_layer_6_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_7_ln_1 : [num_users=1] = call_module[target=transformer.layer.7.ln_1](args = (%add_14,), kwargs = {})\n",
      "    %transformer_layer_7_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.7.attn.attn_mask]\n",
      "    %transformer_layer_7_attn_mha : [num_users=2] = call_module[target=transformer.layer.7.attn.mha](args = (%transformer_layer_7_ln_1, %transformer_layer_7_ln_1, %transformer_layer_7_ln_1), kwargs = {attn_mask: %transformer_layer_7_attn_attn_mask, need_weights: False})\n",
      "    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 0), kwargs = {})\n",
      "    %getitem_15 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 1), kwargs = {})\n",
      "    %add_15 : [num_users=2] = call_function[target=operator.add](args = (%add_14, %getitem_14), kwargs = {})\n",
      "    %transformer_layer_7_ln_2 : [num_users=1] = call_module[target=transformer.layer.7.ln_2](args = (%add_15,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_fc](args = (%transformer_layer_7_ln_2,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_active : [num_users=1] = call_module[target=transformer.layer.7.mlp.active](args = (%transformer_layer_7_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_proj](args = (%transformer_layer_7_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.7.mlp.dropout](args = (%transformer_layer_7_mlp_c_proj,), kwargs = {})\n",
      "    %add_16 : [num_users=2] = call_function[target=operator.add](args = (%add_15, %transformer_layer_7_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_8_ln_1 : [num_users=1] = call_module[target=transformer.layer.8.ln_1](args = (%add_16,), kwargs = {})\n",
      "    %transformer_layer_8_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.8.attn.attn_mask]\n",
      "    %transformer_layer_8_attn_mha : [num_users=2] = call_module[target=transformer.layer.8.attn.mha](args = (%transformer_layer_8_ln_1, %transformer_layer_8_ln_1, %transformer_layer_8_ln_1), kwargs = {attn_mask: %transformer_layer_8_attn_attn_mask, need_weights: False})\n",
      "    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 0), kwargs = {})\n",
      "    %getitem_17 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 1), kwargs = {})\n",
      "    %add_17 : [num_users=2] = call_function[target=operator.add](args = (%add_16, %getitem_16), kwargs = {})\n",
      "    %transformer_layer_8_ln_2 : [num_users=1] = call_module[target=transformer.layer.8.ln_2](args = (%add_17,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_fc](args = (%transformer_layer_8_ln_2,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_active : [num_users=1] = call_module[target=transformer.layer.8.mlp.active](args = (%transformer_layer_8_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_proj](args = (%transformer_layer_8_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.8.mlp.dropout](args = (%transformer_layer_8_mlp_c_proj,), kwargs = {})\n",
      "    %add_18 : [num_users=2] = call_function[target=operator.add](args = (%add_17, %transformer_layer_8_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_9_ln_1 : [num_users=1] = call_module[target=transformer.layer.9.ln_1](args = (%add_18,), kwargs = {})\n",
      "    %transformer_layer_9_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.9.attn.attn_mask]\n",
      "    %transformer_layer_9_attn_mha : [num_users=2] = call_module[target=transformer.layer.9.attn.mha](args = (%transformer_layer_9_ln_1, %transformer_layer_9_ln_1, %transformer_layer_9_ln_1), kwargs = {attn_mask: %transformer_layer_9_attn_attn_mask, need_weights: False})\n",
      "    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 0), kwargs = {})\n",
      "    %getitem_19 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 1), kwargs = {})\n",
      "    %add_19 : [num_users=2] = call_function[target=operator.add](args = (%add_18, %getitem_18), kwargs = {})\n",
      "    %transformer_layer_9_ln_2 : [num_users=1] = call_module[target=transformer.layer.9.ln_2](args = (%add_19,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_fc](args = (%transformer_layer_9_ln_2,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_active : [num_users=1] = call_module[target=transformer.layer.9.mlp.active](args = (%transformer_layer_9_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_proj](args = (%transformer_layer_9_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.9.mlp.dropout](args = (%transformer_layer_9_mlp_c_proj,), kwargs = {})\n",
      "    %add_20 : [num_users=2] = call_function[target=operator.add](args = (%add_19, %transformer_layer_9_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_10_ln_1 : [num_users=1] = call_module[target=transformer.layer.10.ln_1](args = (%add_20,), kwargs = {})\n",
      "    %transformer_layer_10_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.10.attn.attn_mask]\n",
      "    %transformer_layer_10_attn_mha : [num_users=2] = call_module[target=transformer.layer.10.attn.mha](args = (%transformer_layer_10_ln_1, %transformer_layer_10_ln_1, %transformer_layer_10_ln_1), kwargs = {attn_mask: %transformer_layer_10_attn_attn_mask, need_weights: False})\n",
      "    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 0), kwargs = {})\n",
      "    %getitem_21 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 1), kwargs = {})\n",
      "    %add_21 : [num_users=2] = call_function[target=operator.add](args = (%add_20, %getitem_20), kwargs = {})\n",
      "    %transformer_layer_10_ln_2 : [num_users=1] = call_module[target=transformer.layer.10.ln_2](args = (%add_21,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_fc](args = (%transformer_layer_10_ln_2,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_active : [num_users=1] = call_module[target=transformer.layer.10.mlp.active](args = (%transformer_layer_10_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_proj](args = (%transformer_layer_10_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.10.mlp.dropout](args = (%transformer_layer_10_mlp_c_proj,), kwargs = {})\n",
      "    %add_22 : [num_users=2] = call_function[target=operator.add](args = (%add_21, %transformer_layer_10_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_11_ln_1 : [num_users=1] = call_module[target=transformer.layer.11.ln_1](args = (%add_22,), kwargs = {})\n",
      "    %transformer_layer_11_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.11.attn.attn_mask]\n",
      "    %transformer_layer_11_attn_mha : [num_users=2] = call_module[target=transformer.layer.11.attn.mha](args = (%transformer_layer_11_ln_1, %transformer_layer_11_ln_1, %transformer_layer_11_ln_1), kwargs = {attn_mask: %transformer_layer_11_attn_attn_mask, need_weights: False})\n",
      "    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 0), kwargs = {})\n",
      "    %getitem_23 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 1), kwargs = {})\n",
      "    %add_23 : [num_users=2] = call_function[target=operator.add](args = (%add_22, %getitem_22), kwargs = {})\n",
      "    %transformer_layer_11_ln_2 : [num_users=1] = call_module[target=transformer.layer.11.ln_2](args = (%add_23,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_fc](args = (%transformer_layer_11_ln_2,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_active : [num_users=1] = call_module[target=transformer.layer.11.mlp.active](args = (%transformer_layer_11_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_proj](args = (%transformer_layer_11_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.11.mlp.dropout](args = (%transformer_layer_11_mlp_c_proj,), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=operator.add](args = (%add_23, %transformer_layer_11_mlp_dropout), kwargs = {})\n",
      "    %transformer_ln_out : [num_users=1] = call_module[target=transformer.ln_out](args = (%add_24,), kwargs = {})\n",
      "    %linear_out : [num_users=3] = call_module[target=linear_out](args = (%transformer_ln_out,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%linear_out, -1), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%linear_out, -1, %size), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_method[target=view](args = (%targets, -1), kwargs = {})\n",
      "    %cross_entropy : [num_users=1] = call_function[target=torch.nn.functional.cross_entropy](args = (%view, %view_1), kwargs = {weight: None, size_average: None, ignore_index: -1, reduce: None, reduction: mean, label_smoothing: 0.0})\n",
      "    return (linear_out, cross_entropy)\n"
     ]
    }
   ],
   "source": [
    "print(processed_gpt.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 274, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"<eval_with_key>.21 from /home/mabot004/eki-transformer-dev/qtransform/qtransform/model/gpt.py:126 in forward\", line 159, in forward\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "AttributeError: 'NoneType' object has no attribute 'view'\n",
      "\n",
      "Call using an FX-traced Module, line 159 of the traced Module's generated forward function:\n",
      "    view = linear_out.view(-1, size);  size = None\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    cross_entropy = torch.nn.functional.cross_entropy(view, view_1, weight = None, size_average = None, ignore_index = -1, reduce = None, reduction = 'mean', label_smoothing = 0.0);  view = view_1 = None\n",
      "\n",
      "    return (linear_out, cross_entropy)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocessed_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:678\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:282\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_with_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m topmost_framesummary\u001b[38;5;241m.\u001b[39mfilename:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_WrappedCall\u001b[38;5;241m.\u001b[39m_generate_error_message(topmost_framesummary),\n\u001b[1;32m    281\u001b[0m           file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \n",
    "processed_gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mini transformer test\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm1d(64)\n",
    "        self.linear = torch.nn.Linear(256,256)\n",
    "        self.mha = torch.nn.MultiheadAttention(256, 2, batch_first = True)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        x, = self.mha(x,x,x, need_weights = False)\n",
    "        return self.softmax(x)\n",
    "#maybe the order of layers being called in the forward pass\n",
    "nodes = list(torch.fx.symbolic_trace(Layer()).graph.nodes)\n",
    "#nodes[1].args.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;28;01mas\u001b[39;00m brevitas_symbolic_trace\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m brevitas_symbolic_trace(Layer())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mMergeBatchNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/base.py:64\u001b[0m, in \u001b[0;36mUntilFixedPointGraphTransform.apply\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_model: GraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_converged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_model\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_model\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/fixed_point.py:112\u001b[0m, in \u001b[0;36mMergeBatchNorm.is_converged\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m layer \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[0;32m--> 112\u001b[0m bn \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#merging happens here\u001b[39;00m\n\u001b[1;32m    115\u001b[0m merge_bn(layer, bn, get_output_channel_dim(layer))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/nn/utils.py:37\u001b[0m, in \u001b[0;36mmerge_bn\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     35\u001b[0m mul_factor, add_factor \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     36\u001b[0m out_ch_weight_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mweight, output_channel_dim)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmul_factor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ch_weight_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     out_ch_bias_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mbias, channel_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "from brevitas.fx.brevitas_tracer import symbolic_trace as brevitas_symbolic_trace\n",
    "model = brevitas_symbolic_trace(Layer())\n",
    "MergeBatchNorm().apply(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.graph.utils import matches_module_pattern\n",
    "def is_converged(graph_model: GraphModule):\n",
    "        named_modules = dict(graph_model.named_modules())\n",
    "        for node in graph_model.graph.nodes:\n",
    "            for pattern in MergeBatchNorm.DEFAULT_PATTERNS:\n",
    "                if matches_module_pattern(pattern, node, named_modules):\n",
    "                    #potential error since node.args is a list containing tuples\n",
    "                    if len(node.args[0].users) > 1:\n",
    "                        continue\n",
    "                    layer = named_modules[node.args[0].target]\n",
    "                    bn = named_modules[node.target]\n",
    "                    #!!!! check if batchnorm is merged\n",
    "                    print(f'{layer}\\n{bn}')\n",
    "                    return -1\n",
    "                    #merging happens here\n",
    "                    merge_bn(layer, bn, get_output_channel_dim(layer))\n",
    "                    \n",
    "                    \n",
    "                    node.replace_all_uses_with(node.args[0])\n",
    "                    graph_model.graph.erase_node(node)\n",
    "                    del_module(graph_model, node.target)\n",
    "        graph_model.recompile()\n",
    "        graph_model.graph.lint()\n",
    "        return graph_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#not merged as it does not fit the patterns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m is_converged(model)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#not merged as it does not fit the patterns\n",
    "#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\n",
    "out = is_converged(model)\n",
    "assert out == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(64)\n",
    "for i in range(200):\n",
    "    bn(torch.randn(12,64,256))\n",
    "bn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = bn.running_mean\n",
    "bn(torch.randn(12,64,256))\n",
    "assert bn.running_mean.equal(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if merge_bn works with CustomBatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qtransform.quantization.quant_bn import CustomBatchNorm1d\n",
    "bn = torch.nn.BatchNorm1d(64)\n",
    "custom_bn = CustomBatchNorm1d(64)\n",
    "#some dummy values to make sure that weights and biases change\n",
    "bn.running_mean = torch.ones(64) - torch.randn(64)\n",
    "bn.running_var = torch.ones(64) - torch.randn(64)\n",
    "\n",
    "from brevitas.nn.utils import merge_bn\n",
    "merge_bn(layer=custom_bn, bn=bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3404, -0.2761,  3.7941,  0.7905,  2.1353,  2.2053,  1.4461,  0.5135,\n",
       "        -0.4257,  0.8978,  1.4404, -0.7516,  1.1945,  1.9911,  2.3707, -0.6096,\n",
       "         0.0236,  1.8476,  0.5917,  1.5003,  1.9186, -0.2508,  2.4155,  0.1087,\n",
       "         1.5802,  0.4824, -0.1152,  0.7643,  0.6944,  0.4769, -0.3377,  0.8450,\n",
       "         0.9591,  1.1728,  0.9848,  1.2537,  0.5495, -0.7393,  0.0230, -0.2664,\n",
       "         0.4891,  0.2027,  1.0734,  2.6990, -0.0286,  2.5351,  0.8789,  1.6932,\n",
       "         2.8497,  3.5356,  1.7280,  0.0991,  1.4326, -0.3052,  0.3073,  0.5748,\n",
       "         0.9516, -0.2996,  0.6284,  0.7912,  0.1061, -0.4269,  1.1209,  1.1498])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our pathced quantized batchnorm from brevitas is somewhat accurate to regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1788.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "bn = torch.nn.BatchNorm1d(64).to(device=device)\n",
    "bn_quant = qnn.quant_bn.BatchNorm1dToQuantScaleBias(64).to(device=device)\n",
    "x = torch.randn(12,64,256).to(device=device)\n",
    "chunks = x.chunk(12)\n",
    "out = bn(x)[0]\n",
    "#brevitas batchnorm cannot do batches and output shape is weird\n",
    "batches = torch.zeros(12).to(chunks[0].device)\n",
    "x = qnn.QuantIdentity().to(device=device)(x)\n",
    "out_quant = bn_quant(chunks[0]).squeeze(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNQUANTIZED: \n",
      "tensor([[ 0.3717,  0.2948,  0.6914,  ..., -0.5079, -0.7649, -0.9057],\n",
      "        [-1.1437, -1.2347,  1.8752,  ..., -0.0156, -2.3635,  0.8861],\n",
      "        [ 1.1881,  1.2583, -0.9642,  ..., -1.1639,  0.0508,  0.0539],\n",
      "        ...,\n",
      "        [-0.6113, -0.8401, -0.0366,  ..., -0.5180,  0.3790,  0.0749],\n",
      "        [-0.9505,  0.7660, -1.6229,  ...,  1.6479,  0.2074,  0.7169],\n",
      "        [ 0.9767, -0.1875, -0.4149,  ..., -0.1294, -1.6123, -0.1703]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "QUANTIZED: \n",
      "tensor([[ 0.3721,  0.2958,  0.6894,  ..., -0.5011, -0.7562, -0.8960],\n",
      "        [-1.1435, -1.2319,  1.7912,  ..., -0.0469, -2.3293,  0.8297],\n",
      "        [ 1.1743,  1.2445, -0.9773,  ..., -1.1769,  0.0374,  0.0405],\n",
      "        ...,\n",
      "        [-0.6131, -0.8418, -0.0385,  ..., -0.5198,  0.3772,  0.0731],\n",
      "        [-0.9197,  0.8135, -1.5987,  ...,  1.7039,  0.2494,  0.7639],\n",
      "        [ 0.9729, -0.2014, -0.4307,  ..., -0.1428, -1.6385, -0.1841]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "DIFFERENCE: tensor([[ 0.0004,  0.0009, -0.0020,  ...,  0.0068,  0.0087,  0.0097],\n",
      "        [ 0.0002,  0.0027, -0.0840,  ..., -0.0313,  0.0342, -0.0564],\n",
      "        [-0.0138, -0.0139, -0.0131,  ..., -0.0130, -0.0134, -0.0134],\n",
      "        ...,\n",
      "        [-0.0017, -0.0017, -0.0018,  ..., -0.0018, -0.0019, -0.0018],\n",
      "        [ 0.0308,  0.0475,  0.0242,  ...,  0.0561,  0.0421,  0.0470],\n",
      "        [-0.0038, -0.0139, -0.0159,  ..., -0.0134, -0.0262, -0.0137]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "\n",
      "DIFFERENCE IN EACH WORD: tensor([ 3.1339e-03, -3.0899e-02, -1.3413e-02, -2.1600e-02,  1.5570e-02,\n",
      "         2.8991e-02, -3.2045e-02, -5.1656e-05,  1.7745e-03,  3.2011e-03,\n",
      "        -2.7319e-02, -7.9251e-03,  6.6058e-03,  8.1829e-03, -2.0327e-03,\n",
      "        -1.0973e-02, -1.7600e-02, -1.7283e-02, -1.5700e-02,  7.0697e-03,\n",
      "        -3.9060e-03,  3.0110e-03, -4.8141e-03,  1.2407e-02, -5.6185e-03,\n",
      "         1.0696e-02, -2.4323e-02,  3.4389e-02,  1.9996e-02,  1.5830e-02,\n",
      "         1.4918e-02,  1.6751e-02,  1.7072e-02, -4.1615e-04, -2.2849e-02,\n",
      "        -7.2231e-03,  1.0098e-03, -4.1827e-02,  2.7265e-03,  1.8969e-02,\n",
      "         1.1765e-03, -1.2866e-02,  6.4182e-03, -3.2022e-03, -6.6441e-03,\n",
      "         1.4052e-02, -3.2617e-02,  1.2806e-02,  4.4178e-02,  5.5695e-03,\n",
      "        -1.3799e-02, -2.0418e-02, -1.4273e-02,  1.5722e-02, -2.3369e-02,\n",
      "        -1.0008e-02, -3.3145e-02, -3.1600e-02,  7.8381e-03, -9.2784e-04,\n",
      "         1.7084e-02, -1.8157e-03,  4.0906e-02, -1.2321e-02], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'UNQUANTIZED: \\n{out}\\n\\nQUANTIZED: \\n{out_quant}\\n\\nDIFFERENCE: {out_quant - out}\\n\\nDIFFERENCE IN EACH WORD: {(out_quant - out).mean(dim=-1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "def chunk_examples(examples, column_name = \"text\"):\n",
    "    \"\"\"\n",
    "            Splits the text of each row into chunks of length chunk_length. \n",
    "            It is useful when samples have large amounts of text in order to perform\n",
    "            mapping in batches more efficiently.\n",
    "            Parts of the code are inspired from: https://huggingface.co/docs/datasets/process#split-long-examples\n",
    "\n",
    "            Returns: {\"chunks\" : chunks} \n",
    "            where chunks is a list of sentences split after chunk_length characters.\n",
    "    \"\"\"\n",
    "    print(len(examples[column_name]))\n",
    "    chunks = []\n",
    "    CHUNK_LENGTH = 100\n",
    "    for sentence in examples[column_name]:\n",
    "        chunks += [sentence[i:i + CHUNK_LENGTH] for i in range(0, len(sentence), CHUNK_LENGTH)]\n",
    "    return {\"chunks\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rotten_tomatoes = datasets.load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3241354ff7a046da89ff96c2b476a262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0272c3f0ec1440ab9a0ee9978d2de20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015f1c7ba4614a79b1d494bdc28e78a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.map(\n",
    "    lambda x: print(len(x)), \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns =  [\"text\", \"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a7c3040474008af1f1e26ba40c502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6dfa7c3af4814906c6a2e638c9758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a5e8fe2e3422c9543dbcd20170543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.map(\n",
    "    chunk_examples, \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns = [\"text\", \"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 13952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chunks'],\n",
       "        num_rows: 1745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67873aac4d4844b6b5e244497ecee27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787e32691604c79bf5c47d831e4c20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7e83fe7e04818ad2d150a3c8842e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = rotten_tomatoes.select_columns(\"text\").map(\n",
    "    chunk_examples, \n",
    "    batched=True, \n",
    "    batch_size = 1000,\n",
    "    remove_columns = [\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13952, 1761, 1745]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(dataset_splits[x]) for x in dataset_splits.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a0b40bbc4347e994ee8e46c92a6188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/13952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e1e144e4ab4b66822342880ef832ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209dfcfcb8e0428aa348b438f9cd9125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing from chunks:   0%|          | 0/1745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "def encode_batch(batch):\n",
    "    batch_ids = [tokenizer.encode_ordinary(x) for x in batch[\"chunks\"]]\n",
    "    return {\"input_ids\": batch_ids, \"length\": [len(x) for x in batch_ids]}\n",
    "\n",
    "tokens = dataset_splits.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    batch_size = 1000, \n",
    "    remove_columns = \"chunks\",\n",
    "    desc=f'tokenizing from chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 13952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'length'],\n",
       "        num_rows: 1745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for split in tokens:\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = datasets.load_dataset(\"rotten_tomatoes\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-raw-v1', 'wikitext-103-v1', 'wikitext-2-raw-v1', 'wikitext-2-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-raw-v1')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikitext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1852\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1852\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:373\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:525\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    524\u001b[0m     example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    531\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo config specified, defaulting to the single config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilder_config\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-raw-v1', 'wikitext-103-v1', 'wikitext-2-raw-v1', 'wikitext-2-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-raw-v1')`"
     ]
    }
   ],
   "source": [
    "datasets.load_dataset(\"wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikitext = datasets.load_dataset(path = \"wikitext\", name = 'wikitext-103-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikitext[\"something_else\"] = wikitext.pop(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[43mwikitext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2357\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/dataset_dict.py:59\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     63\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "split = wikitext[\"train\"].train_test_split(test_size=0.05, seed=2357, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1711282\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 90068\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801350"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(split.num_rows.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our benchmarking script is faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "cfg_args =\n",
    " dict(\n",
    "    n_layer = 6,\n",
    "    n_head = 6,\n",
    "    n_embd = 384,\n",
    "    n_positions = 256,\n",
    "    n_inner = 384 * 4,\n",
    "    embd_pdrop  = 0.2,\n",
    "    attn_pdrop  = 0.2,\n",
    "    return_dict=False)\n",
    "hf_cfg = transformers.GPT2Config(**cfg_args)\n",
    "#gpt2_hf = transformers.GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gpt2_hf.__class__.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(gpt2_hf, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 384)\n",
       "  (wpe): Embedding(256, 384)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-5): 6 x GPT2Block(\n",
       "      (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ckpt = torch.load(\"test\")\n",
    "test_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Child(transformers.GPT2LMHeadModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[-0.0034, -0.0187, -0.0299,  ...,  0.0290, -0.0106,  0.0040],\n",
       "                      [ 0.0177, -0.0137, -0.0025,  ...,  0.0308,  0.0007, -0.0168],\n",
       "                      [-0.0055, -0.0008, -0.0139,  ..., -0.0303,  0.0064, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0141, -0.0004,  0.0128,  ...,  0.0113, -0.0113,  0.0003],\n",
       "                      [-0.0067,  0.0219,  0.0267,  ...,  0.0151, -0.0265,  0.0033],\n",
       "                      [-0.0253,  0.0030,  0.0068,  ..., -0.0190, -0.0058,  0.0044]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[ 0.0052, -0.0059,  0.0178,  ..., -0.0193,  0.0101, -0.0164],\n",
       "                      [ 0.0018, -0.0123, -0.0057,  ...,  0.0150,  0.0277, -0.0257],\n",
       "                      [ 0.0007,  0.0007,  0.0088,  ..., -0.0067, -0.0308, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0210,  0.0215,  0.0132,  ..., -0.0068,  0.0099,  0.0164],\n",
       "                      [-0.0072,  0.0063,  0.0115,  ...,  0.0059,  0.0046,  0.0108],\n",
       "                      [-0.0447,  0.0051,  0.0168,  ...,  0.0208, -0.0169, -0.0483]])),\n",
       "             ('transformer.h.0.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.0.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.attn.c_attn.weight',\n",
       "              tensor([[ 3.7256e-02, -5.7076e-03,  2.1655e-02,  ...,  1.8264e-02,\n",
       "                        1.3201e-02, -1.9385e-02],\n",
       "                      [-6.7957e-05, -1.8852e-02,  7.9665e-03,  ...,  1.3440e-02,\n",
       "                        1.0891e-02,  5.7556e-03],\n",
       "                      [ 5.4535e-03, -1.5160e-03,  9.2361e-03,  ..., -3.8783e-02,\n",
       "                       -1.8917e-02, -1.1734e-02],\n",
       "                      ...,\n",
       "                      [-3.4595e-02,  1.2689e-02,  2.9578e-02,  ..., -4.6025e-03,\n",
       "                       -3.1350e-02, -3.6187e-02],\n",
       "                      [-2.5702e-02, -3.5004e-02,  1.8364e-03,  ..., -1.3925e-02,\n",
       "                       -2.7972e-05,  2.6473e-03],\n",
       "                      [-4.5683e-03,  2.9496e-02,  1.8853e-02,  ...,  1.5937e-02,\n",
       "                        1.5186e-02,  1.1989e-02]])),\n",
       "             ('transformer.h.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.0.attn.c_proj.weight',\n",
       "              tensor([[ 0.0055, -0.0034, -0.0067,  ...,  0.0014,  0.0020, -0.0057],\n",
       "                      [-0.0002, -0.0059,  0.0052,  ..., -0.0030,  0.0109, -0.0065],\n",
       "                      [-0.0031,  0.0036,  0.0030,  ...,  0.0069,  0.0045,  0.0043],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0040,  0.0044,  ..., -0.0031, -0.0023,  0.0013],\n",
       "                      [ 0.0106,  0.0030, -0.0040,  ...,  0.0153, -0.0046, -0.0050],\n",
       "                      [ 0.0072,  0.0032,  0.0001,  ..., -0.0068,  0.0027,  0.0007]])),\n",
       "             ('transformer.h.0.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.0.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0186, -0.0079, -0.0231,  ..., -0.0090,  0.0049,  0.0112],\n",
       "                      [ 0.0156, -0.0173, -0.0105,  ...,  0.0267,  0.0082, -0.0050],\n",
       "                      [ 0.0273, -0.0035, -0.0220,  ..., -0.0092, -0.0019,  0.0142],\n",
       "                      ...,\n",
       "                      [ 0.0021, -0.0123,  0.0199,  ...,  0.0210, -0.0058, -0.0423],\n",
       "                      [-0.0195, -0.0100, -0.0093,  ...,  0.0105, -0.0299,  0.0084],\n",
       "                      [-0.0104, -0.0248,  0.0136,  ..., -0.0042,  0.0111, -0.0076]])),\n",
       "             ('transformer.h.0.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0087, -0.0064, -0.0061,  ..., -0.0067,  0.0004,  0.0036],\n",
       "                      [-0.0068, -0.0013,  0.0011,  ...,  0.0020,  0.0101,  0.0011],\n",
       "                      [ 0.0062, -0.0091, -0.0032,  ..., -0.0127, -0.0057,  0.0004],\n",
       "                      ...,\n",
       "                      [-0.0076,  0.0023,  0.0027,  ...,  0.0054,  0.0015,  0.0022],\n",
       "                      [-0.0053, -0.0024,  0.0004,  ..., -0.0049,  0.0007,  0.0042],\n",
       "                      [ 0.0012,  0.0019,  0.0059,  ..., -0.0032,  0.0003, -0.0041]])),\n",
       "             ('transformer.h.0.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.1.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0029, -0.0306,  0.0492,  ...,  0.0486,  0.0332,  0.0024],\n",
       "                      [-0.0033, -0.0012, -0.0049,  ..., -0.0322,  0.0253, -0.0083],\n",
       "                      [ 0.0086, -0.0182, -0.0170,  ...,  0.0254, -0.0096, -0.0118],\n",
       "                      ...,\n",
       "                      [-0.0220,  0.0099,  0.0235,  ...,  0.0151,  0.0061, -0.0051],\n",
       "                      [-0.0122, -0.0283,  0.0018,  ..., -0.0091,  0.0160, -0.0282],\n",
       "                      [-0.0208, -0.0002,  0.0039,  ...,  0.0543,  0.0217, -0.0133]])),\n",
       "             ('transformer.h.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.1.attn.c_proj.weight',\n",
       "              tensor([[-0.0073,  0.0082,  0.0022,  ..., -0.0110,  0.0025,  0.0077],\n",
       "                      [-0.0098,  0.0032, -0.0065,  ..., -0.0005, -0.0067,  0.0077],\n",
       "                      [-0.0028,  0.0072, -0.0022,  ...,  0.0061, -0.0002,  0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0030,  0.0010, -0.0073,  ...,  0.0025,  0.0019, -0.0107],\n",
       "                      [-0.0032,  0.0107,  0.0109,  ...,  0.0029, -0.0058,  0.0050],\n",
       "                      [-0.0080, -0.0057,  0.0047,  ..., -0.0043, -0.0038, -0.0035]])),\n",
       "             ('transformer.h.1.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.1.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.1.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0253,  0.0231,  0.0146,  ...,  0.0167,  0.0093, -0.0117],\n",
       "                      [ 0.0079,  0.0042, -0.0050,  ..., -0.0054, -0.0204, -0.0164],\n",
       "                      [ 0.0128,  0.0015, -0.0173,  ...,  0.0259,  0.0095,  0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0068, -0.0010,  0.0471,  ...,  0.0024,  0.0270, -0.0161],\n",
       "                      [ 0.0144,  0.0135, -0.0132,  ..., -0.0020,  0.0230, -0.0305],\n",
       "                      [-0.0324, -0.0064,  0.0334,  ...,  0.0325,  0.0288, -0.0200]])),\n",
       "             ('transformer.h.1.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.1.mlp.c_proj.weight',\n",
       "              tensor([[-0.0074, -0.0062, -0.0133,  ..., -0.0036,  0.0060,  0.0039],\n",
       "                      [-0.0006, -0.0023,  0.0043,  ...,  0.0049, -0.0170,  0.0050],\n",
       "                      [ 0.0014,  0.0034,  0.0025,  ...,  0.0023,  0.0062, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0065, -0.0053, -0.0045,  ..., -0.0061, -0.0049,  0.0043],\n",
       "                      [ 0.0011, -0.0056, -0.0066,  ...,  0.0026,  0.0019,  0.0039],\n",
       "                      [ 0.0068, -0.0038, -0.0037,  ..., -0.0021, -0.0046,  0.0030]])),\n",
       "             ('transformer.h.1.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.2.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.attn.c_attn.weight',\n",
       "              tensor([[-0.0011,  0.0015, -0.0151,  ..., -0.0326,  0.0005,  0.0041],\n",
       "                      [-0.0157, -0.0099,  0.0021,  ..., -0.0436, -0.0027,  0.0200],\n",
       "                      [ 0.0155,  0.0178, -0.0090,  ..., -0.0163, -0.0040,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0092,  0.0178, -0.0356,  ..., -0.0079, -0.0109,  0.0206],\n",
       "                      [-0.0144, -0.0005, -0.0236,  ..., -0.0450, -0.0089,  0.0058],\n",
       "                      [-0.0036, -0.0079,  0.0304,  ..., -0.0107,  0.0041,  0.0448]])),\n",
       "             ('transformer.h.2.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.2.attn.c_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0022,  0.0071,  ..., -0.0022, -0.0165, -0.0026],\n",
       "                      [ 0.0044,  0.0104,  0.0084,  ..., -0.0051,  0.0007, -0.0053],\n",
       "                      [-0.0053,  0.0002,  0.0077,  ..., -0.0005, -0.0017,  0.0043],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0057, -0.0060,  ...,  0.0025,  0.0005, -0.0043],\n",
       "                      [ 0.0010,  0.0026, -0.0043,  ..., -0.0096,  0.0031,  0.0100],\n",
       "                      [-0.0029,  0.0070,  0.0037,  ...,  0.0048,  0.0018,  0.0038]])),\n",
       "             ('transformer.h.2.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.2.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.2.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0315,  0.0344,  0.0009,  ..., -0.0097, -0.0009,  0.0069],\n",
       "                      [ 0.0277, -0.0016,  0.0410,  ..., -0.0333, -0.0150, -0.0140],\n",
       "                      [ 0.0352,  0.0106, -0.0107,  ..., -0.0025,  0.0187,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0449,  0.0141,  0.0310,  ...,  0.0090,  0.0016, -0.0281],\n",
       "                      [ 0.0204,  0.0066, -0.0013,  ...,  0.0184, -0.0160, -0.0169],\n",
       "                      [-0.0097,  0.0511,  0.0376,  ..., -0.0138, -0.0158, -0.0105]])),\n",
       "             ('transformer.h.2.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.2.mlp.c_proj.weight',\n",
       "              tensor([[-0.0015, -0.0046, -0.0029,  ...,  0.0009, -0.0026,  0.0063],\n",
       "                      [-0.0103,  0.0028, -0.0013,  ...,  0.0018,  0.0034, -0.0127],\n",
       "                      [-0.0063, -0.0044,  0.0020,  ..., -0.0087,  0.0043,  0.0091],\n",
       "                      ...,\n",
       "                      [ 0.0037, -0.0074,  0.0025,  ..., -0.0035,  0.0074, -0.0052],\n",
       "                      [-0.0033, -0.0035, -0.0060,  ...,  0.0044, -0.0033, -0.0029],\n",
       "                      [-0.0109,  0.0094,  0.0010,  ..., -0.0081,  0.0031, -0.0030]])),\n",
       "             ('transformer.h.2.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.3.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.attn.c_attn.weight',\n",
       "              tensor([[ 0.0231, -0.0003,  0.0229,  ...,  0.0518, -0.0003, -0.0184],\n",
       "                      [-0.0087,  0.0082,  0.0264,  ...,  0.0022, -0.0101, -0.0039],\n",
       "                      [ 0.0020,  0.0252, -0.0072,  ...,  0.0155,  0.0498,  0.0139],\n",
       "                      ...,\n",
       "                      [-0.0324, -0.0272, -0.0113,  ..., -0.0341, -0.0159, -0.0178],\n",
       "                      [-0.0347,  0.0082, -0.0143,  ..., -0.0131,  0.0047, -0.0249],\n",
       "                      [-0.0255,  0.0048,  0.0283,  ...,  0.0296, -0.0156,  0.0051]])),\n",
       "             ('transformer.h.3.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.3.attn.c_proj.weight',\n",
       "              tensor([[ 0.0015,  0.0085, -0.0011,  ...,  0.0015, -0.0030, -0.0037],\n",
       "                      [ 0.0030,  0.0064,  0.0003,  ...,  0.0150,  0.0033,  0.0081],\n",
       "                      [ 0.0059, -0.0078,  0.0050,  ..., -0.0024,  0.0047,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0108, -0.0008,  0.0020,  ..., -0.0033, -0.0005, -0.0044],\n",
       "                      [ 0.0033,  0.0042,  0.0002,  ..., -0.0029, -0.0072, -0.0088],\n",
       "                      [ 0.0019,  0.0048, -0.0003,  ..., -0.0069, -0.0043,  0.0092]])),\n",
       "             ('transformer.h.3.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.3.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.3.mlp.c_fc.weight',\n",
       "              tensor([[-0.0032,  0.0245,  0.0143,  ...,  0.0049,  0.0247,  0.0229],\n",
       "                      [ 0.0091, -0.0084, -0.0130,  ...,  0.0214, -0.0070, -0.0259],\n",
       "                      [ 0.0163,  0.0213,  0.0448,  ..., -0.0186, -0.0068,  0.0389],\n",
       "                      ...,\n",
       "                      [ 0.0025,  0.0045,  0.0160,  ...,  0.0050, -0.0262, -0.0125],\n",
       "                      [-0.0352, -0.0087, -0.0294,  ..., -0.0090, -0.0193, -0.0132],\n",
       "                      [-0.0088,  0.0146, -0.0012,  ...,  0.0272,  0.0011,  0.0364]])),\n",
       "             ('transformer.h.3.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.3.mlp.c_proj.weight',\n",
       "              tensor([[-4.6481e-03,  1.7276e-03,  6.5588e-03,  ...,  5.9177e-03,\n",
       "                        6.1392e-04, -8.3755e-03],\n",
       "                      [ 6.0920e-06,  5.6963e-04, -4.0137e-03,  ..., -4.3385e-03,\n",
       "                        1.2971e-02,  2.9268e-03],\n",
       "                      [-2.0432e-04, -6.1765e-03,  1.7358e-03,  ..., -6.7340e-03,\n",
       "                       -4.2019e-03,  2.4833e-03],\n",
       "                      ...,\n",
       "                      [-1.0416e-02,  2.1218e-03,  5.0554e-03,  ..., -4.8121e-03,\n",
       "                        4.1022e-03, -1.1739e-02],\n",
       "                      [-1.6727e-03, -5.9600e-03, -4.6248e-03,  ..., -6.9834e-04,\n",
       "                       -1.2813e-03,  2.7497e-03],\n",
       "                      [ 3.1807e-03, -2.7290e-03, -4.0406e-04,  ...,  5.0236e-03,\n",
       "                        3.1861e-03,  5.6393e-04]])),\n",
       "             ('transformer.h.3.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.4.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.attn.c_attn.weight',\n",
       "              tensor([[-0.0512, -0.0201,  0.0244,  ...,  0.0025, -0.0251,  0.0160],\n",
       "                      [ 0.0165,  0.0148, -0.0101,  ...,  0.0132,  0.0243,  0.0126],\n",
       "                      [-0.0276, -0.0339,  0.0122,  ..., -0.0051, -0.0143,  0.0240],\n",
       "                      ...,\n",
       "                      [-0.0311, -0.0140, -0.0056,  ...,  0.0076,  0.0414, -0.0005],\n",
       "                      [ 0.0043,  0.0368, -0.0142,  ..., -0.0159, -0.0248, -0.0092],\n",
       "                      [ 0.0179, -0.0112,  0.0143,  ...,  0.0363,  0.0040, -0.0308]])),\n",
       "             ('transformer.h.4.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.4.attn.c_proj.weight',\n",
       "              tensor([[-0.0038,  0.0040, -0.0114,  ...,  0.0142, -0.0011,  0.0030],\n",
       "                      [ 0.0027,  0.0003, -0.0029,  ..., -0.0066, -0.0062,  0.0026],\n",
       "                      [-0.0050, -0.0049,  0.0053,  ..., -0.0016,  0.0013, -0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0018, -0.0069,  0.0041,  ...,  0.0055,  0.0012, -0.0004],\n",
       "                      [ 0.0014, -0.0029, -0.0058,  ...,  0.0019, -0.0011, -0.0053],\n",
       "                      [ 0.0055, -0.0055, -0.0031,  ..., -0.0052, -0.0034,  0.0001]])),\n",
       "             ('transformer.h.4.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.4.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.4.mlp.c_fc.weight',\n",
       "              tensor([[-0.0723,  0.0188, -0.0038,  ...,  0.0066,  0.0168, -0.0271],\n",
       "                      [ 0.0071, -0.0258,  0.0235,  ...,  0.0219, -0.0212,  0.0034],\n",
       "                      [-0.0140,  0.0407, -0.0035,  ..., -0.0048, -0.0412, -0.0049],\n",
       "                      ...,\n",
       "                      [-0.0241, -0.0222, -0.0407,  ..., -0.0141,  0.0281,  0.0465],\n",
       "                      [-0.0203,  0.0182, -0.0042,  ..., -0.0216, -0.0052, -0.0103],\n",
       "                      [ 0.0189,  0.0079,  0.0197,  ...,  0.0155, -0.0180, -0.0184]])),\n",
       "             ('transformer.h.4.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.4.mlp.c_proj.weight',\n",
       "              tensor([[-0.0035,  0.0121, -0.0011,  ...,  0.0021, -0.0018,  0.0055],\n",
       "                      [-0.0042, -0.0052,  0.0045,  ..., -0.0051,  0.0037, -0.0032],\n",
       "                      [-0.0013,  0.0007, -0.0008,  ...,  0.0021, -0.0059,  0.0150],\n",
       "                      ...,\n",
       "                      [-0.0031, -0.0083,  0.0003,  ...,  0.0037, -0.0003, -0.0057],\n",
       "                      [-0.0052,  0.0024,  0.0038,  ...,  0.0010, -0.0028, -0.0038],\n",
       "                      [-0.0023, -0.0085,  0.0018,  ..., -0.0059,  0.0041, -0.0020]])),\n",
       "             ('transformer.h.4.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.5.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.attn.c_attn.weight',\n",
       "              tensor([[ 0.0065, -0.0118, -0.0069,  ...,  0.0399, -0.0374, -0.0160],\n",
       "                      [-0.0375, -0.0067,  0.0145,  ...,  0.0032, -0.0245,  0.0069],\n",
       "                      [-0.0346,  0.0048,  0.0188,  ..., -0.0069,  0.0224,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0006, -0.0080,  ..., -0.0500, -0.0187,  0.0186],\n",
       "                      [-0.0150, -0.0017, -0.0027,  ..., -0.0125,  0.0406, -0.0197],\n",
       "                      [ 0.0335,  0.0320,  0.0153,  ...,  0.0122, -0.0076,  0.0345]])),\n",
       "             ('transformer.h.5.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.5.attn.c_proj.weight',\n",
       "              tensor([[-4.4844e-03,  1.3152e-03, -3.4908e-03,  ...,  1.0732e-02,\n",
       "                        9.2446e-03,  1.2282e-02],\n",
       "                      [ 3.3737e-03, -7.5732e-03,  1.8066e-03,  ..., -4.9796e-03,\n",
       "                        6.8677e-03,  3.7573e-03],\n",
       "                      [-1.8318e-03, -1.2586e-03,  1.8209e-03,  ..., -5.5451e-03,\n",
       "                       -8.9172e-03,  6.3112e-03],\n",
       "                      ...,\n",
       "                      [-5.5658e-03, -3.5368e-03, -1.1666e-02,  ...,  1.0554e-03,\n",
       "                       -2.2873e-03, -6.9975e-03],\n",
       "                      [ 4.1811e-03,  5.0578e-03,  1.0962e-02,  ...,  2.1638e-03,\n",
       "                       -2.0211e-03, -4.3049e-03],\n",
       "                      [ 2.3144e-03, -3.2657e-05,  9.2031e-03,  ..., -1.0683e-02,\n",
       "                        6.9108e-03, -4.3645e-03]])),\n",
       "             ('transformer.h.5.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.h.5.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.h.5.mlp.c_fc.weight',\n",
       "              tensor([[-0.0117,  0.0228,  0.0034,  ...,  0.0170,  0.0223, -0.0357],\n",
       "                      [ 0.0081,  0.0052,  0.0225,  ..., -0.0146, -0.0184,  0.0036],\n",
       "                      [ 0.0248, -0.0153,  0.0256,  ..., -0.0205, -0.0131, -0.0301],\n",
       "                      ...,\n",
       "                      [-0.0035, -0.0120, -0.0019,  ..., -0.0074, -0.0068, -0.0182],\n",
       "                      [ 0.0216,  0.0306,  0.0594,  ...,  0.0326,  0.0241,  0.0049],\n",
       "                      [-0.0488,  0.0018,  0.0011,  ..., -0.0068, -0.0214, -0.0073]])),\n",
       "             ('transformer.h.5.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.h.5.mlp.c_proj.weight',\n",
       "              tensor([[-7.6916e-03, -7.8936e-03,  8.9618e-03,  ..., -5.4573e-03,\n",
       "                        4.4367e-04,  8.7935e-04],\n",
       "                      [-1.1022e-03, -6.3099e-04, -7.0274e-03,  ...,  3.2784e-03,\n",
       "                        1.2046e-02, -6.4660e-03],\n",
       "                      [-3.8424e-03, -7.7052e-03,  1.0153e-02,  ..., -6.8431e-04,\n",
       "                       -3.5021e-03,  2.7638e-03],\n",
       "                      ...,\n",
       "                      [ 5.7108e-04, -2.3094e-03,  3.5360e-03,  ...,  1.7454e-03,\n",
       "                       -2.6458e-03, -9.5012e-03],\n",
       "                      [ 5.4334e-03,  1.5826e-03,  9.0219e-05,  ..., -9.1541e-04,\n",
       "                       -7.5399e-04, -3.9756e-03],\n",
       "                      [-3.7548e-03,  1.6471e-03,  2.4763e-03,  ..., -1.7112e-03,\n",
       "                       -1.0687e-03, -3.5235e-03]])),\n",
       "             ('transformer.h.5.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_f.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('transformer.ln_f.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-0.0034, -0.0187, -0.0299,  ...,  0.0290, -0.0106,  0.0040],\n",
       "                      [ 0.0177, -0.0137, -0.0025,  ...,  0.0308,  0.0007, -0.0168],\n",
       "                      [-0.0055, -0.0008, -0.0139,  ..., -0.0303,  0.0064, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0141, -0.0004,  0.0128,  ...,  0.0113, -0.0113,  0.0003],\n",
       "                      [-0.0067,  0.0219,  0.0267,  ...,  0.0151, -0.0265,  0.0033],\n",
       "                      [-0.0253,  0.0030,  0.0068,  ..., -0.0190, -0.0058,  0.0044]]))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Child(hf_cfg)\n",
    "test.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "ids = torch.Tensor(tokenizer.encode_ordinary(\"hello world this is a test.\")).unsqueeze(dim=0).to(dtype=torch.long)\n",
    "labels = torch.Tensor(tokenizer.encode_ordinary(\"and the words are the labels.\")).unsqueeze(dim=0).to(dtype=torch.long)\n",
    "out = test(ids, labels=labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_labels = test(ids, labels=None, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_labels.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one extra word appended to input sequence\n",
    "out.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50304 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "config = GPTConfig(block_size=256, \n",
    "                   vocab_size=50304, \n",
    "                   n_layer=6, \n",
    "                   n_head=6, \n",
    "                   n_embd=384, \n",
    "                   dropout=0.2, \n",
    "                   bias=True, \n",
    "                   flash=False, \n",
    "                   transformer_active_func='ReLU', \n",
    "                   norm_layer='BatchNorm', \n",
    "                   single_output=False, \n",
    "                   use_weight_tying=True, \n",
    "                   custom_ln=False)\n",
    "gpt_qt = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, loss = gpt_qt(ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50304])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[-0.0480, -0.0163, -0.0034,  ...,  0.0231,  0.0399,  0.0322],\n",
       "                      [-0.0097, -0.0217, -0.0156,  ..., -0.0078,  0.0002, -0.0066],\n",
       "                      [ 0.0230, -0.0403,  0.0179,  ...,  0.0164,  0.0006, -0.0172],\n",
       "                      ...,\n",
       "                      [-0.0109, -0.0091, -0.0256,  ...,  0.0080, -0.0260,  0.0130],\n",
       "                      [-0.0125,  0.0159,  0.0072,  ..., -0.0025,  0.0050, -0.0154],\n",
       "                      [ 0.0171, -0.0222, -0.0161,  ..., -0.0008,  0.0018, -0.0086]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[ 0.0174, -0.0088, -0.0167,  ..., -0.0200, -0.0163, -0.0277],\n",
       "                      [-0.0096, -0.0254,  0.0014,  ...,  0.0017,  0.0487, -0.0294],\n",
       "                      [ 0.0069,  0.0073,  0.0126,  ...,  0.0032, -0.0227, -0.0187],\n",
       "                      ...,\n",
       "                      [-0.0023, -0.0291, -0.0032,  ...,  0.0274, -0.0156, -0.0070],\n",
       "                      [ 0.0055, -0.0096,  0.0016,  ..., -0.0173, -0.0041, -0.0089],\n",
       "                      [ 0.0360,  0.0023,  0.0097,  ...,  0.0243, -0.0060,  0.0181]])),\n",
       "             ('transformer.layer.0.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.0.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.ln_1.running_mean',\n",
       "              tensor([ 3.3417e-04,  6.4249e-05, -1.7564e-04,  2.5978e-04,  2.5669e-04,\n",
       "                       1.6849e-04, -1.4753e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.0.ln_1.running_var',\n",
       "              tensor([0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8102, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.0.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.0.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.0.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.ln_2.running_mean',\n",
       "              tensor([-0.0004, -0.0003, -0.0004, -0.0002, -0.0002, -0.0001, -0.0002,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])),\n",
       "             ('transformer.layer.0.ln_2.running_var',\n",
       "              tensor([0.9993, 0.9988, 0.9991, 0.9983, 0.9990, 0.9987, 0.9983, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.0.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.0.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.0.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0327,  0.0343,  0.0564,  ...,  0.0229, -0.0261,  0.0239],\n",
       "                      [ 0.0284,  0.0331,  0.0526,  ...,  0.0373, -0.0095,  0.0103],\n",
       "                      [-0.0476, -0.0240,  0.0137,  ..., -0.0208,  0.0301, -0.0612],\n",
       "                      ...,\n",
       "                      [ 0.0134, -0.0077,  0.0258,  ...,  0.0385, -0.0429, -0.0619],\n",
       "                      [ 0.0381, -0.0488, -0.0014,  ..., -0.0604, -0.0477,  0.0044],\n",
       "                      [ 0.0415,  0.0069,  0.0520,  ..., -0.0221, -0.0173, -0.0237]])),\n",
       "             ('transformer.layer.0.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.weight',\n",
       "              tensor([[ 2.1368e-02, -2.2444e-02, -2.8786e-02,  ...,  1.8028e-02,\n",
       "                        1.1564e-02,  3.3639e-03],\n",
       "                      [-4.1301e-02, -5.4199e-03, -2.4576e-02,  ...,  1.3486e-02,\n",
       "                        8.5759e-03, -7.5457e-03],\n",
       "                      [-2.9626e-03,  3.9741e-03, -7.6519e-03,  ..., -1.0736e-02,\n",
       "                        2.3561e-03, -2.3734e-03],\n",
       "                      ...,\n",
       "                      [-3.1809e-05, -2.1282e-03,  3.5508e-02,  ..., -4.9362e-03,\n",
       "                       -8.6604e-03,  5.7153e-04],\n",
       "                      [-8.6202e-04, -9.6304e-03, -1.6080e-03,  ...,  1.3915e-02,\n",
       "                       -2.2579e-02, -3.2772e-02],\n",
       "                      [ 4.6639e-02,  4.8244e-02, -1.3898e-02,  ..., -9.7132e-03,\n",
       "                        3.5791e-02,  2.9523e-02]])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.c_proj.weight',\n",
       "              tensor([[-8.8311e-05,  1.6462e-03,  5.8459e-03,  ...,  9.6361e-03,\n",
       "                       -2.6982e-03,  3.1887e-03],\n",
       "                      [ 9.4704e-04, -5.6763e-03,  2.1477e-03,  ...,  4.9899e-04,\n",
       "                       -4.8711e-03, -8.6017e-03],\n",
       "                      [ 1.1691e-02,  5.3716e-03,  1.4181e-03,  ...,  1.2279e-03,\n",
       "                        8.7907e-03, -2.5356e-03],\n",
       "                      ...,\n",
       "                      [ 1.3080e-02,  1.7922e-03,  9.3596e-03,  ..., -8.9694e-04,\n",
       "                       -1.5338e-02,  1.5187e-03],\n",
       "                      [ 5.9816e-03,  2.4333e-03,  2.5251e-03,  ...,  8.4732e-03,\n",
       "                        1.7239e-03, -4.8735e-04],\n",
       "                      [ 3.4273e-03, -9.2383e-03, -1.6062e-03,  ..., -4.3751e-03,\n",
       "                       -8.9931e-03,  4.8042e-04]])),\n",
       "             ('transformer.layer.0.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.attn.c_attn.weight',\n",
       "              tensor([[-0.0118,  0.0227, -0.0423,  ...,  0.0115,  0.0236,  0.0075],\n",
       "                      [ 0.0292,  0.0084, -0.0145,  ...,  0.0029,  0.0230,  0.0192],\n",
       "                      [-0.0137,  0.0051,  0.0246,  ...,  0.0166, -0.0197, -0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0324,  0.0406, -0.0125,  ..., -0.0089, -0.0051,  0.0079],\n",
       "                      [-0.0163, -0.0042,  0.0137,  ..., -0.0037, -0.0065, -0.0092],\n",
       "                      [-0.0274, -0.0067, -0.0224,  ...,  0.0032, -0.0147, -0.0043]])),\n",
       "             ('transformer.layer.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0093,  0.0113, -0.0179,  ..., -0.0357, -0.0128,  0.0239],\n",
       "                      [ 0.0137,  0.0200,  0.0104,  ..., -0.0175, -0.0169,  0.0014],\n",
       "                      [ 0.0439,  0.0074,  0.0100,  ..., -0.0390, -0.0191,  0.0054],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0137,  0.0092,  ...,  0.0397,  0.0182,  0.0173],\n",
       "                      [-0.0237, -0.0023, -0.0200,  ..., -0.0291,  0.0013,  0.0143],\n",
       "                      [ 0.0125,  0.0181, -0.0033,  ...,  0.0211, -0.0030, -0.0077]])),\n",
       "             ('transformer.layer.0.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0005,  0.0124, -0.0022,  ...,  0.0001,  0.0004,  0.0162],\n",
       "                      [ 0.0057, -0.0037,  0.0039,  ...,  0.0041,  0.0058,  0.0061],\n",
       "                      [ 0.0059,  0.0022, -0.0021,  ...,  0.0015, -0.0082,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0090,  0.0022,  ...,  0.0014, -0.0010,  0.0127],\n",
       "                      [-0.0065, -0.0014,  0.0015,  ..., -0.0022,  0.0050, -0.0054],\n",
       "                      [ 0.0042,  0.0085, -0.0024,  ...,  0.0042,  0.0028,  0.0001]])),\n",
       "             ('transformer.layer.0.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.1.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_1.running_mean',\n",
       "              tensor([-2.5211e-04,  1.9818e-04, -6.1746e-05,  8.6410e-05,  6.2564e-04,\n",
       "                       4.9416e-04,  5.8872e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.1.ln_1.running_var',\n",
       "              tensor([1.0044, 0.9998, 1.0009, 1.0008, 1.0014, 1.0036, 0.9994, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.1.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.1.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.1.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.ln_2.running_mean',\n",
       "              tensor([-3.6266e-04, -1.4068e-04,  4.0739e-06, -1.9030e-04, -7.2249e-05,\n",
       "                      -1.3234e-04, -1.6590e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.1.ln_2.running_var',\n",
       "              tensor([1.0010, 1.0008, 1.0004, 1.0007, 1.0006, 1.0006, 1.0006, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.1.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.1.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.1.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0042,  0.0067,  0.0036,  ...,  0.0401, -0.0550,  0.0283],\n",
       "                      [-0.0450,  0.0131,  0.0262,  ...,  0.0413,  0.0182, -0.0394],\n",
       "                      [-0.0393, -0.0571,  0.0320,  ..., -0.0116,  0.0435,  0.0199],\n",
       "                      ...,\n",
       "                      [-0.0399,  0.0382,  0.0399,  ...,  0.0246,  0.0235, -0.0016],\n",
       "                      [-0.0271, -0.0486,  0.0069,  ..., -0.0101,  0.0258,  0.0368],\n",
       "                      [-0.0372,  0.0099,  0.0414,  ..., -0.0529,  0.0298,  0.0019]])),\n",
       "             ('transformer.layer.1.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0300, -0.0486, -0.0398,  ...,  0.0125, -0.0130, -0.0273],\n",
       "                      [ 0.0231,  0.0073, -0.0064,  ..., -0.0162,  0.0433,  0.0037],\n",
       "                      [-0.0179, -0.0231, -0.0141,  ...,  0.0115, -0.0200,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0223, -0.0098, -0.0025,  ...,  0.0256,  0.0094,  0.0003],\n",
       "                      [ 0.0054,  0.0143,  0.0203,  ..., -0.0119, -0.0181, -0.0241],\n",
       "                      [ 0.0185,  0.0150, -0.0101,  ...,  0.0087, -0.0183, -0.0031]])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.c_proj.weight',\n",
       "              tensor([[ 4.4580e-03, -2.5867e-03, -2.4178e-03,  ..., -3.9064e-03,\n",
       "                       -1.0177e-03,  5.6169e-04],\n",
       "                      [ 9.0653e-03,  3.4227e-03, -1.2240e-03,  ..., -1.7433e-02,\n",
       "                       -1.2715e-02,  3.7085e-03],\n",
       "                      [-8.5925e-05,  1.2168e-02, -2.4109e-03,  ..., -6.6085e-03,\n",
       "                       -9.7568e-03,  4.3224e-03],\n",
       "                      ...,\n",
       "                      [-1.3515e-02,  3.5075e-03,  2.8530e-04,  ..., -2.6873e-03,\n",
       "                        1.0414e-03,  5.0562e-03],\n",
       "                      [ 7.8588e-04,  1.7284e-03,  6.5941e-03,  ...,  3.1925e-03,\n",
       "                       -6.9360e-03, -1.3075e-02],\n",
       "                      [-9.2083e-03,  5.3821e-03, -1.0619e-02,  ...,  8.0773e-03,\n",
       "                       -1.0429e-02, -7.1923e-03]])),\n",
       "             ('transformer.layer.1.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0280,  0.0090, -0.0085,  ...,  0.0041, -0.0172, -0.0110],\n",
       "                      [ 0.0031, -0.0004, -0.0204,  ...,  0.0195,  0.0061,  0.0273],\n",
       "                      [ 0.0144, -0.0163,  0.0090,  ..., -0.0367,  0.0121, -0.0257],\n",
       "                      ...,\n",
       "                      [ 0.0253, -0.0274,  0.0218,  ...,  0.0171,  0.0104,  0.0388],\n",
       "                      [-0.0268,  0.0104, -0.0334,  ...,  0.0065, -0.0011, -0.0149],\n",
       "                      [-0.0183,  0.0222, -0.0180,  ...,  0.0269, -0.0175,  0.0134]])),\n",
       "             ('transformer.layer.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_fc.weight',\n",
       "              tensor([[-0.0298,  0.0238,  0.0399,  ...,  0.0321,  0.0268, -0.0263],\n",
       "                      [-0.0072, -0.0047,  0.0203,  ...,  0.0150, -0.0259,  0.0284],\n",
       "                      [-0.0248,  0.0131, -0.0172,  ..., -0.0035,  0.0540,  0.0424],\n",
       "                      ...,\n",
       "                      [ 0.0059, -0.0083, -0.0415,  ...,  0.0101,  0.0067,  0.0094],\n",
       "                      [ 0.0467, -0.0055, -0.0013,  ...,  0.0315,  0.0131, -0.0411],\n",
       "                      [-0.0124, -0.0042,  0.0094,  ..., -0.0200,  0.0217, -0.0260]])),\n",
       "             ('transformer.layer.1.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_proj.weight',\n",
       "              tensor([[-0.0010,  0.0043, -0.0047,  ..., -0.0163,  0.0028,  0.0060],\n",
       "                      [ 0.0086,  0.0051,  0.0056,  ...,  0.0022,  0.0052, -0.0079],\n",
       "                      [ 0.0052, -0.0021,  0.0062,  ..., -0.0028, -0.0090, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0016,  0.0117,  ..., -0.0059,  0.0060,  0.0035],\n",
       "                      [ 0.0069,  0.0048, -0.0037,  ..., -0.0073, -0.0083, -0.0095],\n",
       "                      [-0.0119,  0.0066,  0.0090,  ..., -0.0060,  0.0140, -0.0137]])),\n",
       "             ('transformer.layer.1.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.2.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_1.running_mean',\n",
       "              tensor([-6.2409e-04, -1.7642e-04, -8.0269e-04, -1.3574e-05, -1.9546e-04,\n",
       "                      -1.3271e-04,  6.8201e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.2.ln_1.running_var',\n",
       "              tensor([1.0024, 1.0019, 1.0015, 1.0014, 1.0004, 1.0010, 1.0031, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.2.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.2.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.2.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.ln_2.running_mean',\n",
       "              tensor([-1.5982e-04,  1.5704e-04, -2.7822e-05, -2.3739e-04, -1.8987e-04,\n",
       "                      -9.8857e-05, -1.7925e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.2.ln_2.running_var',\n",
       "              tensor([1.0004, 1.0008, 1.0004, 1.0003, 1.0011, 1.0004, 1.0011, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.2.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.2.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.2.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0280,  0.0288, -0.0137,  ..., -0.0568,  0.0052,  0.0175],\n",
       "                      [-0.0511,  0.0403, -0.0347,  ..., -0.0531,  0.0129, -0.0051],\n",
       "                      [-0.0304,  0.0528,  0.0500,  ..., -0.0569,  0.0508, -0.0595],\n",
       "                      ...,\n",
       "                      [ 0.0072,  0.0170, -0.0242,  ...,  0.0179, -0.0411,  0.0358],\n",
       "                      [ 0.0251, -0.0500, -0.0565,  ..., -0.0108,  0.0080,  0.0332],\n",
       "                      [ 0.0139,  0.0485,  0.0387,  ...,  0.0458, -0.0067,  0.0022]])),\n",
       "             ('transformer.layer.2.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0011, -0.0009,  0.0147,  ...,  0.0019, -0.0018, -0.0227],\n",
       "                      [-0.0169,  0.0020,  0.0301,  ...,  0.0180,  0.0083,  0.0201],\n",
       "                      [-0.0306,  0.0140, -0.0068,  ...,  0.0108,  0.0201, -0.0155],\n",
       "                      ...,\n",
       "                      [-0.0144,  0.0132,  0.0021,  ..., -0.0264,  0.0014, -0.0027],\n",
       "                      [ 0.0346, -0.0126,  0.0287,  ...,  0.0112,  0.0516, -0.0136],\n",
       "                      [-0.0137,  0.0032, -0.0233,  ..., -0.0073,  0.0307, -0.0182]])),\n",
       "             ('transformer.layer.2.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.c_proj.weight',\n",
       "              tensor([[-0.0021, -0.0029, -0.0014,  ..., -0.0051,  0.0055,  0.0099],\n",
       "                      [-0.0029, -0.0024, -0.0089,  ...,  0.0077, -0.0039,  0.0070],\n",
       "                      [-0.0036, -0.0054, -0.0069,  ...,  0.0112,  0.0108,  0.0085],\n",
       "                      ...,\n",
       "                      [-0.0091, -0.0038,  0.0045,  ...,  0.0042,  0.0034,  0.0002],\n",
       "                      [-0.0147, -0.0010, -0.0075,  ..., -0.0149,  0.0038, -0.0090],\n",
       "                      [ 0.0048,  0.0035,  0.0193,  ...,  0.0072, -0.0047,  0.0067]])),\n",
       "             ('transformer.layer.2.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.attn.c_attn.weight',\n",
       "              tensor([[-0.0125,  0.0097,  0.0008,  ...,  0.0282,  0.0103,  0.0067],\n",
       "                      [-0.0013,  0.0204, -0.0152,  ..., -0.0013,  0.0063, -0.0268],\n",
       "                      [ 0.0238, -0.0226, -0.0007,  ...,  0.0134,  0.0055,  0.0358],\n",
       "                      ...,\n",
       "                      [ 0.0145,  0.0171,  0.0464,  ...,  0.0094, -0.0182,  0.0050],\n",
       "                      [-0.0097, -0.0013,  0.0193,  ...,  0.0087,  0.0102,  0.0118],\n",
       "                      [ 0.0136,  0.0063, -0.0082,  ..., -0.0067,  0.0136, -0.0067]])),\n",
       "             ('transformer.layer.2.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.mlp.c_fc.weight',\n",
       "              tensor([[ 1.0232e-02, -2.7240e-02,  7.1662e-03,  ..., -1.7358e-03,\n",
       "                        1.5643e-02, -9.1390e-03],\n",
       "                      [ 1.0988e-02, -3.3004e-02,  7.6915e-03,  ..., -1.6483e-02,\n",
       "                       -2.1697e-02, -3.6452e-02],\n",
       "                      [-1.7921e-05,  1.0917e-02, -8.4004e-03,  ..., -4.0824e-02,\n",
       "                        4.8517e-03,  4.0391e-02],\n",
       "                      ...,\n",
       "                      [-1.4068e-02,  3.7376e-03,  1.9370e-03,  ..., -1.9469e-02,\n",
       "                        3.3798e-02,  4.5801e-03],\n",
       "                      [ 4.0186e-03,  4.5541e-03,  2.2524e-02,  ...,  1.3113e-02,\n",
       "                        2.9036e-02,  1.6006e-02],\n",
       "                      [ 1.0697e-02,  4.2475e-02,  1.6489e-02,  ...,  1.7569e-02,\n",
       "                       -5.5972e-03, -1.9042e-02]])),\n",
       "             ('transformer.layer.2.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.2.mlp.c_proj.weight',\n",
       "              tensor([[-0.0049, -0.0085, -0.0072,  ..., -0.0013,  0.0046,  0.0025],\n",
       "                      [ 0.0050,  0.0042, -0.0096,  ..., -0.0112, -0.0046,  0.0026],\n",
       "                      [ 0.0045, -0.0121,  0.0038,  ..., -0.0086,  0.0028, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0116,  0.0028,  ..., -0.0032, -0.0044,  0.0009],\n",
       "                      [ 0.0085, -0.0063, -0.0022,  ...,  0.0024,  0.0072,  0.0031],\n",
       "                      [-0.0045, -0.0088, -0.0091,  ..., -0.0094, -0.0008, -0.0040]])),\n",
       "             ('transformer.layer.2.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.3.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_1.running_mean',\n",
       "              tensor([ 3.2829e-04,  7.9684e-05, -3.9067e-04,  3.0088e-04,  1.1539e-03,\n",
       "                       7.9151e-04,  1.5546e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.3.ln_1.running_var',\n",
       "              tensor([1.0032, 1.0001, 0.9981, 1.0005, 1.0018, 1.0012, 1.0020, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.3.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.3.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.3.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.ln_2.running_mean',\n",
       "              tensor([ 5.1885e-04, -1.7353e-05,  6.4138e-05,  4.6098e-05, -2.4954e-05,\n",
       "                       9.7934e-05,  6.0665e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.3.ln_2.running_var',\n",
       "              tensor([1.0016, 0.9996, 1.0002, 1.0009, 1.0013, 1.0005, 1.0008, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.3.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.3.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.3.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0389,  0.0547, -0.0046,  ...,  0.0490,  0.0367,  0.0450],\n",
       "                      [ 0.0443,  0.0137, -0.0537,  ...,  0.0608,  0.0118,  0.0162],\n",
       "                      [ 0.0591, -0.0548,  0.0054,  ..., -0.0152,  0.0218,  0.0065],\n",
       "                      ...,\n",
       "                      [-0.0008, -0.0357, -0.0606,  ...,  0.0283, -0.0586, -0.0312],\n",
       "                      [ 0.0575,  0.0403,  0.0562,  ...,  0.0497,  0.0530,  0.0142],\n",
       "                      [-0.0390, -0.0097, -0.0130,  ..., -0.0046,  0.0108,  0.0455]])),\n",
       "             ('transformer.layer.3.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0132,  0.0290,  0.0266,  ...,  0.0073, -0.0051, -0.0174],\n",
       "                      [-0.0094,  0.0216,  0.0131,  ...,  0.0116,  0.0095,  0.0058],\n",
       "                      [-0.0235, -0.0242, -0.0092,  ...,  0.0021,  0.0518,  0.0051],\n",
       "                      ...,\n",
       "                      [-0.0250,  0.0070,  0.0064,  ...,  0.0292,  0.0006,  0.0115],\n",
       "                      [ 0.0176,  0.0137,  0.0110,  ...,  0.0090,  0.0021,  0.0092],\n",
       "                      [-0.0235, -0.0317,  0.0083,  ..., -0.0108,  0.0442, -0.0281]])),\n",
       "             ('transformer.layer.3.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.c_proj.weight',\n",
       "              tensor([[ 0.0021, -0.0033, -0.0059,  ...,  0.0064,  0.0006,  0.0078],\n",
       "                      [-0.0030, -0.0047, -0.0054,  ...,  0.0047, -0.0081, -0.0058],\n",
       "                      [ 0.0034, -0.0056, -0.0048,  ..., -0.0025,  0.0039,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0068, -0.0001,  ...,  0.0034, -0.0058, -0.0063],\n",
       "                      [-0.0027, -0.0015, -0.0072,  ..., -0.0043,  0.0033,  0.0024],\n",
       "                      [-0.0027, -0.0006, -0.0066,  ...,  0.0054, -0.0063, -0.0006]])),\n",
       "             ('transformer.layer.3.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.attn.c_attn.weight',\n",
       "              tensor([[-0.0051, -0.0094,  0.0362,  ...,  0.0102, -0.0018, -0.0106],\n",
       "                      [-0.0062, -0.0011,  0.0008,  ...,  0.0213,  0.0162, -0.0060],\n",
       "                      [-0.0139,  0.0381, -0.0222,  ..., -0.0003,  0.0173,  0.0157],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0160, -0.0207,  ..., -0.0105, -0.0099, -0.0205],\n",
       "                      [ 0.0029,  0.0304,  0.0013,  ...,  0.0479, -0.0108,  0.0265],\n",
       "                      [-0.0042, -0.0067, -0.0162,  ..., -0.0108, -0.0162, -0.0116]])),\n",
       "             ('transformer.layer.3.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0037,  0.0058,  0.0034,  ...,  0.0187,  0.0023, -0.0295],\n",
       "                      [ 0.0090,  0.0105, -0.0267,  ...,  0.0088,  0.0205,  0.0167],\n",
       "                      [ 0.0158, -0.0439, -0.0019,  ...,  0.0045, -0.0308,  0.0172],\n",
       "                      ...,\n",
       "                      [ 0.0096, -0.0316, -0.0298,  ..., -0.0002, -0.0119,  0.0110],\n",
       "                      [-0.0121,  0.0056,  0.0060,  ..., -0.0046, -0.0116,  0.0021],\n",
       "                      [-0.0257,  0.0124,  0.0161,  ...,  0.0255, -0.0344,  0.0059]])),\n",
       "             ('transformer.layer.3.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.3.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0119, -0.0018, -0.0029,  ..., -0.0068,  0.0053, -0.0110],\n",
       "                      [-0.0005, -0.0049,  0.0059,  ...,  0.0024, -0.0062, -0.0051],\n",
       "                      [-0.0056, -0.0045,  0.0056,  ...,  0.0018, -0.0039,  0.0032],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0023,  0.0012,  ...,  0.0036, -0.0039, -0.0066],\n",
       "                      [-0.0110,  0.0023, -0.0014,  ...,  0.0039,  0.0045,  0.0019],\n",
       "                      [ 0.0022,  0.0035, -0.0093,  ..., -0.0049,  0.0031,  0.0131]])),\n",
       "             ('transformer.layer.3.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.4.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_1.running_mean',\n",
       "              tensor([ 3.2635e-04, -2.8256e-04,  1.0005e-04, -5.3350e-04, -2.8570e-04,\n",
       "                       3.2666e-05,  1.6110e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.4.ln_1.running_var',\n",
       "              tensor([1.0018, 1.0015, 1.0011, 1.0029, 1.0002, 1.0034, 1.0009, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.4.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.4.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.4.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.ln_2.running_mean',\n",
       "              tensor([ 2.7777e-04, -7.5782e-06,  4.6470e-04,  1.7742e-04,  1.7507e-04,\n",
       "                       2.4828e-04,  3.0388e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.4.ln_2.running_var',\n",
       "              tensor([1.0014, 1.0008, 1.0011, 1.0003, 1.0001, 1.0001, 1.0002, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.4.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.4.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.4.attn.mha.in_proj_weight',\n",
       "              tensor([[ 0.0288,  0.0167, -0.0056,  ..., -0.0444, -0.0450, -0.0176],\n",
       "                      [-0.0128, -0.0411, -0.0231,  ...,  0.0228,  0.0208,  0.0347],\n",
       "                      [ 0.0295, -0.0454, -0.0495,  ...,  0.0355,  0.0312,  0.0217],\n",
       "                      ...,\n",
       "                      [-0.0353,  0.0235, -0.0578,  ...,  0.0458,  0.0509, -0.0581],\n",
       "                      [ 0.0560, -0.0616, -0.0558,  ..., -0.0080,  0.0006, -0.0027],\n",
       "                      [ 0.0542, -0.0512,  0.0507,  ..., -0.0287,  0.0274,  0.0274]])),\n",
       "             ('transformer.layer.4.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0025, -0.0269,  0.0087,  ...,  0.0062,  0.0023, -0.0022],\n",
       "                      [-0.0067,  0.0123,  0.0026,  ...,  0.0013,  0.0171,  0.0008],\n",
       "                      [-0.0402, -0.0033,  0.0219,  ...,  0.0130, -0.0149,  0.0257],\n",
       "                      ...,\n",
       "                      [ 0.0193, -0.0064,  0.0281,  ...,  0.0200, -0.0122, -0.0006],\n",
       "                      [ 0.0288, -0.0130, -0.0002,  ..., -0.0119,  0.0167, -0.0388],\n",
       "                      [ 0.0075, -0.0271, -0.0371,  ...,  0.0276,  0.0036,  0.0315]])),\n",
       "             ('transformer.layer.4.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.c_proj.weight',\n",
       "              tensor([[-0.0038,  0.0054,  0.0035,  ...,  0.0027,  0.0002, -0.0078],\n",
       "                      [ 0.0041, -0.0138, -0.0007,  ...,  0.0011,  0.0094, -0.0085],\n",
       "                      [ 0.0023, -0.0053,  0.0041,  ...,  0.0074,  0.0061,  0.0066],\n",
       "                      ...,\n",
       "                      [-0.0043,  0.0004, -0.0061,  ..., -0.0106,  0.0015,  0.0085],\n",
       "                      [-0.0002,  0.0074, -0.0002,  ..., -0.0001,  0.0027,  0.0038],\n",
       "                      [-0.0015, -0.0053, -0.0002,  ...,  0.0065, -0.0018,  0.0041]])),\n",
       "             ('transformer.layer.4.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.attn.c_attn.weight',\n",
       "              tensor([[-2.4835e-02,  1.1955e-02, -6.7136e-02,  ..., -2.4684e-03,\n",
       "                        7.0792e-04, -3.5418e-03],\n",
       "                      [-7.7227e-03,  2.9037e-02,  1.4865e-02,  ...,  2.2977e-03,\n",
       "                        8.7832e-03,  1.9092e-02],\n",
       "                      [ 1.6047e-02,  1.5633e-02, -3.0294e-02,  ..., -2.1802e-02,\n",
       "                        2.0193e-02, -1.7915e-02],\n",
       "                      ...,\n",
       "                      [ 8.3266e-03, -5.5766e-03, -3.4184e-04,  ..., -2.6712e-02,\n",
       "                        1.0628e-02, -7.3286e-03],\n",
       "                      [ 8.0172e-05,  2.5236e-02, -2.4027e-02,  ...,  6.8065e-03,\n",
       "                       -2.4374e-02, -2.3047e-02],\n",
       "                      [ 3.2499e-02,  6.9854e-02, -8.6880e-03,  ..., -2.6728e-02,\n",
       "                        3.2441e-02,  2.2233e-03]])),\n",
       "             ('transformer.layer.4.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0036,  0.0041,  0.0183,  ...,  0.0494, -0.0374,  0.0044],\n",
       "                      [ 0.0034,  0.0164, -0.0052,  ...,  0.0031, -0.0143,  0.0188],\n",
       "                      [ 0.0069,  0.0424, -0.0017,  ..., -0.0451, -0.0181,  0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0040, -0.0087,  0.0132,  ..., -0.0065, -0.0051,  0.0145],\n",
       "                      [ 0.0207,  0.0211,  0.0350,  ...,  0.0248,  0.0468, -0.0027],\n",
       "                      [-0.0557,  0.0222,  0.0033,  ..., -0.0054, -0.0149,  0.0013]])),\n",
       "             ('transformer.layer.4.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.4.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0074, -0.0085, -0.0010,  ..., -0.0004,  0.0031, -0.0042],\n",
       "                      [-0.0058, -0.0042, -0.0045,  ..., -0.0058, -0.0037,  0.0018],\n",
       "                      [-0.0002,  0.0104,  0.0128,  ..., -0.0037, -0.0017,  0.0053],\n",
       "                      ...,\n",
       "                      [ 0.0042, -0.0072, -0.0086,  ...,  0.0043,  0.0024, -0.0052],\n",
       "                      [-0.0003, -0.0025, -0.0014,  ..., -0.0001,  0.0011, -0.0049],\n",
       "                      [-0.0029,  0.0064, -0.0024,  ...,  0.0004, -0.0012, -0.0014]])),\n",
       "             ('transformer.layer.4.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.5.ln_1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_1.running_mean',\n",
       "              tensor([ 0.0003, -0.0008,  0.0007,  0.0001, -0.0007, -0.0008,  0.0003,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])),\n",
       "             ('transformer.layer.5.ln_1.running_var',\n",
       "              tensor([1.0029, 1.0017, 1.0012, 1.0003, 1.0005, 1.0004, 1.0017, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.5.ln_1.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.5.ln_2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.layer.5.ln_2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.ln_2.running_mean',\n",
       "              tensor([ 4.5155e-06,  8.9975e-05, -1.1019e-04,  7.0603e-05, -2.5189e-05,\n",
       "                       1.0571e-04, -2.2604e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.layer.5.ln_2.running_var',\n",
       "              tensor([1.0013, 1.0013, 1.0002, 1.0012, 1.0012, 1.0010, 1.0002, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.layer.5.ln_2.num_batches_tracked', tensor(2)),\n",
       "             ('transformer.layer.5.attn.bias',\n",
       "              tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., -inf, -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., -inf],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('transformer.layer.5.attn.mha.in_proj_weight',\n",
       "              tensor([[-0.0552, -0.0202,  0.0418,  ...,  0.0427,  0.0432,  0.0091],\n",
       "                      [-0.0469, -0.0605, -0.0385,  ...,  0.0140,  0.0039, -0.0097],\n",
       "                      [ 0.0084,  0.0558,  0.0508,  ..., -0.0349,  0.0123, -0.0434],\n",
       "                      ...,\n",
       "                      [ 0.0624, -0.0111, -0.0322,  ...,  0.0138, -0.0269, -0.0398],\n",
       "                      [ 0.0615,  0.0210, -0.0127,  ..., -0.0515, -0.0250,  0.0516],\n",
       "                      [ 0.0623, -0.0553, -0.0314,  ...,  0.0050, -0.0415,  0.0302]])),\n",
       "             ('transformer.layer.5.attn.mha.in_proj_bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0180, -0.0069,  0.0089,  ...,  0.0063,  0.0368,  0.0064],\n",
       "                      [-0.0074, -0.0292, -0.0286,  ...,  0.0039, -0.0064, -0.0240],\n",
       "                      [ 0.0066, -0.0291,  0.0019,  ...,  0.0124, -0.0039,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0042, -0.0409, -0.0125,  ..., -0.0162, -0.0184,  0.0189],\n",
       "                      [ 0.0309,  0.0147,  0.0079,  ...,  0.0212, -0.0107,  0.0009],\n",
       "                      [ 0.0326, -0.0485,  0.0142,  ..., -0.0021, -0.0056, -0.0245]])),\n",
       "             ('transformer.layer.5.attn.mha.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.c_proj.weight',\n",
       "              tensor([[-0.0057,  0.0021,  0.0007,  ...,  0.0046, -0.0018,  0.0094],\n",
       "                      [ 0.0014, -0.0021,  0.0039,  ..., -0.0113, -0.0012,  0.0037],\n",
       "                      [ 0.0072, -0.0049, -0.0042,  ..., -0.0209,  0.0017, -0.0101],\n",
       "                      ...,\n",
       "                      [ 0.0070, -0.0042, -0.0073,  ...,  0.0070,  0.0155,  0.0055],\n",
       "                      [ 0.0035,  0.0048, -0.0021,  ...,  0.0034, -0.0064, -0.0069],\n",
       "                      [-0.0011,  0.0077,  0.0017,  ..., -0.0101,  0.0052,  0.0053]])),\n",
       "             ('transformer.layer.5.attn.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.attn.c_attn.weight',\n",
       "              tensor([[ 0.0086,  0.0317, -0.0269,  ...,  0.0051, -0.0214,  0.0003],\n",
       "                      [ 0.0209,  0.0240,  0.0088,  ...,  0.0183,  0.0033, -0.0197],\n",
       "                      [-0.0232,  0.0034, -0.0039,  ..., -0.0017, -0.0101,  0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0107, -0.0164,  0.0018,  ...,  0.0171, -0.0490, -0.0148],\n",
       "                      [ 0.0103,  0.0081,  0.0278,  ...,  0.0082,  0.0091, -0.0013],\n",
       "                      [-0.0006, -0.0294,  0.0118,  ..., -0.0003, -0.0514, -0.0054]])),\n",
       "             ('transformer.layer.5.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0098, -0.0088, -0.0083,  ..., -0.0403,  0.0086,  0.0004],\n",
       "                      [ 0.0138, -0.0084, -0.0006,  ...,  0.0082, -0.0178,  0.0213],\n",
       "                      [-0.0442, -0.0084,  0.0196,  ..., -0.0098, -0.0343, -0.0152],\n",
       "                      ...,\n",
       "                      [-0.0279, -0.0112,  0.0004,  ...,  0.0158, -0.0050,  0.0031],\n",
       "                      [ 0.0146, -0.0206, -0.0212,  ..., -0.0042, -0.0131, -0.0034],\n",
       "                      [-0.0178,  0.0145, -0.0050,  ..., -0.0256, -0.0048,  0.0042]])),\n",
       "             ('transformer.layer.5.mlp.c_fc.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('transformer.layer.5.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0066,  0.0081, -0.0019,  ..., -0.0005, -0.0044, -0.0004],\n",
       "                      [-0.0058,  0.0005, -0.0085,  ...,  0.0035,  0.0108,  0.0028],\n",
       "                      [ 0.0018, -0.0017,  0.0020,  ...,  0.0001, -0.0069,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0010,  0.0100, -0.0038,  ..., -0.0043,  0.0062,  0.0075],\n",
       "                      [ 0.0003, -0.0051, -0.0006,  ...,  0.0066,  0.0115,  0.0081],\n",
       "                      [-0.0044,  0.0136, -0.0055,  ...,  0.0048, -0.0028,  0.0094]])),\n",
       "             ('transformer.layer.5.mlp.c_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_out.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('transformer.ln_out.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.ln_out.running_mean',\n",
       "              tensor([-3.1858e-04, -1.3067e-04, -3.4808e-04,  3.2735e-05, -5.1440e-04,\n",
       "                      -1.3033e-03, -2.0179e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "                       0.0000e+00])),\n",
       "             ('transformer.ln_out.running_var',\n",
       "              tensor([1.0022, 1.0021, 1.0015, 1.0017, 1.0011, 1.0021, 1.0015, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100, 0.8100,\n",
       "                      0.8100, 0.8100, 0.8100, 0.8100])),\n",
       "             ('transformer.ln_out.num_batches_tracked', tensor(2)),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[-0.0178,  0.0210,  0.0373,  ...,  0.0198, -0.0191,  0.0009],\n",
       "                      [ 0.0117, -0.0071, -0.0389,  ..., -0.0104, -0.0184, -0.0197],\n",
       "                      [-0.0363,  0.0041, -0.0026,  ..., -0.0074, -0.0342, -0.0105],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0309, -0.0007,  ...,  0.0158,  0.0122,  0.0084],\n",
       "                      [-0.0108, -0.0058, -0.0057,  ...,  0.0121, -0.0143, -0.0220],\n",
       "                      [ 0.0136, -0.0058,  0.0052,  ...,  0.0251, -0.0047,  0.0360]]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_qt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9495eafaba47f881f96c38ca5cf0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_mask = model(ids)\n",
    "with_mask = model(ids, attention_mask=torch.ones(ids.size()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mask.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mask.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.37.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward pass of ONNX models with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-03-12 12:59:49.139317627 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.attn.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139560873 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.mlp.c_fc.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139573813 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.mlp.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139585473 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139598593 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139610803 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139623073 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_1.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139634742 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139646122 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139659972 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139671552 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.0.ln_2.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139682982 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.attn.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139694461 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.mlp.c_fc.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139707011 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.mlp.c_proj.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139718641 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139730371 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139741941 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139754841 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_1.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139766430 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139777940 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139789610 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139801160 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.layer.1.ln_2.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139812450 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139829960 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139841369 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139854139 [W:onnxruntime:, graph.cc:1296 Graph] Initializer transformer.ln_out.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139865949 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::Add_402 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139878049 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_407 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139889909 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_430 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2024-03-12 12:59:49.139903238 [W:onnxruntime:, graph.cc:1296 Graph] Initializer onnx::MatMul_431 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Load model from /home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx failed:Fatal error: onnx.brevitas:Quant(-1) is not a registered function/op",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m h0 \u001b[38;5;241m=\u001b[39m example_input\n\u001b[1;32m      6\u001b[0m c0 \u001b[38;5;241m=\u001b[39m example_input\n\u001b[0;32m----> 7\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/onnxruntime-1.17.1-py3.10-linux-x86_64.egg/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/onnxruntime-1.17.1-py3.10-linux-x86_64.egg/onnxruntime/capi/onnxruntime_inference_collection.py:472\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[0;32m--> 472\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Load model from /home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx failed:Fatal error: onnx.brevitas:Quant(-1) is not a registered function/op"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "example_input = np.random.randn(8, 3, 3).astype(np.float32)\n",
    "h0 = example_input\n",
    "c0 = example_input\n",
    "ort_session = ort.InferenceSession(\"/home/mabot004/eki-transformer-dev/qtransform/qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import qonnx\n",
    "from qonnx.core.onnx_exec import execute_onnx\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "# maybe only do this when it is required, for this howiever is always the case\n",
    "from onnx.shape_inference import infer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "File not found: qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      2\u001b[0m infered_shapes \u001b[38;5;241m=\u001b[39m infer_shapes(model\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelWrapper(infered_shapes)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qonnx-0.3.0-py3.10.egg/qonnx/core/modelwrapper.py:61\u001b[0m, in \u001b[0;36mModelWrapper.__init__\u001b[0;34m(self, onnx_model_proto, make_deepcopy, fix_float64, fix_missing_initializer_valueinfo)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a ModelWrapper instance.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03monnx_model_proto can be either a ModelProto instance, or a string\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mwith the path to a stored .onnx file on disk, or serialized bytes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03minitializers that are missing theirs.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(onnx_model_proto), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_proto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_proto \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(onnx_model_proto)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: File not found: qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper('qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx')    \n",
    "infered_shapes = infer_shapes(model.model)\n",
    "model = ModelWrapper(infered_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "File not found: onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_local \u001b[38;5;241m=\u001b[39m \u001b[43mModelWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      2\u001b[0m infered_shapes \u001b[38;5;241m=\u001b[39m infer_shapes(model_local\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m model_local \u001b[38;5;241m=\u001b[39m ModelWrapper(infered_shapes)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/qonnx-0.3.0-py3.10.egg/qonnx/core/modelwrapper.py:61\u001b[0m, in \u001b[0;36mModelWrapper.__init__\u001b[0;34m(self, onnx_model_proto, make_deepcopy, fix_float64, fix_missing_initializer_valueinfo)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a ModelWrapper instance.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03monnx_model_proto can be either a ModelProto instance, or a string\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mwith the path to a stored .onnx file on disk, or serialized bytes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03minitializers that are missing theirs.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(onnx_model_proto), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_proto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_proto \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(onnx_model_proto)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model_proto, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: File not found: onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx"
     ]
    }
   ],
   "source": [
    "model_local = ModelWrapper('onnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_13_51_49__epoch__1.onnx')    \n",
    "infered_shapes = infer_shapes(model_local.model)\n",
    "model_local = ModelWrapper(infered_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_local.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"offsets\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.wpe.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.in_proj_bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.out_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.mha.out_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.in_proj_bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.out_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.mha.out_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_480\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_498\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_521\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_526\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_527\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_542\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_560\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_583\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_588\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_589\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_594\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"output\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.0.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.attn.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_fc.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1024\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.mlp.c_proj.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_1.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.layer.1.ln_2.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.bias\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_mean\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"transformer.ln_out.running_var\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::Add_402\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 64\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_407\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_430\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"onnx::MatMul_431\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 50304\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper('qonnx_1,64_gpt_2_h2l2e256b64_ReBN_wikitext_2024-03-12_10:27:20__epoch:1.onnx')    \n",
    "infered_shapes = infer_shapes(model.model)\n",
    "model = ModelWrapper(infered_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dim_value: 1\n",
       ", dim_value: 64\n",
       "]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.input[0].type.tensor_type.shape.dim[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_checkpoint = torch.load(\"/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_wikitext_2024-03-14_10:36:21__epoch:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics', 'quant_cfg', 'quantized'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"quantized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[ 0.6698, -0.0029,  0.2328,  ..., -1.2054,  0.7732,  0.2316],\n",
       "                      [ 0.0911, -0.1520, -0.7457,  ...,  1.8444, -0.9675, -0.9812],\n",
       "                      [-0.0984,  0.6329,  0.2754,  ..., -0.1391,  2.0210,  0.7870],\n",
       "                      ...,\n",
       "                      [-0.9953, -0.0249, -0.7582,  ...,  0.1705,  0.7253, -0.0414],\n",
       "                      [-2.0492,  2.4153, -0.2697,  ...,  0.5925, -1.0206, -0.3261],\n",
       "                      [ 0.7156, -0.3761,  0.4326,  ...,  0.1460,  0.1682,  0.8418]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[-0.0486, -0.5337,  1.3270,  ..., -0.0831,  1.0253,  0.8613],\n",
       "                      [-0.4642, -0.4871,  0.6089,  ..., -0.3660, -0.6049,  0.8867],\n",
       "                      [-1.4405,  1.7084, -0.1334,  ...,  1.6913,  1.5892, -0.9947],\n",
       "                      ...,\n",
       "                      [-0.2837,  0.1455, -0.1076,  ...,  0.7258,  0.2620,  0.2455],\n",
       "                      [-0.1508, -3.0918, -0.1600,  ..., -1.3561,  0.2651,  0.6371],\n",
       "                      [ 1.2966, -0.6439,  1.2848,  ...,  1.0981,  1.1735,  0.3455]])),\n",
       "             ('transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.1572)),\n",
       "             ('transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(5.9887)),\n",
       "             ('transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.2637)),\n",
       "             ('transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.6196)),\n",
       "             ('transformer.layer.0.attn.bias',\n",
       "              tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('transformer.layer.0.attn.mha.q_proj.weight',\n",
       "              tensor([[-0.0353, -0.0486, -0.0943,  ..., -0.0095, -0.0193, -0.0703],\n",
       "                      [-0.0410, -0.1019,  0.0073,  ...,  0.0352,  0.0982, -0.0390],\n",
       "                      [ 0.0166,  0.0541, -0.1053,  ..., -0.0907, -0.0621, -0.0965],\n",
       "                      ...,\n",
       "                      [ 0.0250, -0.0172,  0.0713,  ..., -0.0990, -0.0499,  0.0022],\n",
       "                      [-0.0779, -0.0164, -0.0013,  ...,  0.0542, -0.1028, -0.0861],\n",
       "                      [ 0.0231, -0.0997,  0.0967,  ..., -0.0261,  0.0255, -0.0447]])),\n",
       "             ('transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.k_proj.weight',\n",
       "              tensor([[ 0.0275, -0.0980, -0.0098,  ..., -0.0963, -0.0482, -0.0581],\n",
       "                      [ 0.0344, -0.0248,  0.0696,  ..., -0.0318, -0.0561, -0.1058],\n",
       "                      [-0.0317, -0.0666,  0.0526,  ...,  0.0559,  0.0522, -0.1081],\n",
       "                      ...,\n",
       "                      [-0.0652,  0.0517,  0.0046,  ..., -0.0033, -0.0121,  0.0610],\n",
       "                      [-0.0806,  0.0302,  0.0080,  ...,  0.0048, -0.0848, -0.0531],\n",
       "                      [-0.0925,  0.1031, -0.0675,  ..., -0.0274, -0.0781, -0.0959]])),\n",
       "             ('transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.v_proj.weight',\n",
       "              tensor([[-0.0694, -0.0180, -0.0642,  ...,  0.0686,  0.0647,  0.0616],\n",
       "                      [ 0.1048,  0.0642,  0.0051,  ..., -0.0601,  0.0435,  0.1044],\n",
       "                      [-0.0006, -0.0121,  0.0265,  ..., -0.0450, -0.0708,  0.0399],\n",
       "                      ...,\n",
       "                      [-0.0965, -0.0945, -0.0519,  ...,  0.0976, -0.0706, -0.0664],\n",
       "                      [-0.0814,  0.0656,  0.0814,  ...,  0.0848, -0.0545,  0.0496],\n",
       "                      [ 0.0422,  0.0063, -0.0545,  ...,  0.0334, -0.0487,  0.0121]])),\n",
       "             ('transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2526)),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0464, -0.0413, -0.0219,  ...,  0.0377,  0.0037,  0.0251],\n",
       "                      [-0.0533,  0.0077,  0.0039,  ..., -0.0615, -0.0360, -0.0285],\n",
       "                      [ 0.0482,  0.0056, -0.0186,  ..., -0.0167, -0.0321,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0234, -0.0076, -0.0600,  ..., -0.0073,  0.0325,  0.0442],\n",
       "                      [ 0.0251, -0.0013,  0.0266,  ..., -0.0392,  0.0052, -0.0356],\n",
       "                      [ 0.0610,  0.0068, -0.0042,  ..., -0.0568, -0.0237,  0.0556]])),\n",
       "             ('transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(3.3341)),\n",
       "             ('transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(1.1101)),\n",
       "             ('transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(0.3764)),\n",
       "             ('transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2667)),\n",
       "             ('transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2057)),\n",
       "             ('transformer.layer.0.attn.c_proj.weight',\n",
       "              tensor([[ 0.0054,  0.0063,  0.0084,  ...,  0.0132, -0.0037, -0.0191],\n",
       "                      [ 0.0158,  0.0194,  0.0109,  ...,  0.0016, -0.0023, -0.0190],\n",
       "                      [-0.0081, -0.0212, -0.0049,  ..., -0.0082, -0.0038,  0.0013],\n",
       "                      ...,\n",
       "                      [-0.0116, -0.0017, -0.0120,  ...,  0.0028,  0.0094,  0.0177],\n",
       "                      [ 0.0013,  0.0196, -0.0094,  ..., -0.0033, -0.0069,  0.0123],\n",
       "                      [-0.0035, -0.0057, -0.0065,  ..., -0.0120, -0.0158, -0.0079]])),\n",
       "             ('transformer.layer.0.attn.c_proj.bias',\n",
       "              tensor([-3.1958e-03, -2.7938e-03,  4.2348e-03,  6.9851e-04, -4.7405e-04,\n",
       "                      -3.9807e-03, -1.1138e-04, -7.5384e-03, -1.4541e-03, -3.8137e-04,\n",
       "                       4.1511e-03, -5.1171e-04,  2.0046e-03,  9.8140e-03, -1.2610e-03,\n",
       "                       8.6387e-03,  6.5137e-03, -9.7377e-04, -4.7080e-04,  5.3193e-03,\n",
       "                      -3.8207e-03, -2.0345e-03,  3.0142e-03, -2.9232e-04, -5.3041e-03,\n",
       "                       2.0776e-03, -2.1777e-03,  2.4488e-03,  7.2095e-04, -6.2109e-04,\n",
       "                       3.1470e-03, -4.7284e-03,  3.2198e-03, -1.0769e-03, -7.4084e-04,\n",
       "                      -7.5731e-03,  2.5854e-03, -1.8355e-03,  1.9806e-03, -8.9894e-06,\n",
       "                      -5.5757e-03, -3.5957e-03, -1.1352e-03, -2.5207e-03,  6.1156e-03,\n",
       "                      -3.0437e-03, -1.9161e-03, -4.1745e-03, -3.6994e-03,  3.0737e-03,\n",
       "                       2.1440e-03,  1.4744e-03, -6.1409e-04, -2.4526e-03,  7.9957e-03,\n",
       "                       7.2305e-04,  5.4262e-03, -4.7437e-03, -9.0757e-04, -5.3126e-03,\n",
       "                      -4.1813e-03, -3.3698e-03,  5.1947e-04,  8.6721e-03, -3.9753e-04,\n",
       "                       1.6587e-03,  5.4407e-04, -3.6200e-03,  1.0749e-03, -1.1938e-03,\n",
       "                       2.4786e-03, -2.8015e-04,  9.7212e-04, -1.7183e-03, -7.3215e-03,\n",
       "                      -3.7554e-04, -3.9826e-03,  1.8572e-03,  6.4269e-04, -9.3058e-04,\n",
       "                       1.1599e-03, -1.2502e-03,  7.7241e-03,  3.8738e-03,  2.1847e-03,\n",
       "                       4.5545e-03,  7.0674e-04, -4.5284e-04,  2.8594e-03,  6.1084e-03,\n",
       "                       3.5313e-03,  4.5137e-04, -1.8374e-03, -3.9396e-04, -1.8007e-03,\n",
       "                       4.1256e-03, -4.0719e-03, -6.5673e-04, -3.0188e-03, -1.6565e-03,\n",
       "                       1.3387e-03,  8.1945e-03, -1.5822e-03,  3.3044e-03,  9.1487e-04,\n",
       "                      -5.1642e-03, -1.5749e-03, -2.8556e-03,  2.7934e-03, -2.8519e-03,\n",
       "                      -7.0998e-04, -5.9756e-03, -2.8088e-03, -5.3180e-03,  1.5826e-03,\n",
       "                      -2.3094e-03, -7.0455e-04,  1.1676e-02,  4.8946e-03, -3.4066e-03,\n",
       "                      -1.4694e-03,  2.7768e-03,  1.0805e-03,  2.5481e-03, -1.0141e-03,\n",
       "                       6.6496e-03, -7.5653e-05, -5.5602e-04,  2.5933e-03, -5.2210e-03,\n",
       "                      -2.4554e-03,  4.6777e-03, -2.7543e-03,  2.3738e-03,  1.1568e-03,\n",
       "                      -1.6824e-03,  9.0484e-05, -1.9676e-03, -2.5284e-04,  8.3513e-03,\n",
       "                       3.0736e-03, -2.4595e-03,  2.0877e-03, -3.8507e-03,  2.9708e-04,\n",
       "                       3.0518e-03, -2.9975e-03,  4.5871e-03, -2.2610e-03, -3.6620e-03,\n",
       "                      -3.2961e-03,  5.5710e-03, -1.7611e-03, -3.3764e-03, -6.1051e-04,\n",
       "                      -3.0379e-03, -1.3521e-03, -4.5735e-03,  2.2358e-03, -5.7713e-04,\n",
       "                      -2.4444e-03, -4.4614e-03,  4.0799e-04,  2.2649e-03, -9.7010e-04,\n",
       "                       1.2003e-03, -4.2445e-04,  3.6652e-03, -4.1346e-03, -4.8639e-03,\n",
       "                      -3.4428e-03, -2.3577e-04,  1.4184e-03,  4.5522e-03,  4.0986e-03,\n",
       "                      -4.8559e-04, -2.8743e-03,  2.1758e-03,  1.8189e-04, -2.6337e-03,\n",
       "                      -2.7351e-03,  7.4681e-04, -1.3457e-03,  2.4508e-03, -3.9711e-03,\n",
       "                      -9.8269e-04,  1.2864e-03, -1.7708e-03,  7.2904e-04,  2.2170e-03,\n",
       "                      -4.3392e-03, -1.4486e-03,  3.6404e-03,  2.7842e-03, -3.5246e-03,\n",
       "                       3.7262e-03,  3.3332e-03, -2.5613e-03,  2.4462e-03,  4.0126e-04,\n",
       "                      -2.9041e-03, -1.6210e-03, -2.9520e-04,  4.9840e-03, -1.2443e-03,\n",
       "                       3.6681e-03, -3.7540e-03, -3.2127e-04, -2.9366e-03,  5.5271e-03,\n",
       "                      -4.6220e-04,  3.7287e-05, -1.4603e-03,  3.1543e-03, -2.2252e-03,\n",
       "                       2.0338e-03, -3.1750e-03, -3.4455e-03, -2.9756e-03,  4.4175e-04,\n",
       "                       2.0203e-03,  3.7114e-04, -6.8246e-04, -1.7589e-03,  3.1209e-03,\n",
       "                      -5.3027e-03, -1.9076e-03, -2.8565e-03,  4.9105e-04,  2.4106e-03,\n",
       "                      -2.1508e-03,  1.3770e-03, -2.6839e-03, -1.3049e-03, -5.9493e-04,\n",
       "                       4.8399e-03, -4.0349e-03, -1.1218e-03,  1.0545e-03,  2.8495e-03,\n",
       "                       1.7935e-03,  2.6863e-03,  1.1054e-04, -8.5209e-04,  2.7937e-03,\n",
       "                       9.3768e-03, -4.6535e-03, -1.8112e-03, -2.8445e-03,  3.9698e-03,\n",
       "                       3.3034e-03, -5.9716e-04,  5.1642e-03,  6.4905e-03,  4.2956e-03,\n",
       "                      -6.7447e-04])),\n",
       "             ('transformer.layer.0.attn.c_attn.weight',\n",
       "              tensor([[-0.0237,  0.0118,  0.0182,  ..., -0.0220, -0.0080,  0.0096],\n",
       "                      [-0.0032, -0.0402, -0.0430,  ..., -0.0288, -0.0002, -0.0157],\n",
       "                      [ 0.0035, -0.0334, -0.0126,  ..., -0.0387, -0.0046,  0.0054],\n",
       "                      ...,\n",
       "                      [-0.0193, -0.0057, -0.0154,  ...,  0.0353, -0.0240,  0.0698],\n",
       "                      [-0.0259, -0.0031,  0.0203,  ...,  0.0257, -0.0133,  0.0197],\n",
       "                      [ 0.0049,  0.0175, -0.0023,  ...,  0.0252, -0.0217,  0.0100]])),\n",
       "             ('transformer.layer.0.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.0.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0454, -0.0613, -0.0267,  ...,  0.0355, -0.0575,  0.0129],\n",
       "                      [ 0.0580, -0.0514, -0.0221,  ...,  0.0337, -0.0567,  0.0515],\n",
       "                      [-0.0287, -0.0080, -0.0489,  ..., -0.0184,  0.0468, -0.0448],\n",
       "                      ...,\n",
       "                      [-0.0503, -0.0550, -0.0385,  ...,  0.0122,  0.0277,  0.0185],\n",
       "                      [-0.0028, -0.0253, -0.0323,  ...,  0.0527,  0.0582,  0.0172],\n",
       "                      [ 0.0035, -0.0061, -0.0270,  ...,  0.0276, -0.0205, -0.0449]])),\n",
       "             ('transformer.layer.0.mlp.c_fc.bias',\n",
       "              tensor([-0.0563, -0.0550, -0.0205,  ...,  0.0098, -0.0153, -0.0202])),\n",
       "             ('transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5672)),\n",
       "             ('transformer.layer.0.mlp.c_proj.weight',\n",
       "              tensor([[ 6.5457e-05,  1.2368e-02,  2.4754e-02,  ...,  1.8065e-02,\n",
       "                       -1.1650e-02, -1.5157e-02],\n",
       "                      [ 3.5525e-03,  2.0345e-02,  6.8030e-03,  ...,  1.3600e-02,\n",
       "                        2.0385e-03,  8.7703e-03],\n",
       "                      [-1.8472e-03, -3.0831e-03, -1.6212e-03,  ..., -2.8142e-03,\n",
       "                        2.7566e-02,  1.9313e-02],\n",
       "                      ...,\n",
       "                      [ 9.2666e-03, -3.1960e-03, -2.4263e-02,  ..., -4.1364e-03,\n",
       "                       -1.8560e-02, -2.1037e-02],\n",
       "                      [ 7.8176e-03, -1.1376e-02, -2.8996e-03,  ...,  1.3448e-02,\n",
       "                       -6.5596e-03, -6.7725e-03],\n",
       "                      [-1.3990e-02, -1.9434e-02, -3.0217e-02,  ...,  2.6123e-02,\n",
       "                       -2.9926e-02,  1.8299e-02]])),\n",
       "             ('transformer.layer.0.mlp.c_proj.bias',\n",
       "              tensor([ 2.5560e-02, -3.6254e-03,  2.1335e-02, -5.9091e-03,  4.8277e-03,\n",
       "                      -2.8918e-02,  1.7541e-04, -1.0784e-02, -3.0900e-02,  3.1523e-03,\n",
       "                       2.2230e-02,  1.9318e-02,  1.9482e-02,  2.3477e-02, -1.7252e-02,\n",
       "                      -2.9714e-02, -2.1485e-02,  2.1345e-02, -6.1715e-03,  1.3268e-02,\n",
       "                      -1.2446e-02, -1.6207e-03,  3.8003e-03,  1.3590e-02, -8.2977e-03,\n",
       "                      -2.1442e-04,  8.8348e-03, -2.2914e-02, -1.5780e-02,  5.2340e-03,\n",
       "                      -2.8551e-02, -1.6331e-02,  1.2741e-02, -1.9215e-02,  1.2352e-02,\n",
       "                       2.0115e-02,  1.3922e-02, -2.7000e-02, -1.2176e-02,  1.7833e-04,\n",
       "                      -2.9944e-02, -2.8255e-02,  5.4421e-03, -5.5175e-03,  1.6391e-02,\n",
       "                       7.5466e-03,  8.2707e-03, -2.7863e-02,  7.6405e-03, -3.0874e-02,\n",
       "                       5.7807e-03, -2.0139e-02, -1.4726e-02, -7.2625e-03,  7.4324e-03,\n",
       "                      -1.2648e-03, -2.1311e-02,  1.4025e-02,  1.2524e-02, -5.9381e-03,\n",
       "                       7.8724e-03, -1.5685e-02, -2.4763e-02, -2.9692e-02,  1.0313e-02,\n",
       "                       1.5932e-02,  8.0940e-03,  2.0063e-03, -2.6461e-02,  2.2808e-03,\n",
       "                      -9.6476e-03,  1.8935e-02, -1.8500e-02, -7.4442e-03,  2.4223e-02,\n",
       "                       1.9091e-02,  1.2080e-02, -2.3178e-02, -1.3817e-02,  1.2120e-02,\n",
       "                       1.3733e-02, -1.9721e-02,  3.4679e-04,  1.4133e-02,  1.0491e-02,\n",
       "                       4.2477e-03,  6.9141e-03, -1.3758e-03,  2.3635e-02,  3.5476e-03,\n",
       "                      -2.9612e-02,  1.6437e-02,  1.2660e-02,  1.1188e-02,  2.7275e-02,\n",
       "                      -2.5706e-02,  2.5976e-02, -1.9942e-02, -2.1187e-02, -3.2883e-03,\n",
       "                      -9.7723e-03,  7.2397e-05, -1.3205e-02,  1.1411e-02, -2.4637e-02,\n",
       "                      -9.2218e-03, -2.6060e-02,  7.1590e-03,  1.5343e-02, -1.9596e-02,\n",
       "                       2.8486e-03, -8.7011e-03,  1.0588e-02,  2.4273e-02,  2.9334e-02,\n",
       "                       1.8043e-02, -9.3513e-03,  1.5989e-02,  2.9546e-02,  1.7701e-02,\n",
       "                       2.8502e-02,  1.6575e-02, -2.2003e-02,  1.8809e-02,  1.7890e-02,\n",
       "                       1.5180e-02, -1.5251e-02,  1.1484e-02,  2.1115e-02, -1.4904e-02,\n",
       "                      -3.0148e-03,  1.0726e-02, -1.0897e-02, -8.2914e-03, -2.5162e-03,\n",
       "                       2.4354e-02, -1.7783e-02,  1.2528e-02,  1.4450e-02, -1.0135e-02,\n",
       "                       2.0730e-03,  2.2086e-02, -1.5699e-02, -1.7384e-02,  2.5103e-02,\n",
       "                      -1.7455e-02, -1.5156e-02, -1.8509e-02,  1.7692e-02,  7.7175e-03,\n",
       "                       2.9849e-02, -4.6452e-03,  2.3786e-02,  4.6916e-03,  1.2176e-03,\n",
       "                      -1.8175e-02,  2.3829e-02,  2.0415e-02, -1.2483e-03, -2.5004e-02,\n",
       "                      -1.8008e-02, -7.0233e-03,  3.2495e-03, -1.5493e-02, -1.0343e-03,\n",
       "                      -2.5326e-02,  4.0800e-03,  1.0004e-02,  3.1022e-02,  1.0129e-02,\n",
       "                       1.4329e-02,  2.5901e-02, -2.5397e-02, -5.4523e-04,  1.0502e-02,\n",
       "                      -2.2719e-02,  2.6889e-02,  9.6757e-03,  5.7213e-04,  1.7956e-02,\n",
       "                       3.5283e-03, -2.7490e-02,  5.7966e-03, -1.3353e-02, -2.7805e-02,\n",
       "                      -1.8380e-02,  2.0327e-02, -2.1421e-02,  2.7386e-02,  2.0206e-02,\n",
       "                       2.1745e-02,  3.1120e-02, -1.2887e-02,  1.8633e-02,  6.9381e-03,\n",
       "                       1.3052e-02, -2.7839e-02, -2.4401e-02,  2.6906e-02,  1.5014e-02,\n",
       "                       5.2837e-03, -1.2443e-02,  6.0333e-03, -1.6458e-02,  2.9455e-02,\n",
       "                       5.3619e-03,  1.6728e-02, -2.0348e-02,  2.0520e-02,  1.8688e-02,\n",
       "                       3.0824e-02,  4.0416e-03,  1.8692e-02,  6.2782e-03,  1.6105e-02,\n",
       "                      -2.4185e-02, -1.9630e-02,  2.6541e-02,  1.3325e-02, -1.9297e-02,\n",
       "                       1.8631e-02,  2.3606e-02,  1.5237e-02, -1.1671e-02,  1.1380e-02,\n",
       "                      -8.6316e-03, -2.8438e-02,  1.2729e-02,  6.3229e-03, -2.4172e-02,\n",
       "                       1.8103e-02, -2.1087e-02, -1.2268e-02, -3.1685e-04, -2.5833e-02,\n",
       "                       2.5406e-02, -1.9458e-02, -2.1070e-02,  6.4020e-03, -7.5140e-03,\n",
       "                       3.0642e-02, -5.8846e-03,  1.8289e-02, -2.1852e-02,  2.1584e-02,\n",
       "                      -2.5861e-02, -2.1236e-03,  7.7794e-03, -2.8326e-04,  2.5003e-02,\n",
       "                      -2.5294e-02, -1.2434e-02,  2.6409e-02,  8.4354e-03, -3.6013e-03,\n",
       "                      -2.0866e-02])),\n",
       "             ('transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4906)),\n",
       "             ('transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5593)),\n",
       "             ('transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4906)),\n",
       "             ('transformer.layer.0.ln_1.bn.weight',\n",
       "              tensor([0.9931, 0.9964, 0.9996, 1.0089, 0.9846, 0.9893, 0.9935, 1.0041, 0.9947,\n",
       "                      0.9868, 0.9935, 1.0011, 0.9954, 0.9945, 0.9981, 0.9823, 0.9943, 0.9905,\n",
       "                      1.0093, 0.9953, 1.0007, 0.9961, 0.9982, 1.0007, 0.9957, 0.9958, 0.9934,\n",
       "                      1.0037, 0.9926, 0.9979, 0.9937, 0.9859, 0.9978, 0.9925, 1.0059, 0.9872,\n",
       "                      0.9991, 0.9992, 0.9887, 0.9935, 0.9950, 0.9894, 0.9937, 0.9980, 0.9960,\n",
       "                      1.0071, 0.9849, 1.0003, 1.0057, 1.0005, 0.9998, 0.9969, 1.0061, 0.9861,\n",
       "                      0.9965, 0.9929, 0.9949, 1.0012, 0.9946, 0.9863, 0.9972, 0.9977, 0.9952,\n",
       "                      1.0003, 0.9967, 0.9910, 1.0025, 0.9942, 1.0001, 0.9992, 0.9872, 1.0018,\n",
       "                      1.0033, 1.0022, 0.9917, 1.0020, 1.0041, 0.9822, 0.9917, 0.9992, 1.0023,\n",
       "                      1.0033, 0.9977, 0.9922, 0.9940, 1.0029, 0.9909, 0.9966, 0.9934, 0.9997,\n",
       "                      1.0022, 0.9961, 0.9876, 0.9942, 0.9997, 0.9958, 0.9957, 0.9874, 0.9925,\n",
       "                      0.9927, 0.9954, 1.0008, 0.9965, 0.9964, 0.9888, 0.9865, 1.0005, 0.9952,\n",
       "                      0.9991, 0.9934, 0.9925, 0.9914, 0.9993, 0.9969, 1.0064, 0.9983, 1.0027,\n",
       "                      0.9896, 0.9994, 0.9823, 0.9940, 0.9944, 0.9917, 0.9996, 0.9988, 0.9996,\n",
       "                      0.9940, 0.9971, 0.9902, 0.9962, 1.0002, 0.9980, 0.9979, 0.9946, 0.9991,\n",
       "                      0.9926, 0.9905, 0.9983, 0.9967, 0.9802, 0.9931, 0.9935, 0.9981, 0.9934,\n",
       "                      0.9946, 0.9999, 0.9920, 0.9984, 0.9955, 0.9990, 0.9793, 1.0057, 0.9948,\n",
       "                      1.0071, 0.9934, 0.9839, 0.9964, 0.9729, 0.9916, 0.9941, 0.9915, 0.9957,\n",
       "                      0.9937, 0.9998, 0.9968, 0.9970, 0.9811, 0.9944, 0.9993, 0.9956, 0.9942,\n",
       "                      0.9990, 1.0001, 1.0033, 0.9995, 0.9908, 0.9855, 1.0001, 0.9945, 0.9962,\n",
       "                      0.9928, 0.9983, 0.9875, 0.9871, 0.9992, 0.9883, 1.0063, 0.9916, 0.9920,\n",
       "                      0.9912, 0.9965, 0.9915, 0.9952, 1.0028, 0.9948, 0.9945, 0.9943, 0.9994,\n",
       "                      1.0002, 1.0039, 0.9967, 1.0038, 1.0025, 1.0036, 0.9988, 1.0007, 0.9897,\n",
       "                      0.9949, 0.9980, 0.9977, 1.0015, 0.9998, 0.9968, 0.9986, 0.9988, 0.9827,\n",
       "                      1.0026, 0.9923, 0.9972, 0.9920, 0.9867, 0.9944, 0.9912, 0.9886, 0.9922,\n",
       "                      0.9883, 0.9926, 1.0009, 1.0035, 0.9835, 1.0175, 1.0001, 0.9956, 0.9975,\n",
       "                      0.9970, 0.9998, 0.9859, 0.9906, 0.9966, 0.9906, 0.9996, 0.9933, 0.9942,\n",
       "                      0.9976, 0.9945, 0.9972, 1.0051, 1.0059, 0.9902, 0.9909, 1.0032, 0.9979,\n",
       "                      0.9900, 1.0000, 0.9957, 0.9918])),\n",
       "             ('transformer.layer.0.ln_1.bn.bias',\n",
       "              tensor([ 3.1101e-04,  3.5736e-03, -1.9809e-04,  2.6513e-03,  9.1775e-03,\n",
       "                      -2.4963e-03,  2.3886e-03,  5.9799e-04,  6.1797e-03,  2.2492e-03,\n",
       "                      -1.9250e-04,  2.9204e-03,  7.4876e-03, -7.0627e-03, -3.7569e-03,\n",
       "                      -8.6532e-04, -4.2497e-03,  1.3495e-03, -4.0327e-04, -8.8196e-04,\n",
       "                       5.5398e-03, -1.0220e-02,  4.6685e-04,  1.7534e-03, -2.9860e-03,\n",
       "                      -6.8689e-03, -1.5521e-03, -2.6503e-03, -6.0754e-03, -3.9257e-03,\n",
       "                       1.5364e-02,  5.2607e-03, -6.0376e-04, -2.1966e-03,  2.7285e-04,\n",
       "                      -2.8595e-03, -6.5942e-04, -4.3354e-04,  1.1670e-02, -1.0760e-02,\n",
       "                       2.1742e-03,  1.6508e-03,  9.5419e-03,  3.2297e-03, -2.5104e-03,\n",
       "                       4.5822e-03,  7.8498e-03,  1.0039e-02,  7.0227e-03,  6.1845e-03,\n",
       "                       1.3724e-02,  3.6781e-03,  7.1948e-03, -1.0618e-03, -1.2964e-03,\n",
       "                       2.8245e-03,  1.0357e-03,  9.5720e-03, -5.4346e-03, -1.0623e-03,\n",
       "                      -3.9675e-03, -1.7652e-03,  8.9791e-03,  4.5481e-03,  3.2018e-03,\n",
       "                      -9.3412e-03,  2.0188e-03, -2.9314e-03, -2.2752e-03,  3.5520e-03,\n",
       "                      -3.3860e-03,  5.0325e-04, -1.7496e-03, -8.2058e-03, -1.5648e-03,\n",
       "                       3.5578e-03, -8.6766e-03, -9.6731e-03,  1.3405e-04, -4.0559e-03,\n",
       "                      -1.3148e-02,  4.3693e-03, -3.8709e-03, -2.1100e-03,  3.6705e-03,\n",
       "                       4.2317e-04, -5.4165e-03,  1.0699e-02, -1.2027e-02, -5.2371e-03,\n",
       "                       3.3795e-03, -9.5000e-03,  4.9961e-03, -5.1793e-03, -4.5933e-03,\n",
       "                       3.2173e-03,  1.0090e-03,  7.6554e-03,  3.7186e-03,  1.8136e-03,\n",
       "                      -7.0848e-03,  5.4481e-03, -3.7139e-03, -8.7814e-03, -7.1024e-03,\n",
       "                      -2.8528e-03, -5.4810e-03,  9.5472e-04,  7.5642e-04, -7.5182e-03,\n",
       "                      -1.9049e-03,  9.2364e-04,  3.3527e-03, -2.8002e-03,  5.2739e-03,\n",
       "                       2.6143e-03, -4.4881e-03, -3.5840e-03, -2.9885e-03,  1.1135e-02,\n",
       "                       3.9166e-03, -6.2252e-03, -4.4337e-03,  2.0772e-03,  1.9236e-03,\n",
       "                       6.8115e-04,  1.2582e-02,  7.9471e-03, -1.2734e-03,  4.6074e-03,\n",
       "                       7.1962e-03,  5.8031e-03, -3.8361e-03,  1.7625e-03,  7.3836e-03,\n",
       "                      -8.2421e-03,  1.3143e-03,  1.3548e-02, -4.6842e-03,  6.5980e-03,\n",
       "                       1.2809e-03,  1.9124e-03, -3.2874e-04,  1.5460e-03, -2.9076e-03,\n",
       "                       5.7855e-03, -7.2487e-03, -3.9049e-03,  2.0109e-03,  8.4380e-03,\n",
       "                       7.5462e-03,  4.4747e-03,  5.5726e-03, -5.7470e-05, -6.4852e-03,\n",
       "                       4.5246e-03, -1.1324e-03, -3.1481e-02,  1.8122e-03,  1.2636e-03,\n",
       "                      -2.7692e-03, -1.1063e-02,  7.7906e-03,  3.4274e-03, -2.8712e-03,\n",
       "                       1.6354e-03,  1.8837e-04,  3.1335e-04,  6.0853e-03,  9.7286e-03,\n",
       "                      -1.1463e-03,  7.6757e-03,  5.2650e-03,  3.4889e-03, -5.5663e-04,\n",
       "                      -1.5003e-03,  2.4190e-03,  4.7878e-04, -1.1356e-02, -4.7840e-04,\n",
       "                       2.4219e-03,  4.7471e-03, -6.9031e-03, -6.0700e-04, -7.4873e-03,\n",
       "                       9.9954e-04,  4.1532e-03,  8.8810e-03,  1.6333e-03, -3.2158e-03,\n",
       "                       8.2802e-03,  1.5118e-03, -2.9279e-03,  3.8373e-03,  1.9230e-03,\n",
       "                       8.7103e-04,  2.1916e-03, -6.3128e-03,  5.2371e-03,  4.4822e-03,\n",
       "                       5.3361e-03,  4.0997e-03, -7.7335e-03,  7.7373e-04,  3.2307e-04,\n",
       "                      -7.1923e-04,  1.0442e-02, -7.5178e-03, -2.8811e-03,  2.0657e-03,\n",
       "                      -8.4339e-03,  5.2320e-03, -4.0905e-04,  1.0169e-03,  7.9803e-03,\n",
       "                       1.5938e-03, -1.6011e-03,  6.0652e-03, -5.2725e-04,  1.9728e-03,\n",
       "                       9.0869e-04, -2.1465e-03, -1.5243e-04, -6.3015e-03, -1.0420e-03,\n",
       "                      -2.2566e-03,  3.5020e-03, -3.5536e-04, -4.9905e-04,  3.7193e-05,\n",
       "                      -2.6920e-02, -3.9275e-03,  1.2735e-03,  3.4936e-03,  5.7107e-03,\n",
       "                      -1.7339e-03, -3.1639e-04,  4.4286e-03,  3.1310e-03,  4.1988e-05,\n",
       "                       4.1158e-03, -1.5461e-03,  4.9999e-04, -1.4872e-03,  2.2923e-04,\n",
       "                       5.5016e-03,  6.0403e-03, -1.8667e-04, -3.1868e-03, -1.2550e-03,\n",
       "                      -1.9593e-03, -2.7976e-03, -1.7939e-03,  7.7737e-03,  2.8359e-03,\n",
       "                      -5.9998e-03])),\n",
       "             ('transformer.layer.0.ln_1.bn.running_mean',\n",
       "              tensor([ 8.9613e-03, -1.2370e-01,  1.2264e-02, -3.6761e-02, -1.2689e-01,\n",
       "                       5.6001e-02,  6.5052e-02,  1.2855e-01, -8.5032e-02,  1.4266e-02,\n",
       "                       6.1345e-02,  1.2547e-01, -1.5445e-02,  3.7252e-01, -1.9607e-01,\n",
       "                      -2.1439e-01,  3.2442e-02, -1.4839e-01,  3.6336e-02,  2.4684e-01,\n",
       "                      -4.8439e-02,  2.1209e-01, -1.8866e-02, -1.6488e-01, -2.7249e-02,\n",
       "                       3.2804e-02, -2.6093e-02,  1.3427e-01, -8.9339e-02,  9.2865e-02,\n",
       "                      -7.5938e-02, -9.5080e-02,  2.4438e-02, -1.6972e-01, -2.4107e-01,\n",
       "                      -4.3595e-02,  1.7633e-01,  3.2695e-02,  3.6585e-03,  1.0136e-01,\n",
       "                      -1.7988e-01, -3.1662e-01,  1.6895e-02, -1.4918e-01, -9.8692e-02,\n",
       "                      -1.1284e-01, -4.4821e-02,  3.6330e-03, -1.4188e-01,  1.0072e-01,\n",
       "                      -1.7571e-01, -8.0654e-02, -1.2633e-01, -2.3966e-02,  7.7860e-03,\n",
       "                       7.1017e-02,  1.3281e-01, -1.2771e-01, -1.2919e-01, -3.2759e-01,\n",
       "                      -2.1682e-01, -1.2878e-01, -1.8552e-01, -2.4980e-01, -7.1415e-02,\n",
       "                       2.4791e-01, -9.0915e-02,  2.2548e-02,  5.9753e-02,  9.4027e-02,\n",
       "                      -4.0220e-02,  1.8115e-01,  5.6399e-02,  4.2331e-02, -6.6840e-02,\n",
       "                      -2.9072e-02, -3.3451e-01, -6.9308e-02, -3.6482e-02,  2.5665e-02,\n",
       "                      -2.5645e-01,  9.5390e-02, -3.5530e-02,  8.6358e-02,  1.2303e-01,\n",
       "                       1.8283e-01, -3.4372e-01, -9.1104e-02, -1.1298e-02, -6.3889e-02,\n",
       "                       6.3522e-02,  2.2962e-01, -7.2341e-02, -4.6083e-02,  1.0854e-01,\n",
       "                      -3.5479e-01,  4.6228e-02,  1.2360e-01,  2.8492e-01, -1.0071e-01,\n",
       "                       7.4886e-02, -4.0985e-02, -7.5252e-03, -9.5699e-02,  1.0469e-01,\n",
       "                       1.2558e-01,  8.2541e-02,  4.9260e-02,  1.0244e-01,  2.6517e-02,\n",
       "                       9.8550e-02,  1.4047e-02, -1.6093e-02, -1.0230e-01, -1.1157e-01,\n",
       "                      -1.1022e-01,  2.6249e-01, -9.7288e-02, -1.1720e-01,  9.1511e-02,\n",
       "                      -4.3909e-02, -2.4690e-01, -2.9092e-01, -1.4486e-01,  1.4933e-01,\n",
       "                      -8.4626e-02, -2.1428e-01, -1.5468e-01, -1.5313e-02,  2.4612e-02,\n",
       "                      -6.0043e-02, -1.0013e-01,  9.2507e-02, -6.6692e-03,  7.0078e-02,\n",
       "                       6.3382e-02,  2.1417e-02,  4.2641e-02, -2.0347e-01, -1.0468e-01,\n",
       "                      -1.7609e-01,  8.5733e-02, -3.3848e-02,  1.2050e-02, -2.6971e-01,\n",
       "                       9.6207e-02, -1.6236e-01,  2.4704e-01, -5.5348e-03, -2.1625e-01,\n",
       "                      -2.2096e-01,  7.0111e-02,  3.1566e-01, -4.7322e-02,  4.3996e-02,\n",
       "                      -4.5475e-02, -5.8858e-02,  1.5261e-01, -1.1475e-01, -3.5395e-03,\n",
       "                       1.8488e-01, -1.8400e-01, -5.6036e-02, -3.0918e-01, -1.7152e-02,\n",
       "                       3.5248e-02,  1.2123e-01, -3.5578e-02,  2.1033e-02,  7.4461e-02,\n",
       "                       2.4327e-02, -9.2737e-02, -2.1670e-01, -3.3930e-02,  9.7354e-02,\n",
       "                      -1.8130e-01,  1.5200e-01,  9.3920e-03,  5.3919e-02,  9.6113e-02,\n",
       "                      -3.3220e-05, -2.1484e-02, -6.2922e-02, -9.7986e-02,  2.3778e-01,\n",
       "                      -6.2653e-02,  4.7574e-02, -1.6614e-01, -5.0956e-02, -1.7901e-01,\n",
       "                      -2.0242e-01,  1.6595e-01,  8.4326e-03, -2.0296e-02,  3.3846e-02,\n",
       "                      -1.0240e-01,  2.2613e-01,  7.4510e-02,  1.8720e-02,  1.2926e-01,\n",
       "                      -6.9275e-03,  1.7067e-01,  8.2102e-02, -4.9122e-04,  1.2157e-01,\n",
       "                       1.0372e-01, -1.9239e-01, -2.7830e-01,  1.9367e-01,  1.2532e-01,\n",
       "                      -7.9966e-02, -1.0177e-01,  6.2981e-02,  9.1120e-04,  8.9377e-02,\n",
       "                      -3.2817e-01,  4.8527e-02,  6.6968e-02,  3.9556e-02,  5.4926e-02,\n",
       "                       1.3325e-01,  1.0052e-02, -2.0190e-01,  3.1939e-01,  8.0386e-03,\n",
       "                      -8.2851e-02, -1.0221e-01,  7.1491e-02, -1.2475e-01,  1.3005e-01,\n",
       "                      -2.4310e-01,  1.3214e-01,  1.7610e-01,  8.8242e-02, -4.4540e-01,\n",
       "                      -1.3117e-01,  1.1320e-01,  1.4869e-02, -2.1633e-01,  7.6324e-02,\n",
       "                      -4.5106e-02, -4.1815e-02,  7.4547e-02,  5.6021e-02, -9.8935e-02,\n",
       "                       2.1980e-01,  1.0372e-01,  9.8856e-02, -2.0176e-01, -1.2405e-01,\n",
       "                       1.5523e-01,  3.4723e-02, -1.2795e-01,  1.2103e-01,  6.1370e-02,\n",
       "                       3.0158e-01])),\n",
       "             ('transformer.layer.0.ln_1.bn.running_var',\n",
       "              tensor([1.8630, 2.7696, 2.0784, 2.2451, 2.5418, 2.1541, 2.0901, 1.9175, 2.4395,\n",
       "                      2.6734, 1.9732, 2.1172, 1.9562, 2.3357, 2.2713, 2.4075, 2.1341, 2.1796,\n",
       "                      2.1201, 2.2759, 2.3647, 2.1635, 2.1766, 2.3060, 1.9830, 2.4331, 1.9320,\n",
       "                      2.3622, 2.1974, 2.4310, 2.3769, 2.2353, 1.8425, 2.3998, 2.8219, 2.0226,\n",
       "                      2.5734, 2.3142, 2.6012, 2.2048, 2.1489, 2.9597, 2.1346, 2.2536, 2.0015,\n",
       "                      2.5245, 2.4192, 1.9835, 2.2949, 1.8772, 2.4173, 2.4063, 2.0481, 1.9125,\n",
       "                      2.0900, 2.2612, 2.2144, 2.1282, 2.1192, 2.7872, 2.5928, 1.9631, 1.8723,\n",
       "                      2.5845, 2.0100, 2.5560, 1.9949, 2.2161, 2.3845, 2.1990, 1.8597, 2.0815,\n",
       "                      2.6053, 2.0464, 1.8208, 1.8994, 2.1686, 2.3446, 2.2001, 2.0110, 2.1137,\n",
       "                      2.1000, 2.5426, 1.9756, 2.1436, 2.0523, 3.0309, 2.1023, 2.1708, 2.0207,\n",
       "                      2.0158, 2.5429, 2.1344, 2.2058, 2.3356, 2.3804, 2.3058, 2.0681, 2.1153,\n",
       "                      2.1283, 2.0516, 2.6495, 2.3169, 1.9235, 1.9665, 2.4516, 2.5483, 2.2798,\n",
       "                      2.1599, 2.0456, 1.9137, 2.2804, 2.3421, 2.3374, 2.6425, 2.1271, 1.7549,\n",
       "                      2.0471, 1.8564, 2.3790, 2.2713, 2.2933, 2.1576, 2.0121, 2.3519, 2.1794,\n",
       "                      2.0212, 2.0229, 2.1946, 1.8762, 2.0886, 2.0122, 2.1047, 2.1856, 2.1119,\n",
       "                      2.2480, 2.2446, 2.2901, 2.1857, 1.9344, 2.1660, 2.2110, 2.1276, 2.6364,\n",
       "                      2.0116, 2.2875, 2.1258, 2.1786, 1.8827, 2.1022, 2.0931, 2.1815, 2.2863,\n",
       "                      2.1692, 2.0579, 1.8030, 2.0218, 2.5830, 2.2119, 2.1736, 2.3562, 1.9443,\n",
       "                      2.1095, 2.2936, 2.0598, 2.4091, 2.2420, 2.4007, 2.2670, 2.5419, 2.4926,\n",
       "                      2.6086, 2.0493, 2.3237, 1.9104, 2.2876, 2.2403, 2.2264, 2.2652, 2.0055,\n",
       "                      2.1918, 2.1658, 2.3181, 1.9615, 2.1852, 2.5133, 1.7870, 1.8803, 2.1345,\n",
       "                      1.9570, 2.3325, 2.0299, 1.9512, 2.1836, 2.2559, 2.1994, 1.8379, 2.4424,\n",
       "                      2.1583, 2.1971, 2.0699, 2.0430, 2.1637, 2.1366, 1.9638, 2.1158, 2.5285,\n",
       "                      2.1851, 2.5068, 2.0872, 2.3871, 2.1194, 2.3151, 2.2502, 2.1883, 2.2507,\n",
       "                      2.0095, 2.6236, 2.1232, 1.8058, 2.7215, 2.4070, 2.2099, 2.3653, 2.0759,\n",
       "                      2.3096, 1.9526, 2.2820, 2.5435, 2.3583, 2.0978, 2.1407, 2.4108, 2.0108,\n",
       "                      2.3646, 1.8780, 2.0884, 2.0445, 2.2944, 1.9166, 1.8874, 2.0484, 2.0168,\n",
       "                      2.1996, 2.3855, 2.0565, 2.3479, 1.9634, 2.2511, 2.2176, 2.0768, 2.1958,\n",
       "                      2.2830, 2.1369, 2.0713, 1.9631])),\n",
       "             ('transformer.layer.0.ln_1.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2659)),\n",
       "             ('transformer.layer.0.ln_2.bn.weight',\n",
       "              tensor([1.0036, 1.0194, 0.9903, 1.0046, 0.9668, 0.9915, 1.0093, 1.0029, 0.9824,\n",
       "                      1.0044, 0.9963, 0.9974, 0.9948, 1.0061, 0.9953, 0.9817, 0.9904, 0.9620,\n",
       "                      0.9871, 0.9723, 0.9851, 0.9824, 0.9792, 0.9925, 1.0087, 1.0253, 0.9839,\n",
       "                      1.0104, 0.9893, 0.9797, 0.9759, 0.9899, 0.9942, 0.9953, 0.9906, 1.0039,\n",
       "                      1.0147, 1.0075, 0.9724, 1.0022, 1.0003, 1.0028, 0.9894, 1.0075, 0.9792,\n",
       "                      0.9887, 0.9926, 1.0121, 0.9879, 0.9824, 1.0104, 1.0076, 0.9739, 0.9821,\n",
       "                      0.9873, 1.0014, 0.9782, 1.0091, 1.0031, 0.9699, 1.0057, 0.9779, 0.9746,\n",
       "                      0.9930, 1.0021, 1.0102, 1.0050, 0.9886, 0.9952, 0.9755, 0.9964, 0.9978,\n",
       "                      0.9852, 0.9986, 0.9861, 1.0213, 0.9749, 0.9961, 0.9969, 0.9917, 0.9977,\n",
       "                      1.0033, 0.9935, 0.9988, 0.9936, 1.0080, 1.0145, 0.9967, 0.9927, 0.9761,\n",
       "                      1.0034, 0.9985, 0.9931, 0.9958, 0.9815, 0.9761, 0.9829, 0.9686, 0.9747,\n",
       "                      0.9854, 0.9794, 0.9897, 1.0121, 0.9647, 0.9761, 1.0046, 0.9853, 0.9969,\n",
       "                      1.0020, 0.9873, 0.9756, 1.0028, 0.9975, 0.9906, 0.9858, 0.9702, 0.9857,\n",
       "                      1.0014, 0.9788, 0.9787, 0.9958, 0.9722, 1.0035, 0.9924, 0.9909, 0.9818,\n",
       "                      0.9825, 1.0154, 0.9793, 1.0082, 0.9834, 1.0029, 0.9846, 0.9994, 0.9830,\n",
       "                      0.9739, 0.9945, 1.0070, 0.9884, 0.9916, 0.9890, 1.0065, 0.9699, 1.0047,\n",
       "                      0.9863, 0.9880, 0.9966, 0.9886, 0.9855, 0.9816, 0.9939, 0.9954, 0.9801,\n",
       "                      0.9811, 1.0019, 1.0037, 0.9901, 1.0134, 0.9840, 0.9893, 0.9925, 0.9750,\n",
       "                      0.9898, 0.9976, 0.9942, 0.9918, 1.0000, 0.9881, 0.9812, 0.9798, 1.0026,\n",
       "                      0.9917, 0.9749, 0.9886, 1.0010, 0.9948, 0.9940, 0.9802, 1.0027, 1.0013,\n",
       "                      0.9873, 0.9914, 0.9821, 0.9932, 1.0190, 0.9907, 0.9944, 0.9961, 0.9908,\n",
       "                      1.0087, 0.9860, 0.9839, 1.0102, 0.9978, 0.9881, 0.9786, 1.0140, 0.9887,\n",
       "                      1.0010, 0.9869, 0.9964, 1.0066, 1.0043, 1.0009, 0.9901, 0.9813, 0.9933,\n",
       "                      0.9950, 0.9918, 1.0021, 0.9979, 0.9838, 1.0092, 0.9905, 0.9613, 0.9999,\n",
       "                      0.9750, 0.9813, 1.0036, 0.9744, 0.9962, 1.0067, 0.9962, 1.0004, 0.9862,\n",
       "                      0.9949, 0.9870, 0.9972, 1.0093, 0.9818, 1.0126, 1.0003, 0.9891, 1.0036,\n",
       "                      1.0111, 0.9960, 1.0039, 0.9942, 0.9822, 0.9877, 1.0042, 0.9809, 0.9979,\n",
       "                      0.9892, 0.9790, 0.9715, 0.9898, 0.9928, 0.9913, 0.9893, 0.9911, 0.9781,\n",
       "                      0.9989, 0.9921, 0.9956, 0.9974])),\n",
       "             ('transformer.layer.0.ln_2.bn.bias',\n",
       "              tensor([-1.1617e-03,  1.5654e-02, -1.8790e-02,  6.1449e-03, -8.6869e-03,\n",
       "                      -1.0334e-02,  8.9562e-03, -1.6320e-02, -1.0743e-02,  1.0246e-02,\n",
       "                      -3.1414e-02,  6.9664e-03,  1.3626e-02,  4.3714e-04, -1.7708e-03,\n",
       "                      -9.5165e-03,  8.9959e-03, -9.7104e-03,  1.3981e-02,  3.5272e-03,\n",
       "                      -2.4233e-02,  3.2937e-03, -1.7318e-02,  3.4604e-05,  1.0131e-02,\n",
       "                       6.7072e-04, -1.0355e-02,  1.3371e-02,  1.7957e-02,  6.2535e-03,\n",
       "                       4.6281e-03,  1.8063e-02,  2.3879e-03,  1.2372e-03,  8.1688e-03,\n",
       "                      -6.8264e-04, -4.9611e-03,  9.2970e-03, -1.0648e-04, -2.6237e-03,\n",
       "                       9.2607e-03,  1.1736e-02,  5.2010e-03,  1.5031e-02, -1.4019e-02,\n",
       "                       1.5601e-03,  2.2779e-02,  8.7433e-03, -2.3392e-02, -5.1128e-03,\n",
       "                       2.0698e-02,  1.4696e-02,  1.8549e-02, -7.3791e-04,  1.1616e-02,\n",
       "                      -3.3466e-03, -1.0966e-02, -2.2087e-02,  2.5562e-02, -5.7451e-03,\n",
       "                       2.2835e-02,  2.9015e-02,  8.1164e-03, -8.2396e-03,  9.1784e-04,\n",
       "                       1.7896e-02, -1.7235e-02,  4.7802e-03,  2.5887e-03, -1.1818e-03,\n",
       "                      -5.8181e-03, -1.4006e-03, -1.5991e-02,  2.2670e-02, -8.7560e-03,\n",
       "                      -1.1325e-02,  1.5039e-02,  3.5482e-03, -1.1875e-02,  2.2314e-02,\n",
       "                      -1.0241e-02, -9.0580e-04,  2.1325e-03,  2.5347e-02,  2.3930e-03,\n",
       "                      -1.7677e-02, -3.2906e-03,  5.5757e-03,  1.0787e-02,  2.9402e-03,\n",
       "                      -1.0092e-02, -1.6945e-02,  1.4501e-02,  4.1303e-03,  1.1130e-02,\n",
       "                       5.4631e-03,  1.5103e-02,  1.0329e-03,  1.8381e-02, -3.9449e-03,\n",
       "                       2.3707e-04,  3.6188e-03, -1.4835e-02,  1.0678e-02,  2.5156e-02,\n",
       "                      -8.0874e-03,  3.5044e-03, -5.0502e-04,  4.1528e-03,  1.5032e-02,\n",
       "                      -5.1855e-03, -5.4355e-03, -2.8857e-03,  7.9513e-03, -1.5374e-02,\n",
       "                      -1.7870e-02, -1.3349e-02,  1.7038e-02, -1.4965e-02, -1.1407e-04,\n",
       "                       1.6526e-02, -1.0807e-02,  3.1279e-03, -1.7861e-02, -1.4402e-02,\n",
       "                       9.9527e-03, -8.1853e-03,  2.6262e-04, -2.0511e-02, -9.5721e-03,\n",
       "                       7.3772e-03,  2.5280e-03, -5.8308e-03, -1.9561e-03,  2.2649e-03,\n",
       "                      -2.0045e-02, -3.7415e-03, -7.8910e-03, -9.0600e-03, -1.0289e-02,\n",
       "                      -8.8055e-04, -6.7048e-04,  1.3263e-02,  2.8746e-04, -1.0058e-03,\n",
       "                       1.2260e-02,  4.9638e-03,  2.6292e-03, -5.1895e-03,  4.8054e-03,\n",
       "                      -1.0795e-02,  4.3272e-03, -3.9734e-03,  1.7559e-03, -5.7020e-03,\n",
       "                       2.2606e-03, -5.9245e-03,  3.8286e-03,  1.5825e-02,  1.9344e-02,\n",
       "                       2.3315e-03, -1.0266e-03,  4.5507e-03, -1.0265e-02, -2.0932e-02,\n",
       "                      -1.5586e-02,  7.2955e-03,  3.9704e-03,  2.4837e-02,  1.6357e-02,\n",
       "                      -1.5650e-02,  6.4151e-03, -2.1876e-02,  1.2612e-02, -5.3367e-03,\n",
       "                      -9.6919e-03, -8.7552e-03,  1.0627e-03, -1.6313e-02, -1.4360e-02,\n",
       "                      -5.0603e-04,  8.6458e-03,  3.1626e-03,  7.5228e-03, -1.5497e-02,\n",
       "                       2.4973e-02,  9.2009e-04,  8.8796e-03, -3.8995e-03, -2.5552e-03,\n",
       "                      -2.3560e-02,  5.5258e-03, -1.3867e-02, -1.4173e-02,  7.4257e-03,\n",
       "                      -1.9307e-02, -1.7087e-02,  2.6576e-03, -1.0954e-02, -3.7312e-03,\n",
       "                       3.0616e-03, -2.0454e-02,  1.6245e-02, -1.7996e-02,  5.9428e-03,\n",
       "                       1.1906e-02,  2.0609e-02, -1.5677e-02,  1.0083e-02, -1.7679e-02,\n",
       "                       2.7291e-02,  1.7250e-02, -1.9806e-02, -4.0750e-03,  3.2714e-03,\n",
       "                      -2.0266e-05, -4.3149e-03, -1.8603e-03,  1.3511e-02,  5.4139e-03,\n",
       "                       2.1898e-02,  9.1348e-03,  1.1247e-02, -8.3234e-03,  4.7891e-03,\n",
       "                      -6.7161e-03, -7.2063e-03,  1.6437e-02, -1.4907e-03, -1.5479e-02,\n",
       "                       2.2923e-02,  4.6226e-03, -1.6084e-02,  1.5069e-02, -9.0403e-03,\n",
       "                       1.7145e-02, -1.4515e-02, -8.0070e-03, -8.0038e-03, -1.4850e-04,\n",
       "                       1.0565e-02,  6.4835e-03, -8.3489e-03,  1.1790e-02,  4.8108e-03,\n",
       "                       1.6109e-02,  4.4754e-04,  1.4808e-02,  1.3281e-03, -4.8641e-03,\n",
       "                      -1.3839e-02,  6.0345e-03,  1.4542e-02,  1.0951e-02, -5.6220e-03,\n",
       "                       3.6378e-03])),\n",
       "             ('transformer.layer.0.ln_2.bn.running_mean',\n",
       "              tensor([-5.0900e-03, -1.0721e-02,  3.8086e-02,  8.2083e-03,  4.0791e-03,\n",
       "                      -1.5757e-02, -1.5869e-03, -4.0722e-03,  1.9033e-02,  6.0358e-03,\n",
       "                       1.6505e-05, -4.9093e-03, -1.2209e-02, -8.7771e-03, -1.4959e-02,\n",
       "                       2.4640e-02, -5.0177e-03,  2.0662e-02,  6.9389e-03, -4.2177e-03,\n",
       "                       2.8362e-03, -8.8493e-03, -3.2864e-03, -3.4676e-03,  8.8807e-03,\n",
       "                      -1.1969e-02,  2.0179e-02,  4.4348e-04, -1.0332e-02, -2.6240e-02,\n",
       "                      -5.4242e-04, -1.5830e-02, -2.7030e-03,  6.7843e-03, -1.9277e-02,\n",
       "                      -3.0499e-02, -6.8424e-04,  3.6796e-03,  4.2282e-02, -2.0701e-02,\n",
       "                      -2.0995e-02, -2.6358e-03,  8.7215e-04,  1.3189e-02,  4.0794e-04,\n",
       "                      -3.1295e-03, -8.1328e-03,  4.5888e-03, -1.1396e-03, -1.0827e-02,\n",
       "                       1.6416e-02,  2.8933e-02,  5.0613e-03,  4.7441e-03,  2.0560e-02,\n",
       "                       7.4190e-03, -7.0605e-03,  2.5724e-02, -2.5804e-03, -1.5834e-02,\n",
       "                      -2.1643e-02, -2.0849e-02, -1.0131e-02,  1.2788e-02,  3.5538e-03,\n",
       "                       1.3842e-03,  9.7056e-03, -1.2455e-02, -2.8297e-03,  1.0676e-02,\n",
       "                       1.5990e-02, -1.9289e-02,  3.8453e-03, -1.2578e-02, -2.4527e-02,\n",
       "                       4.0436e-03, -1.2882e-02,  1.5468e-03,  6.1483e-03, -1.3916e-02,\n",
       "                      -3.4099e-03, -2.1674e-03, -2.4861e-03,  1.8135e-02, -2.8915e-03,\n",
       "                       8.7743e-03,  4.7296e-03, -8.3798e-03, -2.7891e-02, -8.3316e-03,\n",
       "                      -3.3166e-03, -3.2707e-02,  2.6956e-02,  8.5065e-04, -2.0160e-02,\n",
       "                      -9.9133e-03,  4.3786e-04,  1.3307e-02,  4.7673e-03,  3.5659e-03,\n",
       "                       1.9744e-03,  2.4958e-02, -2.6892e-02, -1.2225e-03,  8.3819e-03,\n",
       "                      -5.2288e-02, -1.2676e-02, -1.0421e-02, -1.5855e-03,  8.2935e-03,\n",
       "                       5.7946e-03,  1.5647e-03,  3.9233e-03, -1.1129e-02,  1.0203e-02,\n",
       "                       1.4875e-02, -6.6866e-03, -4.7482e-04,  1.7492e-02, -1.1491e-02,\n",
       "                      -3.1674e-03,  3.1042e-03, -8.4997e-03,  2.2395e-02, -5.1092e-03,\n",
       "                       1.1958e-02, -5.9185e-03,  1.8376e-02,  3.7141e-03, -2.4477e-02,\n",
       "                      -2.3261e-03,  2.7153e-02, -1.7070e-02,  3.2139e-03,  9.8418e-03,\n",
       "                       3.7204e-03, -4.8453e-03,  2.2284e-02,  1.9756e-02,  8.5836e-03,\n",
       "                       1.2711e-02, -3.4019e-04,  1.4558e-02,  4.2319e-03, -1.2487e-03,\n",
       "                       1.2874e-02, -2.9393e-02, -4.4419e-03,  9.6483e-04,  1.7746e-04,\n",
       "                       7.5834e-03,  3.2333e-03, -2.5262e-02, -7.8071e-03, -1.0614e-02,\n",
       "                       5.2585e-03,  1.5115e-02, -2.7218e-02, -1.4371e-03, -1.6751e-02,\n",
       "                      -1.6378e-02, -9.2631e-03,  1.7552e-02,  1.4014e-02, -1.5652e-02,\n",
       "                      -6.2528e-03,  1.9234e-04, -8.2318e-03, -1.2588e-02,  1.1316e-02,\n",
       "                      -1.7938e-02, -3.2783e-03,  2.6472e-02,  2.5226e-02, -3.6355e-03,\n",
       "                       9.7574e-03, -9.1546e-03,  1.2679e-02, -1.3290e-02, -1.8622e-02,\n",
       "                      -2.4983e-02,  2.2928e-02,  2.4071e-03,  1.9825e-03, -2.5308e-02,\n",
       "                       6.0616e-03,  1.5534e-02,  3.6195e-03, -2.6011e-02,  1.3159e-02,\n",
       "                      -7.7617e-03,  8.4955e-03,  2.3088e-02,  1.9208e-03, -3.7350e-03,\n",
       "                       4.9570e-03,  1.1132e-02, -4.8787e-03,  1.2850e-02,  3.1416e-03,\n",
       "                      -6.0059e-03, -9.9623e-03,  1.2791e-02,  1.0964e-02,  1.5833e-02,\n",
       "                      -1.9202e-02,  3.7600e-03, -2.6611e-02, -1.2429e-02,  5.2836e-03,\n",
       "                      -4.5105e-03,  6.8247e-03,  1.8158e-02,  2.7214e-02,  1.6454e-02,\n",
       "                       5.6127e-03, -2.0812e-03, -8.1859e-03, -7.0998e-03,  1.2417e-02,\n",
       "                       1.5451e-02, -1.0423e-02,  1.0884e-02,  7.4714e-03, -1.2460e-02,\n",
       "                      -2.2207e-02, -1.8058e-02, -5.1364e-03, -5.5807e-04,  1.5648e-02,\n",
       "                      -2.0213e-02, -5.0812e-03, -1.2590e-02,  6.0696e-03,  2.8794e-02,\n",
       "                       1.9313e-02, -3.0259e-04, -4.1645e-03,  8.9691e-05,  1.3176e-02,\n",
       "                       5.5752e-03,  3.9541e-05, -3.1730e-02, -6.4912e-03,  1.3219e-02,\n",
       "                       2.2198e-02,  2.8339e-03, -1.5230e-02,  2.6949e-02, -8.7384e-04,\n",
       "                      -3.5156e-03,  1.0783e-03,  1.0803e-02,  2.4274e-02, -2.0325e-03,\n",
       "                       4.9847e-03])),\n",
       "             ('transformer.layer.0.ln_2.bn.running_var',\n",
       "              tensor([0.9524, 0.9480, 0.9745, 0.9642, 0.9369, 0.9231, 0.9267, 0.9414, 0.9541,\n",
       "                      0.9510, 0.9204, 0.9540, 0.9181, 0.9392, 0.9381, 0.9102, 0.9504, 0.9420,\n",
       "                      0.9889, 0.9660, 0.9528, 0.9543, 0.9452, 0.9614, 0.9597, 0.9580, 0.9392,\n",
       "                      0.9682, 0.9393, 0.9807, 0.9612, 0.9404, 0.9574, 0.9452, 0.9588, 0.9421,\n",
       "                      0.9745, 0.9390, 0.9084, 0.9597, 0.9367, 0.9417, 0.9363, 0.9601, 0.9470,\n",
       "                      0.9735, 0.9327, 0.9330, 0.9393, 0.9588, 0.9669, 0.9387, 0.9560, 0.9040,\n",
       "                      0.9693, 0.9274, 0.9404, 0.9456, 0.9145, 0.9405, 0.9656, 0.9415, 0.9318,\n",
       "                      0.9719, 0.9575, 0.9563, 0.9611, 0.9525, 0.9602, 0.9615, 0.9159, 0.9605,\n",
       "                      0.9601, 0.9575, 0.9496, 0.9590, 0.9775, 0.9133, 0.9524, 0.9759, 0.9509,\n",
       "                      0.9684, 0.9610, 0.9416, 0.9586, 0.9711, 0.9317, 0.9646, 0.9555, 0.9489,\n",
       "                      0.9690, 0.9560, 0.9298, 0.9554, 0.9633, 0.9430, 0.9642, 0.9184, 0.9188,\n",
       "                      0.9321, 0.9481, 0.9684, 0.9634, 0.9305, 0.9310, 0.9331, 0.9668, 0.9491,\n",
       "                      0.9537, 0.9424, 0.9204, 0.9325, 0.9622, 0.9564, 0.9721, 0.9327, 0.9599,\n",
       "                      0.9385, 0.9464, 0.9295, 0.9523, 0.9453, 0.9523, 0.9564, 0.9578, 0.9558,\n",
       "                      0.9303, 0.9417, 0.9537, 0.9346, 0.9565, 0.9404, 0.9460, 0.9533, 0.9515,\n",
       "                      0.9413, 0.9485, 0.9590, 0.9346, 0.9152, 0.9464, 0.9497, 0.9769, 0.9398,\n",
       "                      0.9290, 0.9486, 0.9314, 0.9638, 0.9504, 0.9477, 0.9260, 0.9332, 0.9631,\n",
       "                      0.9498, 0.9417, 0.9281, 0.9672, 0.8978, 0.9366, 0.9354, 0.9520, 0.9321,\n",
       "                      0.9478, 0.9342, 0.9473, 0.9590, 0.9379, 0.9532, 0.9730, 0.9348, 0.9544,\n",
       "                      0.9536, 0.9308, 0.9410, 0.9689, 0.9251, 0.9312, 0.9551, 0.9487, 0.9641,\n",
       "                      0.9274, 0.9621, 0.9406, 0.9437, 0.9577, 0.9400, 0.9696, 0.9418, 0.9210,\n",
       "                      0.9086, 0.9626, 0.9421, 0.9564, 0.9679, 0.9514, 0.9489, 0.9381, 0.9375,\n",
       "                      0.9354, 0.9574, 0.9486, 0.9772, 0.9653, 0.9412, 0.9534, 0.9635, 0.9123,\n",
       "                      0.9405, 0.9450, 0.9425, 0.9596, 0.9580, 0.9584, 0.9431, 0.9548, 0.9343,\n",
       "                      0.9396, 0.9576, 0.9459, 0.9407, 0.9378, 0.9256, 0.9456, 0.9437, 0.9296,\n",
       "                      0.9246, 0.9437, 0.9707, 0.9426, 0.9025, 0.9976, 0.9719, 0.9458, 0.9411,\n",
       "                      0.9624, 0.9400, 0.9297, 0.9328, 0.9500, 0.9241, 0.9477, 0.9466, 0.9427,\n",
       "                      0.9615, 0.9483, 0.9376, 0.9634, 0.9532, 0.9119, 0.9369, 0.9760, 0.9560,\n",
       "                      0.9477, 0.9404, 0.9738, 0.9779])),\n",
       "             ('transformer.layer.0.ln_2.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5780)),\n",
       "             ('transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.2201)),\n",
       "             ('transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.6164)),\n",
       "             ('transformer.layer.1.attn.bias',\n",
       "              tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('transformer.layer.1.attn.mha.q_proj.weight',\n",
       "              tensor([[-0.0289,  0.0976,  0.0667,  ..., -0.0131,  0.0932,  0.0024],\n",
       "                      [-0.0678, -0.0605,  0.0941,  ...,  0.0241,  0.0261,  0.0976],\n",
       "                      [ 0.0276, -0.0368, -0.0275,  ...,  0.0095, -0.1039, -0.0963],\n",
       "                      ...,\n",
       "                      [-0.0174, -0.0388,  0.0762,  ..., -0.0933, -0.0762,  0.0182],\n",
       "                      [-0.1064, -0.1046,  0.0996,  ...,  0.0059,  0.0950,  0.0947],\n",
       "                      [ 0.0433, -0.0998,  0.0985,  ..., -0.0557, -0.0753, -0.0908]])),\n",
       "             ('transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.k_proj.weight',\n",
       "              tensor([[ 0.0670,  0.0631, -0.0680,  ...,  0.0421,  0.0778,  0.0756],\n",
       "                      [-0.0360, -0.0908,  0.0439,  ...,  0.0365,  0.0324,  0.0242],\n",
       "                      [ 0.0767,  0.0078,  0.0632,  ...,  0.0169, -0.0303,  0.0467],\n",
       "                      ...,\n",
       "                      [-0.0232,  0.0720,  0.0345,  ...,  0.0828, -0.0528, -0.0473],\n",
       "                      [ 0.0859,  0.1058,  0.0129,  ...,  0.0084, -0.0406, -0.0608],\n",
       "                      [-0.1061,  0.0643, -0.0219,  ...,  0.0850, -0.0083,  0.0746]])),\n",
       "             ('transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.v_proj.weight',\n",
       "              tensor([[ 0.0770, -0.0444,  0.0392,  ...,  0.0213, -0.0336, -0.1049],\n",
       "                      [-0.0791, -0.0693, -0.0372,  ..., -0.0016, -0.0555, -0.0367],\n",
       "                      [ 0.0394,  0.0130, -0.0104,  ..., -0.0837,  0.0782,  0.0857],\n",
       "                      ...,\n",
       "                      [-0.0449,  0.0637, -0.0511,  ...,  0.0733, -0.0521, -0.0959],\n",
       "                      [ 0.0212, -0.0945, -0.0851,  ..., -0.0861,  0.1063, -0.0324],\n",
       "                      [ 0.0266,  0.0173,  0.0705,  ..., -0.0137,  0.0776,  0.0054]])),\n",
       "             ('transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9517)),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.weight',\n",
       "              tensor([[ 0.0383, -0.0247,  0.0034,  ..., -0.0250, -0.0135,  0.0593],\n",
       "                      [ 0.0116,  0.0308,  0.0596,  ..., -0.0100, -0.0307, -0.0493],\n",
       "                      [ 0.0376, -0.0257, -0.0307,  ...,  0.0383,  0.0414, -0.0497],\n",
       "                      ...,\n",
       "                      [-0.0564, -0.0499,  0.0457,  ...,  0.0512, -0.0417,  0.0239],\n",
       "                      [ 0.0428,  0.0313,  0.0073,  ...,  0.0434,  0.0338, -0.0295],\n",
       "                      [-0.0285,  0.0077, -0.0428,  ...,  0.0328,  0.0573,  0.0241]])),\n",
       "             ('transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(3.1205)),\n",
       "             ('transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(1.1111)),\n",
       "             ('transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(0.3889)),\n",
       "             ('transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.2181)),\n",
       "             ('transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(4.3162)),\n",
       "             ('transformer.layer.1.attn.c_proj.weight',\n",
       "              tensor([[-0.0121,  0.0034, -0.0166,  ...,  0.0043,  0.0106, -0.0059],\n",
       "                      [ 0.0002,  0.0207,  0.0165,  ...,  0.0044,  0.0283, -0.0115],\n",
       "                      [-0.0110,  0.0047,  0.0163,  ..., -0.0086, -0.0136, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0152, -0.0017, -0.0024,  ...,  0.0169,  0.0018, -0.0160],\n",
       "                      [-0.0023,  0.0208,  0.0079,  ..., -0.0002, -0.0090, -0.0032],\n",
       "                      [ 0.0041, -0.0305, -0.0199,  ...,  0.0094, -0.0021,  0.0034]])),\n",
       "             ('transformer.layer.1.attn.c_proj.bias',\n",
       "              tensor([-1.0001e-02,  8.5324e-04, -1.0371e-03, -3.6230e-04,  3.3133e-03,\n",
       "                      -1.2187e-04, -4.2462e-03, -9.1936e-04,  6.8946e-04,  2.1431e-03,\n",
       "                      -3.1210e-03, -3.5645e-03, -2.9548e-03, -3.4646e-03,  5.6597e-03,\n",
       "                       6.4772e-04, -1.2795e-04,  4.6478e-03,  2.6459e-03, -5.5229e-03,\n",
       "                       3.3030e-03,  1.3496e-03, -2.6890e-03, -2.0859e-03, -6.0766e-04,\n",
       "                       1.7153e-03, -1.9855e-03, -2.5495e-03,  1.7874e-03, -6.7082e-03,\n",
       "                      -5.6755e-04, -2.3912e-03, -4.3582e-03,  2.7330e-03, -1.6397e-03,\n",
       "                      -1.0015e-04, -8.4450e-04,  7.5479e-04,  3.6866e-03,  2.5952e-03,\n",
       "                       9.9290e-04,  6.2599e-04, -3.0715e-03,  7.0385e-03, -4.3600e-03,\n",
       "                      -1.6166e-03, -2.7170e-03,  3.1233e-03,  5.6342e-04, -1.8623e-03,\n",
       "                       4.1094e-03, -4.2669e-03, -3.2268e-03,  3.1488e-04, -5.0921e-03,\n",
       "                       1.7216e-03, -1.8930e-03,  1.0187e-03,  1.2571e-03, -1.0238e-03,\n",
       "                       3.8939e-03, -7.8471e-04,  6.8841e-04,  2.7645e-03,  9.7573e-04,\n",
       "                       2.8124e-03,  2.1228e-03, -3.2062e-03,  6.6098e-04,  1.6510e-03,\n",
       "                      -4.8358e-04, -8.3054e-04,  1.7340e-03, -3.5693e-03,  1.0891e-03,\n",
       "                       5.2370e-03, -6.6089e-04,  1.1281e-02, -2.5373e-04,  1.2816e-03,\n",
       "                       3.8372e-03, -4.4826e-03,  2.1975e-03,  2.7572e-03,  1.1562e-03,\n",
       "                       1.9154e-03, -1.4028e-03, -6.4799e-03, -5.4713e-03,  4.6193e-04,\n",
       "                       6.2959e-03, -3.5250e-03,  5.6006e-03, -3.1833e-03,  4.2460e-03,\n",
       "                       1.4790e-04, -2.2666e-03,  4.5003e-03, -1.7849e-03,  5.7741e-04,\n",
       "                       7.4971e-03, -4.0401e-03, -1.4316e-03,  1.8538e-04,  2.5096e-03,\n",
       "                      -2.8483e-03, -1.6946e-03, -7.0243e-04,  1.9334e-03, -1.4484e-03,\n",
       "                       4.3673e-03,  6.0470e-03, -1.4517e-03, -1.4012e-03,  2.1070e-03,\n",
       "                      -7.7036e-03,  6.3211e-03, -1.0998e-03, -9.5744e-04,  3.0299e-03,\n",
       "                       1.5667e-03, -1.3419e-03, -5.3833e-03, -5.6620e-04,  9.3516e-03,\n",
       "                       4.8909e-04, -3.6963e-03, -6.9707e-03,  2.1502e-03,  1.1926e-03,\n",
       "                      -1.8200e-03,  2.2529e-03, -2.4530e-03, -3.2041e-03, -2.5172e-03,\n",
       "                       2.1932e-04,  3.1836e-05,  1.5012e-03, -2.8934e-03, -1.2549e-03,\n",
       "                       7.0520e-03,  2.5215e-03,  2.1365e-03,  1.1457e-03,  8.0462e-04,\n",
       "                       2.3927e-03,  3.9076e-03,  1.3535e-03,  2.2825e-03,  4.7756e-03,\n",
       "                       1.4805e-03, -3.4977e-03, -9.9157e-04,  2.6944e-03,  5.5162e-04,\n",
       "                      -3.5070e-03,  1.9986e-03, -3.0507e-03,  2.2575e-03, -3.5908e-03,\n",
       "                       5.0501e-04, -4.6861e-04, -1.1597e-03,  3.1375e-03, -2.1633e-03,\n",
       "                       4.5996e-03, -4.3433e-03,  2.1133e-03, -1.3463e-03,  3.3768e-03,\n",
       "                       5.5218e-03, -4.8867e-04, -1.8089e-03, -8.6995e-04,  3.8831e-03,\n",
       "                      -4.5628e-03,  1.2847e-03, -8.5883e-04,  6.4863e-03, -3.0359e-03,\n",
       "                       2.5516e-03, -5.4719e-03, -3.0461e-03, -9.3430e-04,  1.1867e-03,\n",
       "                      -5.6178e-04, -4.1363e-03,  5.5099e-03,  1.5559e-04, -1.8867e-03,\n",
       "                      -4.8013e-04,  2.1434e-03,  1.7304e-03, -4.8338e-03,  1.3595e-03,\n",
       "                      -5.6351e-03, -3.2486e-03, -1.7714e-03,  3.6017e-03, -8.0431e-04,\n",
       "                      -1.6796e-03,  4.6677e-03,  2.4440e-03,  1.0739e-03, -5.1304e-03,\n",
       "                      -4.2639e-03, -2.7939e-03,  2.9703e-03,  9.2800e-03,  2.2361e-04,\n",
       "                       4.5265e-03,  2.5850e-03, -1.0793e-03, -1.3141e-05,  4.1272e-03,\n",
       "                      -4.2361e-03,  4.8523e-05, -1.1420e-03,  2.8526e-03,  4.5000e-03,\n",
       "                       1.9379e-03,  2.6035e-03,  1.2161e-03, -2.0494e-03,  1.3908e-03,\n",
       "                       3.3852e-03, -4.9066e-03, -1.0401e-03, -2.3964e-04,  8.2049e-03,\n",
       "                       2.0331e-03, -1.8505e-03, -1.7263e-03,  1.6367e-03, -5.3243e-03,\n",
       "                       2.1911e-03, -1.8749e-05, -4.0067e-04,  3.0792e-03,  7.8981e-04,\n",
       "                      -6.1128e-03, -6.4850e-05,  1.0346e-03,  6.0894e-04, -1.6089e-03,\n",
       "                      -2.2540e-03,  1.1035e-03, -2.5577e-03, -1.4807e-03,  5.8240e-03,\n",
       "                       3.8623e-03,  1.0280e-03,  3.6149e-03,  4.3118e-03,  4.4890e-03,\n",
       "                       4.6817e-03])),\n",
       "             ('transformer.layer.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0296,  0.0019,  0.0275,  ..., -0.0367,  0.0327,  0.0085],\n",
       "                      [ 0.0132,  0.0106,  0.0004,  ..., -0.0132, -0.0035,  0.0032],\n",
       "                      [-0.0170,  0.0133,  0.0263,  ...,  0.0311,  0.0103,  0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0189, -0.0202,  0.0250,  ..., -0.0290,  0.0136, -0.0242],\n",
       "                      [ 0.0051,  0.0136, -0.0009,  ..., -0.0116,  0.0404,  0.0048],\n",
       "                      [-0.0009,  0.0096,  0.0206,  ..., -0.0171, -0.0136, -0.0004]])),\n",
       "             ('transformer.layer.1.attn.c_attn.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('transformer.layer.1.mlp.c_fc.weight',\n",
       "              tensor([[-0.0058,  0.0039,  0.0383,  ..., -0.0456,  0.0403, -0.0299],\n",
       "                      [-0.0213,  0.0152,  0.0204,  ...,  0.0470, -0.0493,  0.0346],\n",
       "                      [-0.0433,  0.0593, -0.0227,  ...,  0.0539,  0.0619, -0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0066,  0.0602, -0.0372,  ..., -0.0466,  0.0298, -0.0297],\n",
       "                      [-0.0296, -0.0246,  0.0449,  ...,  0.0440,  0.0335, -0.0466],\n",
       "                      [ 0.0471,  0.0025,  0.0513,  ..., -0.0563,  0.0324, -0.0364]])),\n",
       "             ('transformer.layer.1.mlp.c_fc.bias',\n",
       "              tensor([ 0.0063,  0.0311, -0.0120,  ...,  0.0166,  0.0098, -0.0140])),\n",
       "             ('transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4474)),\n",
       "             ('transformer.layer.1.mlp.c_proj.weight',\n",
       "              tensor([[ 1.3174e-02, -2.1647e-02,  3.5113e-03,  ...,  7.6342e-05,\n",
       "                        8.9009e-03, -1.6953e-02],\n",
       "                      [ 1.1414e-02, -1.1850e-02, -7.3429e-03,  ..., -2.6661e-02,\n",
       "                        1.8389e-02,  6.3918e-03],\n",
       "                      [ 5.1879e-03, -2.9540e-02,  9.4862e-03,  ...,  1.7773e-05,\n",
       "                        1.8000e-02,  9.6450e-03],\n",
       "                      ...,\n",
       "                      [-2.0446e-02,  2.4317e-02,  2.8374e-02,  ..., -2.2716e-02,\n",
       "                       -6.1415e-03,  3.1544e-03],\n",
       "                      [-1.5467e-02,  1.9042e-02,  2.5861e-02,  ...,  2.3330e-03,\n",
       "                       -5.8655e-03,  2.2571e-02],\n",
       "                      [-2.2951e-02,  4.9628e-03,  7.5520e-03,  ...,  2.2974e-02,\n",
       "                        1.7007e-02,  7.2556e-03]])),\n",
       "             ('transformer.layer.1.mlp.c_proj.bias',\n",
       "              tensor([-0.0203,  0.0033, -0.0026,  0.0247, -0.0087, -0.0038, -0.0253, -0.0213,\n",
       "                      -0.0306,  0.0258,  0.0227,  0.0158,  0.0239, -0.0068,  0.0224,  0.0016,\n",
       "                      -0.0198, -0.0067,  0.0080,  0.0026,  0.0126, -0.0081, -0.0180,  0.0100,\n",
       "                       0.0013, -0.0307, -0.0031, -0.0113,  0.0093,  0.0020,  0.0086, -0.0160,\n",
       "                       0.0026,  0.0231, -0.0236,  0.0087, -0.0304, -0.0234, -0.0012,  0.0067,\n",
       "                       0.0145,  0.0221, -0.0309, -0.0225, -0.0281, -0.0106,  0.0173,  0.0129,\n",
       "                       0.0017, -0.0208,  0.0049,  0.0303,  0.0083,  0.0230, -0.0145, -0.0226,\n",
       "                       0.0265, -0.0144, -0.0127,  0.0077,  0.0188,  0.0069, -0.0065, -0.0138,\n",
       "                      -0.0014,  0.0152,  0.0252, -0.0112, -0.0011, -0.0217,  0.0068, -0.0118,\n",
       "                      -0.0071, -0.0150,  0.0170, -0.0167, -0.0134,  0.0001,  0.0169,  0.0284,\n",
       "                       0.0143, -0.0269,  0.0250,  0.0108,  0.0197, -0.0126, -0.0119,  0.0230,\n",
       "                       0.0117,  0.0290,  0.0259, -0.0186,  0.0024, -0.0116,  0.0062,  0.0243,\n",
       "                      -0.0308,  0.0046, -0.0167, -0.0041, -0.0089,  0.0032,  0.0204,  0.0188,\n",
       "                      -0.0165,  0.0219, -0.0044, -0.0301, -0.0226,  0.0079,  0.0158, -0.0265,\n",
       "                       0.0122, -0.0195,  0.0179,  0.0069, -0.0273,  0.0051, -0.0088, -0.0080,\n",
       "                       0.0200, -0.0057, -0.0045, -0.0067, -0.0165,  0.0264,  0.0277, -0.0157,\n",
       "                       0.0066, -0.0221, -0.0218, -0.0252,  0.0178,  0.0267,  0.0227, -0.0050,\n",
       "                      -0.0223, -0.0117,  0.0125,  0.0284, -0.0298, -0.0170, -0.0188, -0.0078,\n",
       "                      -0.0084, -0.0159, -0.0006,  0.0287, -0.0157,  0.0039, -0.0260, -0.0095,\n",
       "                      -0.0052,  0.0206, -0.0014,  0.0123, -0.0103, -0.0253, -0.0295,  0.0218,\n",
       "                       0.0171,  0.0034, -0.0256, -0.0031,  0.0214, -0.0184, -0.0264, -0.0168,\n",
       "                       0.0038, -0.0141, -0.0172, -0.0132, -0.0135,  0.0124,  0.0045, -0.0302,\n",
       "                       0.0056,  0.0130,  0.0113,  0.0204, -0.0084, -0.0086, -0.0048, -0.0040,\n",
       "                      -0.0082,  0.0211, -0.0151,  0.0118,  0.0244,  0.0295, -0.0292, -0.0251,\n",
       "                       0.0092, -0.0117,  0.0219,  0.0050,  0.0057, -0.0171,  0.0158,  0.0151,\n",
       "                      -0.0103, -0.0057,  0.0191,  0.0191, -0.0224,  0.0108,  0.0227,  0.0293,\n",
       "                      -0.0054,  0.0266, -0.0182,  0.0258,  0.0008,  0.0176,  0.0038,  0.0281,\n",
       "                      -0.0247,  0.0310,  0.0082, -0.0075,  0.0054, -0.0289, -0.0208,  0.0090,\n",
       "                       0.0030,  0.0151, -0.0029, -0.0150,  0.0161, -0.0237,  0.0169,  0.0027,\n",
       "                      -0.0119, -0.0142,  0.0015,  0.0200,  0.0107,  0.0082, -0.0165,  0.0166,\n",
       "                       0.0119, -0.0306,  0.0076,  0.0051,  0.0123, -0.0080,  0.0052, -0.0057,\n",
       "                       0.0302, -0.0263, -0.0028,  0.0247, -0.0093,  0.0036,  0.0219, -0.0136])),\n",
       "             ('transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4092)),\n",
       "             ('transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.5107)),\n",
       "             ('transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4092)),\n",
       "             ('transformer.layer.1.ln_1.bn.weight',\n",
       "              tensor([0.9953, 0.9932, 0.9971, 1.0079, 0.9953, 0.9981, 0.9995, 0.9931, 0.9836,\n",
       "                      0.9842, 0.9947, 0.9938, 0.9917, 0.9951, 0.9944, 0.9910, 0.9929, 0.9975,\n",
       "                      0.9935, 1.0084, 0.9960, 0.9917, 0.9960, 0.9947, 0.9932, 0.9956, 1.0002,\n",
       "                      0.9976, 0.9886, 0.9934, 1.0020, 0.9934, 1.0021, 0.9986, 1.0133, 0.9871,\n",
       "                      0.9969, 0.9965, 0.9939, 0.9887, 0.9960, 1.0091, 0.9890, 0.9865, 0.9997,\n",
       "                      1.0014, 0.9993, 0.9897, 1.0014, 0.9966, 0.9872, 1.0013, 0.9941, 0.9872,\n",
       "                      0.9921, 1.0012, 0.9935, 0.9968, 0.9854, 1.0029, 0.9942, 0.9881, 0.9891,\n",
       "                      0.9974, 0.9974, 1.0017, 1.0034, 1.0035, 0.9920, 0.9993, 0.9938, 0.9974,\n",
       "                      1.0005, 1.0010, 0.9961, 0.9963, 0.9994, 0.9768, 0.9938, 1.0028, 0.9967,\n",
       "                      0.9886, 0.9906, 0.9976, 0.9811, 0.9994, 1.0018, 0.9920, 0.9948, 0.9963,\n",
       "                      0.9997, 0.9997, 1.0014, 1.0060, 1.0010, 1.0047, 1.0035, 0.9964, 0.9901,\n",
       "                      0.9854, 0.9958, 0.9922, 1.0070, 0.9946, 0.9877, 1.0087, 0.9858, 1.0002,\n",
       "                      1.0004, 0.9863, 1.0058, 1.0011, 0.9839, 0.9933, 1.0121, 0.9948, 1.0038,\n",
       "                      1.0012, 1.0009, 0.9809, 0.9970, 0.9901, 0.9920, 1.0030, 0.9907, 1.0038,\n",
       "                      0.9949, 0.9970, 0.9884, 0.9918, 0.9939, 0.9959, 0.9922, 0.9875, 0.9960,\n",
       "                      1.0034, 1.0041, 0.9867, 0.9978, 0.9917, 0.9972, 0.9843, 0.9936, 0.9979,\n",
       "                      0.9920, 0.9946, 0.9826, 0.9917, 0.9935, 0.9915, 0.9922, 0.9902, 0.9978,\n",
       "                      0.9868, 0.9886, 0.9908, 1.0002, 0.9890, 0.9904, 0.9992, 0.9971, 0.9788,\n",
       "                      1.0005, 0.9975, 0.9959, 0.9952, 0.9939, 1.0068, 0.9967, 0.9937, 0.9878,\n",
       "                      0.9989, 0.9945, 0.9936, 1.0015, 0.9944, 0.9865, 0.9948, 0.9938, 0.9932,\n",
       "                      0.9998, 1.0011, 1.0017, 0.9924, 0.9962, 0.9965, 0.9976, 0.9984, 0.9953,\n",
       "                      0.9954, 0.9939, 0.9933, 0.9999, 0.9954, 1.0084, 0.9995, 0.9997, 1.0018,\n",
       "                      0.9978, 0.9983, 0.9962, 1.0098, 1.0011, 0.9939, 0.9929, 0.9970, 0.9897,\n",
       "                      0.9959, 1.0044, 1.0011, 0.9889, 1.0035, 0.9832, 0.9904, 0.9997, 0.9959,\n",
       "                      1.0028, 0.9988, 0.9927, 1.0058, 1.0059, 1.0045, 1.0068, 0.9885, 0.9949,\n",
       "                      0.9968, 0.9945, 0.9976, 0.9946, 0.9958, 0.9965, 1.0070, 0.9971, 0.9986,\n",
       "                      0.9945, 0.9939, 1.0013, 0.9985, 0.9964, 0.9972, 1.0188, 0.9984, 0.9891,\n",
       "                      0.9987, 0.9978, 0.9934, 0.9992, 0.9953, 0.9908, 0.9905, 1.0006, 0.9931,\n",
       "                      0.9987, 1.0025, 1.0015, 1.0029])),\n",
       "             ('transformer.layer.1.ln_1.bn.bias',\n",
       "              tensor([-5.8221e-03,  5.1567e-03, -2.7539e-03, -3.2386e-03, -1.4047e-03,\n",
       "                       2.1491e-03,  7.0566e-03, -7.2472e-04,  7.1937e-03, -3.3103e-04,\n",
       "                       7.6884e-03, -5.6532e-03,  4.5102e-03, -2.5344e-03,  4.6091e-03,\n",
       "                       5.8198e-03, -4.4172e-03,  6.7577e-04,  3.1598e-03, -6.0722e-03,\n",
       "                       2.3039e-03, -2.0422e-03, -2.6043e-03,  2.3514e-04, -1.3696e-04,\n",
       "                      -2.7391e-03,  3.0222e-04, -7.9647e-03, -2.0969e-03, -8.2105e-04,\n",
       "                       1.3519e-02,  9.0652e-03, -2.3756e-03, -5.7700e-03,  7.1253e-03,\n",
       "                      -6.1457e-03,  1.3592e-03,  4.0590e-03,  1.9296e-02, -8.8499e-03,\n",
       "                      -2.7389e-03,  1.0754e-02, -3.8546e-04, -7.5427e-03,  1.0755e-02,\n",
       "                       9.2125e-04, -8.2215e-04, -4.6560e-04, -8.4366e-03,  4.2312e-03,\n",
       "                       7.4299e-03, -2.8996e-03, -1.4430e-02, -1.5712e-02,  4.3866e-04,\n",
       "                       6.7002e-03, -2.7704e-03, -3.0450e-03, -7.3768e-03,  1.7011e-03,\n",
       "                      -5.2680e-03, -1.1853e-03, -4.2184e-03, -4.4225e-03,  4.4505e-03,\n",
       "                      -9.1620e-03, -5.2125e-03, -5.6020e-03,  8.8070e-03, -6.4308e-03,\n",
       "                       3.6090e-03, -7.4969e-03,  3.8329e-04, -7.5339e-03, -1.2599e-02,\n",
       "                       1.1228e-02, -1.8359e-03, -1.0769e-02, -6.0324e-03,  1.0948e-02,\n",
       "                      -3.9661e-03,  2.8852e-03, -6.9461e-03,  4.5364e-03, -2.0028e-03,\n",
       "                      -3.2919e-03, -5.0685e-03,  3.4899e-05, -2.6441e-03, -9.3618e-03,\n",
       "                      -4.0739e-03, -9.2865e-03,  2.1343e-03,  6.3682e-03,  1.0055e-02,\n",
       "                      -7.2832e-03, -1.8698e-04, -1.3970e-03,  1.0466e-02, -4.9834e-03,\n",
       "                      -7.1049e-04,  1.9894e-03, -2.7675e-03,  5.0164e-03,  5.1143e-03,\n",
       "                      -8.2889e-03,  9.7298e-03, -2.8950e-04,  1.2949e-03, -3.2998e-04,\n",
       "                       1.2702e-03, -1.8586e-03,  2.2046e-03,  7.5215e-03,  1.9129e-03,\n",
       "                       6.1702e-04, -8.9103e-04,  2.4675e-03, -6.3069e-03,  1.7767e-03,\n",
       "                      -6.6770e-03, -3.2735e-03, -5.1627e-03, -1.1403e-03, -6.3716e-03,\n",
       "                       2.0715e-03,  8.9919e-03,  2.4345e-03, -1.2673e-03,  3.7260e-03,\n",
       "                      -7.0699e-03,  3.8565e-05, -4.2306e-05,  9.5679e-03, -1.0547e-02,\n",
       "                       1.9003e-03, -3.3257e-03,  1.3666e-02,  2.6138e-04, -4.1423e-03,\n",
       "                      -6.5063e-03, -3.4874e-03, -3.7485e-03,  1.0733e-03,  4.7532e-04,\n",
       "                       1.0517e-02, -7.4096e-03, -3.5391e-03,  6.0124e-04,  3.1639e-03,\n",
       "                      -6.1817e-04,  8.0520e-03, -4.7760e-03,  4.6868e-03, -3.4434e-03,\n",
       "                      -4.0857e-04,  5.5039e-05, -1.4874e-02,  1.6638e-02,  5.6338e-03,\n",
       "                      -5.4811e-03, -1.3302e-02,  9.2119e-03,  2.2444e-03,  6.9384e-04,\n",
       "                      -1.2393e-02,  3.9307e-03,  1.3712e-03,  6.2569e-03,  5.7022e-03,\n",
       "                      -1.1866e-02,  2.6987e-04, -2.9723e-03,  4.6817e-04, -5.7003e-03,\n",
       "                       4.4396e-03, -8.1133e-04,  1.2583e-02, -1.0043e-02, -5.6614e-03,\n",
       "                       3.9674e-03,  5.8616e-03,  4.2365e-03,  1.1961e-02,  1.8009e-05,\n",
       "                       1.0455e-03, -4.7964e-03,  8.5359e-03,  1.0323e-02,  1.8004e-04,\n",
       "                      -1.7126e-03,  3.7596e-03,  3.9523e-03,  6.2968e-03, -1.8048e-03,\n",
       "                       1.2506e-03, -5.6832e-03, -4.9432e-03, -6.7194e-03,  5.4706e-03,\n",
       "                       9.6134e-03,  3.5158e-03, -9.5797e-03, -1.3620e-02, -4.0934e-04,\n",
       "                       3.6817e-03,  1.3185e-02, -3.0881e-03, -5.7888e-03, -1.2137e-04,\n",
       "                      -6.4653e-04, -6.1570e-03, -1.1290e-03, -8.3489e-04,  7.9154e-03,\n",
       "                       4.0959e-03, -3.2487e-03,  2.6119e-03,  2.4061e-03,  4.6800e-03,\n",
       "                      -1.8684e-03,  8.0169e-03, -5.1568e-04, -5.0112e-03,  5.9902e-03,\n",
       "                      -2.5044e-03,  1.4882e-02,  6.8808e-03, -1.2065e-03, -6.5927e-03,\n",
       "                      -1.7777e-02,  1.9434e-03,  6.0327e-03, -2.3244e-03,  6.9356e-03,\n",
       "                      -5.0458e-04,  6.5660e-03, -7.7588e-03,  8.1408e-03,  1.4749e-03,\n",
       "                       7.7715e-03, -3.8637e-03,  7.4098e-03, -2.4731e-03, -1.5271e-03,\n",
       "                      -5.2852e-03, -1.0320e-02, -3.9096e-03,  1.0299e-04, -7.2018e-03,\n",
       "                      -3.8862e-03, -3.0657e-03, -3.3569e-03, -1.0884e-03,  1.5932e-03,\n",
       "                       1.8049e-03])),\n",
       "             ('transformer.layer.1.ln_1.bn.running_mean',\n",
       "              tensor([ 1.1139e-02,  1.5441e-01,  1.1539e-01,  1.0086e-01,  1.0197e-01,\n",
       "                       2.2128e-02, -3.2519e-02,  6.2103e-02, -1.3045e-01, -6.4599e-02,\n",
       "                      -7.3170e-02, -5.1458e-03, -1.3689e-01,  7.8431e-02, -5.1795e-02,\n",
       "                       4.6568e-02,  5.8648e-02, -1.3398e-01, -3.9924e-05, -3.2148e-02,\n",
       "                      -1.3576e-01,  1.8537e-01,  1.1509e-01,  2.2635e-02,  7.0844e-02,\n",
       "                      -4.4114e-02,  3.5481e-02, -1.7169e-01,  1.2571e-01, -6.7973e-02,\n",
       "                      -7.8461e-02,  8.5584e-02, -5.9387e-02,  4.3660e-02,  2.6600e-01,\n",
       "                      -1.2397e-01, -5.2547e-03,  4.4324e-02, -1.7845e-01, -4.4629e-03,\n",
       "                      -3.9899e-02,  2.1679e-02,  1.1464e-01,  6.7306e-02, -9.9234e-02,\n",
       "                       4.4555e-02, -1.1312e-02, -2.3754e-01,  4.3854e-03, -1.2301e-01,\n",
       "                       5.2956e-02, -2.3322e-02,  1.2563e-01,  1.1484e-01, -1.7586e-01,\n",
       "                       9.1538e-02, -1.8795e-01, -1.2293e-01, -5.1974e-02,  8.3926e-02,\n",
       "                      -8.7769e-02,  1.2748e-01,  3.1639e-02, -1.3973e-01,  2.7462e-02,\n",
       "                      -1.6644e-01,  4.4277e-02,  1.6907e-02,  1.6516e-01,  7.4672e-02,\n",
       "                       9.7025e-02, -1.9486e-01,  2.0828e-02,  1.6407e-01, -2.1148e-01,\n",
       "                       2.2964e-02,  7.8262e-02, -2.8969e-01,  8.4015e-02,  2.6156e-01,\n",
       "                      -1.2613e-01, -1.0260e-01, -1.2713e-01, -1.5781e-02,  1.4045e-01,\n",
       "                      -2.9894e-01, -2.5795e-02,  1.9799e-01,  2.1032e-01,  3.9289e-03,\n",
       "                       5.3842e-02, -2.9281e-01, -6.1684e-02, -1.1645e-01, -6.0875e-02,\n",
       "                      -3.3407e-02,  9.5081e-02,  5.6504e-02,  1.0790e-01,  1.0986e-01,\n",
       "                      -2.1840e-01,  1.8539e-02, -1.4655e-01,  1.3299e-01,  8.7966e-02,\n",
       "                       5.1052e-02,  2.6262e-02,  1.3373e-01, -3.6845e-02,  4.6378e-02,\n",
       "                       1.8143e-02,  1.3235e-01, -9.9589e-03, -6.1850e-03, -1.3508e-01,\n",
       "                      -1.8862e-01,  3.4351e-02, -2.0028e-01,  3.9759e-02, -7.9982e-02,\n",
       "                      -2.5930e-01,  6.1060e-02,  1.5390e-01,  3.1436e-02,  5.1514e-03,\n",
       "                      -5.5080e-02,  1.5296e-01, -1.7800e-01,  5.4793e-02, -2.1589e-01,\n",
       "                      -1.0736e-01, -6.0281e-03, -2.7824e-02,  2.3527e-01,  6.2133e-02,\n",
       "                      -8.3617e-02, -1.2886e-01, -1.9083e-01,  8.5297e-02, -9.8991e-02,\n",
       "                      -1.0497e-01,  6.5014e-02, -8.6640e-02, -3.5754e-01, -4.0042e-02,\n",
       "                       1.5787e-02, -2.3844e-02,  2.6347e-02, -7.5449e-02,  3.6007e-02,\n",
       "                      -2.4754e-01,  1.3986e-02, -7.2623e-02, -3.2258e-02, -1.4196e-01,\n",
       "                      -4.4362e-02,  9.7508e-03,  4.6639e-02, -3.1924e-02,  5.5722e-02,\n",
       "                      -2.6200e-01,  2.5941e-01, -2.3457e-01,  6.0053e-02,  1.4134e-01,\n",
       "                       4.7252e-02, -1.8339e-01, -1.3771e-01, -2.1559e-01,  9.1178e-02,\n",
       "                      -1.6983e-01,  1.0473e-01,  9.7824e-02,  2.6898e-02, -7.4671e-02,\n",
       "                       3.2569e-02,  9.9439e-02, -4.5330e-02, -1.0039e-01,  3.8105e-02,\n",
       "                      -1.1793e-01, -3.8735e-02,  1.5331e-01,  2.1662e-02,  4.2178e-02,\n",
       "                      -1.1204e-01, -8.0245e-02, -1.2954e-01, -1.6914e-01,  8.1506e-02,\n",
       "                      -1.3791e-01,  3.2912e-01, -1.5844e-01,  5.2776e-02, -8.9005e-03,\n",
       "                      -7.9340e-02, -4.1346e-01,  3.3298e-03, -9.8208e-02,  9.2478e-02,\n",
       "                       1.2854e-01, -1.3172e-01, -1.1384e-01,  1.3338e-01,  1.9197e-01,\n",
       "                      -2.1196e-01,  4.5352e-02,  1.0272e-01, -7.9552e-02,  7.0065e-02,\n",
       "                       2.2445e-02,  8.6010e-03, -2.0632e-02, -1.7339e-01, -2.1702e-01,\n",
       "                      -2.5469e-01,  7.9613e-02, -6.6653e-02, -4.2330e-04,  1.7982e-02,\n",
       "                       2.2468e-01,  1.5981e-01, -1.0608e-01, -2.1494e-01,  1.4774e-01,\n",
       "                       1.7087e-01,  5.2804e-02, -5.2291e-02, -4.7880e-02,  3.7800e-01,\n",
       "                      -4.0258e-02, -3.6016e-02, -9.1592e-02,  4.2890e-02,  8.6158e-02,\n",
       "                      -1.0563e-01, -2.9807e-01, -4.1502e-02, -4.3571e-02,  5.2386e-02,\n",
       "                       6.4171e-03,  6.2174e-02, -1.6760e-01,  2.3647e-01,  2.3520e-02,\n",
       "                      -2.7284e-01, -7.6285e-02,  2.4832e-02, -1.5880e-01, -1.8341e-01,\n",
       "                       5.7653e-02,  4.1724e-02, -2.7600e-02, -9.4982e-02, -2.7544e-01,\n",
       "                      -5.9760e-02])),\n",
       "             ('transformer.layer.1.ln_1.bn.running_var',\n",
       "              tensor([1.1033, 1.0432, 1.0183, 1.0500, 0.9855, 1.0931, 1.0439, 1.0350, 1.0518,\n",
       "                      1.0544, 1.0641, 1.0645, 1.0392, 1.0054, 1.0231, 1.0044, 1.0192, 0.9599,\n",
       "                      1.0473, 1.0104, 0.9968, 1.0258, 1.0443, 1.0178, 1.0672, 1.1176, 1.0329,\n",
       "                      1.0873, 1.0577, 1.0530, 1.0039, 1.0278, 1.0293, 1.0099, 1.0441, 1.0627,\n",
       "                      1.0239, 1.0314, 0.9695, 1.0625, 1.0818, 1.0460, 1.0204, 1.0429, 1.0220,\n",
       "                      1.0164, 1.0350, 1.0767, 1.0520, 1.0461, 1.0272, 1.0502, 0.9929, 1.0492,\n",
       "                      1.0361, 1.0371, 1.0138, 1.0513, 1.0876, 0.9735, 1.0299, 0.9701, 1.0592,\n",
       "                      1.0017, 1.0320, 1.0372, 0.9741, 1.0133, 1.0668, 0.9822, 1.0476, 0.9450,\n",
       "                      0.9798, 1.0441, 0.9788, 1.1448, 0.9832, 1.0937, 1.0427, 1.0188, 1.0787,\n",
       "                      1.0354, 1.0573, 1.0409, 1.0294, 1.0981, 1.1061, 1.0575, 1.0609, 1.0710,\n",
       "                      1.0707, 1.0341, 1.0641, 1.0417, 1.0230, 0.9654, 0.9841, 0.9771, 1.0003,\n",
       "                      1.0244, 1.0036, 1.0452, 1.0454, 0.9848, 0.9974, 1.1427, 0.9959, 0.9853,\n",
       "                      1.0678, 0.9970, 1.0179, 1.0616, 1.0373, 1.0306, 1.0441, 0.9299, 1.0579,\n",
       "                      1.0239, 1.0620, 0.9936, 1.0690, 0.9638, 1.0729, 1.0719, 1.0439, 0.9687,\n",
       "                      1.0046, 1.1030, 1.0135, 1.0096, 0.9952, 1.0575, 1.0608, 1.0719, 0.9842,\n",
       "                      0.9927, 0.9772, 1.0270, 1.0925, 0.9793, 1.0268, 1.0366, 0.9834, 1.0327,\n",
       "                      1.0388, 0.9900, 1.0105, 1.0517, 1.0497, 1.0206, 1.0536, 1.0243, 1.0068,\n",
       "                      1.0358, 1.0978, 1.0729, 1.0664, 1.0748, 1.0141, 1.0272, 1.0366, 0.9710,\n",
       "                      1.0201, 1.0659, 1.0192, 1.0368, 1.0496, 1.0301, 0.9891, 0.9521, 1.0530,\n",
       "                      0.9831, 0.9514, 1.0495, 1.0363, 1.0566, 1.0154, 1.0215, 1.0709, 1.0324,\n",
       "                      1.0471, 1.0398, 1.0214, 1.0146, 1.1199, 1.0366, 1.0296, 1.0396, 1.0525,\n",
       "                      1.0182, 1.0364, 1.0133, 1.0839, 1.0291, 1.0432, 1.0077, 1.0752, 1.0203,\n",
       "                      1.0220, 1.0641, 1.0535, 1.0942, 1.0811, 1.0717, 1.0118, 1.0736, 1.0451,\n",
       "                      1.0070, 0.9879, 1.0629, 1.0582, 0.9886, 1.0738, 1.0105, 0.9976, 1.0374,\n",
       "                      1.0130, 1.0024, 1.1088, 1.0263, 0.9969, 1.0849, 1.0101, 1.0430, 1.0199,\n",
       "                      1.0436, 1.0303, 1.0576, 1.0494, 1.0157, 1.0423, 1.0492, 1.0377, 1.0853,\n",
       "                      1.0643, 1.0718, 1.0914, 1.0004, 1.0318, 1.0022, 1.0329, 0.9753, 1.0275,\n",
       "                      1.0359, 0.9929, 0.9399, 1.0037, 1.0580, 1.0311, 1.0435, 1.0287, 1.0051,\n",
       "                      1.0385, 1.0428, 1.0632, 1.0259])),\n",
       "             ('transformer.layer.1.ln_1.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.9571)),\n",
       "             ('transformer.layer.1.ln_2.bn.weight',\n",
       "              tensor([0.9831, 1.0079, 1.0054, 0.9961, 1.0219, 0.9974, 0.9965, 0.9988, 1.0022,\n",
       "                      0.9969, 0.9748, 0.9880, 0.9796, 0.9950, 1.0020, 0.9958, 0.9953, 0.9800,\n",
       "                      0.9917, 0.9957, 0.9950, 1.0026, 0.9891, 0.9901, 1.0093, 0.9957, 0.9909,\n",
       "                      1.0078, 0.9992, 1.0023, 0.9896, 1.0086, 0.9895, 0.9783, 0.9703, 0.9986,\n",
       "                      0.9989, 1.0226, 0.9950, 1.0076, 0.9941, 0.9928, 0.9742, 0.9934, 0.9953,\n",
       "                      0.9915, 0.9860, 0.9958, 1.0101, 0.9899, 1.0053, 0.9970, 0.9896, 1.0014,\n",
       "                      0.9983, 0.9677, 0.9818, 0.9887, 0.9839, 0.9920, 1.0060, 0.9750, 0.9835,\n",
       "                      0.9916, 0.9750, 1.0108, 0.9988, 0.9874, 0.9782, 1.0152, 0.9805, 0.9864,\n",
       "                      0.9892, 0.9947, 0.9761, 1.0020, 0.9885, 0.9667, 0.9924, 1.0001, 0.9770,\n",
       "                      0.9956, 0.9998, 1.0158, 0.9911, 0.9854, 0.9901, 0.9829, 1.0041, 1.0120,\n",
       "                      0.9811, 0.9989, 1.0080, 0.9922, 1.0077, 0.9867, 0.9968, 0.9811, 1.0070,\n",
       "                      0.9955, 0.9771, 1.0112, 0.9920, 1.0053, 0.9876, 1.0030, 1.0151, 0.9948,\n",
       "                      0.9674, 0.9946, 1.0019, 0.9759, 0.9872, 0.9893, 0.9974, 0.9946, 0.9858,\n",
       "                      0.9860, 0.9886, 0.9968, 0.9902, 0.9883, 0.9659, 0.9846, 0.9814, 0.9808,\n",
       "                      0.9886, 0.9850, 0.9824, 0.9845, 0.9859, 1.0141, 0.9733, 0.9876, 0.9920,\n",
       "                      0.9949, 0.9952, 1.0184, 0.9934, 0.9684, 1.0093, 1.0035, 0.9852, 0.9783,\n",
       "                      0.9836, 0.9887, 0.9958, 0.9815, 0.9965, 0.9884, 0.9741, 0.9836, 1.0124,\n",
       "                      0.9779, 0.9963, 1.0127, 1.0056, 0.9954, 0.9948, 0.9766, 1.0048, 0.9928,\n",
       "                      0.9938, 1.0110, 0.9843, 0.9822, 0.9910, 0.9909, 0.9759, 0.9716, 1.0075,\n",
       "                      0.9870, 0.9887, 0.9977, 0.9949, 0.9852, 0.9749, 0.9874, 0.9857, 1.0015,\n",
       "                      0.9860, 0.9869, 1.0038, 1.0065, 1.0047, 0.9825, 0.9787, 0.9935, 0.9988,\n",
       "                      0.9974, 1.0004, 0.9787, 0.9689, 0.9696, 0.9813, 1.0047, 1.0058, 1.0073,\n",
       "                      0.9890, 1.0005, 0.9837, 1.0025, 0.9964, 0.9877, 1.0060, 1.0121, 0.9818,\n",
       "                      0.9579, 0.9970, 1.0190, 0.9763, 1.0005, 0.9952, 1.0000, 0.9996, 0.9836,\n",
       "                      1.0126, 0.9999, 0.9875, 0.9900, 0.9964, 1.0033, 0.9892, 0.9759, 0.9907,\n",
       "                      0.9822, 0.9948, 0.9778, 0.9799, 0.9828, 0.9693, 1.0185, 1.0125, 0.9905,\n",
       "                      0.9993, 0.9903, 0.9887, 0.9933, 0.9999, 0.9985, 0.9938, 0.9882, 0.9912,\n",
       "                      0.9966, 0.9874, 0.9870, 0.9788, 0.9902, 1.0050, 0.9749, 0.9955, 0.9911,\n",
       "                      0.9913, 0.9864, 0.9788, 1.0012])),\n",
       "             ('transformer.layer.1.ln_2.bn.bias',\n",
       "              tensor([ 5.0810e-04,  2.2260e-03,  2.0139e-02,  1.4007e-02, -1.7009e-02,\n",
       "                       1.4115e-02, -2.3324e-02, -1.2341e-03, -1.2072e-02, -2.0208e-03,\n",
       "                      -1.8786e-02, -3.0154e-03,  3.9364e-03, -3.3066e-03,  1.9050e-02,\n",
       "                      -6.3591e-04,  1.8423e-02,  1.8148e-02,  1.0736e-02,  3.2033e-03,\n",
       "                       4.8249e-03, -8.6844e-03, -9.3156e-03, -9.7410e-03, -1.6282e-02,\n",
       "                      -3.0445e-04, -7.9906e-03,  8.2209e-03, -2.1252e-03, -2.4963e-02,\n",
       "                      -9.3795e-03,  1.6345e-02,  2.2916e-02,  1.9191e-03,  5.3739e-03,\n",
       "                       8.4440e-04, -8.8147e-03, -2.4676e-02,  1.2385e-02, -4.8957e-03,\n",
       "                      -1.1943e-02, -9.0304e-03,  6.3513e-03,  5.4888e-03,  1.1922e-03,\n",
       "                       6.7514e-03,  2.9962e-04, -2.9280e-02, -7.5892e-03,  4.0167e-03,\n",
       "                      -5.4013e-03, -7.1883e-03, -2.1465e-03, -9.5972e-03,  2.8820e-03,\n",
       "                      -6.5771e-03,  1.3990e-03, -1.4911e-02, -1.1819e-02,  6.5806e-03,\n",
       "                       8.5870e-03, -1.4183e-02,  1.0823e-02,  4.9194e-03, -8.6388e-03,\n",
       "                      -3.3424e-03,  3.7726e-06,  4.1957e-03, -4.3190e-03, -2.3190e-03,\n",
       "                      -5.3815e-03, -5.8048e-03,  3.2512e-03,  2.2481e-02,  5.2318e-03,\n",
       "                      -4.0507e-03, -1.5391e-03, -2.0346e-03, -1.4961e-02,  1.6682e-02,\n",
       "                      -9.1186e-03,  8.2513e-03,  2.1799e-02, -9.8468e-03,  5.1663e-05,\n",
       "                       8.1228e-05, -1.1013e-02, -1.7367e-02,  4.6571e-03, -1.3915e-02,\n",
       "                      -2.3020e-03,  2.0314e-02,  9.7351e-03,  2.3743e-02, -1.3724e-02,\n",
       "                       9.7753e-04, -2.8619e-02, -1.0550e-02,  2.2225e-02, -4.1173e-03,\n",
       "                       1.6873e-02, -2.0226e-02, -2.0354e-03,  3.4337e-03, -2.2665e-03,\n",
       "                       7.2060e-03, -3.3002e-03,  1.0266e-02,  4.7099e-03,  2.4212e-02,\n",
       "                      -1.6816e-03,  8.1377e-04,  3.3085e-03,  2.4109e-03, -1.1460e-02,\n",
       "                       2.1251e-02, -1.3491e-03, -1.1962e-02,  3.0592e-03, -1.4343e-02,\n",
       "                       5.1494e-03, -7.3220e-03, -7.0191e-03, -1.3196e-02, -6.9530e-04,\n",
       "                      -1.7020e-02, -2.5450e-02, -1.3397e-02,  9.5504e-03,  8.2364e-03,\n",
       "                       8.6899e-03,  8.8057e-03,  1.3925e-02,  1.6805e-02,  2.1568e-04,\n",
       "                       8.7147e-03, -1.5728e-03, -1.1679e-04,  1.5903e-02, -7.1933e-03,\n",
       "                      -1.2309e-02,  8.7611e-03, -4.4405e-03, -3.6146e-03, -3.8659e-03,\n",
       "                      -9.8101e-03,  1.0793e-04,  3.3641e-03,  5.9730e-03,  1.4035e-02,\n",
       "                       1.3719e-02, -9.8242e-03, -2.7027e-03, -3.9151e-03, -1.1985e-02,\n",
       "                      -4.8718e-03,  7.9249e-03, -2.8365e-03, -2.5403e-03, -9.2564e-03,\n",
       "                       1.9051e-02, -8.1137e-03,  3.1790e-03,  2.2123e-02,  2.0604e-02,\n",
       "                       8.7327e-03,  1.7366e-02,  1.5353e-02, -2.2964e-02,  4.2339e-03,\n",
       "                      -6.1526e-03,  2.0417e-03, -1.4667e-03, -7.8034e-03,  5.0231e-03,\n",
       "                       3.9734e-03, -2.0449e-02, -3.1907e-03, -1.9084e-02, -6.5902e-03,\n",
       "                       7.0325e-03, -3.4281e-03, -4.5021e-04,  1.1948e-02, -8.5247e-03,\n",
       "                       1.5528e-02,  5.2197e-03, -1.9829e-02, -7.3630e-04,  1.8906e-02,\n",
       "                      -5.3263e-03,  7.6433e-03, -6.3328e-03,  1.0419e-02, -3.1822e-02,\n",
       "                      -1.0802e-02,  1.4463e-02, -9.5410e-03, -3.1390e-03, -1.0768e-02,\n",
       "                       1.6894e-02, -1.8262e-02,  3.7256e-03,  1.3838e-02, -3.4028e-03,\n",
       "                       1.2367e-02, -1.8607e-04, -4.1150e-03, -7.6958e-04,  1.4816e-02,\n",
       "                      -3.4654e-03, -2.9064e-03, -4.5450e-03, -4.1109e-03, -8.3334e-03,\n",
       "                       2.0326e-02,  1.2961e-02,  4.9955e-03, -7.9388e-03, -1.9305e-03,\n",
       "                      -1.2993e-02, -1.3283e-03,  9.7526e-03, -5.5189e-03,  3.0794e-03,\n",
       "                      -4.6258e-03,  1.2484e-02, -9.9925e-03, -4.8786e-03,  2.3004e-02,\n",
       "                       4.2997e-03, -9.9293e-03, -7.5440e-03, -1.1227e-03, -3.2404e-03,\n",
       "                       3.1280e-03,  2.8269e-02,  1.3784e-02,  1.4942e-02, -5.6776e-03,\n",
       "                      -1.2616e-02, -1.3355e-02, -3.0417e-02, -9.7531e-04,  4.8611e-03,\n",
       "                      -4.3179e-03,  8.5396e-04,  2.3420e-02,  4.8037e-03,  6.5845e-03,\n",
       "                      -1.4697e-02, -1.8788e-02,  1.1566e-03,  1.4785e-02, -8.8301e-03,\n",
       "                       5.9381e-03])),\n",
       "             ('transformer.layer.1.ln_2.bn.running_mean',\n",
       "              tensor([-1.9021e-02, -1.9254e-02, -3.0956e-04, -5.2507e-03,  6.1926e-03,\n",
       "                      -1.0408e-02, -8.9222e-03,  1.6712e-02,  3.0598e-03, -8.3229e-05,\n",
       "                      -2.7902e-02, -1.4596e-02,  7.3436e-03, -1.4639e-02,  1.3739e-02,\n",
       "                       1.1970e-02, -1.7694e-02,  2.5341e-02,  1.6107e-02, -2.6036e-02,\n",
       "                       1.2762e-02,  1.6440e-02, -1.5854e-02,  3.9164e-03, -7.3323e-04,\n",
       "                      -1.2801e-02,  6.2168e-03, -2.1561e-02, -9.2425e-03, -2.3547e-02,\n",
       "                       1.5407e-02,  5.9060e-04,  1.1714e-02,  1.0355e-03, -5.4140e-03,\n",
       "                      -1.0986e-02,  3.2414e-04,  2.0229e-02,  2.9609e-02, -1.3866e-02,\n",
       "                      -1.6150e-02,  1.5443e-03, -1.9695e-02, -1.4983e-02,  6.5565e-03,\n",
       "                       1.4927e-02, -1.8320e-02,  1.4951e-03, -1.5021e-02, -3.8744e-03,\n",
       "                       2.0430e-02,  1.5196e-02, -3.0891e-02, -1.5315e-02, -1.3729e-02,\n",
       "                       6.9245e-03, -1.3314e-02, -1.0494e-02,  6.5611e-03, -6.9887e-03,\n",
       "                       4.2155e-03,  1.0500e-02, -8.4994e-03, -1.2193e-02,  1.0530e-02,\n",
       "                       5.9812e-03,  5.3299e-03, -5.0148e-03,  1.2544e-02,  6.7684e-04,\n",
       "                       1.8312e-02, -2.7306e-02,  2.5535e-03, -1.6624e-02, -1.3025e-02,\n",
       "                       6.9766e-03, -1.1135e-02, -3.3762e-03, -1.1720e-02,  7.4370e-03,\n",
       "                       3.4904e-03, -3.7858e-03, -1.1269e-02,  9.6979e-03,  8.6640e-03,\n",
       "                      -6.8590e-03,  9.7572e-04, -1.7568e-02, -1.2438e-02, -3.6310e-03,\n",
       "                      -8.3306e-03, -3.6405e-02,  2.1746e-02,  1.2477e-03,  8.7972e-03,\n",
       "                      -5.9339e-04, -8.6709e-03, -3.3926e-04,  2.4160e-02,  1.3363e-04,\n",
       "                      -4.2162e-03, -3.2987e-03, -8.9551e-03, -1.8979e-03,  2.0890e-02,\n",
       "                      -8.0663e-03,  2.5025e-02,  5.1623e-03, -1.1619e-02,  1.5582e-02,\n",
       "                       2.0806e-02, -4.5018e-03,  5.6075e-03,  1.2597e-02,  7.1517e-03,\n",
       "                      -1.6939e-02,  8.2110e-03, -1.4567e-02, -8.2460e-03, -8.5253e-03,\n",
       "                      -1.1154e-02, -1.4337e-02, -1.0535e-02, -6.4409e-03,  1.0007e-02,\n",
       "                       6.2339e-03, -1.7154e-03, -1.2003e-02,  8.3916e-03,  7.0523e-03,\n",
       "                      -1.5099e-02,  1.1364e-02, -7.4600e-03, -5.2077e-03, -1.7737e-02,\n",
       "                       2.2117e-02, -3.1385e-03,  1.7640e-02,  8.0655e-03, -2.6088e-02,\n",
       "                       5.4053e-03,  1.2795e-02,  1.9379e-03,  6.5554e-03, -7.8491e-03,\n",
       "                       3.8630e-03, -1.5895e-02, -2.8392e-02,  2.1838e-02,  1.4942e-02,\n",
       "                       1.0721e-02, -5.2960e-03, -2.2822e-02, -5.4426e-03, -5.0739e-03,\n",
       "                      -8.2867e-04,  1.1658e-02, -1.9404e-02,  9.0003e-03, -8.1483e-03,\n",
       "                      -5.2467e-03, -6.7552e-03, -7.6710e-03,  1.6222e-02, -6.8017e-03,\n",
       "                      -4.9373e-03, -1.6385e-02,  2.5699e-02,  4.3059e-03, -3.1656e-03,\n",
       "                       1.0242e-02, -2.7279e-02, -2.4342e-03, -7.1079e-04,  4.2255e-03,\n",
       "                       1.3536e-02, -5.2789e-03,  5.1763e-03, -3.7729e-03, -8.1752e-03,\n",
       "                      -2.5513e-03, -1.0004e-02,  1.0159e-02,  6.1186e-03, -1.1485e-02,\n",
       "                       1.6314e-02,  4.4675e-03,  1.5057e-02,  8.3205e-03, -7.5839e-04,\n",
       "                      -4.5638e-03, -2.2232e-03,  1.6166e-02, -1.2078e-02,  3.7097e-03,\n",
       "                      -1.3655e-02, -3.1096e-02, -1.0156e-02, -9.8921e-03,  7.8636e-04,\n",
       "                      -7.1055e-03,  9.3466e-04, -1.8699e-02, -2.1860e-02,  1.6112e-02,\n",
       "                      -9.8161e-03, -1.1986e-03, -1.2667e-02, -2.2781e-03,  4.6060e-04,\n",
       "                       2.1649e-02,  6.7031e-03,  1.5357e-02, -6.5635e-03,  3.8374e-02,\n",
       "                       3.5005e-03,  6.6233e-04, -7.4751e-03,  5.8409e-03, -6.5037e-04,\n",
       "                       6.1062e-03, -4.1679e-03, -8.4528e-03, -6.8942e-03,  3.0859e-03,\n",
       "                       1.5446e-02,  1.7444e-02,  8.1855e-03, -9.0888e-03,  1.0062e-02,\n",
       "                      -5.9940e-03,  5.0823e-03,  5.4972e-03, -5.9466e-04,  1.1431e-02,\n",
       "                       1.1935e-02,  2.8431e-02, -1.6094e-02,  2.5942e-02,  2.1838e-02,\n",
       "                      -1.5457e-02,  1.3336e-03, -1.9556e-02, -3.8828e-03, -3.7749e-03,\n",
       "                      -1.6257e-02,  2.0440e-03, -2.1535e-02,  9.8596e-03,  1.0084e-02,\n",
       "                      -1.6165e-02, -9.0303e-03, -7.9722e-03,  1.2207e-02,  1.1675e-03,\n",
       "                       1.6462e-02])),\n",
       "             ('transformer.layer.1.ln_2.bn.running_var',\n",
       "              tensor([0.9857, 0.9734, 0.9868, 1.0032, 0.9843, 0.9848, 0.9754, 0.9659, 0.9696,\n",
       "                      0.9697, 0.9733, 0.9767, 0.9654, 0.9795, 0.9753, 0.9727, 0.9827, 0.9733,\n",
       "                      0.9796, 1.0122, 0.9786, 0.9692, 0.9767, 0.9780, 0.9735, 0.9897, 0.9924,\n",
       "                      0.9929, 0.9659, 0.9800, 0.9989, 0.9835, 0.9972, 0.9910, 1.0051, 0.9697,\n",
       "                      0.9912, 0.9794, 0.9642, 0.9789, 0.9913, 1.0098, 0.9715, 0.9669, 0.9851,\n",
       "                      0.9929, 0.9909, 0.9602, 0.9745, 0.9856, 0.9708, 0.9952, 0.9821, 0.9627,\n",
       "                      0.9823, 0.9859, 0.9792, 0.9828, 0.9555, 0.9971, 0.9930, 0.9668, 0.9653,\n",
       "                      0.9857, 0.9856, 0.9921, 0.9906, 0.9968, 0.9798, 0.9824, 0.9664, 0.9871,\n",
       "                      0.9914, 0.9932, 0.9841, 0.9746, 0.9969, 0.9435, 0.9683, 1.0013, 0.9910,\n",
       "                      0.9657, 0.9771, 0.9729, 0.9644, 0.9932, 0.9969, 0.9785, 0.9841, 0.9827,\n",
       "                      0.9913, 1.0006, 0.9867, 0.9989, 0.9910, 0.9998, 0.9972, 0.9775, 0.9561,\n",
       "                      0.9567, 0.9879, 0.9714, 1.0111, 0.9767, 0.9676, 1.0051, 0.9640, 0.9892,\n",
       "                      0.9939, 0.9685, 0.9915, 0.9878, 0.9640, 0.9802, 1.0161, 0.9549, 0.9923,\n",
       "                      0.9976, 0.9922, 0.9503, 0.9966, 0.9745, 0.9768, 0.9923, 0.9787, 0.9979,\n",
       "                      0.9707, 0.9702, 0.9746, 0.9759, 0.9792, 0.9848, 0.9811, 0.9613, 0.9918,\n",
       "                      0.9969, 0.9960, 0.9699, 0.9805, 0.9745, 0.9879, 0.9658, 0.9805, 0.9841,\n",
       "                      0.9786, 0.9718, 0.9532, 0.9724, 0.9756, 0.9844, 0.9699, 0.9668, 0.9938,\n",
       "                      0.9696, 0.9726, 0.9694, 1.0007, 0.9604, 0.9616, 0.9801, 0.9792, 0.9446,\n",
       "                      0.9889, 0.9754, 0.9748, 0.9850, 0.9730, 1.0038, 0.9982, 0.9691, 0.9790,\n",
       "                      0.9880, 0.9726, 0.9747, 1.0049, 0.9809, 0.9759, 0.9756, 0.9807, 0.9866,\n",
       "                      0.9910, 0.9896, 1.0003, 0.9776, 0.9887, 0.9831, 0.9873, 0.9937, 0.9679,\n",
       "                      0.9642, 0.9866, 0.9733, 0.9906, 0.9869, 0.9946, 0.9901, 0.9919, 0.9895,\n",
       "                      0.9832, 0.9865, 0.9779, 1.0158, 0.9960, 0.9688, 0.9758, 0.9943, 0.9656,\n",
       "                      0.9892, 0.9963, 0.9855, 0.9727, 0.9983, 0.9642, 0.9682, 0.9804, 0.9794,\n",
       "                      0.9844, 0.9871, 0.9743, 1.0132, 0.9948, 0.9875, 1.0067, 0.9745, 0.9773,\n",
       "                      0.9720, 0.9851, 0.9912, 0.9707, 0.9801, 0.9883, 1.0123, 0.9770, 0.9822,\n",
       "                      0.9772, 0.9794, 0.9920, 0.9823, 0.9864, 0.9752, 1.0170, 0.9897, 0.9735,\n",
       "                      0.9933, 0.9880, 0.9744, 0.9864, 0.9718, 0.9652, 0.9748, 1.0104, 0.9867,\n",
       "                      0.9812, 0.9929, 0.9977, 1.0020])),\n",
       "             ('transformer.layer.1.ln_2.bn.num_batches_tracked', tensor(501)),\n",
       "             ('transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       "              tensor(2.4514)),\n",
       "             ('transformer.ln_out.bn.weight',\n",
       "              tensor([1.0098, 1.0182, 1.0206, 0.9940, 1.0315, 1.0193, 1.0004, 1.0023, 1.0057,\n",
       "                      0.9918, 1.0130, 0.9933, 1.0149, 1.0294, 0.9949, 1.0068, 1.0120, 1.0224,\n",
       "                      1.0030, 1.0115, 0.9876, 1.0168, 1.0086, 1.0177, 1.0216, 1.0171, 1.0006,\n",
       "                      1.0164, 1.0143, 1.0215, 1.0114, 0.9880, 0.9893, 1.0057, 1.0144, 0.9998,\n",
       "                      1.0189, 1.0160, 0.9874, 1.0189, 0.9929, 1.0300, 1.0104, 1.0037, 1.0004,\n",
       "                      1.0024, 1.0053, 1.0110, 1.0136, 1.0256, 1.0079, 1.0065, 1.0299, 1.0078,\n",
       "                      1.0012, 0.9975, 0.9710, 1.0101, 1.0126, 1.0157, 1.0074, 1.0027, 1.0170,\n",
       "                      1.0229, 1.0147, 1.0192, 1.0030, 1.0072, 1.0302, 1.0167, 1.0174, 1.0123,\n",
       "                      1.0002, 1.0119, 1.0056, 1.0248, 1.0043, 1.0070, 0.9948, 0.9980, 1.0049,\n",
       "                      0.9948, 0.9829, 0.9788, 1.0201, 1.0257, 1.0108, 0.9941, 1.0030, 1.0316,\n",
       "                      0.9967, 1.0079, 1.0311, 1.0181, 1.0181, 1.0116, 0.9900, 1.0156, 0.9960,\n",
       "                      0.9906, 1.0092, 0.9983, 1.0271, 1.0082, 1.0130, 1.0217, 1.0167, 1.0176,\n",
       "                      0.9991, 1.0043, 1.0212, 1.0026, 1.0277, 0.9908, 1.0168, 1.0010, 1.0118,\n",
       "                      0.9952, 1.0050, 1.0103, 1.0050, 1.0244, 1.0176, 0.9882, 0.9866, 1.0051,\n",
       "                      1.0036, 1.0082, 1.0149, 1.0202, 1.0042, 1.0413, 1.0166, 1.0226, 1.0058,\n",
       "                      1.0240, 0.9976, 1.0226, 1.0226, 1.0170, 1.0164, 1.0039, 1.0253, 1.0155,\n",
       "                      0.9835, 0.9922, 1.0211, 1.0205, 0.9855, 1.0257, 0.9913, 1.0290, 1.0055,\n",
       "                      1.0079, 1.0054, 1.0004, 0.9977, 1.0273, 1.0101, 1.0184, 1.0115, 1.0320,\n",
       "                      1.0169, 1.0317, 1.0170, 1.0118, 1.0101, 1.0018, 0.9991, 1.0060, 1.0218,\n",
       "                      1.0239, 1.0106, 1.0193, 1.0137, 0.9872, 0.9885, 1.0219, 1.0399, 0.9963,\n",
       "                      1.0011, 0.9980, 1.0137, 0.9815, 1.0146, 1.0338, 1.0161, 1.0183, 1.0066,\n",
       "                      1.0055, 0.9909, 0.9987, 1.0081, 1.0107, 0.9875, 1.0136, 1.0337, 1.0043,\n",
       "                      1.0020, 0.9884, 1.0179, 1.0238, 1.0131, 1.0110, 1.0024, 1.0192, 1.0095,\n",
       "                      1.0121, 1.0183, 1.0073, 0.9964, 0.9865, 1.0182, 1.0095, 0.9935, 1.0159,\n",
       "                      1.0196, 0.9940, 1.0054, 1.0104, 1.0103, 1.0225, 1.0099, 1.0282, 0.9893,\n",
       "                      0.9953, 1.0268, 0.9931, 1.0150, 0.9976, 0.9982, 1.0164, 1.0189, 1.0129,\n",
       "                      1.0222, 0.9824, 1.0187, 1.0157, 1.0350, 1.0114, 1.0163, 1.0136, 1.0236,\n",
       "                      1.0001, 1.0041, 1.0137, 1.0097, 1.0074, 1.0075, 1.0170, 1.0172, 0.9880,\n",
       "                      1.0179, 0.9931, 1.0029, 1.0216])),\n",
       "             ('transformer.ln_out.bn.bias',\n",
       "              tensor([ 0.0330,  0.0654,  0.0652,  0.0756,  0.0675,  0.0731,  0.0707,  0.0718,\n",
       "                       0.0746,  0.0751, -0.0680,  0.0748, -0.0683, -0.0693, -0.0662,  0.0733,\n",
       "                      -0.0735, -0.0425,  0.0659, -0.0752,  0.0604,  0.0705, -0.0752, -0.0696,\n",
       "                       0.0505,  0.0651,  0.0608, -0.0721,  0.0683,  0.0671, -0.0751,  0.0733,\n",
       "                       0.0683, -0.0739,  0.0532, -0.0737,  0.0448, -0.0695, -0.0712, -0.0538,\n",
       "                      -0.0666,  0.0757,  0.0750,  0.0731, -0.0688,  0.0650,  0.0709, -0.0397,\n",
       "                       0.0696, -0.0719,  0.0564,  0.0693,  0.0719,  0.0752,  0.0521, -0.0629,\n",
       "                       0.0738,  0.0584,  0.0762, -0.0709, -0.0551, -0.0720,  0.0490, -0.0655,\n",
       "                       0.0526, -0.0558, -0.0724,  0.0747,  0.0628,  0.0675, -0.0693,  0.0616,\n",
       "                       0.0486, -0.0532,  0.0708,  0.0676, -0.0645, -0.0670,  0.0740,  0.0729,\n",
       "                      -0.0592,  0.0711, -0.0745,  0.0725,  0.0494, -0.0753,  0.0739,  0.0396,\n",
       "                      -0.0474,  0.0629,  0.0699, -0.0526,  0.0759,  0.0648,  0.0728, -0.0759,\n",
       "                       0.0728,  0.0699, -0.0589,  0.0699,  0.0758,  0.0709, -0.0755, -0.0739,\n",
       "                      -0.0736, -0.0734, -0.0738,  0.0521, -0.0742, -0.0746,  0.0556, -0.0658,\n",
       "                       0.0699,  0.0730,  0.0560,  0.0733, -0.0560, -0.0692,  0.0716, -0.0656,\n",
       "                      -0.0706,  0.0749, -0.0486,  0.0729,  0.0605, -0.0438, -0.0613,  0.0477,\n",
       "                      -0.0640, -0.0629,  0.0621, -0.0405, -0.0751, -0.0540,  0.0733, -0.0527,\n",
       "                       0.0613, -0.0743,  0.0698,  0.0614,  0.0695, -0.0366, -0.0685,  0.0655,\n",
       "                      -0.0618, -0.0722, -0.0742,  0.0588,  0.0753,  0.0724, -0.0699,  0.0694,\n",
       "                      -0.0749,  0.0669, -0.0711,  0.0755,  0.0709,  0.0723, -0.0599,  0.0701,\n",
       "                       0.0191,  0.0713,  0.0323,  0.0683, -0.0753,  0.0727, -0.0710, -0.0393,\n",
       "                      -0.0240, -0.0694, -0.0456, -0.0534,  0.0637, -0.0730, -0.0724, -0.0710,\n",
       "                       0.0626,  0.0657,  0.0714, -0.0712, -0.0735,  0.0676,  0.0615, -0.0748,\n",
       "                      -0.0603,  0.0575,  0.0534, -0.0685, -0.0753,  0.0715,  0.0334, -0.0625,\n",
       "                       0.0605,  0.0704,  0.0688, -0.0628,  0.0473, -0.0706, -0.0571, -0.0680,\n",
       "                       0.0693, -0.0536,  0.0706,  0.0752,  0.0653, -0.0696, -0.0596, -0.0588,\n",
       "                       0.0684, -0.0736,  0.0707,  0.0701, -0.0735,  0.0670,  0.0702, -0.0584,\n",
       "                      -0.0736, -0.0754, -0.0749,  0.0719, -0.0690, -0.0675, -0.0703,  0.0743,\n",
       "                      -0.0708,  0.0710, -0.0639,  0.0701,  0.0751,  0.0714,  0.0719, -0.0704,\n",
       "                      -0.0401,  0.0741,  0.0501, -0.0580, -0.0751, -0.0735, -0.0518, -0.0737,\n",
       "                       0.0697,  0.0647, -0.0670, -0.0647, -0.0621,  0.0690,  0.0568, -0.0734,\n",
       "                       0.0744, -0.0694,  0.0640, -0.0629,  0.0475, -0.0728,  0.0697, -0.0714])),\n",
       "             ('transformer.ln_out.bn.running_mean',\n",
       "              tensor([-5.8141e-02,  1.3079e-01, -2.1400e-02,  4.9686e-03, -1.0193e-01,\n",
       "                       2.4564e-02, -7.5131e-02,  1.3508e-01,  7.0382e-02,  2.4447e-01,\n",
       "                      -2.2274e-03, -6.0076e-02,  5.6936e-02,  1.6127e-01,  3.1789e-01,\n",
       "                       2.3420e-01,  4.2342e-02, -4.8172e-02, -6.9289e-02, -3.1723e-02,\n",
       "                      -2.2066e-01, -8.1715e-02, -2.5716e-01,  1.5230e-01,  1.7414e-01,\n",
       "                       2.1539e-01, -1.5150e-01, -7.6097e-02, -1.9538e-02, -5.4798e-02,\n",
       "                       6.3223e-02,  2.0898e-01, -1.4062e-02, -7.6384e-02,  2.9279e-02,\n",
       "                       5.0781e-02, -1.6929e-01,  2.7221e-01,  1.7878e-02,  3.6105e-03,\n",
       "                      -1.7082e-01, -7.1669e-02,  1.8795e-01,  6.2661e-02, -1.0697e-01,\n",
       "                      -1.8645e-01, -1.4992e-01,  8.2736e-02,  7.7978e-02, -1.6668e-01,\n",
       "                      -2.7592e-02,  5.3022e-02,  9.3130e-02,  4.6861e-02,  5.0822e-02,\n",
       "                      -1.0599e-01,  5.1424e-02, -1.3053e-02,  3.0266e-01, -3.9892e-02,\n",
       "                      -1.9838e-01, -1.3706e-01, -3.2121e-02,  1.0754e-01,  9.4593e-02,\n",
       "                      -1.0040e-01, -8.7830e-02, -2.4346e-01, -3.7514e-02,  1.3650e-01,\n",
       "                       1.7066e-02,  8.8375e-02, -1.3163e-03, -8.2109e-02, -2.8969e-01,\n",
       "                       4.2483e-02, -5.7037e-02,  2.7972e-02, -3.0250e-01, -2.4733e-01,\n",
       "                       1.3485e-01, -9.5632e-02,  1.1636e-01, -1.5129e-01,  3.2581e-01,\n",
       "                      -1.4147e-01,  1.4585e-01, -1.3530e-01,  8.9173e-02,  7.4837e-03,\n",
       "                       1.9528e-01, -3.0883e-02, -2.0584e-02,  1.2603e-01, -1.9749e-02,\n",
       "                       1.3061e-01,  2.0388e-01,  6.2487e-02, -1.5527e-01, -7.3609e-04,\n",
       "                      -9.3891e-03,  2.0884e-02, -1.5609e-01, -4.6201e-02,  3.3214e-01,\n",
       "                       8.3825e-02,  7.2540e-02, -3.3934e-02, -3.5572e-02, -6.2375e-02,\n",
       "                       3.8312e-02, -1.3194e-01,  1.7720e-01, -3.4696e-04, -1.3001e-01,\n",
       "                      -1.9460e-01,  1.2290e-01,  1.6266e-01,  1.0195e-01,  1.2091e-02,\n",
       "                       2.7424e-02, -6.8262e-02, -9.8489e-02, -1.6596e-01, -2.1163e-01,\n",
       "                      -4.9602e-02,  1.3223e-01,  5.6779e-02,  6.5867e-02,  2.0605e-01,\n",
       "                       1.2081e-01,  2.8241e-01, -2.0585e-01, -1.3696e-01, -1.1372e-01,\n",
       "                       3.4088e-02, -4.5516e-02, -3.7179e-02,  1.4639e-01, -3.9199e-02,\n",
       "                      -2.6581e-02, -2.4225e-01,  1.8658e-01,  2.2573e-01, -1.7433e-01,\n",
       "                       1.7373e-01,  2.0363e-01, -1.1421e-01, -1.0053e-01,  1.5971e-01,\n",
       "                      -2.3361e-02, -1.0206e-01, -3.2853e-01, -3.3045e-02, -7.2461e-02,\n",
       "                       1.0288e-01, -5.9134e-02, -9.3943e-02,  1.1036e-01, -6.8434e-02,\n",
       "                       7.8905e-02, -9.1916e-02, -1.4464e-01,  2.3768e-01,  1.1836e-01,\n",
       "                      -6.0382e-02,  4.3319e-02,  1.0165e-01, -2.3457e-01, -7.9387e-02,\n",
       "                       3.3591e-01,  9.3516e-03,  2.4645e-01,  9.3470e-02,  3.7699e-02,\n",
       "                       1.0106e-01, -1.0120e-01,  3.8802e-02, -1.8633e-02, -1.4278e-01,\n",
       "                      -7.9285e-02, -1.1819e-01, -1.1813e-01, -3.0546e-02, -1.3863e-01,\n",
       "                       1.6456e-01,  2.1189e-01, -2.1941e-01, -1.7039e-01,  1.8242e-01,\n",
       "                      -5.1359e-02,  2.2068e-01,  9.5329e-02, -2.1995e-02,  3.1592e-02,\n",
       "                       1.2317e-01,  8.0677e-02, -2.2732e-01,  2.8156e-02, -1.8660e-01,\n",
       "                      -2.3038e-01, -3.3026e-02,  2.2210e-01, -1.0486e-01, -1.0387e-01,\n",
       "                       1.2544e-01,  1.6322e-01, -1.5510e-02, -1.4063e-01, -1.2700e-02,\n",
       "                       8.4921e-02,  1.8081e-02, -1.1513e-01,  5.9176e-02, -3.0183e-01,\n",
       "                      -8.6616e-02, -1.6011e-01, -7.0512e-02, -5.0375e-02,  2.5973e-01,\n",
       "                       2.1508e-01, -4.7449e-03,  1.1025e-02,  5.0393e-02, -1.0840e-02,\n",
       "                      -5.5376e-02, -3.9630e-02, -1.8986e-01,  3.4925e-01, -1.8029e-01,\n",
       "                       4.4797e-02,  1.9314e-01, -8.9562e-02,  1.3119e-01,  2.1669e-01,\n",
       "                      -4.2978e-02,  1.8271e-02,  6.0693e-02,  5.0944e-02,  6.1720e-02,\n",
       "                      -1.3678e-01, -5.2286e-02,  6.9372e-02, -1.7248e-01, -8.9332e-02,\n",
       "                       4.9449e-02,  4.9949e-02,  2.6415e-01, -5.8301e-02,  9.1511e-02,\n",
       "                       3.0951e-01,  1.1935e-01,  5.5150e-02,  2.3039e-01, -7.3223e-02,\n",
       "                      -1.0753e-01])),\n",
       "             ('transformer.ln_out.bn.running_var',\n",
       "              tensor([0.9969, 1.0467, 1.0465, 1.0654, 1.0545, 1.0892, 1.0856, 1.0478, 1.0312,\n",
       "                      1.0162, 1.0004, 1.0295, 1.0486, 1.0980, 1.0818, 1.0225, 1.0284, 0.9922,\n",
       "                      1.0822, 1.0137, 1.0299, 1.0202, 1.0466, 1.0210, 1.0818, 1.0014, 0.9589,\n",
       "                      1.0730, 1.0639, 1.1062, 1.0362, 1.0269, 1.0055, 1.0400, 0.9799, 1.0768,\n",
       "                      1.0543, 1.0697, 1.0811, 1.0434, 1.0187, 0.9911, 1.0428, 1.0479, 0.9924,\n",
       "                      1.0044, 1.0620, 1.0827, 1.0550, 1.0243, 1.0035, 1.0683, 1.0615, 1.0242,\n",
       "                      1.0879, 0.9367, 1.0786, 1.0521, 0.9815, 1.0367, 1.0704, 1.0031, 1.0126,\n",
       "                      1.0288, 1.0331, 1.1320, 1.0429, 1.0356, 1.0192, 1.0793, 1.0459, 0.9690,\n",
       "                      1.0143, 1.0810, 1.0111, 1.0727, 0.9373, 0.9825, 1.0224, 1.0877, 1.0015,\n",
       "                      1.0800, 1.1019, 1.0943, 1.0470, 1.0198, 1.0126, 0.9919, 1.0553, 1.0873,\n",
       "                      0.9989, 1.0761, 1.0752, 1.0212, 1.0382, 1.0394, 1.0096, 1.0131, 0.9870,\n",
       "                      1.0654, 1.0145, 1.0793, 1.0067, 1.0213, 1.0847, 1.1082, 1.1286, 1.0252,\n",
       "                      0.9953, 1.0613, 1.0191, 1.0053, 1.0397, 1.0859, 1.0029, 1.0888, 1.0446,\n",
       "                      1.0220, 1.0107, 1.0663, 1.0220, 1.0388, 0.9750, 1.0733, 0.9956, 1.0061,\n",
       "                      1.0360, 1.0505, 1.0497, 0.9533, 1.0502, 1.0774, 1.0269, 0.9849, 1.0073,\n",
       "                      1.0246, 1.0756, 1.0644, 1.0312, 0.9985, 1.0837, 1.0771, 1.0211, 0.9838,\n",
       "                      0.9782, 1.0497, 1.0451, 1.0669, 1.0650, 1.0352, 1.0158, 1.0037, 1.1058,\n",
       "                      1.0264, 1.0702, 1.0836, 1.1206, 1.0433, 1.0114, 0.9476, 1.0889, 1.0146,\n",
       "                      1.0243, 1.0046, 0.9884, 1.0500, 1.0404, 1.0060, 1.0365, 1.0114, 1.1147,\n",
       "                      1.0352, 1.0491, 1.0699, 1.0487, 1.0182, 1.0374, 1.0910, 1.0382, 1.0816,\n",
       "                      0.9940, 0.9976, 1.0391, 1.0654, 1.0333, 1.0092, 1.0135, 1.0358, 1.0536,\n",
       "                      1.0609, 1.0580, 1.0203, 0.9887, 1.0132, 0.9878, 1.0918, 1.0612, 1.0295,\n",
       "                      1.0554, 1.0260, 1.0038, 1.0099, 1.1072, 1.1019, 1.0079, 1.0642, 1.0379,\n",
       "                      1.0148, 1.1003, 1.1166, 1.0295, 1.0274, 1.0525, 1.0162, 1.0531, 1.0604,\n",
       "                      1.0649, 1.0421, 1.0173, 1.0357, 1.0720, 1.0892, 1.0170, 0.9748, 1.0283,\n",
       "                      0.9910, 1.0571, 0.9770, 0.9610, 1.0118, 0.9224, 1.0593, 1.0740, 0.9613,\n",
       "                      1.0454, 0.9824, 1.0161, 1.0442, 1.1050, 0.9971, 1.0708, 1.0707, 1.0677,\n",
       "                      1.0247, 1.0695, 1.0039, 1.0344, 1.0423, 1.0319, 0.9838, 1.0555, 1.0578,\n",
       "                      1.0390, 1.0595, 1.0095, 1.1293])),\n",
       "             ('transformer.ln_out.bn.num_batches_tracked', tensor(501)),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[-0.0106, -0.0437, -0.0375,  ...,  0.0632, -0.0366,  0.0532],\n",
       "                      [-0.0259, -0.0383, -0.0318,  ...,  0.0783, -0.0497,  0.0691],\n",
       "                      [-0.0085, -0.0504, -0.0363,  ...,  0.0654, -0.0368,  0.0471],\n",
       "                      ...,\n",
       "                      [-0.0043, -0.0436, -0.0466,  ...,  0.0608, -0.0356,  0.0632],\n",
       "                      [-0.0011, -0.0477, -0.0482,  ...,  0.0582, -0.0489,  0.0669],\n",
       "                      [-0.0216, -0.0454, -0.0616,  ...,  0.0540, -0.0662,  0.0498]]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quantize': True, 'model': {'cls': 'gpt', 'throw_errors_on_duplicate': False, 'layers': {\"transformer.r'w[tp]e'\": {'quantize': True, 'layer_type': 'Embedding', 'quantizers': {'weight': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}}}, 'transformer.dropout': {'quantize': True, 'layer_type': 'Dropout'}, 'transformer.emb_add': {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.residual1\": {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': None}}, \"transformer.layer.r'[0-9]+'.residual2\": {'quantize': True, 'layer_type': 'EltwiseAdd', 'quantizers': {'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'output': None}}, \"transformer.layer.r'[0-9]+'.r'ln_[0-9].id'\": {'quantize': True, 'layer_type': 'Identity', 'quantizers': {'act': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}, 'args': {'return_quant_tensor': True}}, \"transformer.layer.r'[0-9]+'.mlp.r'c_.+'\": {'quantize': True, 'layer_type': 'Linear', 'quantizers': {'weight': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'bias': {'default_quantizer': 'Int8Bias', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.mlp.active\": {'quantize': True, 'layer_type': 'ReLU', 'quantizers': {'act': {'default_quantizer': 'Uint8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'input': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}}}, \"transformer.layer.r'[0-9]+'.mlp.dropout\": {'quantize': True, 'layer_type': 'Dropout'}, \"transformer.layer.r'[0-9]+'.attn.r'.+dropout'\": {'quantize': True, 'layer_type': 'Dropout'}, \"transformer.layer.r'[0-9]+'.attn.mha\": {'quantize': True, 'layer_type': 'MultiheadAttention', 'quantizers': {'in_proj_input_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'in_proj_weight_quant': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'in_proj_bias_quant': {'default_quantizer': 'Int32Bias', 'args': {'bit_width': 8}}, 'attn_output_weights_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'q_scaled_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'k_transposed_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'v_quant': {'type': 'act', 'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_input_quant': {'default_quantizer': 'Int8ActPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_weight_quant': {'default_quantizer': 'Int8WeightPerTensorFloat', 'args': {'bit_width': 8}}, 'out_proj_bias_quant': {'default_quantizer': 'Int32Bias', 'args': {'bit_width': 8}}}, 'args': {'packed_in_proj': False, 'batch_first': True}}}, 'dtype': '${data.dtype}'}, 'kind': 'qat', 'type': 'BrevitasQuantizer'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_checkpoint[\"quant_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT\n",
    "gpt = GPT(quant_checkpoint[\"model_cfg\"][\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.layer.0.attn.mha.in_proj_weight\", \"transformer.layer.0.attn.mha.in_proj_bias\", \"transformer.layer.0.attn.mha.out_proj.bias\", \"transformer.layer.1.attn.mha.in_proj_weight\", \"transformer.layer.1.attn.mha.in_proj_bias\", \"transformer.layer.1.attn.mha.out_proj.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_proj.weight\", \"transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_proj.weight\", \"transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_proj.weight\", \"transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_proj.weight\", \"transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_proj.weight\", \"transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_proj.weight\", \"transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#doesnt work, maybe because gpt_quant is unquantized currently\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_checkpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.layer.0.attn.mha.in_proj_weight\", \"transformer.layer.0.attn.mha.in_proj_bias\", \"transformer.layer.0.attn.mha.out_proj.bias\", \"transformer.layer.1.attn.mha.in_proj_weight\", \"transformer.layer.1.attn.mha.in_proj_bias\", \"transformer.layer.1.attn.mha.out_proj.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.emb_add.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.emb_add.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_proj.weight\", \"transformer.layer.0.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_proj.weight\", \"transformer.layer.0.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_proj.weight\", \"transformer.layer.0.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.0.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual1.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.residual2.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_proj.weight\", \"transformer.layer.1.attn.mha.q_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_proj.weight\", \"transformer.layer.1.attn.mha.k_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_proj.weight\", \"transformer.layer.1.attn.mha.v_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.attn.mha.out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_fc.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.c_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_1.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"transformer.layer.1.ln_2.id.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\". "
     ]
    }
   ],
   "source": [
    "#doesnt work, maybe because gpt_quant is unquantized currently\n",
    "gpt.load_state_dict(quant_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "setting quantizer output for layer transformer.layer.0.residual1 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.1.residual1 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.0.residual2 to None as config is left empty\n",
      "setting quantizer output for layer transformer.layer.1.residual2 to None as config is left empty\n",
      "Using unsigned quantizer for Uint8ActPerTensorFloat at brevitas.quant.scaled_int\n",
      "Using unsigned quantizer for Uint8ActPerTensorFloat at brevitas.quant.scaled_int\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization import get_quantizer\n",
    "quantizer, model_cfg = get_quantizer(quant_checkpoint[\"quant_cfg\"], gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelQuantConfig(cls='gpt', layers={'transformer.wte': LayerQuantConfig(layer=Embedding(50257, 256), quantize=True, layer_type='Embedding', name='transformer.wte', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.wpe': LayerQuantConfig(layer=Embedding(128, 256), quantize=True, layer_type='Embedding', name='transformer.wpe', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.dropout', quantizers={}, replace_later=False, args={}), 'transformer.emb_add': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.emb_add', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.residual1': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.0.residual1', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.1.residual1': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.1.residual1', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.0.residual2': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.0.residual2', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.1.residual2': LayerQuantConfig(layer=EltwiseAdd(), quantize=True, layer_type='EltwiseAdd', name='transformer.layer.1.residual2', quantizers={'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'output': None}, replace_later=False, args={}), 'transformer.layer.0.ln_1.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.0.ln_1.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.0.ln_2.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.0.ln_2.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.1.ln_1.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.1.ln_1.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.1.ln_2.id': LayerQuantConfig(layer=Identity(), quantize=True, layer_type='Identity', name='transformer.layer.1.ln_2.id', quantizers={'act': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'return_quant_tensor': True}), 'transformer.layer.0.mlp.c_fc': LayerQuantConfig(layer=Linear(in_features=256, out_features=1024, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.0.mlp.c_fc', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.c_proj': LayerQuantConfig(layer=Linear(in_features=1024, out_features=256, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.0.mlp.c_proj', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.c_fc': LayerQuantConfig(layer=Linear(in_features=256, out_features=1024, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.1.mlp.c_fc', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.c_proj': LayerQuantConfig(layer=Linear(in_features=1024, out_features=256, bias=True), quantize=True, layer_type='Linear', name='transformer.layer.1.mlp.c_proj', quantizers={'weight': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'bias': BiasQuant(default_quantizer='Int8Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.active': LayerQuantConfig(layer=ReLU(), quantize=True, layer_type='ReLU', name='transformer.layer.0.mlp.active', quantizers={'act': ActQuant(default_quantizer='Uint8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.1.mlp.active': LayerQuantConfig(layer=ReLU(), quantize=True, layer_type='ReLU', name='transformer.layer.1.mlp.active', quantizers={'act': ActQuant(default_quantizer='Uint8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'input': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={}), 'transformer.layer.0.mlp.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.mlp.dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.mlp.dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.mlp.dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.resid_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.attn.resid_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.attn_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.0.attn.attn_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.attn.resid_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.attn.resid_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.1.attn.attn_dropout': LayerQuantConfig(layer=Dropout(p=0.1, inplace=False), quantize=True, layer_type='Dropout', name='transformer.layer.1.attn.attn_dropout', quantizers={}, replace_later=False, args={}), 'transformer.layer.0.attn.mha': LayerQuantConfig(layer=MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "), quantize=True, layer_type='MultiheadAttention', name='transformer.layer.0.attn.mha', quantizers={'in_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'attn_output_weights_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'q_scaled_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'k_transposed_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'v_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'packed_in_proj': False, 'batch_first': True}), 'transformer.layer.1.attn.mha': LayerQuantConfig(layer=MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "), quantize=True, layer_type='MultiheadAttention', name='transformer.layer.1.attn.mha', quantizers={'in_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'in_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'attn_output_weights_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'q_scaled_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'k_transposed_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'v_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_input_quant': ActQuant(default_quantizer='Int8ActPerTensorFloat', template=None, type='act', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_weight_quant': WeightQuant(default_quantizer='Int8WeightPerTensorFloat', template=None, type='weight', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8)), 'out_proj_bias_quant': BiasQuant(default_quantizer='Int32Bias', template=None, type='bias', quantizer_module='brevitas.quant.scaled_int', args=QuantArgs(quant_type=None, bit_width_impl_type=None, float_to_int_impl_type=None, narrow_range=None, signed=None, zero_point_impl=None, scaling_impl_type=None, scaling_stats_op=None, scaling_min_val=None, high_percentile_q=None, low_percentile_q=None, collect_stats_steps=None, affine_shift_scale=None, scaling_stats_permute_dims=None, scaling_per_output_channel=None, restrict_scaling_type=None, bit_width=8))}, replace_later=False, args={'packed_in_proj': False, 'batch_first': True})}, dtype='float32', model=GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 256)\n",
       "    (wpe): Embedding(128, 256)\n",
       "    (emb_add): EltwiseAdd()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (residual1): EltwiseAdd()\n",
       "        (residual2): EltwiseAdd()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (active): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (identity): Identity()\n",
       "        (ln_1): BatchNormTranspose(\n",
       "          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (id): Identity()\n",
       "        )\n",
       "        (ln_2): BatchNormTranspose(\n",
       "          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (id): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_out): BatchNormTranspose(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (id): Identity()\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=256, out_features=50257, bias=False)\n",
       "), quantized=False, throw_errors_on_duplicate=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_gpt, other_layers = quantizer.get_quantized_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_gpt.load_state_dict(quant_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_bn = qnn.BatchNorm1dToQuantScaleBias(64)\n",
    "from qtransform.quantization.quant_bn import QuantBatchnorm1d, CustomBatchNorm1d, replace_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singleton from https://refactoring.guru/design-patterns/singleton/python/example#example-0--main-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton works, both variables contain the same instance.\n"
     ]
    }
   ],
   "source": [
    "class SingletonMeta(type):\n",
    "    \"\"\"\n",
    "    The Singleton class can be implemented in different ways in Python. Some\n",
    "    possible methods include: base class, decorator, metaclass. We will use the\n",
    "    metaclass because it is best suited for this purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        if cls not in cls._instances:\n",
    "            instance = super().__call__(*args, **kwargs)\n",
    "            cls._instances[cls] = instance\n",
    "        return cls._instances[cls]\n",
    "\n",
    "\n",
    "class Singleton(metaclass=SingletonMeta):\n",
    "    \n",
    "    value: int\n",
    "    def some_business_logic(self):\n",
    "        \"\"\"\n",
    "        Finally, any singleton should define some business logic, which can be\n",
    "        executed on its instance.\n",
    "        \"\"\"\n",
    "        print(self.value)\n",
    "\n",
    "s1 = Singleton()\n",
    "s2 = Singleton()\n",
    "\n",
    "if id(s1) == id(s2):\n",
    "    print(\"Singleton works, both variables contain the same instance.\")\n",
    "else:\n",
    "    print(\"Singleton failed, variables contain different instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alright\n"
     ]
    }
   ],
   "source": [
    "s1.some_business_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer singleton requires config to initialize tokenizer\n",
    "\n",
    "class TokenizerSingletonMeta(type):\n",
    "    \"\"\"\n",
    "    The Singleton class can be implemented in different ways in Python. Some\n",
    "    possible methods include: base class, decorator, metaclass. We will use the\n",
    "    metaclass because it is best suited for this purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        if cls not in cls._instances:\n",
    "            instance = super().__call__(*args, **kwargs)\n",
    "            cls._instances[cls] = instance\n",
    "        return cls._instances[cls]\n",
    "\n",
    "\n",
    "class TokenizerSingleton(metaclass=TokenizerSingletonMeta):\n",
    "    def some_business_logic(self):\n",
    "        \"\"\"\n",
    "        Finally, any singleton should define some business logic, which can be\n",
    "        executed on its instance.\n",
    "        \"\"\"\n",
    "        print(\"alright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    _instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        raise RuntimeError('Call instance() instead')\n",
    "\n",
    "    @classmethod\n",
    "    def instance(cls):\n",
    "        if cls._instance is None:\n",
    "            print('Creating new instance')\n",
    "            cls._instance = cls.__new__(cls)\n",
    "            # Put any initialization here.\n",
    "        return cls._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Logger._instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 08:45:55.955411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Property meta_file omited in config. Assuming default: \"meta.pkl\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from tiktoken import get_encoding\n",
    "from qtransform.dataset.files import MemmapDataset\n",
    "from qtransform.tokenizer import TikTokenizer\n",
    "\n",
    "tokenizer = TikTokenizer({\n",
    "    \"wrapper\": \"TikTokenizer\",\n",
    "    \"encoding\": \"gpt2\",\n",
    "    \"module\": \"tiktoken\",\n",
    "    \"name\": \"gpt2\"})\n",
    "block_size = 128\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length', max_length=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = MemmapDataset(\"/home/mabot004/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/train-tiny_shakespeare-float32.bin\", dtype=np.float32, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qtransform.dataset.files.MemmapDataset at 0x7fd637d52920>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TikTokenizer' object has no attribute 'pad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TikTokenizer' object has no attribute 'pad'"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(data_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/home/mabot004/eki-transformer-dev/qtransform/~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m data_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-eval.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-bench.arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m wikitext_hf \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m wikitext_hf\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:1430\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:958\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    957\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[0;32m--> 958\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/data_files.py:686\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    683\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    685\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/data_files.py:591\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 591\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/data_files.py:380\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/home/mabot004/eki-transformer-dev/qtransform/~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow'"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-train.arrow\",\n",
    "    \"eval\": \"~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-eval.arrow\",\n",
    "    \"bench\": \"~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-tokenized-bench.arrow\"}\n",
    "wikitext_hf = load_dataset(\"arrow\" , data_files = data_files)\n",
    "wikitext_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/ is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2252\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2253\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2254\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/ is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "datasets.load_from_disk(\"~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = datasets.Dataset.from_file(\"/home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/tiny_shakespeare/cache-gpt2-128-tokenized-train.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301966"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "log = getLogger(\"map\")\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "from datasets import DatasetDict\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "\n",
    "cache_file_prefix = \"/home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return {\"input_ids\": tokenizer.encode_ordinary(x) for x in examples[\"text\"]}\n",
    "\n",
    "\n",
    "def wrapper(tokenizer, dataset:DatasetDict, block_size, batch_size, cache_file_prefix):\n",
    "    def map_dataset(tokenizer, dataset:DatasetDict, block_size, batch_size, cache_file_prefix):\n",
    "        \"\"\"apllies mapping and tokenizer do dataset\"\"\"\n",
    "\n",
    "        log.info(f\"Dataset has  column names: {dataset.column_names}. Only using \\'text\\' and cutting all other\")\n",
    "        if isinstance(dataset, dict) and 'train' in dataset.keys():\n",
    "            text_column_name = \"text\" if \"text\" in dataset['train'].column_names else dataset['train'].column_names[0]\n",
    "            column_names = dataset['train'].column_names\n",
    "        else:\n",
    "            text_column_name = \"text\" if \"text\" in dataset.column_names else dataset.column_names[0]\n",
    "            column_names = dataset.column_names\n",
    "            \n",
    "        cache_file_names = {\n",
    "          'test':  cache_file_prefix + \"tokenized-\" + 'test.arrow',\n",
    "          'train': cache_file_prefix + \"tokenized-\" + 'train.arrow',\n",
    "          'validation': cache_file_prefix +  \"tokenized-\" + 'validation.arrow'\n",
    "        }\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name])\n",
    "        tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "            cache_file_names=cache_file_names\n",
    "        )\n",
    "        tokenized_datasets = tokenized_datasets.select_columns(\"input_ids\")\n",
    "        print(tokenized_datasets)\n",
    "        # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "        def group_texts(examples):\n",
    "            #print(examples)\n",
    "            # Concatenate all texts.\n",
    "            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "            total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "            # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "            # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "            # Split by chunks of max_len.\n",
    "            result = {\n",
    "                k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "                for k, t in concatenated_examples.items()\n",
    "            }\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            return result\n",
    "\n",
    "        cache_file_names = {\n",
    "          'test':  cache_file_prefix + \"grouped-\" + 'test.arrow',\n",
    "          'train': cache_file_prefix + \"grouped-\" + 'train.arrow',\n",
    "          'validation': cache_file_prefix +  \"grouped-\" + 'validation.arrow'\n",
    "        }\n",
    "\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            #cache_file_names=cache_file_names\n",
    "        )\n",
    "\n",
    "        return lm_datasets\n",
    "    return map_dataset(tokenizer, dataset, block_size, batch_size, cache_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rt \u001b[38;5;241m=\u001b[39m \u001b[43mrt\u001b[49m\u001b[38;5;241m.\u001b[39mselect_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rt' is not defined"
     ]
    }
   ],
   "source": [
    "rt = rt.select_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a29f6a8de454eaca43c9a711d51a993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 64:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1169, 3881, 318, 23985, 284, 307, 262, 2310, 301, 4289, 338, 649, 366, 369, 272, 366, 290, 326, 339, 338, 1016, 284, 787, 257, 22870, 772, 3744, 621, 610, 77, 727, 5513, 5767, 89, 44028, 837, 474, 11025, 12, 565, 3885, 5719, 1801, 1326, 393, 2876, 574, 384, 13528, 764], [1169, 17177, 3481, 15962, 24659, 286, 366, 262, 15876, 286, 262, 13917, 366, 26298, 318, 523, 3236, 326, 257, 5721, 286, 2456, 2314, 22668, 6901, 763, 12, 16002, 14, 35248, 279, 2357, 14509, 1559, 338, 9902, 5761, 286, 474, 764, 374, 764, 374, 764, 284, 75, 74, 2013, 338, 3504, 12, 16442, 764], [16803, 475, 1165, 12, 83, 538, 312, 3182, 16603], [361, 345, 3360, 588, 284, 467, 284, 262, 6918, 284, 423, 1257, 837, 373, 17914, 318, 257, 922, 1295, 284, 923, 764], [24677, 3212, 355, 1223, 4071, 837, 281, 2071, 3807, 326, 338, 523, 5508, 290, 13795, 306, 6515, 326, 340, 1595, 470, 1254, 588, 530, 764], [1169, 2646, 3769, 617, 1049, 11281, 656, 262, 7669, 13370, 20527, 286, 477, 12770, 1377, 772, 883, 508, 423, 4251, 262, 4112, 1353, 286, 262, 983, 764], [2364, 364, 326, 4071, 6087, 286, 9739, 290, 3707, 764], [28998, 645, 4286, 1683, 925, 468, 517, 7360, 3751, 326, 262, 2975, 284, 5968, 318, 31285, 351, 922, 14953, 764], [4169, 364, 4962, 287, 257, 3013, 7774, 39769, 326, 49507, 379, 262, 13015, 2162, 340, 338, 523, 14169, 345, 765, 284, 5465, 340, 764, 475, 339, 7599, 16194, 340, 572, 764], [20657, 1337, 286, 616, 3797, 4394, 257, 23056, 306, 1180, 16416, 286, 355, 666, 22041, 764], [5661, 318, 257, 2646, 880, 2861, 4379, 837, 3375, 290, 13777, 6665, 290, 477, 764], [10919, 1107, 24072, 546, 10787, 36960, 318, 663, 1877, 12, 2539, 3081, 290, 8768, 15403, 1108, 764], [7, 266, 437, 14031, 318, 1267, 1521, 356, 467, 284, 262, 22041, 1058, 284, 307, 11672, 832, 262, 4151, 837, 262, 2612, 837, 262, 2000, 764], [505, 286, 262, 6000, 1641, 12, 17107, 837, 8842, 12, 324, 5388, 6918, 1683, 764], [586, 3358, 837, 340, 16723, 364, 262, 3840, 356, 761, 3923, 523, 881, 764], [272, 15950, 13206, 705, 8727, 2630, 340, 6, 287, 543, 262, 8507, 286, 262, 749, 5863, 1772, 508, 1683, 5615, 2058, 656, 1808, 764], [359, 388, 6010, 611, 17698, 1561, 88, 11648, 764], [64, 30669, 1440, 812, 287, 262, 1642, 764], [1169, 3807, 338, 29036, 837, 551, 430, 457, 870, 8737, 481, 12475, 883, 4684, 284, 12774, 663, 1035, 6098, 18187, 26045, 764], [2364, 364, 257, 8033, 286, 262, 4713, 1633, 286, 2081, 44809, 764], [64, 22677, 837, 28695, 837, 16361, 1473, 1692, 2890, 2646, 764], [4480, 257, 3350, 326, 3407, 617, 286, 262, 1353, 10544, 1762, 287, 4795, 2646, 837, 14081, 1222, 4998, 9018, 514, 780, 340, 318, 523, 753, 13911, 837, 523, 30942, 306, 28297, 546, 703, 356, 467, 546, 674, 3160, 764], [64, 14851, 290, 23101, 306, 819, 23466, 10474, 286, 19506, 290, 36580, 291, 2647, 13160, 416, 5206, 541, 5405, 764], [1662, 329, 2506, 837, 475, 329, 883, 351, 4150, 340, 481, 2018, 837, 340, 338, 257, 3621, 12928, 422, 3210, 3807, 5146, 14505, 764], [1416, 2850, 257, 1178, 2173, 329, 1804, 644, 340, 857, 351, 257, 7256, 290, 922, 12, 20122, 42554, 764], [13966, 31775, 7758, 375, 859, 1512, 837, 340, 338, 635, 4457, 4050, 764], [2777, 1304, 805, 12586], [272, 7306, 2569, 1842, 1621, 326, 6774, 503, 262, 41270, 1315, 12, 1941, 12, 727, 14348, 287, 2506, 764], [265, 546, 6957, 2431, 837, 14068, 5440, 16047, 257, 35984, 8761, 355, 340, 9558, 832, 262, 5385, 1621, 764, 2158, 837, 340, 16523, 4490, 23365, 290, 326, 12191, 3081, 1690, 3917, 351, 2876, 574, 1559, 338, 12838, 355, 880, 355, 351, 2961, 595, 1681, 4040, 764], [270, 5419, 326, 42280, 9563, 28796, 764, 764, 764, 23755, 866, 465, 35245, 12, 13982, 7706, 38031, 719, 284, 711, 2130, 508, 22960, 257, 1103, 5141, 764], [5162, 4741, 2308, 284, 1445, 2687, 508, 1683, 14682, 837, 48588, 837, 393, 11686, 764], [64, 4958, 913, 2646, 422, 257, 4958, 26479, 837, 3748, 287, 663, 34036, 18288, 1108, 837, 13206, 287, 663, 10800, 396, 29081, 764], [2971, 837, 13779, 290, 6044, 11487, 764], [361, 612, 338, 257, 835, 284, 6840, 4545, 3988, 546, 262, 16169, 286, 5010, 837, 1312, 892, 340, 338, 287, 4493, 588, 262, 357, 12716, 374, 12, 4111, 1267, 3432, 764], [4514, 340, 561, 307, 2562, 284, 1577, 19813, 262, 649, 3670, 286, 734, 37377, 290, 257, 14825, 837, 340, 338, 257, 1290, 517, 22677, 2646, 621, 597, 16416, 286, 289, 6724, 7264, 29923, 1837, 764], [2016, 2279, 1244, 307, 4187, 378, 290, 4451, 837, 340, 1239, 1718, 572, 290, 1464, 3947, 9037, 764], [66, 415, 316, 7138, 23007, 262, 7541, 14194, 444, 837, 734, 12, 33533, 27239, 837, 290, 39479, 46221, 326, 29298, 378, 410, 42816, 338, 1528], [907, 764, 46246, 3841, 12, 86, 959, 14969, 624, 72, 318, 2048, 599, 29655, 287, 607, 33154, 2584, 837, 26019, 19462, 5350, 1210, 764], [2016, 340, 318, 416, 645, 1724, 465, 1266, 670, 837, 8591, 20782, 89, 12, 6603, 263, 318, 257, 18876, 290, 18778, 3626, 416, 257, 41602, 12, 69, 485, 4958, 837, 257, 13899, 2646, 1128, 5807, 351, 11530, 284, 307, 550, 416, 477, 4684, 284, 787, 262, 3626, 284, 35860, 606, 764], [2339, 749, 6314, 47280, 287, 2274, 812, 837, 617, 286, 262, 49772, 389, 523, 47284, 326, 484, 4865, 319, 852, 16251, 2339, 764, 257, 4334, 24126, 319, 269, 12397, 3037, 318, 3726, 284, 13972, 656, 262, 2168, 764], [3605, 1122, 14293, 674, 3241, 588, 257, 19972, 837, 290, 6529, 13332, 1088, 607, 1365, 1900, 763, 12, 7364, 837, 1317, 266, 15668, 3900, 764], [1169, 1621, 14754, 663, 13197, 287, 257, 938, 12, 11374, 3772, 7464, 326, 338, 772, 1342, 19756, 621, 262, 1334, 286, 262, 4286, 764, 881, 286, 262, 835, 837, 996, 837, 428, 318, 257, 23056, 306, 5337, 6594, 764], [12853, 263, 561, 10403, 423, 1444, 428, 12500, 1837, 290, 379, 1661, 47029, 803, 3807, 257, 1049, 21181, 764], [6, 785, 1154, 6592, 304, 493, 9509, 723, 434, 68, 1005, 324, 5799, 837, 1288, 9717, 81, 18840, 390, 393, 421, 8836, 2934, 292, 1658, 555, 78, 390, 1658, 418, 2646, 274, 8358, 410, 1000, 8591, 3112, 64, 3326, 3718, 271, 3263, 68, 16964, 424, 2656, 32482, 764, 705], [1169, 2646, 1838, 257, 1913, 1339, 329, 262, 6817, 286, 262, 17245, 287, 4441, 262, 2369, 593, 2128, 764], [74, 1670, 268, 6100, 588, 18662, 2346, 837, 607, 11914, 31138, 284, 262, 4405, 837, 607, 890, 837, 8290, 1384, 4190, 1804, 1310, 284, 19916, 1497, 262, 16927, 276, 36116, 286, 15488, 764], [70, 418, 1359, 3769, 281, 4998, 2854, 326, 15594, 9501, 2279, 2073, 287, 262, 2646, 764], [64, 1103, 3807, 837, 546, 1103, 661, 837, 326, 3607, 514, 257, 4071, 19350, 656, 257, 3968, 749, 286, 514, 836, 470, 760, 764], [83, 2194, 1865, 300, 11736, 803, 290, 3223, 306, 8258, 277, 540, 764], [11261, 307, 42078, 278, 281, 2562, 2496, 1377, 883, 1468, 705, 1120, 338, 6175, 7185, 3033, 1377, 475, 764, 764, 764, 340, 23477, 290, 24538, 511, 27384, 1272, 355, 262, 1738, 1521, 661, 651, 257, 4829, 503, 286, 4964, 606, 1909, 764], [272, 11932, 16700, 286, 45610, 1559, 338, 29303, 3451, 764], [259, 663, 374, 14655, 837, 7026, 290, 555, 32935, 835, 837, 262, 3807, 2499, 764], [11246, 10544, 423, 523, 881, 45581, 326, 345, 1549, 307, 3772, 284, 6004, 284, 606, 3555, 262, 3072, 1492, 764, 289, 6724, 7264, 290, 6450, 430, 6473, 735, 389, 734, 884, 588, 540, 10544, 764], [82, 15918, 2010, 37524, 27343, 21104, 28127, 689, 262, 13389, 286, 262, 49018, 837, 7669, 13370, 837, 290, 2116, 12, 46303, 3077, 1667, 12898, 355, 607, 2612, 6140, 284, 1280, 764], [42200, 262, 6729, 1830, 290, 300, 21985, 264, 1856, 666, 4791, 43724, 357, 5556, 530, 15900, 3290, 1267, 837, 262, 4286, 11453, 257, 3952, 64, 12, 29988, 1496, 10742, 286, 2612, 764], [16833, 2435, 345, 892, 23557, 3956, 468, 1057, 503, 286, 13324, 837, 340, 7228, 257, 649, 835, 284, 5975, 290, 26072, 764], [805, 1095, 284, 307, 2656, 837, 772, 996, 340, 374, 2419, 572, 867, 286, 663, 4213, 764], [82, 3889, 14, 785, 1930, 263, 275, 29038, 512, 4105, 22625, 257, 26891, 286, 7259, 1587, 245, 257, 1178, 2785, 7127, 837, 257, 1178, 517, 2391, 39930, 284, 262, 1621, 1587, 245, 475, 262, 2187, 5301, 3729, 23007, 262, 5292, 837, 1931, 837, 4437, 286, 262, 3704, 764], [5832, 1549, 892, 416, 783, 45630, 64, 561, 423, 550, 1576, 286, 458, 5309, 275, 799, 680, 21399, 298, 10466, 351, 11954, 286, 3869, 764, 1865, 262, 719, 318, 991, 23332, 994, 764], [25356, 393, 407, 345, 821, 34964, 416, 597, 286, 4587, 81, 3755, 338, 25917, 319, 366, 262, 584, 366, 290, 366, 262, 2116, 837, 366, 4587, 81, 3755, 318, 281, 47589, 13899, 290, 34264, 5891, 764], [64, 15497, 1576, 3807, 837, 2714, 1978, 416, 14297, 34549, 10544, 764], [5661, 318, 262, 1266, 45630, 272, 3807, 546, 17840, 15508, 1201, 7795, 338, 4232, 764], [6381, 1681, 468, 1464, 587, 2277, 12, 273, 12, 3927, 618, 6079, 14142, 3988, 6, 3835, 284, 262, 3159, 764, 764, 764, 256, 1347, 45697, 318, 257, 1310, 286, 1111, 764], [3137, 262, 10515, 2950, 287, 4441, 262, 37748, 46792, 286, 262, 19506, 287, 428, 442, 12571, 17500, 1434, 286, 23208, 290, 1657, 318, 21994, 764], [1169, 15108, 850, 29487, 13795, 306, 28539, 262, 8434, 12766, 286, 674, 28680, 10281, 532, 31955, 837, 47913, 837, 290, 8157, 764], [1169, 46038, 266, 263, 1008, 607, 89, 519, 318, 6776, 290, 880, 290, 2877, 287, 8591], [30171, 261, 318, 257, 1049, 14549, 42458, 257, 3716, 2095, 837, 475, 2146, 933, 869, 283, 13676, 1342, 13206, 262, 18485, 340, 502, 45070, 422, 663, 14702, 923, 764], [3911, 286, 262, 20024, 286, 3332, 259, 13805, 469, 318, 326, 340, 30940, 262, 3489, 351, 31049, 290, 1657, 1108, 764], [1559, 286, 262, 26619, 743, 307, 257, 922, 2063, 12, 9769, 1165, 890, 475, 2058, 1128, 5807, 351, 257, 48259, 2565, 286, 10715, 290, 5897, 1108, 764], [64, 32857, 278, 10590, 10512, 287, 543, 262, 29404, 286, 4802, 3685, 389, 477, 262, 517, 28027, 329, 262, 3105, 40502, 326, 468, 27165, 606, 764], [64, 256, 2306, 837, 12661, 10590, 10512, 764], [64, 13206, 2406, 12, 1659, 12, 496, 10512, 546, 262, 50065, 516, 7002, 286, 257, 8564, 1862, 2576, 832, 257, 2168, 286, 17016, 5682, 290, 257, 14800, 6531, 284, 2834, 1479, 422, 607, 4923, 290, 2401, 500, 1586, 2802, 338, 1745, 625, 607, 764], [64, 4988, 3867, 1998, 837, 290, 257, 2818, 1672, 286, 703, 1242, 1377, 618, 1760, 826, 1377, 460, 1037, 12035, 837, 18282, 837, 290, 4467, 764], [5661, 8675, 1286, 6515, 1621, 837, 7744, 2936, 290, 4958, 2759, 22152, 1143, 837, 318, 257, 15499, 329, 663, 285, 8770, 624, 3437, 764], [265, 2612, 262, 3807, 318, 257, 46208, 306, 38733, 43527, 21181, 3025, 26192, 427, 324, 654, 670, 355, 33988, 2138, 621, 9136, 764], [1169, 5585, 286, 5509, 39433, 290, 467, 297, 388, 338, 9902, 2597, 481, 2035, 423, 345, 14442, 644, 345, 821, 4379, 837, 393, 10708, 534, 2951, 764, 1312, 6151, 340, 5145, 467, 297, 388, 338, 705, 26585, 6, 318, 8082, 5145], [64, 39769, 517, 27016, 6819, 12006, 621, 366, 285, 972, 78, 366], [361, 428, 3807, 547, 257, 1492, 837, 340, 561, 307, 257, 2443, 12, 15344, 263, 837, 345, 460, 470, 4043, 284, 766, 644, 4325, 1306, 764], [71, 1531, 365, 6459, 514, 284, 7239, 262, 3950, 286, 3206, 42499, 1358, 764], [46303, 4623, 290, 14851, 1377, 3737, 517, 14851, 621, 6198, 5292, 1377, 475, 257, 1310, 16287, 561, 423, 3750, 257, 890, 835, 764], [270, 338, 262, 1266, 2646, 286, 262, 614, 523, 1290, 837, 262, 18335, 1028, 543, 477, 584, 1266, 4286, 29467, 815, 307, 8630, 764], [35436, 913, 284, 2342, 837, 475, 10209, 4684, 284, 1011, 257, 2863, 481, 307, 20945, 351, 734, 286, 262, 614, 338, 749, 13013, 290, 40112, 889, 2646, 13289, 764], [5661, 318, 257, 28027, 2646, 326, 3607, 345, 257, 13899, 837, 18244, 31193, 1570, 286, 4173, 38336, 10016, 1204, 1969, 284, 262, 4173, 30188, 72, 4865, 764], [272, 41138, 10997, 14, 400, 81, 4665, 764], [64, 1178, 10848, 88, 10601, 5614, 7263, 837, 16946, 318, 355, 39679, 355, 257, 3807, 3011, 777, 1528, 764], [4514, 262, 318, 293, 318, 1111, 3143, 6197, 516, 290, 13770, 22795, 2569, 837, 663, 410, 37503, 389, 8131, 4950, 284, 804, 379, 764], [45525, 837, 284, 74, 290, 267, 17771, 23104, 257, 36675, 415, 837, 3223, 306, 8258, 9280, 286, 1918, 764, 287, 262, 1429, 837, 484, 10176, 326, 612, 338, 991, 257, 1256, 286, 1204, 287, 289, 506, 479, 506, 22041, 764], [35248, 479, 499, 333, 318, 257, 26479, 351, 257, 1103, 37457, 329, 12191, 32964, 290, 8855, 837, 290, 428, 318, 257, 1365, 2646, 621, 465, 2961, 46932, 12, 16129, 3807, 837, 262, 625, 79, 49309, 1288, 9924, 764], [1169, 3807, 318, 257, 11975, 286, 9856, 2568, 837, 355, 31283, 948, 11034, 290, 46124, 7259, 27675, 345, 832, 262, 2104, 7600, 2431, 764], [64, 5701, 3807, 351, 2223, 326, 338, 7895, 319, 262, 2214, 290, 257, 1621, 345, 1337, 546, 572, 340, 764], [67, 20805, 1761, 272, 837, 262, 3437, 286, 38043, 710, 837, 32254, 262, 4979, 880, 837, 3011, 257, 3621, 266, 600, 563, 804, 422, 465, 7064, 837, 42909, 514, 351, 262, 3807, 338, 13997, 3323, 290, 3544, 1801, 261, 338, 2694, 284, 307, 5670, 290, 17082, 764], [1169, 15403, 1108, 286, 262, 3704, 318, 991, 16572, 764], [74, 27906, 3544, 3934, 2473, 9640, 837, 31906, 4963, 286, 31432, 354, 654, 837, 991, 12566, 290, 23332, 1468, 35672, 12, 1462, 12, 260, 417, 18813, 286, 502, 263, 39704, 17774, 465, 1751, 284, 2251, 465, 3496, 2106, 837, 475, 749, 3665, 286, 477, 318, 262, 3496, 2346], [2339, 262, 2646, 338, 2048, 17911, 13437, 6496, 23258, 286, 1903, 19355, 1795, 82, 23200, 544, 837, 340, 338, 2383, 1231, 852, 625, 21989, 764], [4514, 285, 12993, 49344, 338, 11034, 27103, 262, 2646, 14245, 2029, 262, 1241, 286, 584, 2406, 12, 1659, 12, 496, 7328, 764, 764, 764, 340, 338, 635, 523, 47300, 326, 340, 338, 1327, 284, 651, 736, 656, 262, 6510, 6, 1621, 764], [361, 2147, 2073, 837, 428, 3807, 20718, 257, 11781, 837, 8468, 1611, 286, 10590, 9961, 764], [259, 257, 3487, 3159, 1429, 837, 777, 275, 398, 1460, 561, 307, 8523, 1576, 284, 8080, 281, 987, 18167, 1430, 319, 262, 9412, 6518, 764, 475, 287, 545, 897, 513, 12, 67, 837, 262, 35478, 20954, 10921, 656, 262, 9421, 328, 29823, 22582, 4721, 510, 416, 262, 16223, 764], [16002, 12, 35248, 26593, 25007, 9404, 3296, 262, 795, 1213, 286, 257, 41038, 2260, 18522, 290, 20136, 326, 468, 42302, 1431, 656, 10726, 49509, 290, 3252, 764], [13, 764, 764, 257, 24471, 12, 1073, 1603, 6594, 286, 257, 3807], [72, 8359, 640, 286, 2661, 981, 1312, 373, 4964, 340, 837, 475, 1312, 373, 6655, 379, 703, 2952, 340, 24887, 422, 616, 4088, 764], [354, 4549, 318, 13767, 837, 865, 1077, 837, 264, 446, 9229, 837, 3190, 48977, 287, 663, 9706, 764], [4169, 303, 4173, 5404, 338, 2446, 318, 220, 1142, 395, 16869, 2229, 1014, 379, 23312, 2866, 290, 6115, 764], [64, 23056, 479, 29456, 2646, 546, 1936, 4048, 1029, 1524, 2460, 508, 1986, 281, 35256, 3344, 618, 484, 1949, 284, 1011, 511, 6958, 656, 9211, 10150, 764], [261, 262, 4417, 837, 340, 338, 257, 20175, 12, 261, 12, 1169, 12, 5143, 4065, 26810, 837, 475, 340, 468, 257, 1256, 287, 2219, 351, 47199, 48596, 338, 290, 479, 444, 75, 12079, 338, 2961, 670, 837, 7328, 588, 262, 4274, 1204, 286, 3326, 261, 2350, 764], [1169, 3815, 326, 423, 2714, 262, 13953, 5462, 1978, 832, 2180, 17545, 290, 583, 4487, 466, 523, 757, 12, 66, 32885, 837, 2116, 12, 30584, 31932, 290, 16336, 739, 3833, 764], [361, 340, 338, 1744, 329, 257, 16304, 284, 503, 19489, 262, 2656, 837, 788, 1017, 17, 857, 655, 326, 764], [64, 14348, 10997, 326, 14051, 416, 262, 3173, 286, 663, 898, 2116, 12, 45964, 6881, 764], [19, 2460, 837, 362, 11886, 837, 4751, 4608, 837, 290, 477, 262, 279, 397, 301, 4171, 29092, 6099, 484, 460, 4144, 532, 340, 338, 262, 8713, 2266, 27235, 2975, 12, 39813, 764], [1169, 2646, 318, 1690, 5901, 351, 257, 2565, 286, 5899, 4240, 434, 290, 14067, 407, 1690, 1775, 287, 1909, 338, 22041, 7043, 40638, 8597], [270, 1244, 307, 29850, 284, 2754, 285, 81, 764, 290, 1809, 290, 465, 37886, 355, 5629, 21591, 837, 475, 285, 81, 764, 5160, 18647, 338, 48258, 605, 837, 23332, 3807, 3578, 514, 284, 766, 606, 837, 3443, 837, 355, 7912, 764], [64, 1254, 12, 11274, 4286, 287, 262, 1266, 2565, 286, 262, 3381, 764], [42131, 290, 2823, 351, 257, 17510, 404, 515, 3918, 17007, 7958, 262, 670, 286, 465, 7481, 837, 12472, 4962, 262, 2126, 286, 262, 11648, 319, 663, 1182, 837, 1642, 340, 374, 12752, 837, 800, 36274, 803, 1257, 14394, 597, 285, 14981, 33679, 1924, 764], [64, 4632, 12661, 837, 1786, 1214, 278, 290, 39705, 14309, 415, 4012, 45268, 764], [270, 338, 428, 4088, 12, 292, 12, 738, 414, 909, 47625, 326, 3607, 3200, 1204, 663, 38361, 17809, 589, 837, 30384, 29808, 326, 890, 12, 10217, 34739, 389, 5600, 3950, 837, 290, 326, 1931, 2313, 606, 664, 5773, 262, 2116, 764], [1056, 12, 8548, 468, 257, 2106, 837, 290, 340, 338, 257, 23094, 329, 428, 1842, 1621, 764], [259, 8354, 837, 20505, 290, 29340, 837, 1751, 286, 262, 4289, 764, 764, 764, 2753, 479, 333, 893, 6, 3451, 284, 257, 2187, 649, 1241, 764], [5661, 743, 407, 423, 262, 10092, 12500, 12, 86, 3532, 278, 2928, 286, 584, 6039, 16377, 7328, 837, 475, 340, 338, 257, 13206, 1621, 837, 8384, 780, 286, 262, 835, 340, 338, 1297, 416, 262, 661, 508, 547, 612, 764], [23395, 262, 10512, 286, 23441, 5633, 264, 2614, 19111, 5115, 644, 262, 6128, 1724, 287, 262, 1263, 4286, 837, 14133, 3435, 45756, 349, 11711, 306, 832, 262, 1621, 837, 351, 23332, 2482, 764], [64, 10296, 837, 32533, 10512, 546, 18522, 290, 11516, 764], [82, 28030, 1456, 1790, 286, 10023, 669, 319, 262, 3660, 275, 12, 29734, 1058, 6159, 355, 8258, 4249, 355, 14169, 837, 996, 281, 4236, 1346, 555, 5310, 43787, 835, 284, 4341, 37989, 2431, 764], [34725, 12, 15588, 11648, 546, 1302, 12, 929, 43520, 318, 257, 1049, 19350, 656, 257, 845, 1180, 995, 764], [403, 2339, 749, 6036, 277, 49191, 837, 14899, 2753, 663, 640, 284, 1560, 663, 1621, 837, 26217, 4632, 1310, 12, 4002, 23827, 287, 1994, 9176, 837, 290, 20718, 617, 19827, 33985, 764], [272, 920, 11840, 9221, 837, 34264, 2646, 326, 7558, 6730, 9700, 674, 6227, 284, 760, 262, 705, 35310, 6, 546, 428, 582, 837, 981, 37431, 7249, 278, 262, 845, 5794, 286, 262, 26444, 287, 257, 5642, 326, 4587, 81, 3755, 561, 48662, 1577, 465, 20027, 284, 764], [1, 3257, 39628, 366, 21695, 9027, 764, 922, 1257, 837, 922, 2223, 837, 922, 7205, 837, 922, 10721, 837, 922, 8761, 837, 922, 13483, 45501, 764], [5832, 815, 1414, 5193, 24780, 329, 428, 1058, 780, 345, 460, 3285, 546, 7195, 6580, 6064, 8015, 319, 262, 1705, 290, 991, 307, 35290, 764, 43972, 588, 428, 787, 340, 1692, 764], [64, 18355, 516, 6594, 379, 717, 837, 5897, 20603, 3007, 286, 5899, 17176, 325, 389, 1178, 290, 1290, 1022, 2162, 511, 18772, 11844, 1769, 262, 35950, 286, 4306, 27721, 2223, 764, 991, 837, 428, 26810, 318, 1257, 837, 290, 2583, 284, 617, 4988, 6275, 16311, 764], [270, 338, 6189, 7425, 257, 21802, 25594, 351, 867, 5366, 479, 382, 504, 837, 290, 815, 670, 663, 5536, 287, 584, 3354, 286, 262, 995, 764], [5143, 837, 836, 470, 2513, 837, 284, 766, 428, 2318, 3077, 290, 865, 4092, 10997, 319, 262, 1263, 3159, 764], [64, 48486, 2378, 416, 257, 8177, 508, 743, 423, 2147, 1364, 284, 5879, 475, 991, 468, 262, 44512, 290, 3708, 284, 905, 703, 663, 1760, 764], [270, 318, 3450, 1028, 4371, 764, 287, 277, 408, 437, 268, 338, 9961, 26298, 837, 428, 7505, 468, 8302, 1593, 284, 683, 290, 318, 2592, 523, 287, 262, 19523, 764], [270, 338, 407, 3446, 257, 308, 39094, 9799, 475, 262, 14505, 318, 3148, 837, 772, 2406, 422, 262, 3708, 12, 400, 622, 764], [5661, 318, 644, 545, 897, 373, 925, 329, 1058, 23016, 319, 257, 5166, 286, 513, 12, 67, 42762, 837, 4423, 503, 262, 1103, 995, 837, 290, 1011, 257, 20429, 27129, 31505, 284, 262, 938, 27580, 1377, 2272, 764], [34671, 306, 355, 257, 6276, 837, 41088, 2218, 837, 374, 31562, 610, 74, 8849, 257, 29932, 23554, 764], [58, 20601, 732, 8254, 318, 60, 12356, 290, 22121, 34996, 837, 14482, 6393, 284, 1111, 3807, 5788, 290, 1919, 29676, 764], [270, 338, 257, 1049, 1730, 286, 264, 44461, 290, 845, 1310, 26320, 764, 475, 644, 15013, 264, 44461, 340, 318, 5145, 764, 764, 764, 287, 428, 29185, 663, 277, 6457, 318, 25246, 764], [272, 2656, 16840, 546, 281, 22286, 351, 640, 764], [270, 481, 10974, 29661, 284, 262, 1621, 290, 883, 508, 760, 340, 422, 416, 21260, 1528, 764], [70, 4685, 6819, 44089, 357, 290, 308, 652, 1267, 15896, 3807, 3404, 764], [1169, 2646, 625, 8988, 262, 3218, 6164, 3245, 286, 2406, 12, 1659, 12, 496, 537, 291, 956, 351, 16739, 16376, 286, 24345, 290, 14233, 764], [361, 534, 17627, 4398, 470, 587, 19222, 276, 416, 1017, 31218, 7328, 290, 46835, 69, 3558, 837, 611, 345, 821, 257, 369, 3919, 20782, 333, 286, 10590, 9961, 837, 428, 318, 534, 7846, 764], [270, 338, 257, 4159, 10997, 326, 8404, 284, 5236, 32073, 351, 763, 945, 9449, 837, 981, 340, 32281, 257, 6507, 4286, 286, 262, 25287, 3715, 764], [270, 318, 31146, 2614, 290, 1865, 1377, 5023, 627, 2171, 1377, 46208, 306, 2523, 514, 262, 4124, 286, 262, 1661, 764], [292, 2376, 12, 12463, 355, 262, 2041, 3048, 389, 837, 262, 7974, 508, 22843, 9342, 36945, 9339, 1978, 36941, 262, 2700, 286, 9265, 625, 6890, 287, 257, 835, 326, 4903, 3643, 17115, 292, 468, 890, 11564, 764], [2339, 285, 522, 1595, 470, 1592, 597, 2173, 329, 2656, 414, 764, 340, 857, 6758, 416, 1708, 257, 1254, 12, 11274, 10451, 351, 257, 5442, 3918, 837, 290, 416, 6011, 663, 2496, 5386, 286, 7876, 3988, 617, 7062, 2597, 4981, 290, 24323, 764], [270, 338, 257, 289, 1025, 290, 257, 2063, 837, 290, 257, 1049, 835, 329, 262, 45630, 272, 661, 284, 766, 644, 257, 4540, 318, 588, 618, 339, 338, 407, 3501, 262, 976, 1315, 12, 1087, 42497, 4046, 764], [16370, 422, 2818, 837, 475, 663, 2612, 318, 287, 262, 826, 1295, 764, 764, 764, 10218, 290, 880, 12, 24815, 764], [64, 6507, 837, 9098, 1692, 10997, 2826, 503, 319, 262, 736, 9725, 286, 1204, 764], [1014, 67, 22748, 318, 416, 645, 1724, 257, 2818, 2646, 837, 475, 663, 22103, 257, 3236, 20024, 5766, 290, 895, 4595, 286, 2656, 414, 764], [16514, 477, 268, 318, 1049, 287, 465, 2597, 475, 1239, 289, 18463, 262, 8188, 422, 465, 5891, 3350, 837, 355, 612, 389, 6088, 286, 22051, 290, 922, 3951, 329, 2506, 287, 428, 10997, 764], [3549, 257, 3440, 286, 20050, 837, 369, 272, 12, 28939, 537, 2373, 2416, 621, 262, 28641, 837, 2041, 12, 34435, 5848, 21832, 262, 38567, 5986, 2380, 764], [268, 579, 3481, 4300, 540, 837, 11476, 780, 340, 318, 3910, 286, 663, 898, 13180, 286, 262, 12986, 764], [1456, 338, 257, 275, 799, 680, 26810, 26852, 7549, 21254, 49990, 351, 48923, 2247, 837, 1865, 655, 355, 5295, 284, 8204, 345, 764], [270, 338, 281, 1468, 1621, 837, 475, 257, 29696, 4226, 837, 7786, 7205, 290, 12387, 15108, 987, 75, 8401, 787, 655, 257, 9245, 1283, 21043, 88, 4713, 764], [27238, 307, 1775, 284, 307, 4762, 764], [2433, 7649, 12375, 290, 474, 888, 1458, 1173, 466, 617, 286, 511, 1266, 670, 287, 511, 739, 15266, 9176, 837, 475, 836, 470, 307, 35820, 1058, 8168, 14071, 597, 21740, 994, 764], [37814, 326, 468, 284, 466, 351, 331, 10438, 290, 1149, 75, 11404, 837, 290, 2279, 326, 468, 284, 466, 351, 331, 10438, 338, 374, 4131, 16260, 699, 837, 12711, 680, 6621, 290, 607, 1729, 12, 47483, 5229, 837, 5300, 8258, 290, 2081, 764], [34751, 1363, 435, 8809, 366, 318, 644, 340, 318, 1587, 244, 257, 3621, 837, 23585, 3128, 2646, 764, 764, 764], [1169, 614, 338, 49414, 5975, 837, 257, 3807, 326, 7529, 351, 257, 1103, 2426, 287, 281, 1464, 6452, 835, 764], [69, 504, 286, 1372, 272, 338, 670, 290, 286, 4173, 680, 6918, 287, 2276, 481, 307, 20945, 416, 275, 273, 7757, 2933, 764], [896, 26045, 389, 13245, 306, 3489, 837, 290, 340, 338, 1165, 6364, 44756, 284, 307, 257, 32251, 764, 685, 4360, 340, 338, 60, 2861, 34639, 780, 286, 734, 43937, 13289, 416, 285, 40302, 269, 5718, 290, 1449, 358, 272, 1216, 6005, 764], [1169, 2646, 318, 17074, 284, 644, 530, 906, 8139, 389, 262, 1492, 338, 15203, 17095, 1377, 326, 356, 1716, 508, 356, 389, 319, 262, 12983, 286, 674, 3397, 837, 475, 356, 423, 645, 2126, 508, 484, 547, 379, 674, 2479, 2162, 290, 326, 640, 318, 257, 42738, 290, 14186, 19464, 645, 2300, 703, 1468, 345, 389, 764], [9662, 831, 5160, 18647, 338, 5682, 35512, 11648, 285, 2261, 4168, 1008, 25570, 468, 2147, 475, 1842, 329, 663, 1426, 325, 286, 12268, 3952, 2853, 44908, 764], [64, 49549, 32085, 18906, 442, 4665, 764], [361, 345, 460, 651, 1613, 262, 5113, 32044, 7612, 290, 11859, 22072, 286, 366, 262, 318, 293, 366, 345, 1183, 651, 257, 32263, 12, 5832, 12, 259, 12, 1169, 12, 25379, 26810, 326, 318, 257, 5874, 4205, 12, 2934, 12, 3174, 290, 257, 1621, 326, 318, 5023, 597, 345, 481, 1884, 766, 6609, 2073, 764], [8117, 389, 355, 867, 18297, 355, 7127, 837, 475, 6165, 837, 340, 7228, 14733, 287, 262, 11511, 18764, 286, 1692, 4069, 837, 290, 340, 338, 257, 7062, 1441, 284, 262, 11135, 286, 257, 12121, 326, 815, 4745, 319, 24072, 764], [64, 880, 12, 9727, 32251, 351, 257, 1728, 1241, 286, 4430, 290, 1729, 12, 260, 2673, 560, 18016, 764], [8117, 338, 1576, 3783, 284, 787, 340, 954, 355, 9856, 837, 290, 1576, 8737, 284, 787, 340, 45504, 764], [2787, 1299, 257, 4735, 837, 611, 6454, 4334, 12, 13638, 837, 1848, 286, 262, 1474, 12, 6381, 1603, 764, 764, 764, 1760, 510, 416, 703, 446, 351, 257, 11831, 837, 611, 407, 845, 41138, 837, 1021, 764], [76, 461, 7617, 65, 1878, 5679, 257, 581, 16780, 12653, 3108, 287, 428, 34318, 47112, 11281, 656, 262, 11859, 6224, 286, 262, 479, 2799, 680, 8015, 286, 4173, 272, 338, 4865, 4447, 764], [1640, 257, 922, 16058, 286, 663, 2491, 640, 837, 13640, 318, 281, 4050, 290, 26435, 436, 10051, 20803, 32251, 764], [1712, 286, 19813, 318, 257, 14169, 290, 3144, 39438, 14348, 10997, 351, 257, 7062, 25394, 286, 35842, 1108, 764], [77, 958, 857, 8006, 262, 13357, 286, 257, 1263, 1641, 290, 663, 9867, 290, 8433, 5768, 764, 764, 764], [1169, 21547, 485, 4328, 18738, 290, 19337, 837, 4950, 661, 389, 3621, 284, 804, 379, 981, 345, 4043, 329, 262, 1621, 284, 651, 1016, 764], [430, 260, 318, 262, 705, 32679, 10997, 6, 326, 772, 6370, 262, 11281, 290, 24345, 286, 428, 595, 18052, 19907, 764], [81, 2283, 1871, 481, 4105, 6, 1266, 3159, 670, 764], [1516, 3039, 306, 23007, 262, 285, 2860, 3101, 290, 14091, 304, 11848, 290, 5202, 286, 14738, 764], [272, 1998, 523, 1786, 1214, 278, 340, 318, 588, 852, 11694, 287, 257, 649, 2858, 764], [270, 338, 4569, 1409, 26597, 868, 477, 262, 835, 837, 475, 340, 338, 1760, 351, 257, 1256, 286, 8161, 2278, 3241, 355, 880, 355, 617, 845, 7062, 20868, 764], [400, 7985, 306, 20050, 764], [25991, 340, 338, 655, 780, 428, 1613, 614, 468, 1775, 262, 2650, 286, 617, 286, 262, 5290, 2646, 14577, 444, 287, 4647, 764, 764, 764, 475, 12698, 837, 16602, 326, 1107, 2125, 470, 477, 326, 2089, 764], [64, 288, 2487, 837, 880, 12, 23800, 837, 2095, 12, 15808, 10997, 351, 10059, 18190, 286, 4203, 764], [5661, 318, 2391, 262, 749, 1257, 345, 1183, 1683, 423, 351, 257, 11648, 5145], [64, 845, 8258, 3807, 764], [50042, 289, 1531, 365, 338, 2646, 318, 837, 47166, 1576, 837, 257, 4427, 290, 257, 9837, 764, 475, 4964, 289, 7211, 861, 837, 257, 1049, 14549, 24447, 656, 257, 20533, 2597, 837, 318, 40112, 889, 764], [64, 2243, 1621, 326, 14759, 262, 7090, 32513, 880, 764], [505, 286, 262, 1266, 837, 749, 739, 21989, 13289, 286, 685, 19650, 299, 488, 32836, 338, 60, 3451, 764], [65, 799, 1681, 468, 587, 6793, 284, 262, 1263, 3159, 3338, 290, 2128, 837, 262, 835, 356, 588, 674, 1160, 12, 1941, 12, 727, 28422, 4813, 284, 3067, 319, 262, 16117, 39042, 764], [25591, 2354, 905, 1597, 481, 2883, 257, 1969, 804, 379, 661, 484, 836, 470, 1107, 765, 284, 760, 764], [1169, 1611, 286, 10927, 2646, 326, 481, 2035, 1577, 345, 257, 11607, 24902, 393, 47029, 378, 345, 764], [50042, 26394, 494, 290, 465, 7706, 1234, 1978, 465, 1017, 31218, 2008, 422, 13952, 3354, 290, 22546, 5696, 318, 355, 881, 1257, 355, 340, 1276, 423, 587, 329, 606, 284, 787, 340, 764], [17197, 743, 407, 1833, 2279, 326, 4325, 1377, 1312, 1101, 407, 1654, 772, 285, 7745, 32276, 2241, 857, 1377, 475, 484, 481, 2048, 3729, 307, 30103, 837, 290, 17713, 20707, 764], [64, 13899, 290, 1257, 2646, 764], [83, 324, 36869, 318, 257, 13767, 837, 8258, 290, 922, 12, 32353, 1522, 2190, 837, 3731, 475, 257, 9476, 764], [5661, 41696, 837, 267, 13034, 12, 26601, 3898, 11648, 837, 287, 543, 1751, 319, 1111, 5389, 286, 262, 1683, 12, 47647, 803, 5358, 423, 511, 910, 1497, 422, 2342, 913, 21694, 2951, 837, 3607, 4167, 1865, 1194, 2863, 764], [72, 29382, 428, 670, 257, 1256, 764], [25356, 345, 821, 3888, 290, 1842, 340, 837, 393, 17533, 393, 14718, 416, 262, 2646, 837, 345, 1183, 991, 1254, 1223, 764], [13, 764, 764, 612, 389, 1576, 7188, 286, 37154, 24345, 284, 1394, 530, 39783, 284, 262, 3159, 764], [1820, 20437, 837, 16599, 3042, 361, 993, 468, 257, 1256, 284, 2897, 290, 673, 3947, 284, 423, 645, 1917, 781, 20706, 607, 3288, 13201, 764, 673, 1276, 423, 257, 845, 1913, 736, 764], [64, 4451, 837, 6029, 290, 34264, 14348, 10997, 764], [64, 436, 1373, 666, 8674, 14, 35248, 45610, 755, 1559, 290, 5764, 12, 14463, 46932, 13483, 265, 18539, 308, 2915, 6701, 25297, 641, 787, 257, 23754, 3626, 379, 17360, 1710, 262, 3489, 351, 2568, 290, 11044, 764], [19419, 4334, 12, 13638, 1108, 837, 288, 506, 3769, 6650, 351, 465, 12661, 13180, 286, 1692, 11511, 18764, 290, 32962, 764], [39390, 837, 23844, 12, 259, 12, 1169, 12, 26110, 265, 1641, 9739, 326, 37453, 663, 1176, 416, 17274, 284, 262, 6419, 764], [292, 281, 9739, 837, 262, 3807, 7622, 345, 35673, 290, 1266, 286, 477, 837, 340, 1657, 641, 534, 13008, 1231, 4305, 257, 24276, 764], [270, 318, 3499, 290, 1257, 284, 766, 922, 439, 290, 607, 47724, 319, 262, 5749, 12, 14813, 12, 6042, 3159, 764], [270, 1839, 470, 13076, 534, 12500, 1377, 290, 340, 338, 407, 5292, 284, 1377, 340, 338, 6974, 257, 34377, 306, 29932, 21998, 12452, 286, 644, 1838, 257, 9707, 257, 9707, 764], [64, 6454, 14897, 306, 12006, 475, 39931, 837, 1235, 278, 804, 379, 257, 1048, 523, 41503, 351, 2116, 12, 5439, 26927, 837, 339, 4329, 281, 4472, 284, 465, 898, 3234, 764], [270, 14582, 262, 19336, 286, 474, 11025, 2429, 316, 290, 45610, 302, 29658, 837, 262, 7328, 286, 277, 562, 65, 5540, 837, 3737, 772, 262, 645, 310, 35735, 2499, 286, 467, 3972, 764], [77, 5605, 743, 407, 651, 281, 705, 64, 6, 329, 2656, 414, 837, 475, 340, 17326, 663, 275, 12, 41364, 15012, 588, 257, 23009, 286, 7522, 764], [4480, 262, 2646, 338, 8871, 7464, 837, 530, 23660, 326, 356, 423, 257, 890, 835, 284, 467, 878, 356, 3938, 1833, 477, 262, 3206, 9943, 32855, 2950, 764], [58, 67, 6582, 1370, 60, 318, 17774, 329, 644, 340, 857, 837, 290, 37959, 329, 644, 340, 1595, 470, 466, 764], [265, 663, 1266, 1903, 319, 355, 340, 5341, 262, 3968, 21022, 1022, 262, 9397, 764], [58, 64, 60, 4071, 837, 4950, 2646, 764], [272, 40986, 5263, 306, 5513, 76, 2501, 7357, 290, 13770, 20050, 2081, 1621, 764], [64, 22677, 804, 379, 257, 12132, 4519, 326, 925, 14408, 287, 8735, 764], [5832, 2513, 503, 286, 262, 922, 2576, 351, 7668, 10825, 1587, 245, 40174, 286, 655, 500, 5929, 351, 257, 256, 11912, 286, 4547, 329, 607, 4028, 764], [912, 1872, 42434, 12, 4528, 648, 468, 2077, 465, 16028, 3918, 290, 20449, 340, 284, 257, 31578, 500, 966, 764], [37424, 306, 11613, 837, 257, 670, 286, 40986, 5263, 4293, 11892, 837, 340, 318, 19018, 1377, 290, 1884, 29243, 1377, 257, 19376, 290, 30438, 16992, 15438, 286, 262, 2597, 326, 334, 764, 264, 764, 3215, 2450, 468, 2826, 287, 262, 4485, 286, 3350, 305, 764], [2197, 40325, 416, 546, 1160, 2431, 837, 428, 38957, 1115, 12, 1941, 12, 727, 3227, 468, 1576, 4490, 23365, 290, 5046, 284, 15959, 355, 7334, 12, 929, 3671, 499, 1042, 764], [732, 651, 617, 4988, 3748, 2095, 3640, 290, 257, 3272, 12, 5458, 286, 45630, 2271, 326, 289, 31777, 3521, 470, 5457, 19812, 1096, 290, 307, 4762, 764], [2016, 428, 2646, 460, 307, 39120, 837, 663, 22455, 389, 8603, 1377, 290, 6178, 343, 1346, 1377, 8820, 15790, 764], [67, 1723, 837, 46814, 2890, 290, 32436, 1327, 284, 6044, 764], [1169, 4923, 3160, 286, 18890, 6510, 6, 1011, 319, 37258, 5300, 32258, 2081, 764], [5908, 382, 338, 2854, 14947, 274, 2048, 355, 881, 355, 607, 670, 351, 27678, 2516, 287, 8735, 338, 3338, 764], [4703, 896, 13997, 12, 41364, 7674, 588, 257, 5337, 345, 460, 470, 1234, 866, 837, 28025, 257, 38512, 284, 2106, 25129, 3181, 284, 1657, 319, 262, 3159, 837, 290, 7622, 345, 25260, 422, 717, 5739, 284, 938, 764], [272, 34418, 837, 16416, 12, 1659, 12, 10378, 2234, 1204, 326, 18105, 25377, 290, 13917, 2081, 764], [43395, 764, 3952, 263, 468, 37928, 6153, 465, 2723, 290, 45569, 663, 12799, 837, 49760, 257, 24140, 913, 290, 20105, 8216, 21247, 546, 41258, 4827, 837, 393, 281, 12986, 396, 15383, 37521, 764], [1169, 1255, 318, 1223, 2407, 4713, 290, 32327, 764], [439, 475, 262, 749, 2774, 17172, 2963, 662, 660, 641, 815, 2883, 428, 1729, 26159, 475, 31610, 8855, 764], [41081, 663, 867, 35905, 803, 17978, 1377, 407, 262, 1551, 286, 543, 318, 38544, 338, 2116, 12, 46303, 3077, 8806, 1377, 38544, 338, 267, 338, 24345, 481, 1592, 345, 625, 764], [5661, 318, 530, 286, 755, 44978, 338, 1266, 7328, 764], [820, 318, 407, 257, 1049, 6314, 3807, 837, 475, 340, 318, 257, 922, 6314, 3807, 837, 543, 991, 1838, 340, 881, 1365, 621, 534, 7226, 6314, 10643, 12, 8210, 764], [16104, 1348, 479, 29456, 1964, 12, 2673, 2646, 318, 655, 355, 922, 1377, 290, 2089, 1377, 355, 289, 31777, 2223, 2462, 873, 764, 318, 428, 4371, 5633], [9417, 837, 19152, 351, 5897, 3049, 33243, 1108, 416, 583, 33826, 666, 1288, 293, 69, 6248, 837, 318, 257, 4988, 18032, 2095, 837, 530, 3025, 5306, 2326, 444, 389, 691, 4622, 7842, 1431, 6300, 286, 262, 3392, 326, 41548, 3016, 2506, 764], [6559, 271, 290, 763, 12, 16002, 12314, 258, 293, 4273, 259, 338, 45707, 540, 39769, 11958, 9700, 351, 257, 2179, 675, 408, 326, 326, 318, 1111, 42880, 8589, 278, 290, 46937, 2890, 764, 1085, 899, 420, 33419, 364, 1332, 463, 290, 1582, 434, 959, 1577, 2208, 75, 876, 13289], [272, 34418, 5296, 656, 262, 9017, 290, 28140, 286, 661, 739, 5503, 355, 880, 355, 257, 13795, 837, 5576, 298, 9134, 804, 379, 13991, 319, 262, 7505, 286, 2802, 2894, 764], [72, 29382, 340, 837, 3573, 326, 10059, 866, 263, 286, 281, 7464, 764], [1169, 30477, 38832, 416, 262, 36446, 1022, 1468, 290, 649, 13817, 389, 900, 1028, 262, 6283, 837, 19278, 8737, 286, 262, 285, 485, 459, 10326, 837, 523, 42900, 4420, 290, 34953, 2280, 18976, 326, 345, 460, 2048, 6938, 262, 748, 44240, 515, 1633, 764], [2787, 668, 1346, 9857, 290, 13891, 764], [12081, 2000, 1771, 345, 2822, 262, 3404, 546, 2318, 2442, 852, 257, 269, 544, 2277, 582, 764, 262, 479, 29655, 1865, 40175, 5761, 28050, 1419, 5630, 1299, 3690, 318, 27939, 837, 47602, 290, 8036, 764], [64, 15499, 286, 1242, 4571, 625, 8689, 837, 475, 644, 1242, 4571, 5145], [1350, 7637, 2241, 2993, 703, 284, 7906, 257, 12838, 290, 530, 460, 470, 1037, 475, 892, 339, 1549, 9144, 428, 2230, 284, 1210, 465, 1204, 656, 1242, 764], [73, 343, 8836, 12575, 330, 338, 4226, 318, 257, 16840, 764, 465, 3435, 389, 11932, 837, 16584, 290, 262, 10721, 318, 12653, 290, 9257, 3867, 764, 262, 8354, 286, 262, 3313, 527, 5714, 1641, 318, 1588, 290, 356, 1663, 7223, 284, 511, 3160, 837, 1336, 286, 4202, 837, 23125, 290, 41687, 764, 764], [5908, 382, 338, 3716, 290, 1593, 2646, 318, 635, 837, 1975, 340, 393, 407, 837, 29731, 17774, 837, 257, 21970, 290, 308, 43009, 1621, 326, 338, 991, 845, 881, 2712, 2346, 503, 764], [1169, 3224, 22992, 318, 3499, 290, 17774, 837, 475, 340, 1595, 470, 423, 262, 976, 10883, 3081, 355, 262, 3726, 286, 262, 1621, 764, 1312, 588, 262, 649, 9640, 290, 991, 1842, 262, 1468, 3404, 764], [2016, 285, 1689, 2753, 257, 1643, 1165, 890, 284, 1064, 663, 18662, 290, 257, 2368, 12, 529, 7110, 2478, 318, 6454, 7758, 375, 859, 1512, 837, 663, 12183, 1940, 14733, 290, 15241, 30889, 389, 1654, 284, 3387, 2687, 287, 2989, 286, 257, 474, 5028, 290, 474, 320, 329, 262, 649, 39210, 764], [5832, 1244, 407, 2822, 262, 4213, 764, 475, 345, 1183, 4753, 765, 262, 256, 12, 15600, 764], [15234, 1460, 281, 19827, 4324, 656, 262, 13843, 290, 607, 15103, 3781, 286, 23646, 1540, 623, 89, 764], [7972, 16620, 364, 318, 427, 499, 5321, 306, 14586, 4035, 837, 262, 1611, 286, 3807, 326, 27671, 345, 284, 2298, 5475, 663, 31025, 772, 355, 345, 423, 284, 9159, 326, 7599, 340, 2277, 345, 810, 345, 2107, 764], [79, 6629, 281, 6468, 1133, 45436, 286, 3504, 45630, 272, 10530, 7332, 1819, 290, 262, 12111, 6531, 284, 6654, 340, 764], [3137, 644, 1838, 514, 3772, 837, 6949, 5633], [64, 22677, 837, 3867, 3704, 326, 6698, 2408, 2428, 351, 24345, 290, 8737, 764], [505, 286, 262, 6000, 14348, 14577, 444, 286, 262, 1613, 5707, 764], [5832, 3636, 470, 869, 262, 922, 2576, 257, 3128, 3807, 357, 281, 3098, 12, 4475, 3807, 318, 517, 588, 340, 1267, 837, 475, 618, 340, 338, 922, 837, 340, 338, 922, 290, 3076, 6058, 764], [36934, 896, 422, 257, 1913, 2854, 422, 1976, 23778, 837, 475, 340, 338, 288, 506, 474, 494, 338, 1986, 345, 3505, 379, 262, 886, 764], [5661, 318, 257, 2646, 865, 27428, 351, 3703, 290, 47128, 290, 530, 326, 9209, 15343, 546, 262, 2694, 286, 262, 1692, 4437, 284, 1064, 1540, 558, 287, 2995, 326, 714, 3538, 19813, 340, 8097, 764], [1169, 3437, 837, 2876, 574, 427, 391, 3900, 837, 468, 14131, 416, 10759, 493, 1473, 319, 465, 3435, 837, 1642, 606, 37276, 3925, 2138, 621, 5538, 286, 1257, 764], [270, 6165, 6296, 6071, 355, 281, 1593, 16199, 1548, 286, 262, 19544, 286, 530, 286, 3042, 259, 45630, 64, 338, 749, 30983, 25879, 764], [1169, 3807, 468, 257, 2705, 837, 583, 4033, 803, 5536, 837, 257, 2636, 6839, 43527, 764], [64, 880, 12, 9727, 290, 1690, 14081, 33016, 286, 262, 26045, 286, 14738, 764], [3500, 465, 5386, 355, 257, 31474, 876, 2493, 12, 1659, 12, 13345, 837, 288, 506, 16194, 465, 772, 12, 13638, 15735, 4074, 284, 511, 23423, 329, 555, 25138, 837, 878, 339, 4477, 465, 2392, 7002, 991, 4058, 764], [13, 764, 764, 14759, 326, 257, 5270, 15738, 663, 2647, 355, 881, 355, 262, 2647, 15738, 257, 5270, 764], [1169, 48490, 318, 355, 29696, 290, 355, 1257, 355, 340, 318, 555, 46407, 16877, 13526], [292, 257, 4973, 284, 1811, 308, 10316, 12, 2382, 7490, 37377, 1377, 475, 837, 18177, 837, 257, 3117, 286, 4844, 1377, 1312, 460, 19671, 284, 262, 29270, 9922, 286, 13845, 764, 410, 446, 41823, 6, 9846, 290, 17218, 764], [10134, 340, 1683, 587, 1744, 284, 910, 326, 481, 1789, 82, 468, 4988, 30671, 257, 2095, 5633, 340, 318, 783, 764], [1525, 17728, 281, 5340, 19661, 287, 281, 5340, 995, 837, 30089, 288, 3565, 514, 284, 910, 1521, 2035, 318, 5340, 1377, 543, 3386, 514, 284, 7239, 644, 338, 1744, 290, 644, 356, 1244, 466, 284, 787, 340, 523, 764], [272, 8036, 8886, 329, 717, 12, 2435, 6260, 12, 35248, 1317, 374, 5185, 988, 837, 2592, 6402, 465, 4469, 318, 287, 2647, 2008, 764], [272, 47716, 837, 7744, 1807, 12, 15234, 5730, 804, 379, 530, 286, 262, 749, 19501, 357, 290, 19501, 306, 30318, 516, 1267, 25015, 1678, 287, 674, 6481, 23101, 262, 17818], [439, 262, 13289, 389, 1353, 29838, 290, 837, 1752, 345, 651, 832, 262, 39271, 837, 477, 393, 2147, 4329, 281, 7016, 837, 996, 991, 3967, 837, 34561, 286, 257, 1650, 764], [1, 663, 23682, 389, 635, 45735, 351, 4847, 543, 5879, 262, 1277, 26092, 8497, 286, 644, 340, 3011, 826, 764, 366], [270, 338, 4735, 290, 13891, 290, 3446, 355, 1807, 12, 15234, 5730, 355, 340, 815, 307, 764], [5661, 318, 884, 257, 41535, 306, 2116, 12, 562, 1522, 3437, 498, 8886, 326, 340, 338, 1327, 284, 760, 644, 284, 13463, 717, 764], [20928, 263, 6622, 2081, 284, 4295, 68, 338, 898, 5761, 286, 257, 5899, 10997, 351, 5543, 645, 3616, 837, 290, 645, 6227, 284, 307, 1997, 475, 257, 23895, 837, 13767, 9739, 326, 318, 287, 1842, 351, 663, 898, 1190, 933, 408, 764], [76, 9116, 77, 354, 338, 8768, 11281, 1838, 262, 2646, 338, 12209, 625, 521, 377, 12745, 5784, 21911, 764], [40716, 2759, 837, 262, 2646, 837, 543, 42370, 326, 8902, 43229, 1627, 1022, 8842, 290, 3950, 764, 764, 764, 2753, 257, 11880, 12, 259, 12, 2395, 988, 9408, 772, 355, 340, 20070, 262, 6763, 66, 19177, 8666, 764], [586, 3358, 837, 262, 3275, 286, 5876, 790, 1110, 2331, 284, 307, 326, 477, 3206, 6227, 10114, 82, 1204, 338, 336, 17765, 764], [361, 345, 821, 588, 502, 837, 257, 48431, 329, 257, 922, 1468, 6977, 19661, 290, 2130, 508, 41564, 306, 10408, 284, 4483, 837, 788, 4632, 1667, 12898, 4394, 477, 262, 2818, 9391, 284, 517, 621, 15959, 534, 20788, 764], [1169, 2646, 468, 655, 1576, 286, 2279, 1377, 302, 12, 268, 529, 902, 837, 3934, 2473, 9640, 837, 3375, 12, 2256, 9299, 1377, 290, 262, 2647, 318, 2391, 41674, 764], [8117, 389, 257, 1178, 8303, 82, 379, 12986, 396, 10997, 764, 764, 764, 475, 4632, 262, 14733, 318, 286, 262, 6029, 837, 10296, 290, 10491, 537, 726, 278, 1611, 326, 468, 1716, 281, 4173, 38336, 22847, 764], [64, 7932, 2095, 12, 3106, 10997, 764], [270, 561, 307, 3499, 284, 3285, 422, 262, 584, 1735, 837, 475, 287, 1561, 284, 607, 837, 262, 1466, 389, 866, 329, 262, 954, 764], [272, 886, 451, 4420, 572, 12945, 14348, 10997, 351, 257, 1049, 1826, 12, 66, 1133, 46029, 764], [1169, 3748, 27762, 12, 1659, 12, 5767, 351, 19091, 9027, 318, 36332, 837, 611, 407, 257, 9476, 287, 663, 898, 826, 764], [270, 3544, 281, 1468, 12, 2435, 10451, 837, 340, 338, 407, 22121, 2656, 290, 340, 338, 2138, 24097, 1377, 475, 345, 655, 423, 284, 1842, 262, 1263, 837, 13526, 837, 3772, 3807, 616, 1263, 3735, 308, 10316, 10614, 764], [270, 338, 2048, 5340, 407, 284, 307, 3888, 416, 262, 3807, 338, 33016, 286, 11728, 290, 663, 26547, 2462, 346, 5119, 287, 1281, 12, 47272, 1155, 374, 31269, 764], [8727, 4206, 644, 3446, 5770, 446, 318, 319, 546, 287, 428, 2646, 837, 475, 465, 2456, 290, 4263, 836, 470, 423, 284, 751, 510, 284, 46814, 1096, 345, 764], [1169, 8216, 318, 12974, 837, 28962, 290, 6397, 764], [1169, 44998, 287, 428, 3350, 389, 477, 3734, 837, 475, 24233, 290, 2876, 574, 1559, 389, 1302, 5269, 764], [270, 714, 1487, 45630, 64, 837, 407, 691, 780, 340, 318, 1336, 286, 3306, 5114, 2173, 837, 475, 780, 340, 318, 523, 9857, 326, 340, 1838, 3716, 4819, 21977, 284, 10209, 2045, 329, 2147, 475, 26758, 9739, 764], [10919, 338, 749, 8871, 546, 428, 5688, 4681, 2870, 2646, 764, 764, 764, 318, 262, 2565, 286, 15133, 326, 29298, 689, 777, 19918, 507, 286, 48960, 287, 281, 220, 522, 64, 995, 764], [13, 764, 764, 611, 345, 821, 287, 257, 2000, 900, 329, 44089, 10997, 837, 262, 38817, 481, 8204, 351, 511, 10319, 12198, 837, 275, 707, 9892, 10997, 290, 1182, 1830, 764], [82, 28030, 5183, 38258, 837, 475, 479, 3732, 451, 338, 2854, 318, 26811, 7786, 764], [292, 257, 3437, 837, 285, 81, 764, 4227, 75, 733, 32773, 28317, 262, 29062, 284, 787, 1257, 286, 465, 7481, 764], [1640, 2687, 508, 18140, 262, 705, 1899, 82, 393, 318, 4609, 287, 530, 582, 338, 2882, 284, 14000, 837, 15770, 288, 562, 1058, 14800, 11542, 318, 2861, 6095, 503, 764], [600, 4359, 4250, 290, 4950, 2646, 837, 475, 883, 286, 345, 508, 1100, 262, 1492, 389, 1884, 284, 307, 11679, 764], [1169, 649, 3516, 857, 423, 257, 2612, 764, 783, 837, 611, 340, 691, 550, 257, 3632, 764], [64, 32800, 13936, 286, 34370, 290, 33837, 287, 45630, 64, 338, 3968, 286, 3252, 764], [1455, 437, 560, 4173, 680, 6260, 1449, 358, 272, 1372, 272, 338, 24649, 837, 275, 273, 7757, 2933, 837, 468, 587, 1813, 257, 14442, 3159, 4351, 1373, 764], [1169, 2646, 338, 6000, 11171, 318, 703, 881, 340, 338, 407, 655, 1194, 2018, 12, 1169, 12, 67, 1747, 837, 13997, 12, 261, 12, 1169, 12, 5143, 4286, 764], [5661, 14169, 1451, 263, 3807, 468, 30953, 12733, 286, 21970, 285, 321, 316, 290, 318, 9812, 1257, 329, 3612, 15579, 764], [270, 338, 530, 286, 262, 17766, 395, 7328, 1312, 423, 1683, 1775, 326, 991, 15314, 284, 307, 28996, 13309, 475, 407, 17698, 46908, 764], [30171, 261, 318, 837, 355, 6678, 837, 10457, 764], [10197, 351, 477, 883, 5210, 13015, 11512, 6450, 276, 866, 837, 262, 45630, 272, 47104, 318, 991, 2495, 39189, 276, 922, 764], [72, 836, 470, 760, 10582, 644, 284, 787, 286, 2876, 574, 264, 12342, 527, 456, 338, 1336, 30424, 837, 996, 326, 1422, 470, 2245, 502, 422, 13226, 881, 286, 340, 764], [1169, 27762, 286, 1175, 326, 3140, 947, 318, 355, 881, 257, 27479, 286, 3660, 442, 1437, 287, 4580, 6966, 76, 355, 340, 318, 257, 7014, 1781, 287, 3807, 25871, 764], [77, 11458, 28201, 837, 288, 47883, 287, 48718, 837, 428, 318, 645, 2829, 3807, 837, 290, 345, 1183, 307, 2263, 257, 2526, 611, 345, 3853, 284, 766, 340, 764, 1312, 8359, 262, 6594, 357, 34029, 290, 477, 1267, 837, 27892, 6795, 837, 290, 8713, 7505, 764], [5832, 714, 910, 326, 340, 338, 3105, 379, 1661, 837, 345, 714, 910, 326, 257, 1178, 286, 262, 3435, 719, 287, 2842, 326, 1103, 661, 3636, 470, 837, 475, 530, 1517, 345, 3521, 470, 910, 318, 326, 16144, 731, 774, 318, 20039, 764], [23218, 7035, 5223, 355, 1529, 64, 6274, 12523, 837, 281, 340, 7199, 28422, 290, 31483, 1277, 601, 508, 655, 4325, 284, 307, 607, 898, 5290, 4472, 764], [47119, 763, 381, 5708, 743, 1239, 1716, 262, 26479, 465, 9955, 373, 837, 475, 22574, 1587, 244, 1178, 28303, 481, 764, 475, 1912, 319, 269, 80, 837, 1312, 1183, 3729, 307, 5291, 281, 4151, 503, 329, 465, 1306, 1628, 764], [272, 28297, 837, 20780, 89, 813, 2471, 13781, 11648, 546, 1204, 319, 262, 1923, 8025, 764], [8929, 319, 7758, 375, 20058, 764, 475, 340, 338, 17991, 1786, 1214, 278, 837, 1165, 837, 5176, 284, 1913, 837, 18409, 13289, 422, 262, 2187, 3350, 764], [69, 3289, 837, 257, 12121, 3807, 326, 16316, 1377, 287, 257, 3155, 286, 27962, 837, 645, 1342, 764], [270, 338, 407, 523, 881, 20050, 284, 2342, 355, 340, 318, 19128, 3101, 284, 6004, 284, 649, 5389, 286, 257, 2180, 3950, 837, 290, 284, 3187, 351, 617, 286, 262, 661, 508, 547, 1498, 284, 787, 281, 2928, 287, 262, 13766, 995, 764], [2777, 8207, 3900, 318, 262, 4071, 3437, 508, 857, 407, 765, 284, 14037, 10209, 284, 308, 19301, 379, 393, 22517, 465, 2041, 3048, 764, 339, 655, 3382, 606, 284, 307, 636, 286, 262, 2223, 837, 262, 39328, 286, 465, 7147, 3950, 764, 994, 837, 36308, 837, 484, 389, 764], [7353, 860, 14, 1157, 262, 17580, 3275, 286, 366, 2614, 4925, 717, 366, 1244, 407, 307, 355, 6340, 21156, 355, 5292, 764], [13415, 290, 7649, 84, 2897, 3288, 837, 2300, 12, 1659, 12, 22584, 13289, 326, 1278, 600, 351, 24140, 837, 40314, 290, 1842, 764], [5661, 10758, 290, 47188, 605, 717, 3895, 422, 374, 27792, 716, 2743, 27513, 262, 1458, 9495, 326, 3504, 12, 1886, 1466, 655, 18869, 423, 1257, 656, 257, 374, 12752, 2190, 786, 286, 3054, 723, 39309, 764], [30412, 959, 284, 2461, 621, 920, 71, 1904, 625, 837, 290, 364, 1559, 338, 22888, 2614, 5761, 318, 407, 691, 1233, 2903, 475, 1233, 5077, 764], [36960, 3750, 4295, 290, 3750, 3026, 757], [13, 764, 764, 6278, 1681, 318, 3142, 284, 1382, 281, 19185, 306, 1692, 2095, 837, 281, 2048, 1103, 12, 12583, 2576, 1844, 351, 5876, 290, 2911, 764], [4514, 428, 2646, 318, 407, 287, 262, 1551, 6452, 837, 340, 318, 991, 6165, 845, 19201, 764, 892, 286, 340, 355, 257, 3297, 286, 4467, 2057, 329, 262, 2000, 764], [2375, 332, 837, 12399, 290, 29381, 5848, 913, 3807, 764], [13, 764, 764, 1464, 3793, 3867, 306, 8768, 764], [272, 12661, 10165, 546, 4673, 832, 6467, 19122, 764], [10594, 5552, 534, 1751, 416, 262, 13843, 290, 716, 6201, 606, 290, 26072, 606, 764], [64, 11004, 27228, 12, 11374, 16901, 319, 262, 3450, 286, 5854, 764], [25591, 508, 561, 1061, 289, 1531, 365, 319, 465, 23387, 39180, 602, 764, 764, 764, 389, 20945, 416, 12399, 837, 5364, 13289, 422, 289, 7211, 861, 290, 2153, 320, 417, 764], [272, 7411, 2081, 1621, 286, 257, 442, 3762, 8674, 508, 2753, 510, 5010, 290, 13520, 510, 287, 281, 9901, 438, 23800, 4632, 416, 262, 4036, 661, 2950, 764], [43365, 866, 262, 614, 338, 749, 1807, 12, 15234, 5730, 2646, 764, 475, 340, 13831, 257, 2756, 329, 663, 28746, 9028, 1830, 25428, 764], [270, 338, 257, 23754, 45630, 272, 5701, 3807, 290, 288, 10679, 627, 1698, 318, 663, 15177, 2612, 764], [5661, 318, 884, 257, 1029, 12, 22554, 3807, 810, 262, 13026, 2229, 290, 262, 26409, 389, 523, 6275, 837, 508, 16609, 611, 262, 1621, 338, 257, 1310, 4939, 764], [785, 35025, 15827, 32251, 837, 996, 6454, 24135, 416, 257, 12747, 459, 3756, 10846, 764], [270, 338, 32513, 583, 25867, 287, 663, 11800, 837, 16443, 475, 5576, 298, 9134, 804, 379, 262, 8849, 1641, 764], [64, 2187, 1256, 15626, 837, 2030, 15492, 290, 8258, 764], [17989, 14505, 764], [1078, 282, 33237, 10997, 351, 257, 2726, 13936, 286, 19225, 290, 35394, 1626, 257, 9775, 384, 25924, 4845, 764], [1169, 9573, 286, 262, 7912, 7997, 837, 1111, 287, 2846, 286, 3918, 290, 26289, 837, 15174, 262, 13106, 422, 4203, 46152, 25253, 837, 355, 857, 262, 20431, 4506, 2319, 12, 11374, 2491, 640, 764], [1169, 43923, 396, 318, 257, 3734, 1188, 276, 713, 652, 670, 329, 755, 44978, 837, 925, 26192, 416, 465, 898, 6461, 837, 1642, 465, 584, 6918, 7599, 26192, 287, 262, 22803, 764], [69, 6197, 23361, 262, 2597, 837, 3501, 257, 5381, 837, 5670, 2854, 35162, 416, 39991, 286, 4203, 764], [10197, 611, 345, 460, 470, 28942, 366, 21486, 305, 366, 9380, 837, 345, 1183, 9144, 881, 286, 410, 446, 41823, 6, 14733, 837, 543, 23589, 2412, 9450, 13215, 764], [271, 2607, 670, 1107, 355, 8756, 803, 355, 705, 16575, 293, 1525, 6, 523, 6840, 1838, 340, 5633], [69, 47769, 764, 764, 764, 36308, 15314, 284, 503, 19489, 262, 2597, 290, 7675, 5341, 262, 25501, 284, 481, 271, 338, 995, 12, 13927, 88, 46969, 764], [3885, 10035, 31964, 284, 1972, 49671, 88, 625, 473, 48915, 12, 18834, 837, 866, 88, 12, 2395, 988, 276, 6941, 381, 1039, 290, 511, 795, 8071, 6587, 1337, 83, 3979, 481, 2192, 1254, 17991, 37264, 416, 262, 2646, 338, 35842, 837, 7543, 12, 5787, 20868, 764], [65, 48151, 338, 4850, 265, 1634, 286, 607, 2614, 18598, 656, 1281, 12, 9032, 929, 583, 67, 653, 468, 257, 34813, 5198, 326, 338, 5802, 284, 13279, 764], [272, 19827, 290, 17774, 9793, 284, 45610, 1559, 764], [292, 2938, 837, 910, 829, 6, 4451, 1573, 1759, 290, 14169, 7110, 542, 15104, 1817, 389, 355, 7786, 355, 1683, 837, 996, 484, 743, 307, 43861, 416, 617, 1913, 13289, 764], [64, 2746, 286, 644, 7328, 588, 428, 815, 307, 588, 764], [292, 356, 527, 290, 356, 747, 805, 10176, 351, 884, 11281, 290, 4681, 2870, 3326, 303, 837, 262, 7540, 23014, 6304, 470, 355, 881, 546, 5279, 837, 3206, 12741, 393, 1964, 556, 270, 22930, 355, 484, 547, 2391, 257, 15499, 286, 262, 773, 296, 4674, 1692, 481, 284, 14034, 837, 2018, 290, 2251, 764], [43669, 837, 777, 277, 49191, 389, 655, 326, 12270, 922, 764, 2125, 470, 340, 1049, 5633], [272, 48943, 1257, 2646, 655, 257, 3756, 582, 1497, 422, 20187, 764], [2502, 12, 1169, 12, 4852, 290, 257, 1643, 23619, 298, 265, 699, 837, 428, 318, 257, 3807, 326, 338, 1392, 267, 375, 829, 286, 3918, 290, 9136, 764], [13, 764, 764, 257, 45516, 290, 3665, 8689, 326, 10069, 326, 3555, 3597, 290, 34768, 389, 407, 262, 691, 7481, 284, 2193, 287, 1204, 764], [44460, 306, 9179, 355, 281, 12452, 286, 257, 3592, 287, 6801, 764], [2127, 1694, 516, 837, 44018, 10997, 764], [64, 15403, 290, 15241, 10512, 837, 1912, 319, 262, 2081, 1621, 286, 257, 17840, 6580, 37189, 12, 2382, 7490, 338, 1235, 284, 1282, 284, 2846, 351, 465, 15587, 837, 10069, 262, 614, 768, 356, 477, 423, 287, 674, 11954, 329, 13427, 1626, 262, 1641, 9197, 764], [292, 257, 374, 10757, 2646, 546, 17144, 661, 287, 18857, 4113, 852, 7121, 290, 5954, 357, 7360, 290, 31474, 9404, 1267, 416, 6227, 764, 764, 764, 685, 8044, 290, 17115, 29690, 60, 1838, 329, 281, 22015, 278, 922, 640, 764], [46303, 4623, 2095, 2050, 416, 290, 29350, 7858, 11635, 764], [49840, 1671, 515, 379, 37437, 590, 837, 428, 3731, 10997, 286, 33558, 468, 5442, 13289, 290, 257, 40551, 837, 1278, 571, 20024, 326, 338, 1327, 284, 4405, 764], [918, 1008, 338, 2854, 355, 288, 993, 647, 318, 45504, 837, 7744, 34418, 764], [361, 645, 530, 25287, 503, 597, 286, 777, 13289, 355, 5764, 12, 18275, 837, 340, 338, 691, 780, 356, 561, 1607, 2147, 1342, 422, 428, 7684, 764], [361, 345, 1842, 3555, 290, 14, 273, 19518, 837, 788, 416, 477, 1724, 2198, 340, 503, 764, 345, 1183, 2192, 1842, 340, 764], [2016, 286, 1948, 1393, 284, 2444, 290, 33471, 286, 3230, 9280, 290, 995, 2647, 837, 262, 2646, 318, 3562, 284, 787, 10209, 286, 477, 9337, 837, 6467, 19063, 290, 14405, 9383, 2694, 765, 284, 651, 510, 290, 9280, 764], [877, 24321, 290, 42831, 28695, 764], [7364, 9976, 318, 736, 287, 257, 1688, 835, 764], [270, 338, 257, 3807, 1377, 290, 281, 5062, 1377, 345, 1839, 470, 765, 284, 2051, 764], [270, 338, 4071, 284, 1064, 257, 2646, 326, 32282, 829, 262, 4151, 837, 6459, 262, 3632, 837, 290, 45104, 674, 22279, 329, 3049, 12, 32416, 2223, 837, 475, 9137, 989, 16316, 477, 326, 290, 257, 2187, 1256, 517, 764], [4514, 407, 477, 27188, 284, 28003, 389, 523, 39730, 837, 612, 338, 881, 3872, 290, 645, 1402, 2033, 286, 19518, 287, 4813, 460, 470, 9422, 764], [361, 612, 338, 2147, 4713, 546, 266, 1236, 397, 274, 837, 543, 373, 3194, 416, 285, 81, 764, 1357, 68, 78, 837, 508, 4635, 290, 7924, 262, 2646, 351, 1149, 829, 257, 764, 751, 408, 72, 837, 881, 286, 262, 640, 262, 3807, 5300, 16425, 764], [30482, 421, 313, 338, 284, 1416, 64, 318, 257, 2190, 764], [1525, 262, 886, 286, 645, 884, 1517, 262, 5386, 837, 588, 4405, 20970, 837, 468, 257, 2342, 913, 17696, 329, 262, 9234, 764], [361, 345, 8288, 884, 6918, 355, 407, 889, 12788, 837, 1440, 37377, 290, 257, 14825, 837, 865, 17484, 474, 1952, 6, 26339, 393, 1029, 37744, 837, 788, 345, 1839, 470, 765, 284, 2051, 546, 257, 2933, 764], [13, 764, 764, 262, 10296, 502, 335, 278, 286, 10512, 290, 10997, 1838, 366, 644, 640, 318, 340, 612, 5633, 366, 1223, 262, 2081, 2646, 6940, 481, 2883, 764], [47119, 988, 7622, 262, 2646, 7558, 256, 2306, 764, 764, 764, 20252, 262, 2095, 338, 24842, 351, 257, 13738, 12409, 5874, 3918, 290, 281, 41231, 14344, 837, 36051, 12, 2339, 4776, 764], [72, 2187, 12, 44407, 4313, 326, 2506, 766, 428, 3807, 438, 329, 663, 6754, 12085, 3436, 764], [20342, 837, 508, 2073, 2476, 257, 14643, 5633], [6511, 1636, 468, 12006, 257, 21196, 24870, 837, 19447, 453, 21002, 27479, 286, 883, 42291, 1528, 764], [16670, 340, 20188, 82, 257, 845, 537, 291, 704, 13026, 379, 1661, 837, 428, 4315, 12, 1154, 6005, 338, 4713, 10721, 837, 26758, 2647, 837, 290, 922, 12, 32353, 1522, 599, 2954, 389, 1690, 25246, 764], [28950, 1036, 518, 2680, 290, 37154, 284, 4973, 837, 475, 384, 335, 15668, 290, 266, 692, 353, 338, 46499, 13289, 5298, 428, 1290, 2029, 262, 1241, 286, 262, 6678, 285, 3885, 2815, 4369, 3807, 764], [2188, 766, 340, 290, 2883, 764], [1169, 13393, 837, 4320, 2339, 27329, 481, 14947, 772, 883, 10209, 508, 423, 1310, 16336, 329, 11063, 12, 26240, 2181, 3004, 764], [469, 3643, 28050, 1419, 17021, 339, 338, 2407, 257, 12356, 3437, 290, 6072, 3881, 4053, 2523, 514, 339, 338, 257, 995, 12, 4871, 8674, 351, 47646, 286, 257, 4923, 2000, 764], [8117, 338, 257, 5909, 1108, 17142, 287, 1138, 25986, 326, 318, 655, 35589, 764], [28582, 1082, 516, 25920, 82, 743, 880, 307, 262, 749, 9815, 286, 777, 7328, 290, 635, 5587, 11706, 284, 262, 3872, 764], [1169, 661, 287, 3290, 12735, 290, 1976, 12, 13202, 389, 523, 8258, 837, 8361, 290, 6776, 837, 345, 423, 284, 2342, 606, 780, 345, 460, 470, 4043, 284, 766, 644, 484, 466, 1306, 764], [292, 4077, 12, 70, 5500, 9234, 6918, 467, 837, 340, 338, 257, 3566, 764], [292, 12207, 88, 837, 285, 40302, 302, 6548, 4356, 365, 357, 19318, 25790, 362, 1058, 7850, 286, 3252, 1267, 468, 655, 262, 826, 2033, 286, 45581, 290, 36292, 764], [64, 12738, 1412, 306, 2829, 2646, 837, 530, 326, 13676, 287, 1176, 287, 23583, 764], [2271, 318, 257, 21002, 837, 21266, 1981, 290, 262, 3807, 338, 2962, 2402, 607, 1838, 340, 4388, 290, 9857, 764], [64, 29395, 837, 5032, 913, 1310, 9961, 2646, 764], [64, 845, 45466, 1011, 319, 1487, 837, 2526, 290, 19661, 837, 290, 262, 2646, 3544, 31049, 284, 787, 663, 2173, 546, 13427, 290, 3349, 764], [58, 392, 882, 60, 3544, 257, 2277, 12, 273, 12, 3927, 19713, 326, 7127, 1690, 1576, 284, 1394, 262, 2646, 17774, 772, 611, 4844, 286, 340, 1838, 257, 36728, 286, 2565, 764], [46176, 7012, 8169, 12, 83, 538, 318, 257, 7932, 2646, 351, 257, 49025, 5330, 1085, 2854, 416, 18145, 344, 1413, 7923, 326, 1595, 470, 10925, 284, 2666, 262, 2615, 1566, 2506, 318, 3910, 286, 340, 764], [41081, 262, 890, 2491, 640, 837, 262, 8761, 1239, 5300, 30740, 1377, 612, 338, 645, 3715, 326, 26557, 366, 12436, 2270, 5145, 366], [16308, 735, 857, 257, 922, 1693, 994, 286, 1762, 1028, 607, 3288, 4300, 1799, 764], [64, 2646, 286, 14186, 41867, 1242, 2759, 31962, 1886, 355, 10908, 4568, 764], [74, 3732, 451, 3607, 257, 12465, 2854, 764], [1169, 1266, 3807, 286, 663, 1611, 1201, 705, 65, 7098, 764, 705, 17115, 292, 837, 1011, 4710, 764, 428, 318, 703, 345, 779, 2041, 3048, 764], [1, 5306, 6267, 366, 468, 587, 3194, 523, 880, 837, 326, 772, 257, 2829, 366, 17495, 321, 2781, 5145, 366, 1474, 262, 886, 2753, 319, 257, 2187, 584, 3616, 764], [505, 1711, 4590, 318, 281, 19827, 27479, 286, 530, 582, 290, 465, 47456, 2162, 340, 338, 655, 1165, 2089, 340, 1595, 470, 423, 517, 29227, 286, 11281, 764], [74, 559, 35826, 8075, 281, 41128, 2565, 286, 407, 691, 852, 612, 379, 262, 640, 286, 777, 2995, 475, 262, 845, 1755, 23963, 6391, 373, 2923, 764], [354, 971, 340, 510, 284, 616, 512, 6944, 329, 1111, 390, 299, 7058, 290, 4636, 6883, 837, 475, 1312, 550, 257, 2495, 922, 640, 351, 428, 3807, 532, 3805, 663, 24862, 17978, 764], [896, 8188, 290, 3054, 2247, 389, 477, 517, 621, 5385, 837, 475, 340, 409, 8401, 257, 1611, 286, 40459, 13997, 12, 41364, 20024, 290, 837, 379, 262, 976, 640, 837, 318, 523, 4713, 290, 1479, 286, 262, 6678, 32251, 18149, 326, 340, 477, 2331, 284, 307, 5836, 329, 262, 717, 640, 764], [270, 6870, 1365, 12, 14813, 12, 23913, 3807, 12, 8601, 326, 1595, 470, 3512, 257, 13526, 837, 22943, 5386, 764], [64, 23332, 1865, 45516, 12838, 286, 262, 11331, 18893, 540, 8470, 326, 11007, 764], [272, 23260, 278, 15013, 329, 1787, 353, 3296, 18116, 284, 6594, 262, 40476, 26586, 4911, 3812, 257, 649, 614, 286, 5536, 290, 38625, 764], [1169, 18054, 286, 262, 10544, 5419, 366, 8824, 2971, 10591, 366, 4485, 2029, 663, 2612, 12, 261, 12, 896, 12, 82, 49189, 3597, 764], [270, 338, 257, 19695, 3626, 837, 475, 599, 3711, 351, 266, 563, 14733, 290, 8768, 3108, 418, 837, 2592, 1022, 2146, 1030, 290, 2266, 41711, 764], [5661, 12452, 286, 37115, 1204, 572, 262, 29963, 286, 262, 275, 27792, 2386, 361, 3317, 28080, 286, 502, 87, 3713, 4394, 281, 1786, 1214, 278, 835, 284, 10176, 262, 27494, 286, 262, 545, 897, 5794, 764], [21953, 290, 14851, 837, 475, 635, 12362, 8258, 764], [1169, 3807, 468, 281, 44128, 286, 4151, 12, 7501, 2105, 5874, 3048, 764], [301, 5889, 572, 351, 257, 20188, 837, 475, 788, 277, 6457, 829, 588, 257, 9583, 4859, 286, 6382, 578, 379, 262, 845, 886, 764, 340, 338, 991, 2861, 257, 804, 764], [1712, 8036, 837, 996, 837, 318, 262, 2646, 338, 1280, 12, 1631, 19523, 326, 17567, 284, 5000, 1969, 663, 3435, 6, 7016, 14129, 764], [64, 10359, 6594, 656, 8718, 12, 2435, 837, 8801, 301, 37186, 318, 257, 29696, 290, 20050, 8855, 329, 477, 9337, 379, 597, 640, 764], [32762, 959, 318, 23754, 837, 6079, 281, 555, 12072, 837, 5801, 12, 6495, 7585, 284, 284, 1891, 338, 339, 485, 26679, 12, 290, 299, 1155, 37467, 12, 5420, 14226, 2259, 10721, 764], [13, 764, 764, 257, 23895, 290, 5365, 17082, 3704, 286, 3671, 499, 1042, 764], [1169, 1621, 27521, 736, 1088, 319, 2346, 287, 262, 1611, 286, 19992, 40686, 326, 338, 4071, 287, 2646, 1909, 837, 475, 307, 7728, 1058, 340, 338, 257, 3105, 25801, 284, 651, 612, 764], [1169, 2187, 3350, 3073, 284, 307, 1719, 523, 881, 1257, 351, 262, 23905, 13915, 39343, 290, 14397, 4675, 1458, 10924, 837, 37432, 1088, 18611, 14700, 588, 8966, 5362, 290, 35024, 21141, 837, 326, 262, 16001, 9849, 2431, 20780, 12271, 416, 764], [13, 764, 764, 468, 2030, 15492, 8188, 810, 262, 5462, 4240, 611, 484, 821, 25899, 34140, 2405, 355, 6776, 764, 340, 338, 257, 49822, 41955, 284, 262, 1854, 1231, 5033, 257, 1281, 23922, 9707, 837, 925, 23387, 416, 663, 366, 1450, 287, 257, 264, 446, 500, 460, 366, 43062, 9156, 764], [6511, 706, 345, 2666, 655, 500, 837, 345, 1183, 307, 11263, 644, 481, 1645, 284, 607, 290, 24433, 607, 262, 1266, 1377, 4232, 326, 1244, 1612, 764], [24219, 2181, 43787, 290, 5901, 351, 850, 5239, 837, 475, 17774, 1576, 379, 705, 2550, 1988, 6, 284, 4313, 284, 2687, 2045, 329, 1223, 1180, 764], [13345, 502, 257, 266, 11011, 837, 475, 1312, 16896, 837, 407, 1752, 837, 475, 1115, 1661, 287, 428, 15108, 6029, 2646, 764], [1662, 9982, 269, 764, 289, 764, 267, 764, 468, 267, 375, 829, 286, 31016, 11330, 764], [272, 20886, 290, 2612, 12, 2001, 478, 278, 2646, 546, 262, 12111, 6370, 286, 410, 1155, 22678, 8015, 2877, 287, 334, 764, 264, 764, 35703, 11665, 284, 1394, 511, 7176, 6776, 287, 15231, 764], [1169, 1241, 286, 24841, 9066, 416, 428, 4747, 12, 1941, 12, 727, 717, 12, 2435, 3895, 3437, 318, 21994, 837, 6402, 607, 31830, 1240, 290, 607, 2426, 2300, 764], [64, 37196, 9739, 837, 1862, 287, 4437, 475, 13013, 287, 477, 7612, 351, 262, 1336, 1108, 286, 4437, 290, 2565, 286, 10152, 326, 2058, 691, 351, 1998, 764], [6381, 1681, 338, 2107, 12, 2673, 7297, 468, 257, 2106, 286, 13011, 29932, 781, 1747, 321, 837, 475, 428, 318, 530, 6695, 618, 484, 423, 41562, 257, 4071, 16840, 764], [361, 262, 3275, 2331, 517, 1777, 576, 621, 262, 2961, 7328, 837, 262, 4263, 423, 884, 257, 7818, 8737, 345, 743, 407, 1337, 764], [25356, 9245, 318, 257, 2003, 2285, 6833, 393, 23985, 284, 307, 3190, 11564, 318, 1280, 284, 1808, 837, 475, 262, 2526, 12, 83, 3979, 287, 262, 4315, 815, 2198, 340, 503, 290, 1296, 511, 898, 4459, 764], [8117, 389, 7188, 287, 428, 1848, 286, 262, 1204, 286, 6802, 1216, 3755, 479, 993, 5439, 326, 389, 1871, 22041, 338, 18822, 428, 614, 764, 12716, 837, 484, 821, 26155, 704, 287, 1022, 262, 749, 848, 20846, 5894, 1848, 286, 479, 993, 5439, 338, 1204, 40758, 764], [8117, 389, 7188, 340, 460, 307, 2612, 12, 10920, 278, 287, 281, 5508, 290, 35290, 357, 290, 10296, 1267, 835, 764], [31712, 1598, 286, 30047, 3511, 326, 340, 338, 257, 366, 2081, 1621, 366, 290, 345, 821, 1884, 284, 423, 530, 5968, 84, 6862, 640, 379, 262, 6918, 764], [8117, 389, 655, 1576, 30953, 287, 262, 12838, 284, 787, 340, 1290, 517, 19201, 621, 2048, 597, 9961, 2646, 287, 2274, 4088, 764], [1169, 37437, 590, 2646, 10876, 468, 1716, 523, 14713, 12, 8158, 6676, 326, 3296, 290, 11408, 15350, 2402, 3384, 993, 1123, 42897, 2838, 284, 11354, 1186, 503, 262, 1306, 1049, 1517, 764, 705, 83, 324, 36869, 6, 373, 530, 286, 262, 7328, 523, 6875, 428, 614, 837, 475, 340, 338, 1107, 517, 286, 262, 1306, 2495, 922, 1517, 764], [16090, 422, 304, 15516, 1252, 338, 24649, 837, 686, 71, 647, 277, 1077, 507, 262, 3297, 286, 19217, 837, 32154, 2095, 12, 290, 12, 2776, 2050, 339, 338, 19344, 329, 4647, 764], [1169, 1621, 5300, 517, 588, 257, 2726, 1100, 837, 5901, 351, 4334, 16376, 286, 1464, 47460, 910, 829, 10721, 764], [12518, 340, 1107, 9853, 764, 764, 764, 15222, 37437, 323, 20417, 319, 257, 38259, 1241, 326, 23589, 2412, 3303, 764], [1169, 4065, 6067, 1342, 621, 262, 3435, 837, 3584, 262, 28303, 5127, 1576, 19481, 837, 1969, 3848, 290, 4274, 12, 19692, 274, 284, 15959, 514, 764], [58, 272, 60, 20105, 14348, 10997, 764], [1169, 10544, 389, 9623, 764, 484, 389, 644, 1838, 340, 2861, 262, 5296, 284, 262, 21421, 764], [32319, 422, 8258, 284, 48883, 290, 9593, 617, 286, 262, 614, 338, 1266, 7205, 837, 2614, 15432, 43609, 6088, 286, 10092, 12858, 764], [72, 13121, 477, 262, 640, 546, 4379, 262, 976, 4213, 5100, 287, 7328, 625, 290, 625, 757, 837, 475, 262, 38043, 710, 5369, 17021, 326, 257, 4713, 1011, 318, 1464, 1744, 764], [8344, 5691, 5897, 22601, 12, 5269, 588, 300, 6, 615, 1151, 5330, 290, 1128, 15204, 764], [8807, 281, 12191, 11648, 714, 651, 340, 477, 866, 837, 290, 20240, 443, 68, 338, 474, 320, 7586, 1058, 477, 45630, 272, 379, 890, 938, 3607, 663, 2426, 257, 3807, 12733, 286, 465, 18054, 764], [13, 764, 764, 355, 262, 1621, 369, 469, 874, 345, 1254, 262, 5207, 286, 262, 3491, 9976, 26784, 7463, 656, 1295, 287, 257, 835, 326, 1838, 534, 19656, 256, 17697, 351, 16084, 290, 14067, 764], [64, 1049, 10997, 26479, 4206, 1049, 10997, 761, 77, 470, 1464, 787, 514, 6487, 764, 4628, 1621, 338, 407, 612, 1865, 532, 475, 705, 5657, 1213, 8548, 6, 2523, 339, 338, 319, 465, 835, 764], [1169, 3807, 318, 530, 286, 262, 1266, 6096, 286, 1242, 913, 1588, 5794, 49291, 345, 389, 1884, 284, 766, 17949, 2582, 764], [75, 2412, 2346, 284, 262, 16946, 313, 2890, 34377, 357, 26592, 837, 996, 407, 3016, 523, 26592, 355, 262, 3182, 27779, 595, 1681, 2576, 3807, 1267, 3235, 7352, 286, 262, 3182, 27779, 595, 1681, 2933, 3807, 764], [4053, 12, 15266, 837, 16576, 13134, 290, 21104, 2823, 290, 7781, 837, 262, 2646, 2499, 319, 1811, 2974, 837, 11764, 14085, 1919, 517, 82, 981, 3140, 77, 1723, 262, 5386, 351, 663, 7016, 2834, 764], [73, 888, 2124, 468, 45002, 3048, 290, 257, 8169, 560, 7110, 837, 475, 663, 8352, 46241, 837, 2116, 12, 10378, 8344, 803, 2565, 286, 14733, 1838, 510, 329, 257, 1256, 764], [58, 83, 323, 4491, 60, 34547, 262, 2126, 286, 1642, 479, 993, 5439, 338, 1242, 257, 2877, 837, 12704, 636, 286, 262, 3807, 837, 1690, 48786, 278, 262, 6802, 656, 607, 898, 670, 764, 428, 2125, 470, 257, 649, 2126, 764, 340, 338, 587, 1760, 878, 475, 1239, 523, 46641, 393, 351, 523, 881, 7506, 764], [272, 8036, 611, 19556, 3626, 326, 9217, 1103, 7401, 764], [11545, 10439, 1626, 530, 1641, 1332, 13215, 287, 428, 12661, 290, 31531, 2406, 12, 1659, 12, 496, 10512, 764], [270, 5238, 6639, 290, 19074, 837, 475, 262, 20820, 286, 427, 391, 3900, 338, 2646, 318, 326, 340, 4988, 318, 19661], [17080, 333, 4623, 290, 10457, 11648, 764], [13, 764, 764, 46814, 2890, 837, 281, 4151, 12, 29443, 4205, 286, 3660, 307, 11030, 3968, 287, 257, 7002, 286, 21540, 837, 13703, 656, 909, 26018, 290, 1441, 764], [505, 286, 262, 1266, 6096, 286, 703, 284, 2190, 257, 2426, 837, 345, 821, 407, 3938, 3910, 318, 852, 11068, 837, 881, 588, 257, 4590, 286, 3511, 345, 1422, 470, 760, 373, 852, 2077, 764], [1662, 1165, 1290, 2174, 262, 21194, 345, 460, 991, 1254, 3437, 2853, 271, 4489, 1734, 45177, 338, 11226, 2612, 290, 262, 16245, 1108, 339, 468, 329, 465, 3435, 764], [292, 611, 284, 5879, 257, 4048, 3437, 460, 787, 257, 3807, 351, 645, 2705, 13015, 837, 479, 776, 29441, 1263, 417, 322, 4394, 645, 7543, 12, 1073, 803, 393, 987, 75, 8401, 286, 1657, 1108, 764, 607, 2646, 318, 23232, 298, 4420, 26435, 436, 10051, 20803, 290, 22029, 764], [58, 41082, 1734, 45177, 60, 2331, 284, 6537, 19933, 306, 326, 772, 18016, 318, 5322, 284, 281, 3038, 416, 262, 8713, 26045, 286, 1204, 290, 1918, 764], [1169, 1255, 318, 46814, 2890, 1377, 5901, 351, 36292, 290, 2809, 282, 273, 764], [69, 4828, 468, 275, 1144, 465, 5848, 290, 16675, 465, 898, 31731, 994, 287, 257, 835, 764, 764, 764, 326, 5300, 845, 1692, 290, 845, 2081, 284, 1204, 764], [270, 338, 1257, 837, 475, 262, 2438, 12, 16620, 481, 6129, 826, 625, 2506, 338, 1182], [12544, 837, 474, 888, 38043, 710, 764, 339, 460, 5046, 257, 2615, 588, 257, 2208, 4293, 837, 339, 460, 503, 12, 4169, 1094, 597, 5797, 837, 339, 1183, 651, 262, 2576, 764, 339, 338, 2208, 13997, 5145], [10919, 1838, 262, 3807, 257, 10997, 318, 262, 835, 340, 30940, 262, 517, 2726, 10825, 2950, 764], [272, 47029, 803, 1998, 764], [5661, 269, 4185, 306, 16304, 284, 262, 7358, 2277, 318, 257, 1310, 517, 22632, 23895, 837, 257, 1310, 36090, 959, 837, 290, 257, 1310, 517, 8805, 11128, 764], [1169, 35396, 286, 2208, 38817, 743, 307, 42738, 837, 475, 484, 1183, 7881, 7634, 351, 9599, 508, 991, 27452, 257, 2705, 4136, 329, 3718, 349, 1455, 9386, 14733, 764], [1169, 2646, 318, 47029, 803, 284, 2342, 780, 6450, 1754, 837, 35112, 422, 262, 17778, 286, 10451, 837, 10069, 10059, 21593, 355, 281, 8674, 764], [64, 12899, 837, 772, 38697, 837, 1865, 27954, 2280, 2342, 540, 804, 379, 262, 264, 585, 312, 1204, 286, 289, 9632, 338, 10281, 3491, 29202, 41175, 764], [1169, 2646, 16316, 407, 655, 262, 1336, 4641, 286, 302, 3919, 338, 13964, 20868, 290, 11281, 837, 475, 257, 640, 3067, 736, 284, 644, 340, 2936, 588, 1141, 883, 28371, 1136, 83, 1346, 8627, 1528, 764], [10919, 1244, 423, 587, 257, 45741, 2612, 48133, 12838, 318, 3027, 1484, 351, 13357, 764], [23661, 262, 19997, 1039, 1058, 329, 262, 717, 640, 1201, 16459, 6095, 2341, 272, 837, 8805, 6415, 1595, 470, 10110, 355, 281, 14549, 764], [16670, 845, 881, 588, 262, 717, 3807, 1912, 319, 474, 764, 479, 764, 5752, 1359, 338, 28187, 8842, 1266, 23531, 837, 428, 1218, 467, 12, 744, 22194, 257, 2407, 28790, 837, 1182, 6511, 14613, 290, 257, 4300, 1346, 46026, 9408, 764], [58, 366, 1011, 1337, 286, 616, 3797, 366, 2361, 318, 281, 12698, 3621, 1310, 2646, 326, 2753, 514, 319, 281, 12452, 286, 1862, 4044, 1204, 287, 7876, 5366, 479, 46215, 832, 262, 11954, 290, 9017, 286, 262, 1936, 44998, 764], [292, 262, 1621, 6100, 16087, 273, 1346, 832, 663, 3598, 1110, 41352, 837, 262, 4286, 4329, 6481, 46814, 2890, 764], [76, 11433, 557, 318, 257, 12362, 4050, 279, 2357, 14, 2777, 1304, 12, 805, 764], [1662, 257, 37438, 393, 5347, 81, 7246, 803, 670, 837, 475, 340, 338, 9389, 837, 3360, 14169, 837, 290, 1464, 3499, 837, 290, 883, 389, 3840, 1576, 284, 766, 340, 764], [1169, 2646, 4539, 319, 4961, 3354, 286, 24211, 290, 11501, 1377, 11501, 326, 2058, 351, 1998, 764, 340, 468, 1257, 852, 7334, 510, 764], [2339, 1468, 22886, 290, 4240, 19490, 26843, 6580, 3447, 764], [430, 38015, 466, 7328, 1282, 1863, 326, 389, 355, 12661, 837, 409, 18478, 415, 837, 290, 3867, 355, 937, 36194, 10614, 764], [505, 32335, 2476, 262, 44344, 284, 2883, 428, 20239, 2223, 1290, 344, 764], [37121, 8258, 329, 262, 2099, 286, 3807, 340, 318, 764, 764, 764], [270, 338, 1690, 35905, 803, 306, 1278, 571, 290, 1281, 870, 837, 290, 1865, 340, 468, 587, 925, 351, 1049, 10678, 1337, 290, 15314, 284, 5203, 510, 262, 582, 287, 257, 835, 284, 257, 46494, 2252, 20136, 287, 772, 262, 749, 39472, 278, 19091, 764], [505, 286, 685, 372, 89, 519, 338, 60, 1551, 7867, 2499, 764], [5661, 1489, 1694, 516, 10997, 9179, 510, 257, 12177, 15438, 286, 262, 10030, 286, 5179, 286, 4138, 286, 442, 3762, 837, 530, 543, 460, 691, 12780, 355, 257, 7818, 13574, 764], [9417, 1107, 318, 546, 257, 3155, 286, 7165, 3730, 837, 290, 340, 338, 21546, 284, 6487, 1863, 351, 606, 764], [12081, 685, 82, 2973, 60, 656, 17238, 764], [272, 45420, 6087, 286, 257, 374, 12752, 922, 1621, 900, 319, 257, 4988, 4490, 5046, 764], [8117, 338, 645, 17086, 262, 10170, 15013, 14482, 286, 262, 2646, 764, 764, 764, 393, 262, 7016, 11540, 286, 262, 13289, 764], [32146, 7328, 428, 614, 423, 587, 355, 581, 3552, 287, 511, 7016, 12105, 1108, 764], [1069, 421, 271, 3973, 13134, 290, 4958, 2759, 611, 3718, 6819, 987, 86, 16206, 126, 227, 685, 1169, 2646, 60, 9405, 287, 257, 13899, 837, 12661, 5642, 262, 987, 2229, 1359, 286, 3234, 837, 4819, 290, 1957, 19497, 764], [4169, 574, 1559, 338, 2854, 318, 379, 1752, 44865, 290, 33285, 5364, 837, 257, 22750, 6087, 764], [5661, 318, 257, 845, 3734, 3807, 1377, 467, 766, 340, 764], [292, 35335, 355, 262, 7110, 318, 837, 479, 559, 35826, 338, 4226, 318, 991, 18078, 329, 617, 1049, 530, 12, 34380, 764], [41081, 663, 17978, 837, 7705, 14768, 287, 534, 1182, 290, 1838, 345, 1808, 534, 898, 14245, 2714, 6116, 764], [505, 286, 883, 4071, 837, 47029, 803, 29932, 1619, 2337, 326, 3011, 772, 1365, 287, 36412, 837, 355, 345, 35024, 625, 663, 790, 47128, 287, 534, 2000, 764], [1662, 2279, 2499, 837, 475, 262, 2811, 318, 2440, 621, 287, 285, 560, 290, 749, 584, 2274, 14577, 444, 764], [64, 416, 89, 29003, 7758, 375, 20058, 326, 49881, 262, 2440, 3632, 5499, 355, 880, 355, 262, 9195, 17305, 764], [64, 8564, 290, 5887, 306, 13134, 4315, 12, 1154, 6005, 326, 2125, 470, 2029, 257, 1310, 3154, 10997, 290, 257, 1178, 40986, 5263, 306, 46908, 10953, 764], [1169, 2646, 338, 7786, 837, 1690, 2984, 3043, 31222, 2565, 286, 14733, 481, 4929, 617, 572, 4860, 764, 764, 764], [22437, 644, 257, 3734, 11648, 857, 1266, 1058, 340, 14582, 257, 5814, 17023, 656, 281, 22594, 995, 837, 788, 2801, 7230, 689, 340, 3938, 290, 3578, 262, 4025, 10939, 286, 262, 7002, 284, 14595, 287, 41511, 2213, 385, 2280, 764], [28177, 790, 3715, 287, 428, 2646, 318, 257, 16840, 326, 714, 1302, 3436, 837, 257, 7138, 6939, 13432, 286, 10038, 837, 4069, 290, 6824, 764], [64, 39705, 5527, 290, 43527, 913, 6573, 32251, 351, 257, 25041, 2854, 416, 435, 23503, 2879, 764], [5832, 1839, 470, 1975, 881, 286, 340, 837, 475, 345, 481, 6487, 379, 262, 2709, 4355, 837, 379, 262, 508, 338, 508, 13092, 290, 262, 15163, 30949, 286, 340, 477, 764], [5661, 2196, 338, 645, 6833, 588, 663, 18476, 837, 475, 663, 35396, 389, 991, 39976, 764], [1169, 38043, 710, 5369, 318, 644, 3931, 3159, 3671, 499, 1042, 973, 284, 307, 287, 262, 4647, 618, 340, 373, 31394, 517, 284, 7334, 4739, 764], [15234, 485, 58, 82, 60, 17864, 12, 65, 1780, 43527, 290, 18409, 3435, 1231, 17965, 319, 3037, 12, 1659, 12, 1169, 12, 32542, 298, 8173, 393, 2181, 43787, 10721, 764], [361, 340, 3088, 284, 466, 1997, 517, 837, 340, 561, 2038, 290, 3737, 22818, 837, 475, 379, 428, 1241, 286, 38705, 29923, 1837, 837, 340, 318, 655, 546, 826, 764], [18820, 17082, 284, 14561, 663, 7481, 290, 1165, 5508, 284, 18510, 663, 5386, 764], [1169, 36275, 13471, 286, 842, 27769, 338, 4263, 290, 5405, 6, 819, 23466, 2647, 764, 764, 764, 6165, 5667, 10209, 351, 262, 4876, 286, 2659, 3191, 3616, 764], [1640, 477, 663, 2726, 2565, 286, 4007, 764, 764, 764, 685, 270, 60, 7228, 257, 835, 284, 3830, 6247, 262, 42546, 286, 663, 4634, 351, 257, 922, 1730, 286, 23125, 290, 14733, 764], [64, 31193, 12641, 286, 2279, 883, 286, 514, 508, 836, 470, 2134, 284, 262, 6764, 366, 555, 28604, 366, 423, 9885, 477, 1863, 1058, 4903, 3643, 266, 764, 24484, 318, 281, 753, 16421, 837, 555, 10641, 1042, 1512, 837, 625, 22377, 1216, 265, 2933, 351, 257, 1612, 15113, 257, 10591, 3094, 764], [5661, 2975, 3807, 3607, 345, 7016, 348, 24705, 1077, 837, 290, 345, 1183, 307, 9675, 345, 1816, 1863, 329, 262, 6594, 764], [19532, 837, 340, 338, 517, 286, 262, 976, 837, 475, 355, 262, 2646, 17021, 837, 326, 338, 407, 1464, 257, 2089, 1517, 764], [64, 1657, 20122, 837, 1254, 12, 11274, 2646, 326, 37872, 262, 640, 12, 24130, 1850, 3872, 326, 262, 749, 3665, 1517, 287, 1204, 318, 1842, 764], [64, 35711, 12, 66, 2799, 1359, 837, 2612, 12, 301, 33307, 8364, 329, 3246, 764], [29642, 422, 46078, 648, 318, 257, 2646, 326, 815, 307, 1775, 416, 477, 837, 2592, 883, 508, 3588, 470, 3910, 286, 837, 393, 423, 11564, 546, 262, 555, 17181, 4970, 286, 1175, 764], [23548, 648, 331, 320, 280, 16316, 5814, 837, 8768, 3435, 508, 6486, 407, 832, 32045, 9673, 837, 475, 780, 484, 17170, 1975, 340, 338, 262, 691, 835, 284, 2222, 12157, 284, 511, 6151, 3392, 764], [13, 764, 764, 4882, 956, 6452, 649, 1204, 656, 262, 5385, 416, 45541, 321, 803, 27962, 290, 4375, 2081, 1692, 13357, 284, 663, 407, 12, 568, 12, 13578, 3435, 764], [6, 764, 764, 764, 1111, 289, 2088, 88, 290, 2208, 12, 24494, 837, 290, 4753, 407, 287, 257, 23290, 837, 523, 1650, 736, 837, 8960, 290, 423, 257, 1178, 22051, 981, 262, 1310, 3392, 651, 257, 34669, 2190, 764, 705], [64, 15497, 14348, 10997, 764], [270, 338, 257, 954, 329, 674, 1661, 764], [14809, 29815, 468, 6793, 281, 3318, 15973, 22152, 2569, 4205, 12, 2934, 12, 3174, 837, 290, 468, 5257, 4847, 884, 355, 2128, 290, 13483, 45501, 351, 5032], [21453, 338, 966, 318, 2829, 290, 3489, 1377, 661, 338, 5682, 389, 18366, 286, 2405, 837, 290, 3573, 29303, 661, 423, 3573, 29303, 2877, 9029, 1377, 475, 465, 7481, 389, 1149, 11056, 764], [64, 14348, 10997, 837, 3763, 837, 475, 530, 351, 3435, 508, 892, 290, 1561, 546, 511, 4661, 837, 290, 389, 1762, 319, 1327, 5370, 764], [85, 1699, 306, 24748, 893, 1111, 262, 45716, 290, 262, 35396, 286, 625, 12, 1169, 12, 4852, 1842, 764], [13, 764, 764, 257, 4939, 837, 47859, 837, 21613, 12, 40871, 1621, 326, 318, 30352, 18117, 1498, 284, 8204, 6949, 764], [64, 386, 12, 17359, 1290, 344, 326, 625, 8988, 881, 286, 663, 13181, 6573, 30466, 5176, 284, 734, 16403, 1085, 13289, 764], [1640, 262, 717, 734, 12, 17936, 286, 428, 39072, 306, 47602, 290, 1242, 913, 837, 1464, 3049, 290, 21799, 12838, 837, 3988, 481, 467, 18177, 1863, 329, 262, 6594, 764], [76, 1228, 19830, 338, 38273, 1842, 1621, 318, 257, 24343, 3929, 10510, 12, 430, 5847, 837, 611, 257, 1643, 6715, 1360, 379, 1661, 764], [1169, 44730, 9970, 2256, 10997, 286, 262, 3931, 764], [10760, 2280, 21318, 674, 17627, 351, 262, 28010, 21343, 290, 5238, 422, 1626, 262, 1413, 284, 2251, 257, 3190, 997, 4623, 1998, 764], [72, 1842, 262, 835, 326, 340, 1718, 8395, 290, 1107, 7893, 345, 284, 1011, 777, 1049, 37312, 286, 4562, 290, 13831, 572, 764], [259, 465, 8886, 355, 257, 2646, 3437, 837, 2853, 17396, 20518, 1122, 16316, 257, 10904, 290, 11932, 670, 764], [8807, 734, 2456, 481, 1560, 345, 644, 345, 760, 618, 14615, 284, 766, 340, 1058, 26794, 1647, 764, 1725, 5331, 764], [1169, 3807, 338, 5897, 46365, 286, 6232, 3815, 3607, 340, 281, 5508, 837, 5615, 12, 259, 19634, 764], [64, 34417, 10512, 3025, 25488, 922, 12, 67, 2308, 14, 14774, 12, 67, 2308, 10372, 874, 389, 655, 3499, 1576, 284, 787, 257, 264, 5083, 588, 502, 12472, 329, 281, 772, 517, 3499, 837, 1342, 23606, 34546, 837, 1342, 6189, 3272, 12, 16760, 6282, 764], [71, 323, 988, 318, 13393, 355, 1216, 3755, 290, 764, 764, 764, 257, 3491, 12, 8601, 1628, 764], [270, 338, 1111, 257, 3306, 1964, 670, 290, 257, 13899, 11648, 764, 764, 764], [71, 1794, 699, 837, 42987, 275, 799, 10997, 764], [292, 257, 15827, 32251, 837, 262, 3807, 318, 2139, 540, 837, 475, 340, 1595, 470, 1107, 5203, 262, 12625, 6717, 9476, 286, 262, 1365, 2646, 6300, 764], [272, 25304, 13367, 319, 7996, 287, 257, 3968, 555, 4134, 22646, 284, 340, 764], [270, 338, 407, 1204, 12, 2001, 29808, 1587, 245, 663, 31016, 290, 1612, 837, 475, 1312, 8288, 340, 764], [28116, 282, 7370, 15800, 286, 262, 10319, 12, 448, 22830, 530, 13423, 422, 1459, 6036, 14505, 764], [1169, 11519, 4202, 286, 262, 2587, 355, 880, 355, 262, 11540, 286, 262, 28303, 3607, 428, 2406, 12, 1659, 12, 496, 1621, 24596, 355, 880, 355, 23125, 764], [992, 416, 1036, 42022, 338, 4451, 306, 34150, 2854, 290, 17131, 837, 262, 3350, 468, 257, 1256, 286, 1257, 351, 262, 2587, 764], [83, 1347, 45697, 41885, 257, 19217, 5236, 286, 14348, 24211, 290, 17580, 6795, 764], [64, 10296, 13516, 286, 1944, 1110, 8844, 261, 8231, 837, 16997, 9640, 286, 4356, 5714, 290, 465, 1641, 9489, 837, 6754, 22415, 837, 290, 5149, 991, 82, 764], [64, 5270, 2124, 24127, 837, 21430, 257, 4506, 6980, 286, 30949, 287, 262, 5701, 13478, 326, 10403, 2314, 938, 764], [79, 49809, 318, 1288, 9924, 2318, 11489, 7586, 278, 11185, 299, 3883, 9859, 837, 290, 340, 338, 7924, 416, 764, 764, 764, 497, 346, 2248, 1133, 764, 289, 3020, 764], [272, 30690, 475, 19827, 10512, 326, 318, 636, 31044, 290, 636, 28763, 286, 262, 340, 7199, 30669, 764], [7972, 16620, 364, 24538, 262, 1692, 4437, 290, 15186, 281, 7016, 3355, 404, 764], [40965, 1239, 587, 257, 3236, 4336, 286, 19317, 641, 6, 10460, 12, 7700, 5337, 837, 340, 6655, 502, 703, 881, 9476, 1312, 550, 4964, 36650, 2164, 776, 338, 2196, 764], [1169, 1266, 1517, 262, 2646, 857, 318, 284, 905, 514, 407, 691, 644, 326, 2000, 3073, 588, 837, 475, 703, 262, 7325, 1429, 2346, 14051, 764], [1640, 477, 663, 4054, 8787, 837, 11871, 13141, 286, 262, 21349, 12, 3972, 6621, 2894, 318, 47416, 837, 287, 257, 14885, 7357, 837, 288, 40861, 835, 764], [5661, 318, 2495, 17963, 88, 2587, 764, 475, 617, 10059, 1976, 9235, 290, 1976, 3775, 1037, 764], [785, 35025, 306, 2342, 540, 764], [1169, 28303, 5032, 2759, 44672, 262, 2565, 286, 36292, 326, 3450, 6622, 329, 867, 7876, 14046, 364, 764], [1169, 12855, 12, 16302, 276, 21641, 2148, 257, 4822, 12, 19913, 8737, 837, 981, 374, 385, 7255, 290, 288, 4364, 69, 385, 389, 257, 14348, 27356, 286, 11954, 837, 3718, 6819, 7362, 355, 2106, 14371, 606, 764], [5832, 836, 470, 423, 284, 307, 281, 2592, 5802, 3915, 263, 284, 1577, 257, 21803, 275, 12, 40191, 284, 262, 23129, 338, 3430, 764], [5661, 14348, 32251, 318, 14559, 276, 287, 262, 8137, 286, 35382, 1786, 1044, 837, 290, 450, 306, 23007, 262, 4046, 7572, 837, 6573, 12416, 290, 22247, 286, 262, 16236, 82, 764], [7146, 500, 13141, 286, 262, 21349, 12, 3972, 6621, 2894, 743, 407, 307, 3446, 11871, 837, 475, 340, 338, 4753, 1377, 36219, 306, 1377, 21349, 21349, 837, 644, 351, 477, 286, 883, 23754, 7259, 290, 46875, 13289, 764], [1177, 276, 319, 663, 898, 2846, 837, 14068, 5440, 318, 1365, 12, 14813, 12, 23913, 1641, 9739, 837, 475, 2081, 3296, 286, 262, 2876, 574, 1559, 338, 5337, 481, 1884, 4702, 595, 1681, 338, 517, 17074, 11445, 2107, 12, 2673, 1509, 1077, 27041, 1359, 6833, 764], [64, 7002, 832, 4088, 837, 257, 14395, 286, 2877, 837, 290, 257, 24281, 278, 7440, 1883, 319, 3735, 1483, 837, 1398, 1042, 837, 290, 17010, 764], [31092, 913, 290, 44207, 9739, 764], [1, 1885, 86, 505, 17685, 366, 318, 281, 23176, 837, 416, 12, 1169, 12, 77, 17024, 3626, 416, 20518, 1122, 764, 340, 1839, 470, 3881, 597, 16659, 475, 318, 4735, 6174, 12, 392, 12, 13059, 15048, 49291, 764], [64, 6754, 12191, 351, 262, 11917, 286, 663, 19131, 546, 1111, 8354, 290, 3703, 764], [732, 761, 685, 5908, 382, 338, 60, 31210, 837, 7540, 88, 2568, 837, 465, 7506, 290, 1398, 10510, 2162, 356, 761, 465, 427, 83, 3378, 837, 356, 761, 465, 14966, 764], [16670, 262, 12857, 1244, 423, 587, 28619, 837, 289, 1530, 5145, 10558, 265, 31786, 23007, 262, 1690, 35322, 12263, 286, 1862, 661, 287, 3660, 474, 2674, 764], [58, 70, 1872, 60, 2058, 5699, 284, 597, 14549, 1312, 460, 3505, 284, 1048, 4035, 10404, 287, 663, 5899, 301, 290, 837, 3763, 837, 749, 29301, 1296, 764], [27218, 389, 3160, 2861, 4964, 837, 13532, 2861, 1708, 764], [270, 338, 2138, 588, 257, 10869, 2041, 1377, 15497, 837, 6029, 290, 6044, 11487, 764], [64, 10038, 88, 9961, 14, 400, 81, 4665, 15321, 416, 46208, 29475, 290, 262, 3437, 338, 880, 12, 4002, 8689, 1830, 25428, 764], [292, 257, 18032, 2095, 2050, 837, 340, 338, 2818, 764, 340, 338, 635, 262, 614, 338, 6029, 395, 3807, 764], [64, 44363, 837, 19377, 876, 2646, 326, 11835, 290, 1242, 2759, 14293, 514, 656, 257, 995, 810, 262, 2614, 290, 262, 1964, 651, 27546, 45905, 764], [4514, 407, 355, 19855, 8036, 355, 663, 45630, 272, 11283, 837, 366, 287, 262, 14043, 837, 366, 517, 35671, 338, 2646, 1838, 663, 898, 837, 43880, 13050], [1169, 1998, 286, 4964, 698, 11369, 1468, 12, 14347, 269, 12397, 11034, 287, 428, 2208, 11664, 5794, 318, 655, 28201, 1576, 284, 307, 12312, 889, 764], [2435, 1488, 263, 743, 407, 307, 262, 749, 18078, 22041, 6246, 475, 663, 11982, 2116, 12, 18206, 2288, 3275, 546, 674, 21049, 6224, 290, 262, 8889, 286, 8557, 11154, 815, 379, 1551, 30419, 281, 20038, 286, 9017, 1039], [1, 262, 23129, 338, 649, 8242, 366, 6140, 351, 257, 2829, 1410, 764, 764, 764, 764, 880, 837, 379, 1551, 326, 338, 262, 1410, 764], [71, 323, 2516, 468, 523, 4336, 4142, 32870, 1143, 790, 13699, 1468, 12, 41364, 49875, 4107, 351, 884, 937, 3477, 25808, 345, 821, 407, 1654, 611, 345, 815, 22517, 393, 804, 656, 1719, 683, 5364, 764], [58, 35248, 279, 2357, 60, 14509, 1559, 290, 465, 5462, 423, 523, 14559, 276, 2405, 287, 262, 48565, 286, 284, 75, 74, 2013, 338, 3597, 326, 790, 5739, 11073, 649, 8716, 82, 837, 1771, 345, 821, 257, 4336, 286, 262, 3835, 393, 407, 764], [4514, 262, 5405, 1017, 14710, 1595, 470, 2407, 4197, 837, 30089, 318, 4753, 257, 3748, 3660, 3148, 20760, 1000, 764], [1169, 10512, 318, 2826, 503, 351, 884, 257, 10813, 8737, 290, 3872, 326, 340, 6774, 10953, 284, 534, 2951, 764], [272, 7895, 290, 7411, 3881, 2647, 2205, 837, 257, 4451, 290, 19201, 804, 2641, 326, 47312, 995, 764], [272, 572, 12945, 837, 3360, 10319, 290, 12362, 16403, 15108, 2646, 546, 262, 2081, 3616, 286, 262, 17122, 764], [5661, 2196, 22201, 689, 262, 49054, 1492, 287, 257, 835, 772, 663, 2748, 278, 1772, 1244, 21099, 764], [29810, 837, 2147, 45104, 588, 1468, 12, 28776, 1509, 1077, 27041, 1359, 764, 290, 287, 428, 2754, 837, 319, 4860, 16316, 764], [13, 764, 764, 20505, 318, 287, 1790, 5127, 287, 262, 22041, 837, 290, 29206, 726, 272, 18034, 465, 13460, 290, 25409, 465, 3435, 6, 25712, 351, 32095, 290, 15213, 764], [272, 5340, 19661, 837, 475, 356, 6808, 329, 262, 19686, 1143, 4173, 38336, 9717, 764], [2339, 19317, 641, 351, 465, 22674, 837, 36650, 2164, 776, 28229, 2407, 3867, 8188, 3690, 465, 581, 16780, 10092, 12291, 319, 262, 5337, 764], [8117, 338, 257, 595, 260, 48840, 1633, 546, 262, 2187, 1517, 837, 290, 326, 338, 644, 1838, 340, 45420, 764], [272, 32436, 14169, 3704, 286, 22041, 764, 1194, 1049, 1587, 239, 10919, 345, 836, 470, 766, 6, 318, 881, 517, 17623, 621, 644, 345, 466, 766, 32251, 837, 18064, 351, 617, 33936, 3048, 837, 753, 392, 45470, 23755, 290, 336, 929, 437, 516, 13289], [64, 7773, 20793, 8196, 286, 10510, 326, 318, 22485, 290, 38192, 438, 4360, 38766, 1346, 6776, 764], [64, 12703, 28962, 290, 47886, 649, 27689, 392, 2646, 546, 281, 1785, 913, 3931, 287, 257, 1511, 12, 1941, 12, 727, 2576, 338, 1204, 764], [66, 1133, 837, 8258, 837, 2612, 48133, 34491, 15108, 3895, 2646, 351, 6088, 286, 23905, 13915, 14733, 329, 262, 3988, 837, 6041, 286, 287, 12, 73, 3369, 329, 262, 6490, 290, 2612, 1576, 329, 2506, 764], [548, 4735, 837, 845, 2342, 540, 717, 3895, 329, 3437, 279, 2357, 15059, 27610], [64, 4466, 14669, 326, 32142, 262, 4143, 6507, 6224, 286, 262, 3996, 280, 1040, 981, 4955, 257, 14186, 665, 19894, 286, 11281, 656, 511, 3160, 764], [270, 5644, 262, 3094, 12, 32319, 3048, 286, 2056, 17512, 837, 422, 262, 1611, 286, 6447, 326, 318, 1760, 416, 262, 13519, 7270, 2056, 764, 764, 764, 284, 262, 16584, 290, 6165, 15444, 2612, 4891, 286, 285, 8770, 624, 3925, 588, 6877, 3245, 290, 289, 3378, 764], [1818, 805, 2339, 837, 3863, 837, 475, 991, 257, 2646, 351, 477, 262, 4847, 326, 925, 262, 584, 1115, 1049, 837, 14343, 1661, 379, 262, 6918, 764], [64, 15497, 1576, 10997, 326, 815, 423, 1043, 257, 3931, 1295, 764], [1671, 272, 10471, 837, 287, 465, 749, 42262, 1729, 12, 1477, 20946, 3159, 2854, 837, 9384, 772, 262, 2705, 395, 7188, 287, 262, 7954, 27360, 286, 465, 20868, 764], [2016, 262, 3685, 318, 1290, 1342, 6507, 2569, 621, 6678, 837, 262, 2646, 318, 7226, 21504, 522, 1058, 3049, 837, 21799, 290, 1336, 286, 572, 12, 1169, 12, 66, 1648, 41138, 10601, 5614, 764], [785, 35025, 355, 340, 318, 21036, 837, 3049, 17490, 468, 257, 7110, 326, 14987, 427, 20946, 329, 38520, 837, 32550, 88, 290, 5123, 764], [10919, 340, 16523, 287, 2656, 414, 340, 1838, 510, 329, 287, 4430, 290, 275, 12, 9526, 30511, 1108, 764], [1169, 5814, 4931, 286, 1976, 23778, 275, 641, 7637, 1838, 262, 3143, 6197, 516, 9105, 4293, 656, 1223, 517, 621, 339, 13025, 815, 307, 764], [5661, 318, 355, 3665, 257, 900, 286, 2370, 355, 345, 1183, 1683, 1064, 286, 1521, 1242, 6067, 837, 290, 703, 340, 460, 41523, 1290, 3675, 13257, 7714, 290, 832, 284, 262, 749, 32258, 14461, 3160, 764], [35248, 3857, 22397, 439, 1816, 503, 2485, 768, 284, 787, 257, 1049, 530, 764], [48267, 670, 284, 766, 340, 379, 262, 717, 3663, 764], [8176, 338, 1266, 7188, 389, 618, 339, 338, 1972, 8179, 319, 262, 9669, 2184, 780, 326, 338, 618, 339, 1107, 8198, 764], [2364, 364, 1576, 34264, 1257, 284, 8204, 262, 38725, 900, 981, 26005, 257, 17950, 462, 9408, 764], [259, 262, 886, 837, 10862, 12, 7109, 2954, 1842, 318, 530, 286, 883, 7328, 326, 1312, 2227, 284, 588, 881, 517, 621, 1312, 1682, 750, 764, 3360, 837, 326, 338, 1576, 764], [272, 16584, 837, 922, 12, 17047, 1850, 9450, 10997, 588, 6409, 1854, 475, 6630, 9211, 621, 2938, 764], [501, 23441, 6622, 262, 2646, 1978, 351, 281, 11932, 290, 5814, 2854, 764, 764, 764], [16885, 7744, 7650, 290, 23332, 306, 13674, 764], [292, 19861, 355, 340, 318, 287, 27561, 1200, 5076, 837, 1288, 275, 5708, 318, 257, 3807, 14559, 276, 287, 281, 33985, 326, 37733, 663, 12333, 257, 18975, 29371, 764], [41081, 257, 1621, 20039, 1576, 284, 787, 262, 2128, 286, 2647, 711, 588, 257, 17864, 12, 65, 1780, 32251, 837, 663, 2612, 318, 523, 881, 287, 262, 826, 1295, 340, 318, 2408, 284, 651, 1107, 32638, 1079, 379, 340, 764], [270, 338, 257, 30669, 764], [272, 8131, 1877, 12, 1156, 288, 7115, 2646, 837, 340, 6774, 257, 1448, 286, 661, 1978, 287, 257, 6029, 290, 23332, 835, 837, 611, 257, 1310, 11282], [270, 338, 262, 29932, 7548, 286, 257, 922, 2443, 12, 15344, 263, 837, 290, 772, 611, 340, 338, 18149, 837, 663, 28421, 3100, 12362, 2769, 764], [35248, 299, 14414, 3425, 1595, 470, 466, 881, 284, 10164, 597, 7159, 530, 835, 393, 262, 584, 764, 339, 2391, 10969, 465, 966, 286, 1570, 326, 38762, 333, 1079, 64, 2499, 764, 645, 1808, 764], [10919, 366, 13735, 366, 16523, 287, 6795, 340, 1838, 510, 329, 351, 663, 2612, 764], [565, 64, 2507, 3939, 263, 37112, 503, 257, 5381, 7110, 351, 281, 2562, 8761, 290, 257, 2962, 319, 2095, 10512, 625, 4065, 12, 26240, 19481, 764], [10919, 1336, 30424, 16523, 287, 606, 1512, 763, 23545, 340, 5688, 1838, 510, 329, 355, 9155, 88, 12, 2188, 577, 88, 837, 11992, 9739, 764, 991, 837, 1312, 1101, 407, 2407, 1654, 644, 262, 966, 318], [7527, 287, 3703, 837, 17177, 3481, 2823, 290, 21104, 13134, 837, 10287, 2244, 500, 274, 318, 837, 287, 663, 5897, 837, 12191, 835, 837, 27939, 837, 47602, 290, 23056, 306, 8468, 764], [7, 257, 1267, 289, 31777, 673, 268, 3996, 1990, 4487, 262, 2646, 422, 262, 845, 3726, 764, 764, 764, 357, 475, 1267, 300, 1219, 805, 338, 13394, 837, 7744, 7016, 2951, 18340, 832, 428, 33418, 410, 1734, 263, 764, 764, 764], [4598, 356, 1107, 761, 257, 8541, 12, 11374, 2646, 284, 1560, 514, 3446, 1521, 257, 14348, 2776, 1022, 257, 1315, 12, 1941, 12, 727, 2933, 290, 257, 2319, 12, 1941, 12, 727, 2415, 1595, 470, 670, 5633], [3841, 14071, 284, 307, 12086, 379, 267, 13034, 640, 329, 21671, 428, 7932, 18560, 286, 257, 46060, 10686, 764], [1169, 2646, 338, 4153, 12, 11374, 2491, 640, 9911, 15800, 286, 625, 12728, 837, 996, 10209, 743, 307, 517, 19064, 621, 262, 10856, 319, 9612, 764], [9099, 470, 1607, 597, 24072, 287, 428, 41859, 286, 48424, 537, 291, 956, 764, 764, 764], [292, 16573, 416, 885, 7114, 18605, 1647, 422, 985, 261, 443, 893, 6, 5337, 366, 262, 1918, 286, 25422, 25637, 366, 290, 7924, 416, 435, 272, 256, 7167, 837, 25422, 25637, 338, 7002, 318, 3499, 475, 465, 1582, 271, 666, 43446, 318, 991, 6286], [1169, 3807, 9405, 257, 14720, 761, 329, 23241, 12, 4111, 837, 1729, 26159, 1641, 6918, 837, 475, 340, 1595, 470, 467, 1165, 881, 2252, 764], [5661, 5814, 290, 10296, 14348, 10997, 468, 1576, 3499, 3435, 284, 6070, 1811, 6918, 837, 290, 663, 23933, 41700, 815, 1592, 625, 262, 749, 1327, 12, 20122, 19768, 873, 764], [64, 21181, 326, 19410, 262, 22672, 2196, 1231, 5033, 3140, 77, 1144, 416, 340, 764], [5661, 318, 257, 3772, 3714, 1891, 284, 262, 640, 618, 30070, 547, 22041, 338, 749, 49875, 1512, 1296, 2427, 286, 530, 286, 663, 749, 20039, 764], [41887, 837, 13891, 290, 24139, 435, 4666, 10205, 7785, 837, 262, 2646, 819, 3369, 1913, 10825, 290, 20070, 10209, 284, 1808, 511, 25420, 26226, 286, 6573, 826, 290, 2642, 764], [11274, 25776, 6, 7876, 8177, 3404, 764], [1662, 523, 881, 257, 3807, 355, 257, 4286, 1492, 329, 262, 1263, 3159, 764, 428, 2125, 470, 616, 4004, 287, 262, 2168, 837, 991, 1312, 8359, 340, 1576, 284, 4313, 764], [270, 338, 530, 286, 262, 749, 5508, 7328, 1683, 925, 546, 289, 31777, 764], [270, 318, 257, 2646, 326, 481, 423, 661, 6155, 503, 19487, 832, 837, 481, 7898, 1854, 284, 1302, 510, 290, 22517, 837, 290, 481, 837, 17713, 837, 2666, 1111, 11665, 7953, 287, 257, 40831, 4384, 329, 812, 284, 1282, 764], [261, 663, 898, 29932, 2846, 837, 340, 7675, 45064, 262, 30477, 286, 1111, 262, 3437, 290, 37986, 416, 1078, 764], [2971, 837, 14397, 837, 25880, 351, 9568, 290, 6795, 837, 290, 2138, 257, 922, 640, 764], [1050, 323, 338, 2646, 2499, 880, 290, 481, 5198, 772, 284, 883, 508, 3588, 470, 1165, 5385, 351, 7858, 429, 23117, 1042, 764], [83, 472, 11108, 290, 3665, 764], [11274, 3807, 764, 922, 14549, 764, 475, 611, 345, 1607, 1657, 14348, 10997, 837, 922, 308, 3768, 837, 481, 345, 307, 11472, 764], [270, 468, 262, 11917, 284, 4240, 546, 1263, 2683, 351, 44053, 290, 25808, 764, 340, 7476, 31792, 3105, 290, 2181, 43787, 837, 780, 340, 6834, 262, 32385, 318, 2861, 262, 6991, 764], [4480, 35444, 1029, 13747, 837, 256, 2306, 280, 3793, 3144, 39438, 3690, 12314, 258, 293, 338, 4158, 290, 14348, 21359, 837, 290, 673, 318, 9763, 416, 257, 4300, 540, 3350, 764], [270, 338, 281, 1672, 286, 13767, 837, 9389, 49291, 326, 6296, 837, 3805, 663, 19787, 3092, 286, 7016, 339, 701, 837, 287, 7062, 6273, 284, 262, 19441, 6783, 2636, 12, 437, 29315, 286, 262, 3437, 338, 2180, 1336, 30424, 764], [64, 845, 8258, 804, 379, 703, 1194, 3968, 17105, 262, 1429, 286, 1093, 889, 290, 4845, 764], [4360, 11880, 12, 259, 12, 2395, 988, 3143, 6197, 516, 1108, 468, 1464, 587, 636, 286, 329, 262, 749, 636, 4295, 68, 338, 288, 2487, 29923, 1837, 5419, 366, 852, 23176, 366, 10980, 663, 20256, 290, 3952, 263, 338, 7325, 14517, 764, 764, 764], [29482, 286, 262, 3807, 338, 20024, 7363, 287, 262, 10517, 2005, 9449, 286, 336, 19986, 290, 6145, 14057, 764, 511, 3644, 12, 11227, 515, 6698, 389, 845, 38084, 764], [1169, 3108, 4771, 2479, 5679, 749, 7173, 837, 996, 837, 318, 262, 530, 4920, 416, 1175, 1008, 1379, 82, 764, 6175, 20539, 474, 1952, 837, 508, 3724, 257, 2300, 286, 2745, 878, 262, 3807, 338, 2650, 764], [3702, 1850, 416, 257, 23754, 2854, 416, 450, 42933, 837, 3332, 259, 13805, 469, 2523, 326, 262, 2126, 286, 1466, 338, 2116, 12, 50039, 1634, 4206, 1178, 29843, 36319, 764], [707, 12378, 475, 17082, 290, 837, 6165, 837, 340, 7864, 345, 625, 764], [21453, 16545, 1936, 11359, 45630, 272, 5682, 837, 290, 780, 262, 4393, 1283, 3938, 3910, 286, 262, 3544, 290, 19544, 286, 16117, 837, 340, 338, 257, 9476, 284, 2883, 511, 29303, 871, 764], [2016, 262, 7110, 318, 20039, 837, 262, 3807, 1239, 5300, 10451, 291, 837, 780, 262, 3241, 318, 319, 262, 42160, 286, 262, 7016, 2478, 286, 262, 19217, 3435, 764], [37687, 474, 1952, 2627, 257, 845, 9670, 26479, 262, 1110, 39716, 1073, 1392, 5710, 422, 511, 1700, 6167, 837, 17742, 326, 530, 582, 338, 16866, 743, 307, 1194, 338, 15807, 764], [70, 35301, 338, 39769, 290, 4571, 389, 36308, 739, 21989, 837, 290, 339, 468, 7428, 6275, 13289, 422, 465, 3350, 764], [8800, 30848, 290, 2153, 320, 417, 389, 2818, 287, 777, 9176, 764], [12518, 534, 3756, 17308, 389, 257, 3155, 286, 3159, 12, 30041, 7462, 265, 8609, 274, 588, 3869, 494, 387, 675, 290, 2341, 272, 29008, 5063, 379, 511, 374, 11429, 88, 1266, 837, 772, 289, 482, 388, 2925, 866, 3538, 764], [4514, 23557, 3956, 318, 4753, 530, 329, 262, 14568, 837, 340, 338, 635, 1336, 286, 7786, 837, 4451, 35704, 764], [11407, 739, 262, 4168, 286, 257, 582, 508, 468, 655, 2626, 465, 3656, 764], [270, 743, 407, 307, 366, 938, 13875, 78, 287, 1582, 271, 366, 475, 764, 764, 764], [3919, 4240, 484, 821, 3375, 546, 366, 1561, 284, 607, 764, 366, 340, 338, 21994, 764], [1640, 663, 32095, 837, 1029, 16716, 27337, 290, 13393, 7205, 837, 262, 2646, 460, 691, 307, 38911, 764], [5460, 837, 428, 318, 257, 23754, 26810, 1128, 5807, 351, 41535, 4676, 12, 1818, 837, 15360, 290, 2647, 764], [270, 318, 40840, 287, 2095, 2890, 703, 661, 422, 884, 10084, 13817, 2648, 262, 976, 1692, 290, 8557, 2476, 764], [270, 338, 6547, 2116, 12, 9685, 287, 663, 13526, 1108, 764], [64, 15499, 837, 25488, 290, 4950, 287, 663, 866, 12945, 11854, 764], [13199, 1850, 284, 8204, 5145], [64, 13206, 837, 3867, 2646, 326, 19410, 663, 5386, 290, 663, 2723, 2587, 764], [10134, 257, 7110, 1336, 286, 30953, 2402, 33620, 764, 764, 764, 290, 257, 1729, 11338, 16134, 286, 15290, 12, 83, 4741, 2879, 629, 10277, 21454, 3858, 326, 4940, 503, 14169, 475, 1569, 364, 656, 625, 12728, 764], [64, 670, 286, 21994, 8675, 1590, 290, 2700, 764], [1169, 2646, 4034, 9257, 422, 257, 1342, 38705, 8216, 621, 663, 18476, 837, 355, 1727, 3568, 284, 423, 10282, 24341, 656, 607, 4168, 764], [1640, 262, 717, 640, 287, 1811, 812, 837, 285, 81, 764, 477, 268, 468, 30612, 2241, 351, 262, 5536, 339, 338, 26843, 351, 262, 289, 31777, 795, 8439, 286, 13845, 764, 443, 14651, 338, 30004, 494, 764], [271, 77, 470, 2407, 262, 4961, 286, 36440, 338, 1266, 2961, 670, 837, 475, 340, 338, 3538, 465, 18822, 45630, 272, 2646, 764, 764, 764, 2058, 1969, 284, 36791, 870, 262, 37451, 286, 465, 289, 506, 479, 506, 7328, 764], [1169, 2646, 41442, 319, 663, 13289, 837, 290, 1111, 5983, 389, 510, 284, 262, 4876, 764], [272, 12661, 837, 23176, 837, 16584, 2646, 326, 10532, 262, 2613, 691, 618, 340, 37622, 329, 19861, 45357, 284, 787, 1654, 345, 821, 1972, 663, 42031, 966, 764], [64, 12949, 9476, 326, 6424, 5614, 663, 4661, 351, 10152, 290, 6628, 764], [64, 20780, 7357, 837, 12312, 889, 837, 10224, 837, 880, 12, 23800, 12838, 286, 734, 1450, 8970, 287, 281, 7044, 983, 286, 3797, 12, 392, 12, 9246, 764], [10919, 14509, 1559, 468, 13013, 994, 318, 4998, 319, 257, 6276, 1241, 764], [292, 6036, 6918, 467, 837, 366, 10912, 7968, 366, 318, 257, 23056, 1487], [49123, 264, 5, 76, 1283, 845, 14348, 837, 290, 2153, 22699, 21486, 297, 268, 3099, 282, 318, 257, 10974, 764], [64, 8675, 6819, 285, 585, 415, 837, 12922, 2042, 10997, 764], [16670, 1204, 393, 1223, 588, 340, 318, 845, 881, 287, 262, 15936, 286, 1254, 12, 11274, 6918, 837, 262, 3350, 290, 3437, 2239, 831, 994, 74, 338, 23895, 4571, 12797, 10974, 2759, 279, 1557, 415, 8237, 422, 9722, 14096, 764], [270, 318, 17564, 837, 12661, 837, 14348, 290, 38404, 29277, 422, 923, 284, 5461, 764], [1169, 3807, 16461, 881, 5699, 284, 12718, 1525, 338, 4268, 12, 25124, 1013, 12743, 8216, 621, 262, 2646, 2196, 286, 1029, 37744, 750, 764], [64, 15497, 15770, 903, 832, 262, 3297, 286, 4686, 16426, 2047, 6098, 1512, 15510, 326, 1931, 3225, 2146, 2442, 468, 1690, 11829, 351, 764, 764, 764, 340, 857, 8588, 257, 9155, 837, 3092, 4763, 271, 605, 20024, 764], [13, 764, 764, 599, 3711, 351, 14733, 357, 705, 72, 2740, 43472, 6228, 4712, 837, 705, 34138, 2853, 39590, 706, 257, 2138, 837, 1931, 837, 10015, 36874, 5163, 351, 281, 8756, 6203, 4993, 1267, 290, 45466, 2325, 265, 654, 357, 8465, 338, 1582, 10599, 468, 587, 6928, 351, 17488, 837, 257, 13779, 8756, 7185, 508, 17007, 873, 2506, 290, 2279, 1088, 1267], [5661, 318, 257, 8246, 290, 14851, 12838, 326, 1718, 1936, 812, 284, 787, 837, 290, 262, 19886, 338, 34418, 8689, 318, 257, 2612, 12, 86, 3532, 278, 21742, 5600, 764], [64, 4950, 290, 36660, 12452, 286, 262, 3923, 356, 1560, 6731, 284, 787, 2565, 286, 262, 30741, 26611, 286, 262, 995, 764], [292, 485, 422, 852, 262, 36090, 6386, 3807, 286, 262, 614, 837, 985, 505, 837, 290, 1809, 9200, 4033, 338, 10457, 3098, 12, 71, 31777, 35704, 837, 468, 257, 20589, 306, 29303, 43706, 284, 340, 764], [47261, 5714, 1021, 813, 32254, 290, 31671, 1088, 465, 39769, 338, 473, 381, 959, 4847, 764, 764, 764, 290, 5630, 1299, 572, 262, 8011, 338, 40502, 351, 11004, 13933, 1108, 329, 257, 717, 12, 45016, 764], [3137, 1194, 5916, 12, 448, 12, 1659, 12, 7050, 1621, 326, 8523, 14768, 45759, 764], [8117, 338, 281, 2568, 284, 331, 12777, 285, 321, 6557, 256, 4131, 72, 35942, 764, 881, 286, 340, 2058, 422, 262, 14802, 837, 26329, 44139, 13289, 416, 663, 1085, 10544, 764], [270, 338, 262, 1611, 286, 46387, 13207, 12, 411, 9665, 374, 3361, 326, 289, 31777, 1165, 8365, 3769, 764], [260, 259, 27087, 262, 1690, 11564, 1109, 286, 262, 995, 338, 21196, 15874, 1692, 3265, 290, 20527, 837, 290, 663, 5339, 284, 12035, 1262, 7325, 837, 3288, 290, 6156, 41744, 6421, 764], [5832, 460, 1254, 262, 4894, 326, 3627, 2737, 428, 39931, 12838, 837, 290, 262, 14733, 290, 9265, 326, 6808, 340, 287, 4203, 764], [270, 338, 1327, 407, 284, 307, 10081, 19513, 416, 685, 86, 1555, 2777, 2049, 338, 60, 45581, 837, 772, 287, 428, 1057, 12, 1659, 12, 1169, 12, 17805, 4038, 837, 780, 428, 2576, 4206, 703, 284, 3708, 340, 284, 262, 3509, 764], [64, 3807, 329, 1367, 12, 1941, 12, 727, 6510, 351, 5701, 10625, 286, 511, 898, 290, 262, 662, 7821, 4813, 508, 11892, 42280, 6, 9563, 28796, 764], [64, 23056, 306, 16425, 2406, 12, 1659, 12, 496, 12838, 764], [361, 345, 821, 407, 656, 262, 43962, 8663, 837, 428, 5544, 15108, 3807, 287, 1440, 812, 1839, 470, 10385, 345, 1377, 393, 772, 1394, 534, 2951, 1280, 764, 475, 3296, 815, 423, 1257, 3249, 257, 4508, 12, 3605, 43962, 1444, 3879, 8482, 764], [6738, 262, 1263, 6175, 8714, 286, 262, 4756, 10824, 284, 1288, 647, 275, 1142, 5714, 338, 7138, 7758, 29512, 4776, 837, 27678, 2516, 3011, 655, 546, 2279, 826, 764], [25356, 1775, 319, 257, 838, 12, 8589, 5581, 3159, 393, 379, 534, 1957, 3294, 87, 837, 262, 5743, 12, 1659, 12, 14108, 12, 24073, 837, 9856, 39343, 286, 2876, 303, 4173, 5404, 389, 49083, 9739, 764], [10134, 257, 427, 15366, 20024, 764, 764, 764, 257, 14042, 2759, 753, 40819, 1843, 34851, 764], [2232, 81, 3301, 32254, 262, 2104, 2646, 351, 262, 1611, 286, 42925, 326, 1838, 597, 1813, 5739, 804, 588, 257, 1641, 338, 2183, 12, 9727, 33826, 5356, 2657, 764], [1169, 3807, 468, 6041, 286, 15360, 290, 28294, 2647, 764, 612, 389, 3105, 290, 28585, 3354, 837, 475, 340, 468, 655, 1576, 25721, 284, 1394, 340, 3499, 764], [272, 8131, 14169, 290, 21840, 306, 44756, 1451, 263, 5901, 351, 41718, 1626, 41718, 1626, 41718, 764], [8117, 338, 407, 881, 517, 284, 428, 16711, 286, 262, 14428, 12718, 1525, 5337, 621, 20024, 1377, 3626, 1203, 837, 21289, 11970, 837, 17972, 6551, 20024, 764], [292, 257, 894, 515, 18666, 284, 617, 24007, 477, 12, 30783, 837, 5055, 287, 262, 16187, 286, 2369, 593, 318, 6467, 2106, 286, 262, 1266, 1611, 1058, 30304, 837, 13477, 290, 5527, 306, 17774, 764], [10197, 611, 262, 6594, 338, 257, 1310, 13852, 88, 837, 351, 257, 2457, 14779, 326, 338, 477, 1165, 13678, 306, 7209, 837, 345, 17753, 1577, 3437, 686, 1362, 12314, 12758, 837, 1266, 1900, 329, 262, 48713, 516, 407, 889, 12788, 837, 3884, 329, 2111, 764], [1662, 355, 18778, 393, 772, 355, 36102, 355, 663, 2476, 284, 307, 284, 1302, 503, 837, 475, 340, 468, 4084, 587, 925, 351, 17696, 290, 1337, 764], [5661, 318, 1097, 295, 338, 8886, 3895, 475, 465, 4226, 290, 4571, 1311, 82, 351, 257, 6628, 326, 867, 4341, 2104, 16179, 2111, 284, 3151, 764], [272, 12661, 837, 3867, 290, 800, 36274, 803, 2646, 764], [1659, 260, 344, 555, 64, 809, 8107, 267, 634, 403, 32482, 390, 2285, 5330, 357, 257, 403, 4188, 5417, 1779, 641, 4763, 1267, 8358, 275, 2013, 410, 1000, 8591, 3112, 64, 257, 1676, 303, 10641, 764], [13, 764, 764, 530, 286, 262, 749, 44207, 290, 17774, 28576, 364, 1312, 1053, 1775, 287, 2407, 257, 890, 640, 764], [64, 14169, 13516, 286, 1109, 290, 10165, 764], [64, 21002, 29932, 18560, 764], [71, 1794, 699, 837, 15241, 290, 33138, 13147, 431, 17459, 764], [274, 36583, 3755, 837, 5874, 434, 68, 1658, 806, 12754, 331, 285, 4669, 920, 1186, 268, 3755, 764, 2829, 331, 3308, 20346, 3263, 68, 573, 25655, 3866, 681, 6557, 764], [1169, 17062, 318, 257, 2829, 290, 2612, 12, 48133, 1621, 837, 1336, 286, 285, 3333, 326, 815, 20024, 477, 475, 262, 749, 28858, 764], [1169, 2646, 318, 281, 20050, 1641, 2646, 1377, 2495, 881, 8998, 379, 597, 40944, 508, 10408, 14260, 764], [64, 1216, 34041, 290, 4713, 14348, 10997, 1033, 3255, 3206, 4819, 290, 262, 6459, 286, 34596, 1022, 1466, 764], [270, 338, 257, 922, 2646, 1377, 407, 257, 6833, 837, 475, 5629, 837, 17774, 290, 16425, 764], [2704, 5570, 913, 290, 14348, 837, 345, 714, 869, 428, 703, 1667, 12898, 1392, 607, 35096, 736, 1377, 13148, 837, 326, 318, 837, 673, 1683, 550, 530, 284, 2221, 351, 764], [71, 1324, 813, 329, 285, 81, 764, 22531, 1377, 996, 14274, 1324, 813, 329, 465, 7481, 1377, 262, 14836, 1021, 286, 262, 17432, 2630, 257, 4226, 326, 645, 1692, 3159, 16002, 714, 423, 10719, 284, 2872, 764], [11098, 805, 290, 443, 86, 271, 389, 20105, 3690, 764], [1169, 7110, 318, 523, 28297, 306, 542, 36207, 290, 47284, 287, 663, 11194, 44845, 326, 645, 530, 714, 1683, 7457, 340, 329, 1997, 29474, 3950], [71, 896, 530, 503, 286, 262, 3952, 329, 262, 705, 9930, 836, 470, 787, 705, 368, 588, 326, 7471, 6, 5011, 764], [270, 288, 3565, 284, 307, 257, 1310, 1180, 837, 290, 326, 49065, 318, 644, 1838, 340, 24769, 764], [58, 69, 408, 437, 268, 60, 318, 881, 517, 656, 33985, 290, 4441, 10038, 621, 339, 318, 329, 319, 3159, 5636, 2171], [1169, 9048, 13289, 389, 477, 4136, 319, 837, 2592, 443, 68, 686, 824, 338, 1210, 355, 479, 268, 764], [64, 13206, 7002, 764, 764, 764, 290, 366, 465, 1266, 1545, 18140, 366, 318, 510, 612, 351, 262, 18822, 286, 38102, 764], [265, 3016, 1115, 2250, 837, 262, 2187, 286, 3338, 3189, 318, 1342, 621, 262, 2160, 286, 663, 3354, 764], [1169, 2250, 1838, 345, 10716, 534, 898, 1204, 287, 881, 262, 976, 835, 663, 3435, 466, 837, 290, 262, 1998, 318, 11982, 764, 262, 2250, 318, 644, 6918, 389, 4385, 284, 307, 764, 764, 764], [64, 10758, 290, 49685, 2646, 326, 6630, 1973, 262, 13020, 286, 644, 318, 2968, 290, 3665, 287, 428, 1029, 12, 13670, 2479, 837, 5486, 663, 24279, 351, 4822, 30786, 19506, 290, 262, 24481, 5077, 2647, 286, 5206, 541, 5405, 764], [37784, 39189, 922, 837, 3805, 663, 4451, 88, 12, 38895, 22491, 764], [568, 1862, 837, 523, 4451, 837, 884, 7401, 837, 884, 257, 10787, 1635, 1635, 1635, 764], [86, 2238, 338, 11418, 423, 257, 7310, 37457, 764, 465, 17119, 46592, 287, 47735, 291, 11278, 326, 15565, 281, 10238, 1502, 3690, 262, 11918, 764], [5657, 1681, 468, 2727, 257, 4205, 390, 2700, 326, 318, 7650, 837, 266, 36053, 290, 7932, 764], [1169, 7464, 857, 2666, 345, 555, 913, 20286, 837, 475, 777, 389, 13289, 284, 2883, 287, 257, 18078, 34549, 3704, 764], [13, 764, 764, 281, 49534, 640, 12, 86, 9222, 3335, 1377, 475, 4903, 3643, 6340, 338, 1877, 12, 13670, 9507, 2196, 991, 3173, 262, 36835, 82, 764], [270, 338, 257, 14802, 2230, 284, 9814, 656, 262, 36051, 286, 262, 995, 837, 257, 34967, 284, 262, 10112, 3303, 286, 18662, 290, 257, 1976, 41214, 19232, 286, 5238, 764], [2364, 364, 281, 8468, 3663, 284, 12414, 262, 11082, 871, 287, 262, 1918, 7389, 837, 407, 655, 262, 11519, 2296, 273, 1483, 475, 635, 262, 387, 746, 26267, 3662, 286, 340, 290, 1171, 2984, 525, 4516, 286, 703, 262, 2187, 1517, 2499, 764], [72, 836, 470, 892, 1312, 1053, 587, 355, 920, 2596, 771, 290, 41586, 416, 281, 355, 666, 2646, 1201, 22441, 64, 256, 2385, 74, 25384, 338, 6953, 582, 764], [270, 318, 523, 23056, 284, 766, 3857, 259, 481, 1789, 82, 1210, 11546, 7370, 422, 262, 4731, 286, 13277, 4420, 45427, 290, 473, 14097, 49023, 274, 339, 338, 587, 1642, 329, 262, 938, 1811, 812, 764], [35248, 1888, 30711, 474, 43561, 313, 837, 1642, 465, 717, 27296, 12, 1462, 12, 26240, 11059, 351, 284, 1416, 64, 837, 24748, 893, 262, 339, 2703, 7506, 286, 279, 18863, 5362, 338, 5863, 1842, 12, 73, 15746, 88, 12, 5123, 12, 2385, 5285, 277, 392, 14208, 351, 1049, 29932, 11044, 764], [75, 17517, 338, 13389, 422, 7646, 2802, 284, 3054, 723, 264, 24080, 318, 21176, 291, 1927, 3143, 6197, 516, 837, 475, 450, 12093, 1167, 2664, 262, 2597, 351, 281, 28418, 431, 34446, 4755, 286, 7016, 3872, 764], [8310, 3755, 338, 17290, 37451, 318, 36332, 1377, 340, 338, 1871, 262, 749, 35589, 306, 3562, 7328, 1312, 1053, 1683, 1775, 764], [1169, 2818, 2646, 329, 883, 508, 588, 6639, 14577, 444, 326, 460, 307, 3013, 485, 764], [6, 10641, 306, 6, 481, 14083, 663, 5386, 287, 734, 4553, 2628, 837, 883, 8978, 329, 517, 21379, 290, 883, 26732, 329, 17703, 764, 764, 764], [77, 712, 88, 290, 8564, 837, 340, 34531, 656, 8768, 17290, 307, 69, 4185, 1732, 837, 290, 379, 262, 976, 640, 10969, 257, 44523, 19975, 286, 644, 10182, 289, 31777, 764], [64, 1667, 29333, 516, 7002, 422, 9963, 7306, 1042, 284, 28680, 2116, 12, 46303, 1159, 764], [1169, 2646, 318, 655, 257, 1263, 837, 18857, 837, 2000, 12, 2436, 7855, 837, 8033, 12, 26103, 2085, 764], [48554, 837, 29696, 837, 8258, 290, 6165, 24281, 278, 2646, 764], [2016, 262, 2646, 338, 8883, 318, 3729, 407, 4534, 1477, 868, 837, 428, 33016, 286, 19180, 11927, 4048, 16641, 468, 734, 5442, 1085, 13289, 290, 20024, 284, 13952, 764], [64, 12733, 17547, 284, 257, 1049, 15706, 290, 607, 21266, 705, 1073, 12, 30783, 764, 705], [64, 2274, 12507, 379, 37437, 590, 837, 428, 2330, 12, 2213, 1077, 35704, 481, 18330, 262, 17696, 286, 772, 883, 39411, 661, 508, 1239, 6898, 257, 42812, 286, 825, 443, 43988, 338, 12972, 398, 5411, 764], [1169, 8296, 6246, 318, 262, 691, 636, 286, 262, 2646, 326, 318, 19128, 3101, 1377, 290, 703, 5763, 876, 345, 389, 286, 428, 8338, 319, 534, 1241, 286, 35387, 764], [13966, 31775, 8258, 290, 9835, 5629, 837, 290, 340, 2499, 13025, 880, 355, 257, 3491, 4038, 329, 1976, 23778, 764], [29199, 2331, 3983, 1286, 36979, 290, 36943, 351, 428, 2587, 837, 290, 339, 460, 470, 1037, 9644, 287, 257, 1178, 286, 465, 898, 18105, 764], [1169, 513, 67, 4263, 691, 9494, 262, 2646, 338, 584, 49366, 3081, 837, 3501, 340, 257, 6283, 14831, 286, 345, 12, 533, 12, 8117, 3542, 9449, 351, 262, 595, 13989, 278, 14880, 1483, 286, 262, 9775, 5445, 12, 2902, 5544, 3355, 286, 262, 3807, 3159, 764], [45070, 1559, 8075, 257, 995, 326, 338, 379, 1752, 28201, 290, 17037, 4420, 5385, 2162, 12986, 837, 1865, 34355, 6507, 764], [270, 338, 20039, 837, 475, 340, 18045, 832, 262, 2938, 46730, 351, 3918, 290, 772, 617, 6795, 764], [28950, 20105, 837, 880, 12, 9442, 290, 837, 11003, 837, 17774, 837, 5968, 2156, 318, 257, 13899, 3188, 286, 281, 1785, 326, 468, 284, 307, 1775, 284, 307, 4762, 764], [2934, 19450, 8704, 8075, 281, 17991, 5527, 837, 745, 16877, 458, 931, 290, 22632, 277, 5753, 462, 837, 475, 1239, 905, 88, 837, 2646, 3025, 48666, 7277, 13460, 389, 23738, 290, 37928, 1048, 1431, 416, 12314, 2978, 8301, 4033, 72, 764], [13, 764, 764, 281, 23310, 3704, 286, 2646, 764], [1169, 2646, 338, 1103, 5198, 1839, 470, 307, 284, 28050, 1419, 3296, 393, 8855, 34636, 837, 475, 284, 3807, 31006, 508, 2883, 3612, 546, 13206, 2683, 351, 645, 2562, 7429, 764], [1169, 1109, 326, 262, 12302, 318, 257, 3016, 45707, 540, 29932, 1998, 1377, 290, 257, 7932, 477, 12, 1095, 15499, 13769, 1377, 318, 257, 20820, 22107, 284, 262, 1621, 262, 2646, 44771, 764], [64, 1614, 3014, 39867, 10997, 543, 318, 8258, 422, 923, 284, 5461, 764], [64, 28027, 290, 4713, 12452, 286, 703, 262, 7161, 991, 3793, 281, 27102, 7196, 287, 442, 3762, 3592, 764], [64, 4047, 19827, 32251, 837, 18064, 351, 617, 44207, 7110, 4410, 290, 617, 21606, 29735, 3170, 6460, 764, 764, 340, 338, 257, 24769, 11808, 287, 14821, 11887, 290, 24632, 12, 42460], [292, 289, 6724, 7264, 1139, 7830, 3690, 262, 3807, 837, 705, 23205, 306, 5145, 10457, 5145, 705], [6679, 338, 43157, 287, 10868, 5475, 1692, 11511, 18764, 837, 407, 7787, 284, 3830, 607, 1204, 6247, 287, 2166, 286, 281, 5386, 764, 607, 7585, 290, 10576, 389, 38870, 764], [5225, 780, 837, 329, 262, 749, 636, 837, 340, 30940, 262, 8531, 537, 291, 956, 290, 10451, 291, 279, 849, 4316, 326, 307, 7207, 663, 29644, 764], [265, 663, 1266, 837, 262, 922, 2576, 318, 257, 23056, 306, 4044, 1011, 319, 38713, 764, 764, 764], [272, 4998, 290, 47716, 3807, 326, 46638, 3892, 656, 262, 5210, 10150, 286, 25741, 764], [10755, 12062, 3988, 508, 32316, 7858, 9501, 355, 484, 1043, 606, 290, 1716, 2116, 12, 9727, 16527, 10856, 1377, 257, 1877, 12, 2902, 2196, 286, 262, 45630, 272, 4320, 764], [13966, 31775, 837, 287, 262, 1781, 286, 17217, 1242, 12, 4803, 10666, 10886, 290, 21158, 12, 65, 321, 2223, 277, 49191, 837, 257, 474, 5286, 4014, 895, 4595, 656, 1223, 4988, 649, 764], [64, 949, 2304, 2261, 1310, 7245, 538, 319, 262, 2646, 13428, 837, 475, 530, 326, 867, 517, 661, 815, 2198, 503], [16520, 64, 1569, 89, 837, 951, 2178, 385, 3144, 333, 280, 267, 279, 17902, 390, 267, 1434, 764], [1, 1511, 10275, 366, 6622, 663, 36374, 1969, 837, 475, 318, 5365, 3105, 284, 1282, 284, 262, 966, 764], [64, 29395, 837, 880, 12, 78, 3902, 4572, 837, 33954, 271, 3973, 23895, 290, 510, 3937, 301, 1068, 764], [9099, 470, 1410, 319, 262, 2818, 7464, 837, 475, 6029, 1363, 435, 8809, 7127, 262, 1317, 351, 9188, 508, 13537, 422, 257, 1402, 3240, 1204, 764], [270, 468, 257, 11800, 835, 286, 1972, 739, 534, 4168, 290, 17274, 351, 345, 890, 706, 340, 338, 625, 764], [1169, 3807, 14768, 45759, 5176, 284, 663, 23251, 23132, 3227, 1486, 764], [270, 5419, 326, 262, 4318, 23827, 389, 5924, 10544, 837, 290, 326, 484, 760, 511, 9176, 523, 880, 764], [64, 28695, 3807, 546, 2994, 837, 8993, 837, 25474, 837, 35394, 837, 26359, 290, 1842, 764], [9268, 262, 3626, 284, 2342, 764], [5562, 374, 3301, 1196, 271, 1058, 262, 12661, 14348, 10997, 351, 4036, 4213, 319, 663, 2000, 764], [2127, 1694, 516, 290, 12379, 701, 11648, 764], [26615, 365, 14293, 503, 262, 1266, 422, 465, 1588, 3350, 287, 21104, 36877, 10993, 874, 326, 389, 11800, 290, 523, 38084, 484, 460, 8080, 262, 38273, 13956, 287, 8746, 5857, 338, 10721, 764], [64, 670, 286, 262, 27878, 11800, 774, 290, 11202, 837, 340, 8849, 262, 11660, 3895, 8886, 286, 6260, 12, 35248, 1931, 291, 416, 1754, 837, 508, 14759, 262, 1176, 286, 262, 16992, 290, 262, 27494, 286, 21654, 290, 3773, 764], [12853, 30424, 318, 262, 50131, 329, 264, 12342, 527, 456, 3296, 508, 892, 339, 338, 3750, 1165, 5068, 1201, 465, 734, 267, 13034, 19332, 7328, 287, 4751], [270, 4962, 503, 284, 307, 257, 2005, 2029, 262, 2593, 837, 5176, 284, 617, 14169, 3597, 290, 599, 3506, 306, 7205, 764], [5832, 1244, 407, 765, 284, 8181, 503, 351, 6072, 32589, 837, 475, 345, 1183, 2192, 766, 257, 1643, 286, 3511, 287, 607, 34419, 1621, 764], [64, 670, 286, 28746, 49198, 837, 16716, 43086, 1042, 290, 11982, 2219, 2565, 764], [270, 338, 355, 1969, 355, 356, 1183, 1683, 1282, 284, 2045, 832, 257, 16413, 338, 1570, 22805, 355, 339, 2499, 764], [28895, 913, 837, 28695, 290, 17774, 764], [86, 9760, 837, 15241, 290, 880, 44756, 764], [7197, 11223, 506, 12, 12114, 648, 4952, 340, 523, 42900, 4420, 290, 7328, 340, 523, 21104, 326, 1312, 3521, 470, 1037, 852, 3144, 30829, 416, 340, 764], [5832, 423, 284, 1414, 3241, 284, 1061, 477, 262, 3923, 837, 475, 484, 821, 1123, 3499, 764, 262, 3807, 318, 880, 2823, 290, 845, 15444, 837, 290, 530, 284, 37375, 706, 262, 10824, 4836, 764], [268, 2633, 340, 329, 644, 340, 318, 2162, 345, 460, 5465, 3511, 1568, 764], [64, 3975, 286, 262, 8434, 39804, 286, 1842, 290, 35394, 290, 11728, 7428, 351, 257, 4958, 338, 11831, 14000, 764], [76, 40138, 40638, 6557, 301, 3970, 837, 36583, 3755, 331, 1357, 268, 2413, 8358, 424, 14060, 274, 5799, 837, 1658, 555, 809, 268, 304, 73, 18856, 78, 390, 2376, 8358, 1658, 1288, 269, 500, 390, 920, 1186, 268, 320, 1153, 78, 279, 1434, 331, 7813, 1224, 73, 418, 764], [64, 10590, 32251, 351, 257, 4451, 4226, 290, 281, 36681, 12, 5589, 22220, 338, 3241, 284, 3703, 764], [28950, 20105, 764], [2164, 415, 3011, 284, 3359, 465, 20603, 1108, 284, 20187, 837, 475, 635, 284, 905, 7205, 2837, 326, 743, 5975, 617, 508, 1807, 1657, 12, 20122, 10997, 373, 465, 329, 660, 764], [265, 1661, 8258, 290, 379, 584, 1661, 2960, 306, 13477, 837, 340, 338, 281, 19827, 804, 379, 734, 23827, 508, 1234, 2405, 503, 612, 780, 484, 1842, 644, 484, 466, 764], [7038, 16265, 83, 290, 7544, 6422, 18756, 409, 2507, 257, 16585, 290, 4467, 1241, 326, 338, 1111, 473, 84, 948, 290, 886, 6648, 764], [71, 5406, 837, 4050, 11648, 319, 1204, 287, 262, 318, 2510, 72, 12, 28756, 6340, 395, 24605, 16771, 764], [1169, 2646, 318, 477, 257, 1310, 6578, 1955, 8949, 837, 475, 340, 338, 4457, 880, 2826, 290, 1690, 845, 8258, 764], [451, 5907, 663, 22051, 422, 4283, 2266, 27235, 705, 19199, 6, 290, 422, 262, 867, 837, 867, 7188, 618, 356, 7564, 772, 1231, 262, 1288, 9924, 272, 27149, 837, 262, 711, 2157, 262, 1517, 764], [64, 1103, 1621, 546, 1103, 661, 2877, 511, 3160, 5213, 546, 262, 2003, 286, 281, 13830, 837, 14946, 29103, 6320, 1641, 2888, 764], [270, 338, 5543, 599, 29655, 703, 300, 32681, 9619, 262, 427, 363, 1706, 826, 866, 284, 262, 2656, 1339, 88, 479, 292, 368, 12, 69, 700, 1348, 3809, 764], [64, 4320, 3350, 286, 4735, 4048, 7401, 508, 1382, 257, 32354, 34549, 764, 612, 2125, 470, 257, 4939, 393, 36138, 2854, 12077, 606, 764], [27004, 3783, 10165, 329, 7334, 12, 4739, 837, 351, 691, 257, 1178, 3991, 4831, 1863, 262, 835, 764], [270, 338, 257, 23056, 1487, 422, 262, 2116, 12, 9446, 290, 34370, 326, 5485, 749, 45630, 272, 24612, 286, 3350, 305, 764], [28950, 3867, 290, 25409, 262, 23597, 11519, 287, 262, 13961, 1022, 262, 45630, 272, 705, 4774, 82, 6, 290, 511, 705, 5162, 3558, 764, 705], [2016, 262, 8381, 479, 29456, 26479, 338, 3452, 3626, 318, 407, 329, 477, 18221, 837, 340, 4394, 18857, 19506, 837, 4050, 13289, 837, 290, 281, 6481, 38192, 2565, 286, 1674, 65, 7656, 764], [75, 6696, 290, 3100, 14542, 423, 11091, 2614, 20024, 837, 290, 511, 3159, 49498, 1838, 262, 1468, 1621, 1283, 649, 764], [1169, 1621, 743, 407, 307, 649, 837, 475, 38132, 1373, 666, 3437, 45610, 755, 1559, 837, 1642, 465, 45630, 272, 3895, 8886, 837, 21274, 274, 340, 510, 512, 7775, 306, 764], [270, 338, 886, 6648, 284, 3285, 8805, 480, 288, 764, 3522, 284, 607, 5229, 355, 705, 19650, 494, 6, 1377, 290, 339, 857, 787, 329, 6275, 1664, 837, 407, 1551, 355, 257, 2116, 12, 16796, 26960, 764], [1169, 2646, 1690, 41885, 257, 46814, 2890, 19518, 764], [3549, 621, 1838, 510, 329, 663, 285, 19301, 680, 24380, 416, 6011, 374, 12752, 599, 689, 286, 8768, 4203, 764], [270, 338, 6159, 355, 14348, 4249, 355, 31610, 355, 340, 815, 307, 764, 475, 340, 4394, 6088, 284, 37375, 290, 34722, 319, 355, 663, 8468, 2776, 6364, 45995, 764], [13966, 31775, 8258, 837, 1464, 845, 20239, 290, 2883, 1346, 625, 31290, 287, 262, 4569, 435, 4666, 10205, 7785, 3918, 764], [647, 8907, 6840, 23677, 12385, 541, 2518, 338, 29696, 5022, 286, 3435, 422, 262, 2443, 284, 3159, 764], [11246, 6918, 389, 588, 257, 25103, 45334, 12, 67, 6, 37600, 260, 2162, 428, 530, 318, 257, 26951, 764], [10919, 714, 423, 1716, 655, 1194, 13041, 560, 277, 540, 318, 3142, 284, 711, 503, 355, 257, 14169, 837, 23332, 12838, 1587, 244, 355, 42683, 287, 663, 898, 835, 355, 663, 2116, 12, 67, 859, 265, 2890, 3435, 764], [67, 23401, 468, 5901, 503, 465, 3350, 351, 16403, 4713, 6698, 764], [620, 17974, 257, 3297, 286, 2646, 291, 2462, 49915, 326, 9890, 82, 287, 262, 2081, 2785, 286, 262, 7090, 764], [27078, 345, 651, 656, 663, 18662, 764, 764, 764, 262, 3807, 4329, 257, 1182, 88, 1998, 764], [1, 8295, 2962, 366, 2499, 355, 281, 8468, 3182, 16603, 290, 3188, 286, 4257, 9628, 364, 287, 262, 711, 7081, 6980], [361, 285, 81, 764, 1976, 33255, 338, 2426, 2300, 318, 837, 284, 617, 4922, 379, 1551, 837, 28533, 408, 3746, 45630, 272, 837, 465, 3164, 284, 23689, 1244, 307, 1444, 4173, 38336, 764], [64, 3049, 12, 31462, 290, 11004, 2646, 326, 3568, 23985, 284, 1716, 257, 20533, 287, 474, 2674, 2771, 11034, 764], [13, 764, 764, 257, 11348, 1310, 3807, 379, 663, 4755, 2162, 281, 13936, 286, 262, 49333, 326, 739, 10724, 262, 25488, 31986, 1905, 286, 262, 14062, 338, 764, 764, 764, 262, 2646, 338, 7464, 468, 257, 366, 644, 373, 340, 477, 329, 5633, 366, 4203, 284, 340, 837, 475, 588, 262, 14062, 338, 837, 262, 5296, 612, 318, 257, 1049, 1730, 286, 1257, 764], [64, 12733, 5726, 656, 257, 845, 2408, 12121, 764], [58, 65, 3823, 3245, 60, 555, 1073, 690, 257, 1621, 3665, 1576, 284, 2666, 262, 3159, 264, 6457, 1359, 351, 38520, 764], [26022, 7165, 12513, 318, 257, 21742, 329, 6450, 1754, 338, 867, 18054, 764], [64, 6029, 12, 32353, 1522, 26540, 341, 286, 530, 286, 5336, 46754, 4861, 338, 749, 9204, 837, 611, 1551, 6768, 8018, 837, 7325, 29420, 16600, 764], [5661, 318, 530, 286, 262, 749, 22632, 13393, 290, 606, 4142, 3867, 2462, 873, 287, 2274, 4088, 837, 290, 287, 15275, 286, 6409, 4159, 17978, 837, 629, 669, 2771, 338, 1266, 287, 517, 621, 257, 5707, 764], [16833, 3003, 262, 4676, 3073, 612, 318, 1223, 2861, 4379, 764], [64, 5527, 306, 15758, 290, 6178, 343, 1346, 15345, 670, 422, 257, 22527, 3437, 508, 4753, 468, 1223, 319, 465, 2000, 764], [270, 338, 257, 16576, 6496, 995, 286, 29649, 82, 837, 27538, 290, 21557, 837, 286, 266, 10321, 287, 44852, 88, 736, 9649, 393, 37293, 17039, 764], [64, 23332, 837, 37276, 290, 24638, 306, 44756, 629, 1252, 680, 10997, 1377, 2845, 351, 281, 23077, 4318, 46029, 326, 714, 423, 587, 257, 4968, 422, 40689, 88, 21015, 338, 3616, 286, 1204, 764], [270, 1239, 10143, 284, 8209, 514, 764], [896, 4571, 837, 663, 4226, 837, 290, 356, 8770, 338, 2854, 355, 257, 29627, 16141, 4714, 2415, 286, 9136, 787, 329, 257, 33544, 17774, 8541, 2431, 837, 611, 326, 338, 644, 345, 821, 287, 262, 10038, 329, 764], [64, 23332, 14348, 10997, 326, 318, 416, 1290, 262, 1657, 395, 3290, 1326, 2646, 290, 1871, 262, 749, 20050, 764], [5661, 318, 262, 1611, 286, 3807, 326, 973, 284, 307, 826, 379, 1363, 379, 262, 264, 3658, 2603, 500, 68, 837, 290, 340, 991, 318, 764], [1169, 9009, 286, 2041, 11984, 5536, 994, 318, 40016, 29033, 290, 1327, 284, 4180, 764], [2339, 663, 734, 27677, 837, 13540, 338, 479, 23790, 272, 271, 80, 1381, 72, 290, 12122, 338, 7182, 30188, 80, 1381, 72, 837, 262, 29932, 2927, 496, 12385, 80, 726, 80, 1381, 72, 714, 307, 262, 749, 12385, 626, 12, 70, 4070, 2646, 1683, 764], [5657, 272, 2125, 470, 262, 749, 32331, 393, 39931, 2646, 422, 4173, 272, 1377, 393, 837, 5600, 837, 416, 663, 3437, 1377, 475, 340, 338, 257, 12733, 15185, 284, 262, 867, 3734, 837, 5670, 7328, 11823, 422, 326, 749, 6452, 286, 7027, 764], [1169, 27329, 3436, 787, 1138, 25986, 2861, 4379, 764], [21953, 837, 14309, 415, 837, 8067, 2280, 6496, 290, 11856, 351, 11026, 4962, 286, 7110, 290, 257, 26951, 286, 5874, 40642, 972, 764], [64, 4286, 326, 1070, 10220, 262, 27494, 286, 24863, 1056, 290, 2055, 287, 257, 26843, 2584, 837, 46875, 6977, 764], [64, 14309, 415, 12838, 286, 10713, 837, 15827, 290, 40788, 764], [77, 726, 344, 338, 2646, 318, 19377, 876, 290, 25722, 2759, 28962, 764], [1456, 837, 512, 4484, 22404, 710, 2058, 355, 1969, 284, 1534, 917, 414, 355, 339, 318, 1884, 284, 651, 764], [1990, 3369, 257, 1310, 286, 262, 3252, 326, 3397, 423, 329, 262, 1744, 25650, 286, 511, 1751, 438, 392, 262, 3360, 2089, 7747, 12289, 290, 17150, 787, 287, 262, 5353, 286, 1804, 606, 922, 764], [36909, 390, 22346, 1825, 498, 274, 285, 40138, 493, 68, 411, 39781, 390, 22346, 6184, 118, 2528, 320, 418, 256, 26597, 1930, 764], [3201, 318, 257, 1402, 14068, 837, 16441, 278, 262, 19091, 287, 257, 18875, 290, 8557, 7332, 1819, 326, 318, 1997, 475, 32171, 433, 291, 764], [272, 19992, 837, 33954, 271, 3973, 953, 4817, 10590, 32251, 764], [5661, 33463, 596, 837, 523, 13699, 284, 262, 4044, 2000, 837, 318, 1682, 257, 23332, 15499, 810, 663, 5292, 739, 12, 1065, 5386, 318, 5213, 764], [67, 2487, 1451, 263, 12, 785, 4716, 28763, 286, 366, 1263, 1730, 319, 8805, 6415, 4675, 366, 326, 338, 257, 49822, 837, 28297, 837, 6487, 12, 20286, 1310, 16840, 287, 543, 262, 8713, 366, 8966, 5362, 366, 6140, 284, 804, 588, 257, 366, 1103, 479, 499, 5500, 1349, 1134, 764, 366], [270, 338, 257, 21104, 13013, 47188, 605, 16901, 319, 257, 7684, 286, 11267, 623, 298, 290, 8826, 3435, 2877, 287, 262, 20287, 1125, 75, 8583, 7541, 764, 764, 764], [271, 340, 257, 2472, 1943, 5633, 645, 764, 318, 340, 1223, 597, 2081, 2646, 19678, 481, 765, 284, 2198, 503, 5633, 345, 731, 764], [89, 1092, 837, 409, 18478, 3875, 11331, 303, 1156, 15108, 2272, 8855, 764], [67, 349, 1655, 290, 1216, 47699, 6977, 257, 13899, 18560, 286, 257, 410, 1155, 22678, 12, 6286, 40944, 508, 30130, 290, 3538, 36659, 515, 355, 281, 477, 12, 2382, 7490, 2576, 351, 257, 4508, 649, 1438, 287, 8372, 256, 1697, 10702, 764], [1169, 595, 18052, 11676, 1894, 8137, 468, 257, 835, 286, 7580, 278, 262, 2104, 4315, 355, 262, 2646, 14088, 319, 764], [64, 23056, 306, 5508, 290, 6165, 15241, 12838, 286, 262, 3297, 286, 661, 3221, 9514, 287, 11811, 45630, 272, 2646, 764, 2989, 340, 503, 764], [1516, 1214, 278, 290, 13891, 837, 611, 6165, 407, 2407, 19201, 764], [1169, 1621, 837, 588, 1204, 837, 17567, 284, 307, 2829, 837, 290, 262, 1255, 318, 257, 13206, 16416, 286, 13006, 10825, 764], [64, 49822, 983, 286, 3797, 290, 10211, 326, 338, 8157, 290, 31610, 379, 1661, 837, 475, 10491, 23687, 1250, 85, 1799, 284, 663, 7095, 290, 16507, 319, 20039, 7110, 542, 15104, 1817, 764], [12543, 3281, 290, 837, 379, 1661, 837, 45516, 837, 262, 2646, 422, 3437, 4903, 3643, 289, 5973, 5439, 3575, 477, 2753, 1295, 287, 38836, 38047, 837, 366, 257, 1748, 810, 661, 991, 1100, 764, 366], [5661, 9961, 12, 785, 4716, 1595, 470, 467, 329, 262, 6678, 3489, 22051, 379, 262, 10907, 286, 7026, 12, 11534, 10963, 1377, 4556, 345, 954, 1288, 85, 8704, 338, 289, 48316, 764], [1169, 3807, 338, 19657, 1943, 815, 307, 18141, 284, 288, 10679, 627, 1698, 837, 287, 4330, 15797, 5485, 355, 281, 16076, 355, 880, 355, 281, 8674], [1662, 257, 2089, 7002, 379, 477, 764], [82, 896, 17809, 292, 813, 355, 257, 9961, 4286, 764, 764, 764, 475, 7228, 6452, 6795, 287, 663, 804, 379, 262, 37354, 286, 257, 1402, 1641, 764], [7972, 16620, 364, 20385, 428, 835, 290, 326, 837, 475, 612, 338, 645, 4020, 868, 262, 26479, 287, 262, 7331, 8701, 837, 2081, 284, 2241, 764], [8117, 318, 257, 23056, 8889, 286, 49509, 287, 336, 19986, 1310, 362, 438, 37121, 257, 32823, 837, 772, 287, 262, 1641, 2646, 1910, 764, 4191, 837, 340, 7864, 345, 625, 764], [77, 726, 344, 7328, 340, 517, 355, 257, 14702, 2106, 11483, 621, 355, 10512, 764], [2339, 257, 5366, 12, 1659, 12, 1169, 12, 20192, 7758, 13698, 1295, 764], [25591, 351, 281, 1393, 287, 649, 393, 18032, 10524, 286, 2646, 6461, 481, 1064, 644, 640, 318, 340, 612, 5633, 880, 2861, 262, 640, 764], [64, 20278, 8258, 3770, 1451, 263, 764], [71, 7211, 861, 3607, 1931, 9232, 257, 27822, 326, 318, 523, 19827, 326, 345, 1064, 3511, 16143, 36580, 1146, 379, 607, 837, 2111, 284, 1833, 607, 290, 11263, 611, 673, 1183, 8469, 764], [41081, 644, 2687, 5804, 546, 262, 3061, 286, 663, 14429, 837, 262, 905, 764, 764, 764, 6870, 257, 15013, 3704, 286, 13766, 837, 290, 612, 338, 645, 17086, 262, 7401, 286, 262, 7325, 3386, 2157, 340, 764], [5832, 1183, 307, 1364, 351, 262, 18098, 286, 1719, 655, 13923, 257, 1049, 2854, 290, 837, 3737, 837, 1577, 287, 284, 262, 14960, 284, 651, 319, 534, 3625, 290, 13279, 340, 764], [1169, 10544, 389, 523, 23754, 379, 24748, 1112, 511, 1862, 48973, 837, 356, 466, 5600, 1254, 329, 606, 764], [1169, 1738, 428, 4286, 2499, 1365, 621, 663, 27677, 318, 326, 616, 364, 318, 645, 2392, 2391, 42078, 278, 262, 9927, 12, 4666, 12, 9937, 1108, 286, 705, 1899, 82, 13997, 6918, 764], [270, 318, 257, 4829, 562, 837, 15715, 20681, 12, 12463, 2223, 32251, 14554, 326, 16316, 290, 788, 617, 764, 1312, 4398, 470, 1775, 530, 287, 523, 890, 837, 645, 4240, 1312, 1422, 470, 7564, 340, 379, 717, 764], [64, 13206, 18560, 286, 6573, 49333], [259, 512, 20391, 837, 26289, 318, 407, 655, 262, 25721, 837, 475, 379, 262, 2612, 286, 517, 10112, 4786, 764], [270, 318, 11441, 837, 286, 1781, 764, 764, 764, 475, 340, 318, 635, 23056, 837, 595, 18052, 837, 290, 655, 15828, 20050, 3805, 663, 11441, 1108, 764], [13, 764, 764, 11865, 21065, 318, 517, 20050, 621, 262, 2656, 764], [64, 2646, 326, 2753, 345, 2641, 262, 39804, 286, 663, 2426, 1058, 345, 1998, 340, 355, 345, 2342, 764], [1169, 3807, 7160, 329, 663, 11783, 2223, 290, 663, 3734, 7205, 764], [1169, 3807, 318, 7448, 422, 45253, 1657, 1108, 416, 262, 21654, 286, 262, 23689, 290, 262, 26275, 286, 262, 13289, 764], [1169, 2646, 4940, 503, 355, 19701, 475, 49733, 45543, 764, 764, 764, 290, 11835, 13676, 656, 1223, 286, 11091, 1176, 764], [22366, 2853, 271, 468, 925, 878, 837, 588, 307, 559, 1291, 2991, 290, 299, 268, 5857, 2123, 5351, 72, 837, 714, 8335, 514, 329, 428, 308, 652, 837, 583, 13658, 837, 1714, 12, 568, 4335, 36738, 319, 262, 39904, 12121, 764], [260, 259, 27087, 262, 18054, 286, 3159, 16002, 1149, 14485, 479, 559, 35826, 837, 13172, 286, 16711, 290, 852, 45610, 285, 971, 18198, 764], [14809, 68, 16316, 257, 6032, 4735, 2854, 287, 257, 2597, 326, 318, 257, 1643, 286, 257, 12928, 422, 262, 15581, 3435, 339, 468, 2826, 287, 262, 1613, 837, 290, 339, 318, 14451, 416, 5513, 732, 328, 837, 508, 10732, 262, 2646, 319, 465, 3154, 837, 22665, 12450, 764], [19796, 82, 257, 835, 284, 1560, 257, 2829, 1621, 837, 3737, 262, 24043, 1621, 286, 477, 837, 287, 257, 835, 326, 2331, 13206, 290, 772, 2656, 764], [64, 13393, 3704, 286, 5874, 19518, 326, 481, 837, 11481, 837, 307, 12086, 355, 530, 286, 262, 749, 1593, 3923, 284, 307, 1297, 287, 38132, 1373, 544, 338, 2646, 2106, 764], [5661, 318, 1242, 5989, 31044, 284, 1242, 764], [13, 764, 764, 257, 9707, 379, 1752, 781, 15492, 290, 14309, 415, 837, 18700, 290, 13699, 306, 2656, 764], [16340, 33494, 318, 257, 7932, 3807, 764], [13, 764, 764, 257, 13779, 290, 3360, 1735, 12, 22018, 2535, 306, 8258, 13516, 286, 11119, 21541, 290, 4268, 2636, 18857, 837, 20495, 279, 9346, 583, 34748, 287, 644, 714, 307, 607, 19304, 2597, 764], [67, 8101, 1359, 290, 7543, 12, 34751, 837, 257, 11975, 286, 19337, 7842, 811, 594, 326, 691, 1714, 837, 10731, 837, 290, 257, 26441, 1627, 286, 4923, 1801, 14002, 460, 5203, 764], [13966, 31775, 18585, 29735, 925, 475, 257, 7864, 462, 3350, 290, 3621, 10721, 7622, 340, 1016, 764], [73, 2674, 338, 18256, 22152, 396, 286, 1714, 290, 2910, 7127, 15579, 351, 644, 743, 307, 465, 749, 24251, 276, 2646, 284, 3128, 764], [3129, 5116, 837, 508, 338, 287, 9826, 790, 3715, 837, 32481, 355, 257, 1862, 582, 508, 3544, 47037, 7363, 588, 257, 7614, 764], [23779, 826, 832, 262, 275, 764, 264, 764, 3501, 257, 1263, 3504, 12, 28825, 1068, 366, 4423, 510, 366, 284, 883, 508, 1561, 510, 644, 318, 2147, 517, 621, 734, 3730, 11226, 262, 5968, 503, 8326, 530, 1194, 764], [1169, 716, 12, 37004, 21751, 290, 983, 3350, 1377, 14249, 1681, 290, 262, 287, 320, 4674, 2513, 268, 2592, 1377, 1394, 428, 8468, 10997, 422, 39302, 319, 663, 898, 8571, 270, 764], [13, 764, 764, 857, 884, 257, 3734, 1693, 286, 30443, 278, 345, 287, 663, 995, 290, 477, 1112, 345, 351, 663, 3435, 6, 7747, 837, 922, 290, 2801, 837, 326, 663, 31731, 389, 12086, 691, 355, 281, 706, 28895, 764], [3876, 626, 516, 837, 44570, 290, 837, 3763, 837, 47886, 2646, 764], [6738, 8557, 43446, 284, 45468, 7433, 837, 410, 42816, 338, 16298, 23784, 14309, 689, 287, 257, 11982, 835, 837, 13975, 284, 262, 6833, 7328, 286, 474, 11025, 302, 3919, 343, 764], [37302, 461, 15314, 284, 8006, 257, 1067, 518, 12810, 20105, 27208, 286, 2042, 10997, 287, 262, 3074, 351, 465, 3350, 286, 1729, 12, 529, 669, 290, 257, 39679, 837, 645, 12, 37315, 3164, 764], [1040, 37340, 318, 7411, 764, 991, 837, 1312, 1807, 340, 714, 423, 587, 517, 764], [8117, 373, 640, 319, 326, 1218, 2835, 284, 766, 262, 13284, 1616, 444, 286, 374, 4105, 323, 338, 18560, 286, 18522, 764], [732, 460, 766, 262, 13666, 6225, 837, 290, 356, 1244, 20315, 340, 3360, 837, 475, 428, 318, 991, 257, 3621, 1310, 4286, 837, 925, 416, 6016, 290, 8030, 15625, 351, 257, 1256, 286, 922, 14042, 764], [64, 9815, 290, 28695, 2646, 1377, 530, 326, 20070, 262, 13215, 286, 26444, 837, 290, 6459, 663, 5386, 764], [1169, 835, 763, 381, 5708, 2992, 274, 465, 1842, 329, 6918, 1377, 1111, 20239, 1461, 18556, 290, 262, 29039, 326, 47001, 12780, 355, 1242, 1377, 318, 308, 1638, 813, 17774, 764], [64, 12949, 30669, 764], [64, 24769, 835, 284, 4341, 734, 2250, 764], [8310, 1192, 2522, 2915, 481, 3013, 15799, 26526, 290, 345, 1183, 765, 284, 23905, 606, 764], [30176, 837, 41696, 290, 21104, 15111, 2646, 764, 530, 286, 262, 1266, 286, 262, 614, 764], [64, 1842, 329, 7328, 32481, 832, 1123, 5739, 290, 262, 6980, 318, 11027, 515, 351, 3489, 17696, 837, 7781, 284, 20187, 351, 617, 25103, 1489, 519, 282, 2238, 17825, 764], [400, 11577, 13041, 284, 262, 2344, 351, 281, 17023, 284, 262, 339, 9099, 396, 287, 514, 477, 837, 299, 958, 468, 12006, 428, 6268, 4286, 287, 884, 257, 835, 326, 772, 262, 749, 28858, 1090, 41650, 6281, 351, 1064, 2241, 393, 5223, 16755, 379, 530, 640, 393, 1194, 764], [49123, 281, 450, 273, 4623, 611, 1822, 7153, 1339, 329, 262, 582, 338, 27951, 764], [272, 33903, 13899, 837, 20533, 3807, 326, 318, 355, 10758, 355, 1997, 262, 22041, 468, 1775, 287, 812, 764], [13, 764, 764, 257, 36660, 5761, 837, 351, 4263, 326, 1283, 517, 588, 14851, 40371, 764], [9930, 19813, 1123, 584, 739, 5006, 837, 3714, 1123, 584, 503, 9168, 837, 1742, 12204, 1133, 290, 18099, 1491, 511, 4970, 287, 1336, 10510, 764, 290, 356, 836, 470, 45347, 674, 2951, 329, 257, 2589, 764], [270, 318, 407, 257, 2347, 12, 10728, 9739, 475, 281, 34318, 47112, 2230, 416, 530, 6802, 284, 892, 546, 1194, 764], [69, 430, 6267, 2125, 470, 355, 308, 652, 393, 7952, 764, 475, 287, 663, 1200, 12, 38050, 837, 26435, 436, 10051, 20803, 4732, 837, 340, 460, 307, 655, 355, 23101, 290, 14851, 1377, 772, 28641, 764], [19816, 274, 588, 540, 20929, 837, 47602, 16223, 290, 7720, 837, 290, 3355, 12, 1462, 12, 11930, 21189, 12, 83, 5912, 2647, 284, 7521, 257, 4286, 286, 257, 850, 25584, 326, 318, 379, 1752, 47029, 803, 837, 14397, 837, 43060, 837, 17836, 290, 1464, 1257, 764], [1169, 890, 12, 9521, 5198, 286, 366, 9137, 989, 366, 815, 26231, 597, 13304, 340, 11668, 764, 428, 318, 530, 329, 262, 9337, 764], [58, 64, 60, 21840, 306, 6856, 837, 15347, 16711, 286, 7933, 2763, 4077, 68, 338, 25325, 5337, 764], [29482, 21657, 12543, 329, 477, 764], [272, 23260, 278, 2646, 326, 10969, 281, 2709, 14209, 4205, 286, 262, 1613, 290, 2753, 1626, 663, 5814, 12553, 262, 49937, 444, 286, 6467, 20316, 2641, 336, 764, 4273, 364, 7423, 338, 607, 2781, 496, 13257, 764], [58, 3099, 675, 338, 2095, 60, 271, 523, 44514, 3194, 837, 1231, 257, 12854, 286, 15598, 1483, 837, 290, 523, 40770, 4420, 5447, 837, 326, 790, 584, 2095, 2331, 21655, 290, 739, 15266, 764], [1169, 25556, 40686, 286, 428, 649, 14, 727, 22041, 11497, 26786, 1838, 262, 2646, 257, 44757, 1998, 837, 588, 281, 1468, 1545, 26688, 416, 262, 409, 328, 3976, 286, 640, 764], [1169, 5635, 1074, 468, 35458, 257, 10997, 351, 517, 22051, 621, 867, 837, 645, 1808, 764, 475, 428, 640, 612, 338, 617, 15936, 319, 262, 3869, 764], [4514, 12362, 17082, 837, 428, 2811, 1310, 1621, 318, 41860, 351, 617, 7427, 2223, 16223, 290, 36254, 764], [270, 318, 1290, 422, 262, 5290, 837, 5176, 284, 262, 39867, 2428, 340, 12073, 837, 262, 13289, 286, 20798, 433, 290, 1327, 88, 837, 290, 326, 6393, 3895, 1377, 257, 7709, 1336, 12, 261, 2272, 3344, 764], [64, 2646, 326, 318, 257, 18560, 286, 11542, 287, 281, 23162, 995, 764], [64, 21289, 333, 1346, 474, 6021, 12, 929, 3704, 286, 2223, 1409, 26597, 868, 764], [6988, 12456, 5206, 1856, 83, 34526, 1204, 2641, 257, 530, 12, 3823, 1524, 4803, 287, 7840, 1216, 590, 287, 465, 11648, 284, 307, 290, 284, 423, 837, 3538, 530, 286, 262, 1266, 7328, 286, 262, 614, 764], [64, 43060, 1310, 491, 18137, 837, 288, 391, 774, 10590, 3246, 319, 262, 2354, 351, 257, 27892, 12591, 286, 48376, 35394, 290, 555, 7856, 298, 415, 5928, 3795, 27189, 764], [5661, 9940, 13437, 35661, 837, 15599, 8030, 2646, 17324, 922, 14458, 981, 17774, 351, 663, 21254, 4018, 453, 266, 36053, 475, 14442, 1641], [272, 2883, 1346, 2063, 12, 39289, 28763, 286, 262, 45682, 340, 7199, 10997, 1263, 1730, 319, 8805, 6415, 4675, 764], [270, 2753, 428, 1239, 12, 1571, 10802, 290, 13763, 837, 7584, 257, 1692, 1986, 319, 340, 837, 819, 3369, 10195, 1871, 477, 508, 389, 2151, 284, 340, 290, 772, 21068, 4547, 764], [260, 570, 286, 2046, 743, 307, 1310, 517, 621, 1194, 458, 1436, 286, 22891, 515, 16269, 837, 475, 340, 338, 991, 2495, 25103, 764], [8117, 389, 1661, 618, 257, 24989, 286, 21981, 5341, 588, 281, 7083, 4471, 286, 12615, 416, 281, 18304, 1377, 257, 1310, 1165, 881, 15360, 837, 257, 1178, 1165, 867, 45705, 8188, 1377, 475, 1312, 8288, 663, 2612, 290, 663, 4437, 764], [11545, 2250, 286, 7758, 375, 859, 1512, 10530, 6405, 284, 734, 2250, 286, 44288, 5701, 38520, 837, 611, 262, 4286, 635, 7303, 262, 20256, 286, 1111, 27962, 837, 517, 338, 262, 26246, 764], [5661, 1125, 1924, 837, 866, 12, 1462, 12, 16442, 2646, 318, 5814, 351, 262, 37438, 4203, 286, 28175, 1088, 1468, 2460, 764], [400, 81, 4509, 837, 28695, 290, 3223, 306, 8258, 837, 428, 19376, 20681, 12, 12463, 10715, 2499, 319, 523, 867, 1180, 2974, 326, 340, 407, 691, 27671, 837, 340, 8665, 5100, 1570, 654, 764], [64, 12838, 286, 9961, 290, 15827, 326, 318, 3016, 2818, 287, 663, 25488, 18598, 284, 262, 21593, 286, 530, 582, 338, 22485, 5848, 764], [272, 12191, 286, 4490, 23365, 290, 5046, 326, 338, 587, 4647, 3750, 422, 262, 38914, 7796, 2128, 9539, 286, 289, 31777, 764], [5235, 8327, 306, 15241, 780, 340, 338, 12653, 546, 477, 6982, 286, 1842, 764], [5031, 23532, 4915, 13698, 2058, 6776, 739, 262, 3241, 422, 734, 18981, 287, 3240, 532, 351, 5508, 13289, 290, 12653, 10375, 1022, 262, 3435, 837, 428, 318, 257, 2406, 12, 1659, 12, 496, 1621, 351, 257, 14528, 764], [8117, 468, 587, 881, 15575, 1732, 1871, 9188, 546, 644, 262, 3071, 6194, 4340, 764, 1312, 1975, 262, 3275, 318, 287, 262, 31228, 1058, 262, 5797, 318, 257, 2415, 764], [272, 20050, 2646, 329, 262, 1641, 837, 28297, 290, 13779, 329, 1111, 6490, 290, 3988, 764], [1, 262, 44400, 805, 16951, 3171, 366, 318, 257, 2408, 2646, 284, 13279, 422, 534, 18346, 618, 1755, 8953, 764], [1169, 1218, 6843, 286, 262, 3971, 563, 1787, 353, 2168, 318, 772, 517, 10883, 621, 262, 717, 290, 2391, 262, 1266, 1641, 2646, 286, 262, 614, 764], [3549, 5508, 546, 435, 89, 16288, 338, 4369, 837, 1312, 892, 837, 621, 4173, 271, 764], [1169, 7205, 3436, 318, 2861, 262, 2756, 286, 13938, 764], [272, 6275, 374, 3361, 326, 22103, 1111, 257, 2612, 290, 257, 2000, 764], [3849, 27362, 16067, 439, 12, 1462, 12, 2959, 1765, 439, 290, 21189, 12, 1462, 12, 44579, 837, 1725, 5331, 290, 17605, 261, 389, 257, 5442, 6087, 1377, 475, 25912, 42573, 28052, 705, 445, 10441, 6, 826, 422, 739, 511, 41760, 764], [5661, 318, 257, 23754, 2095, 2050, 837, 257, 12774, 656, 262, 1204, 286, 257, 3716, 582, 764], [320, 8439, 274, 345, 351, 663, 1280, 12, 1631, 1108, 290, 24072, 764], [5661, 2125, 470, 257, 8689, 2646, 1377, 1312, 836, 470, 760, 611, 340, 338, 1744, 284, 787, 257, 8689, 2646, 546, 384, 457, 1491, 1367, 400, 837, 996, 1312, 1101, 1654, 617, 481, 1949, 1377, 475, 340, 338, 355, 1969, 355, 2687, 468, 28765, 284, 1282, 764], [1820, 11752, 616, 837, 318, 428, 281, 800, 36274, 803, 837, 5186, 3807, 764], [1169, 734, 5983, 442, 3361, 15394, 517, 31068, 351, 511, 7205, 621, 2046, 12, 65, 630, 722, 10963, 36646, 351, 511, 8033, 764, 764, 764], [771, 283, 2753, 257, 845, 1280, 12, 14543, 3164, 284, 428, 8564, 2587, 837, 4478, 8036, 1630, 837, 1111, 22632, 290, 287, 262, 3597, 764], [12081, 1752, 20039, 764], [65, 6950, 494, 290, 256, 929, 330, 318, 523, 2060, 12, 14543, 306, 27939, 837, 340, 7584, 1290, 517, 23895, 43014, 284, 10195, 764], [568, 867, 43014, 588, 428, 45077, 577, 4158, 28849, 393, 1976, 15746, 6701, 83, 1272, 286, 663, 50178, 837, 475, 1641, 33099, 11298, 257, 4071, 6979, 329, 42880, 8589, 278, 32521, 414, 764], [1169, 3350, 318, 42096, 6275, 290, 18397, 764], [8499, 1642, 1811, 35030, 286, 584, 8786, 6, 670, 837, 3211, 268, 666, 12, 5171, 18425, 3437, 22037, 29206, 726, 272, 1379, 2317, 281, 2656, 3513, 286, 257, 7744, 2614, 2426, 764], [1169, 2646, 318, 32258, 16425, 837, 290, 262, 13289, 286, 262, 1862, 1938, 389, 15950, 17101, 764], [361, 340, 2331, 588, 257, 4159, 20820, 326, 663, 384, 457, 84, 11286, 3699, 3491, 318, 1862, 1576, 284, 307, 262, 1729, 11286, 3699, 26479, 338, 3367, 837, 517, 8082, 991, 389, 262, 1598, 12, 18834, 10758, 1108, 290, 5897, 21296, 351, 543, 8674, 290, 3437, 1011, 319, 1204, 338, 18039, 2683, 764], [64, 2960, 290, 1690, 13899, 11648, 546, 257, 28145, 721, 455, 282, 4928, 287, 288, 7826, 326, 11156, 829, 281, 15962, 26688, 2156, 1123, 614, 284, 19437, 17366, 656, 11969, 2594, 764], [69, 504, 286, 262, 15108, 15599, 8855, 905, 481, 307, 287, 1175, 400, 519, 9538, 2162, 1854, 761, 407, 6646, 4174, 764], [19419, 12600, 278, 284, 8718, 45693, 837, 1312, 460, 1181, 326, 25847, 474, 408, 3970, 2876, 259, 743, 307, 262, 1266, 976, 12, 8044, 19661, 1312, 423, 1775, 764], [77, 16617, 14802, 306, 23153, 82, 810, 1178, 45630, 272, 7328, 16498, 284, 39130, 1377, 656, 262, 995, 286, 4915, 2473, 594, 290, 33985, 764, 764, 764], [403, 2339, 262, 24480, 803, 277, 9278, 36145, 992, 416, 884, 705, 14150, 12, 14108, 944, 12, 64, 12, 34191, 12, 31629, 12, 3937, 16377, 6, 6918, 355, 1204, 318, 4950, 290, 474, 461, 672, 262, 31866, 837, 262, 13791, 6516, 318, 5508, 1576, 284, 10129, 262, 5885, 286, 2911, 287, 257, 385, 36297, 764], [64, 16739, 2868, 12409, 1842, 1621, 764], [10197, 883, 508, 561, 588, 284, 6735, 262, 2646, 15828, 815, 1064, 881, 284, 35024, 290, 4384, 764], [5661, 318, 3608, 837, 29395, 3404, 837, 3492, 284, 627, 24421, 262, 24613, 286, 281, 5386, 326, 18297, 262, 3931, 2512, 30181, 764], [1169, 3807, 318, 1336, 286, 3734, 13289, 837, 2957, 416, 474, 577, 69, 275, 959, 65, 488, 1754, 355, 1449, 21474, 290, 937, 3970, 7245, 571, 33945, 84, 355, 932, 1734, 356, 47709, 837, 465, 3656, 764], [64, 3144, 39438, 3272, 12, 30844, 10997, 286, 33558, 764], [10757, 5482, 33743, 20393, 530, 286, 465, 24348, 9176, 287, 812, 290, 285, 624, 474, 7928, 3607, 465, 1266, 3807, 2854, 1201, 837, 880, 837, 2854, 764], [1169, 3807, 2125, 470, 1464, 2562, 284, 804, 379, 764, 475, 611, 340, 318, 5600, 257, 7077, 286, 1242, 284, 4079, 1204, 837, 621, 443, 394, 468, 2727, 257, 4958, 913, 3704, 286, 1242, 4592, 826, 994, 764], [270, 338, 685, 1173, 979, 338, 60, 1266, 670, 1865, 837, 428, 2576, 12, 8580, 508, 29093, 5804, 673, 460, 36162, 262, 995, 338, 24672, 351, 7770, 922, 481, 764], [8929, 8091, 389, 262, 23754, 13289, 416, 33826, 8803, 22802, 647, 837, 355, 262, 6994, 16687, 837, 290, 299, 6696, 11193, 355, 410, 42816, 1067, 13929, 829, 837, 262, 29303, 13766, 1664, 4706, 764], [58, 4919, 446, 60, 523, 922, 355, 443, 261, 2318, 9319, 764, 764, 764, 326, 339, 8941, 2331, 284, 307, 7205, 764], [272, 28996, 13309, 837, 1474, 12, 9866, 12239, 764], [16668, 1504, 12121, 23689, 837, 543, 3011, 739, 674, 4168, 2391, 416, 12538, 262, 4523, 1627, 764], [1525, 2263, 9739, 9975, 2426, 2300, 290, 3501, 340, 14733, 290, 745, 570, 3883, 837, 8295, 2962, 4329, 1111, 12500, 12, 65, 436, 4420, 8258, 290, 24949, 306, 31193, 764], [270, 338, 257, 48666, 7277, 290, 47188, 605, 5022, 286, 4847, 764], [7266, 40099, 837, 1117, 12464, 837, 8668, 290, 38273, 837, 262, 19132, 4701, 318, 257, 27939, 670, 286, 15632, 764]]}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "dataset = wrapper(tokenizer_hf, rt, 64, 12, cache_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property meta_file omited in config. Assuming default: \"meta.pkl\"\n"
     ]
    }
   ],
   "source": [
    "from qtransform.tokenizer import TikTokenizer\n",
    "tiktokenizer = TikTokenizer({\"encoding\": \"gpt2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'PaddingStrategy'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils.generic import PaddingStrategy\n",
    "PaddingStrategy.MAX_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktokenizer.decode(tiktokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_eval = datasets.load_dataset(\"rotten_tomatoes\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_eval.select_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    EVAL: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetDict({\"EVAL\": rt_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rt.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117e7dce1d1c443f90f85aa0e7d878bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d217e1146d78436a9cf8c6815b2bb0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d03997ba6cf422bbdf8dbc8563473df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.formatting.formatting.LazyBatch'> 1000\n",
      "<class 'datasets.formatting.formatting.LazyBatch'> 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.map(\n",
    "    lambda x: print(f'{type(x)} {len(x[\"text\"])}'),\n",
    "    batched=True,\n",
    "    batch_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb46ac9e26ee49fea41cda3902bcfe7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d576e3df6d42cbb1a3030c6119c77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d85ced171e4bb8b8a8577880702ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenizer_function(batch):\n",
    "    return {\"input_ids\": [tiktokenizer.encode(x) for x in batch[\"text\"]]}\n",
    "text_column_name = rt[list(rt.keys())[0]].column_names[0] \n",
    "batch_size = 300\n",
    "rt_test = rt.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns=[text_column_name],\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers.models.gpt2.tokenization_gpt2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1142ef0b6534433c956e146ce7406c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094175b245c0449faeda0b1a6e0498e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81baf69e725246f39c0c7cd5d95918b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "rt = datasets.load_dataset(\"rotten_tomatoes\")\n",
    "tokenizer_hf = transformers.AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "dataset = wrapper(tokenizer_hf, rt, 128, 12, cache_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .effective but too-tepid biopicif you sometimes like to go to the movies to have fun , wasabi is'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['textnew', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['textnew', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['textnew', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.rename_columns({\"text\": \"textnew\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt[\"train\"].column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7677\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt[\"train\"].train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchEncoding("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_dataset(\"rotten_tomatoes\") is rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'test']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {}\n",
    "import requests\n",
    "API_URL = \"https://datasets-server.huggingface.co/splits?dataset=ibm/duorc&config=SelfRC\"\n",
    "response = requests.get(API_URL, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(\"did not work\")\n",
    "data = response.json()\n",
    "[split[\"split\"] for split in data[\"splits\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': 'ibm/duorc', 'config': 'ParaphraseRC', 'split': 'train'},\n",
       " {'dataset': 'ibm/duorc', 'config': 'ParaphraseRC', 'split': 'validation'},\n",
       " {'dataset': 'ibm/duorc', 'config': 'ParaphraseRC', 'split': 'test'},\n",
       " {'dataset': 'ibm/duorc', 'config': 'SelfRC', 'split': 'train'},\n",
       " {'dataset': 'ibm/duorc', 'config': 'SelfRC', 'split': 'validation'},\n",
       " {'dataset': 'ibm/duorc', 'config': 'SelfRC', 'split': 'test'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid value for train_size: test of type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_dataset.py:4409\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[0;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[1;32m   4403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan the number of samples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or a float in the (0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4406\u001b[0m     )\n\u001b[1;32m   4408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_size, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[0;32m-> 4409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for train_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(test_size, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[1;32m   4411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for test_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(test_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid value for train_size: test of type <class 'str'>"
     ]
    }
   ],
   "source": [
    "rt[\"train\"].train_test_split(0.2, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset.map() got an unexpected keyword argument 'cache_file_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_eval \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_hf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrt_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(tokenizer, dataset, block_size, batch_size, cache_file_prefix)\u001b[0m\n\u001b[1;32m     66\u001b[0m     lm_datasets \u001b[38;5;241m=\u001b[39m tokenized_datasets\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     67\u001b[0m         group_texts,\n\u001b[1;32m     68\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrouping texts in chunks of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m#cache_file_names=cache_file_names\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lm_datasets\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mwrapper.<locals>.map_dataset\u001b[0;34m(tokenizer, dataset, block_size, batch_size, cache_file_prefix)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[text_column_name])\n\u001b[0;32m---> 36\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning tokenizer on dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_datasets)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset.map() got an unexpected keyword argument 'cache_file_names'"
     ]
    }
   ],
   "source": [
    "dataset_eval = wrapper(tokenizer_hf, rt_eval, 64, 12, cache_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['text', 'label'],\n",
       " 'validation': ['text', 'label'],\n",
       " 'test': ['text', 'label']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(rt, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_tokenized = datasets.Dataset.from_file(\"/home/mabot004/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-tokenized-train.arrow\")\n",
    "rt_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '/home/a/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-tokenized-train.arrow'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/a/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-tokenized-train.arrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_dataset.py:762\u001b[0m, in \u001b[0;36mDataset.from_file\u001b[0;34m(cls, filename, info, split, indices_filename, in_memory)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     in_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    744\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Instantiate a Dataset backed by an Arrow table at filename.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m        [`Dataset`]\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indices_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         indices_pa_table \u001b[38;5;241m=\u001b[39m ArrowReader\u001b[38;5;241m.\u001b[39mread_table(indices_filename, in_memory\u001b[38;5;241m=\u001b[39min_memory)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:357\u001b[0m, in \u001b[0;36mArrowReader.read_table\u001b[0;34m(filename, in_memory)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03mRead table from file.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    pyarrow.Table\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/table.py:1059\u001b[0m, in \u001b[0;36mMemoryMappedTable.from_file\u001b[0;34m(cls, filename, replays)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, replays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1059\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_mapped_arrow_table_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_replays(table, replays)\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(table, filename, replays)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/table.py:65\u001b[0m, in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_mapped_arrow_table_from_file\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[0;32m---> 65\u001b[0m     opened_stream \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_mapped_record_batch_reader_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m opened_stream\u001b[38;5;241m.\u001b[39mread_all()\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/table.py:50\u001b[0m, in \u001b[0;36m_memory_mapped_record_batch_reader_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_mapped_record_batch_reader_from_file\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mRecordBatchStreamReader:\n\u001b[0;32m---> 50\u001b[0m     memory_mapped_stream \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mipc\u001b[38;5;241m.\u001b[39mopen_stream(memory_mapped_stream)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/io.pxi:1009\u001b[0m, in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/io.pxi:956\u001b[0m, in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/error.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/home/a/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-tokenized-train.arrow'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "datasets.Dataset.from_file(\"/home/a/.cache/huggingface/datasets/qtransform_tokenized/rotten_tomatoes/cache-gpt2-128-tokenized-train.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3335\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 417\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 421\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" entertainment and education .perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .take care of my cat offers a refreshingly\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[\"train\"][3][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3335\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6888e9c2b0b498985f8a7fe1cd4c960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "def gen_samples():\n",
    "    for sample in dataset[\"train\"]:\n",
    "        yield {\"input_ids\": sample[\"input_ids\"]}\n",
    "test = DatasetDict({\"train\": datasets.Dataset.from_generator(gen_samples)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset[\"train\"], collate_fn = collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_nocollate = DataLoader(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      " is\n",
      " a\n",
      " entertainment\n",
      " different\n",
      ".\n",
      " masterpiece\n",
      " independent\n",
      " departure\n",
      " 95\n",
      " someone\n"
     ]
    }
   ],
   "source": [
    "#collate_fn necessary for huggingface datasets\n",
    "for i, data in enumerate(dataloader_nocollate):\n",
    "    ids = data[\"input_ids\"][0]\n",
    "    print(tokenizer_hf.decode(ids))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.the gorgeously elaborate continuation of \" the lord of the rings \" trilogy\n",
      " is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j. r. r. tolkien's middle-earth.effective but too-tepid biopicif you sometimes like to go to the movies to have fun, wasabi is\n",
      " a good place to start.emerges as something rare, an issue movie that's so honest and keenly observed that it doesn't feel like one.the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game.offers that rare combination of\n",
      " entertainment and education.perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions.steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it. but he somehow pulls it off.take care of my cat offers a refreshingly\n",
      " different slice of asian cinema.this is a film well worth seeing, talking and singing heads and all.what really surprises about wisegirls is its low-key quality and genuine tenderness.( wendigo is ) why we go to the cinema : to be fed through the eye, the heart, the mind\n",
      ".one of the greatest family-oriented, fantasy-adventure movies ever.ultimately, it ponders the reasons we need stories so much.an utterly compelling 'who wrote it' in which the reputation of the most famous author who ever lived comes into question.illuminating if overly talky documentary.a\n",
      " masterpiece four years in the making.the movie's ripe, enrapturing beauty will tempt those willing to probe its inscrutable mysteries.offers a breath of the fresh air of true sophistication.a thoughtful, provocative, insistently humanizing film.with a cast that includes some of the top actors working in\n",
      " independent film, lovely & amazing involves us because it is so incisive, so bleakly amusing about how we go about our lives.a disturbing and frighteningly evocative assembly of imagery and hypnotic music composed by philip glass.not for everyone, but for those with whom it will connect, it's a nice\n",
      " departure from standard moviegoing fare.scores a few points for doing what it does with a dedicated and good-hearted professionalism.occasionally melodramatic, it's also extremely effective.spiderman rocksan idealistic love story that brings out the latent 15-year-old romantic in everyone.at about\n",
      " 95 minutes, treasure planet maintains a brisk pace as it races through the familiar story. however, it lacks grandeur and that epic quality often associated with stevenson's tale as well as with earlier disney efforts.it helps that lil bow wow... tones down his pint-sized gangsta act to play\n",
      " someone who resembles a real kid.guaranteed to move anyone who ever shook, rattled, or rolled.a masterful film from a master filmmaker, unique in its deceptive grimness, compelling in its fatalist worldview.light, cute and forgettable.if there's a way to effectively teach kids about the dangers\n"
     ]
    }
   ],
   "source": [
    "tokenizer_hf.pad_token = tokenizer_hf.eos_token\n",
    "#iterate sequentially through dataset\n",
    "for i, data in enumerate(dataloader):\n",
    "    ids = data[\"input_ids\"][0]\n",
    "    print(tokenizer_hf.decode(ids))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_ids = list()\n",
    "for i in range(len(train[\"input_ids\"])):\n",
    "    len_ids.append(len(train[\"input_ids\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301966"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import fields, InitVar\n",
    "from typing import ClassVar, List, Dict\n",
    "\n",
    "class DatasetRunType(IntEnum):\n",
    "    TRAIN = 0\n",
    "    EVAL = 1\n",
    "    BENCH = 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetSplits:\n",
    "    \"\"\"\n",
    "        Dataclass containing the datasets for training, eval, testing, benchmark along with the name of the dataset.\n",
    "        After construction, a simple type check is done with the __post_init__ hook.\n",
    "    \"\"\"\n",
    "    splits: InitVar[Dict[DatasetRunType, Dataset]] = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.splits = {\n",
    "            DatasetRunType.TRAIN: None,\n",
    "            DatasetRunType.EVAL: None,\n",
    "            DatasetRunType.BENCH: None\n",
    "        }\n",
    "    \n",
    "    # make class subscritable aka: self['train'] works\n",
    "    def __getitem__(self, item):\n",
    "        return self.splits[item]\n",
    "    \n",
    "    def __setitem__(self, index, item):\n",
    "        self.splits[index] = item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'something'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDatasetRunType\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msomething\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m DatasetRunType\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/enum.py:440\u001b[0m, in \u001b[0;36mEnumMeta.__getitem__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name):\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_member_map_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'something'"
     ]
    }
   ],
   "source": [
    "getattr(DatasetRunType, 'something', None) in DatasetRunType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = dict( \n",
    "  names = dict( #mapping of our split names to huggingface split names\n",
    "    train = \"train\",\n",
    "    eval =  \"validation\",\n",
    "    bench = \"test\"),\n",
    "  sizes = dict(\n",
    "    train = 0.9, \n",
    "    eval =  0.05,\n",
    "    bench = 0.05,)\n",
    ")\n",
    "def get_split_mapping(split: DatasetRunType) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "        Get the mapping of a split as specified in the untokenized dataset config.\n",
    "        It includes the name of the original dataset split and the size if the split does not exist.\n",
    "    \"\"\"\n",
    "    name = splits[\"names\"].get(split.name.lower())\n",
    "    size = splits[\"sizes\"].get(split.name.lower())\n",
    "    return (name, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('validation', 0.05)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split_mapping(DatasetRunType.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DatasetRunType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(DatasetRunType.TRAIN.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = DatasetSplits()\n",
    "test[DatasetRunType.EVAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetSplits' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetSplits' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "test.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[DatasetRunType.EVAL] = \"1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDatasetRunType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "DatasetRunType.EVAL.name.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = DatasetSplits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"bruh\". Should be one of ['train', 'validation', 'test'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrotten_tomatoes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbruh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/load.py:2166\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n\u001b[0;32m-> 2166\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;66;03m# To avoid issuing the same warning twice\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:1190\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1187\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1202\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/utils/py_utils.py:456\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Singleton\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m disable_tqdm \u001b[38;5;241m=\u001b[39m disable_tqdm \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    459\u001b[0m iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_struct\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:1220\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m-> 1220\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/builder.py:1294\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_legacy_cache():\n\u001b[1;32m   1293\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m-> 1294\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:240\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    221\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    225\u001b[0m ):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[1;32m    242\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:213\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     files \u001b[38;5;241m=\u001b[39m file_instructions\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:130\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m ReadInstruction\u001b[38;5;241m.\u001b[39mfrom_spec(instruction)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m \u001b[43minstruction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m file_instructions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:653\u001b[0m, in \u001b[0;36mReadInstruction.to_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    642\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:653\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    642\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/datasets-2.14.7-py3.10.egg/datasets/arrow_reader.py:465\u001b[0m, in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    463\u001b[0m split \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39msplitname\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown split \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    466\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m name2len[split]\n\u001b[1;32m    467\u001b[0m from_ \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39mfrom_\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"bruh\". Should be one of ['train', 'validation', 'test']."
     ]
    }
   ],
   "source": [
    "load_dataset(\"rotten_tomatoes\", split=\"bruh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = \"~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/\"\n",
    "cache_file_prefix = \"cache-gpt2-128\"\n",
    "FILE_SUFFIX = \"-tokenized.arrow\"\n",
    "import os\n",
    "def check_tokenized(cache_dir, cache_file_prefix, FILE_SUFFIX) -> dict[str,  bool]:\n",
    "    splits = [split for split in DatasetRunType]\n",
    "    filepath_splits =  {split: os.path.join(cache_dir, cache_file_prefix + \"-\" + split.name.lower() +FILE_SUFFIX) for split in splits}\n",
    "    status_splits = {split:{\"exists\": os.path.exists(filepath_splits[split]), \"filepath\": filepath_splits[split]} for split in splits}\n",
    "    return status_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<DatasetRunType.TRAIN: 0>: {'exists': False,\n",
       "  'filepath': '~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-train-tokenized.arrow'},\n",
       " <DatasetRunType.EVAL: 1>: {'exists': False,\n",
       "  'filepath': '~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-eval-tokenized.arrow'},\n",
       " <DatasetRunType.BENCH: 2>: {'exists': False,\n",
       "  'filepath': '~/.cache/huggingface/datasets/qtransform_tokenized/wikitext_wikitext-2-raw-v1/cache-gpt2-128-bench-tokenized.arrow'}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_tokenized(cache_dir, cache_file_prefix, FILE_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'input_ids': tensor([[ 8667,  7328,   837,   475,   428,  1948,  1255,   318,  6165,  2714,\n",
    "           736,   422,   852,  1223,  3744,   764,  1662,   257, 31240,   318,\n",
    "         22532,  4249,   257, 43973,  1364,  5576,  1698,   764,   292,   307,\n",
    "         21013,   663,  3670,   837,   428, 23241,    12,  1485,    12,  4111,\n",
    "           279,   733,   293,   318,  6165,   355, 11123,   355,   262,  3013,\n",
    "         33498,  9664,  2705,   877,  6842,   764,  1078,  1791,    82,   416,\n",
    "           428, 34549,  2646,   284, 23553,   257,  3275,   389,   523,  4334,\n",
    "            12, 13638,   326,   484,  2427,   279, 13929,   417,   262,  5386,\n",
    "           764,   270,   477,  5300,   588,   257, 40689,    88, 21015, 17548,\n",
    "          3750, 33437,  2642,   764,    77,   712,   516, 14608,    82,   389,\n",
    "           407, 17774,   764,  1416,   669,  2771,  1595,   470,  1577,   514,\n",
    "           257,  2095,  2861,  3501,   257, 12270,   546,   764,    64, 21104,\n",
    "           925,  3704,   286,  7379,   963,   540,  1454,   626]]), 'labels': tensor([[ 8667,  7328,   837,   475,   428,  1948,  1255,   318,  6165,  2714,\n",
    "           736,   422,   852,  1223,  3744,   764,  1662,   257, 31240,   318,\n",
    "         22532,  4249,   257, 43973,  1364,  5576,  1698,   764,   292,   307,\n",
    "         21013,   663,  3670,   837,   428, 23241,    12,  1485,    12,  4111,\n",
    "           279,   733,   293,   318,  6165,   355, 11123,   355,   262,  3013,\n",
    "         33498,  9664,  2705,   877,  6842,   764,  1078,  1791,    82,   416,\n",
    "           428, 34549,  2646,   284, 23553,   257,  3275,   389,   523,  4334,\n",
    "            12, 13638,   326,   484,  2427,   279, 13929,   417,   262,  5386,\n",
    "           764,   270,   477,  5300,   588,   257, 40689,    88, 21015, 17548,\n",
    "          3750, 33437,  2642,   764,    77,   712,   516, 14608,    82,   389,\n",
    "           407, 17774,   764,  1416,   669,  2771,  1595,   470,  1577,   514,\n",
    "           257,  2095,  2861,  3501,   257, 12270,   546,   764,    64, 21104,\n",
    "           925,  3704,   286,  7379,   963,   540,  1454,   626]]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quantize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_roneneldan__TinyStories_2024-03-28_11:10:56__epoch:2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quantize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint_refactor = torch.load(\"/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_roneneldan__TinyStories_2024-03-28_11:10:56__epoch:2\")\n",
    "checkpoint = torch.load(\"/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_roneneldan__TinyStories_2024-03-28_12:31:27__epoch:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.1, 'bias': True, 'block_size': 128, 'vocab_size': 50257, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNormTranspose', 'flash': False, 'single_output': False, 'use_weight_tying': True, 'shift_targets': True}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.get(\"model_cfg\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.v = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Foo:\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        \n",
    "def my_new_method(self):\n",
    "    print(\"self.v =\", self.v)\n",
    "    \n",
    "def other(self, value, iterations, verbose = False):\n",
    "    return self.v + value * iterations\n",
    "\n",
    "\n",
    "test = Foo(5)\n",
    "\n",
    "setattr(Foo, 'print_v', my_new_method)\n",
    "setattr(Foo, 'other', other)\n",
    "test.print_v()\n",
    "Foo(10).other(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Foo"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Foo"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n"
     ]
    }
   ],
   "source": [
    "def something(self):\n",
    "    print(\"something\")\n",
    "setattr(test.__class__, \"something\", something)\n",
    "test.something()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ec4a7e2d27417da2382605b3055a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a1bf61109540e5a2343f484ce9dc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef79b642ae24e9c80804ef7e0caa5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a14de8d7b14ac099fa42c79e9ba9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "PATH = \"/home/mabot004/.qtransform/datasets/huggingface/wikitext/wikitext-2-raw-v1/\"\n",
    "data_files = {'train': PATH+'cache-gpt2-128-BENCH-grouped.arrow', 'test': PATH+'cache-gpt2-128-EVAL-grouped.arrow'}\n",
    "test = load_dataset(\"arrow\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[796,\n",
       " 5199,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 796,\n",
       " 220,\n",
       " 198,\n",
       " 5199,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 318,\n",
       " 281,\n",
       " 3594,\n",
       " 2646,\n",
       " 837,\n",
       " 5581,\n",
       " 290,\n",
       " 21421,\n",
       " 8674,\n",
       " 764,\n",
       " 679,\n",
       " 550,\n",
       " 257,\n",
       " 8319,\n",
       " 2488,\n",
       " 12,\n",
       " 31,\n",
       " 20495,\n",
       " 2597,\n",
       " 319,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 383,\n",
       " 3941,\n",
       " 287,\n",
       " 4751,\n",
       " 764,\n",
       " 770,\n",
       " 373,\n",
       " 3940,\n",
       " 416,\n",
       " 257,\n",
       " 20495,\n",
       " 2597,\n",
       " 287,\n",
       " 262,\n",
       " 711,\n",
       " 2332,\n",
       " 684,\n",
       " 3194,\n",
       " 416,\n",
       " 11288,\n",
       " 37072,\n",
       " 837,\n",
       " 543,\n",
       " 373,\n",
       " 6157,\n",
       " 287,\n",
       " 5878,\n",
       " 379,\n",
       " 262,\n",
       " 8111,\n",
       " 3078,\n",
       " 15752,\n",
       " 764,\n",
       " 679,\n",
       " 550,\n",
       " 257,\n",
       " 8319,\n",
       " 2597,\n",
       " 287,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 8974,\n",
       " 1757,\n",
       " 1024,\n",
       " 276,\n",
       " 287,\n",
       " 6244,\n",
       " 764,\n",
       " 554,\n",
       " 5472,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 11406,\n",
       " 257,\n",
       " 2597,\n",
       " 355,\n",
       " 366,\n",
       " 13854,\n",
       " 366,\n",
       " 287,\n",
       " 262,\n",
       " 4471,\n",
       " 366,\n",
       " 29345,\n",
       " 705,\n",
       " 82,\n",
       " 8362,\n",
       " 366,\n",
       " 286,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 383,\n",
       " 5882,\n",
       " 31623,\n",
       " 2162,\n",
       " 339,\n",
       " 31636,\n",
       " 7848,\n",
       " 10544,\n",
       " 2940,\n",
       " 13535,\n",
       " 290,\n",
       " 20893,\n",
       " 12806,\n",
       " 72,\n",
       " 764,\n",
       " 679,\n",
       " 373,\n",
       " 3350,\n",
       " 287]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "tokenizer.decode(test[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.memmap('/home/mabot004/.qtransform/datasets/files/tiny_shakespeare/cache-gpt2-128-EVAL-.bin', mode='r', dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hat's but a cavil: he is old, I young.\\n\\nGREMIO:\\nAnd may not young men die, as well as old?\\n\\nBAPTISTA fearful head is on!\\n\\nTYRREL:\\nThe tyrannous and bloody deed is done.\\nThe most arch of piteous massac.\\n\\nKING RICHARD II:\\nGive me the crown. Here, cousin, seize the crown;\\nHere cousin:\\nOn this\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_stories = load_dataset(\"roneneldan/TinyStories\")\n",
    "tiny_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 09:33:05.758918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 09:33:09.487400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 09:33:11.207566: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-04-04 09:33:11.207641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7431 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB MIG 1g.10gb, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "2024-04-04 09:33:24.702541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/home/mabot004/eki-transformer-dev/qtransform/outputs/models/BENCH_gpt2_ReBNT_tiny_roneneldan__TinyStories_2024-04-09_09:19:29__epoch:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CHECKPOINT', 'from_file': {'model_dir': 'models', 'filename': None}, 'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.1, 'bias': True, 'block_size': 128, 'vocab_size': 50257, 'transformer_active_func': 'ReLU', 'norm_layer': 'BatchNormTranspose', 'flash': False, 'single_output': False, 'use_weight_tying': True, 'shift_targets': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"model_cfg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singleton testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    _instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        raise RuntimeError('Call instance() instead')\n",
    "\n",
    "    @classmethod\n",
    "    def instance(cls):\n",
    "        if cls._instance is None:\n",
    "            print('Creating new instance')\n",
    "            cls._instance = cls.__new__(cls)\n",
    "            # Put any initialization here.\n",
    "        return cls._instance\n",
    "    def method():\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new instance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Logger at 0x7f7a4a7dd120>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logger.instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Logger at 0x7f7a4a7deb00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logger._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "Logger.method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "class SingletonMeta(type):\n",
    "    \"\"\"\n",
    "    The Singleton class can be implemented in different ways in Python. Some\n",
    "    possible methods include: base class, decorator, metaclass. We will use the\n",
    "    metaclass because it is best suited for this purpose.\n",
    "    \"\"\"\n",
    "\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Possible changes to the value of the `__init__` argument do not affect\n",
    "        the returned instance.\n",
    "        \"\"\"\n",
    "        if cls not in cls._instances:\n",
    "            instance = super().__call__(*args, **kwargs)\n",
    "            cls._instances[cls] = instance\n",
    "        return cls._instances[cls]\n",
    "\n",
    "    \n",
    "class BaseClass():\n",
    "    #called only once\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        \n",
    "    def __call__(self, args):\n",
    "        self.args = args\n",
    "    \n",
    "\n",
    "class Singleton(BaseClass, metaclass=SingletonMeta):\n",
    "    def some_business_logic(self):\n",
    "        \"\"\"\n",
    "        Finally, any singleton should define some business logic, which can be\n",
    "        executed on its instance.\n",
    "        \"\"\"\n",
    "        print(\"something\")\n",
    "\n",
    "class OtherSingleton(BaseClass, metaclass=SingletonMeta):\n",
    "    def other(self):\n",
    "        print(self)\n",
    "\n",
    "#args are not overwritten as only one instance is created\n",
    "#not the best when changing args though\n",
    "s1 = Singleton(1)\n",
    "s2 = Singleton(2)\n",
    "s3 = OtherSingleton(3)\n",
    "s4 = OtherSingleton(4)\n",
    "print(s1 is s2)\n",
    "print(s3 is s4)\n",
    "print(s1 is s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Singleton' object has no attribute '_instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ms1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instance\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Singleton' object has no attribute '_instance'"
     ]
    }
   ],
   "source": [
    "s1._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type 'torch.device' is not an acceptable base type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDeviceSingleton\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mdevice, metaclass\u001b[38;5;241m=\u001b[39mSingletonMeta):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: type 'torch.device' is not an acceptable base type"
     ]
    }
   ],
   "source": [
    "class DeviceSingleton(torch.device, metaclass=SingletonMeta):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "from logging import getLogger\n",
    "\n",
    "log = getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class FromFile():\n",
    "    \"\"\"\n",
    "    Keep args for composing checkpoint/ onnx model path inside of dataclass to avoid dict checks in multiple\n",
    "    places.\n",
    "    \"\"\"\n",
    "    filename: str\n",
    "    model_dir: str\n",
    "    _filename: str = field(init=False, repr = False)\n",
    "    _model_dir: str = field(init=False, repr = False)\n",
    "\n",
    "    def __init__(self, filename: str, model_dir: str):\n",
    "        self.model_dir = model_dir\n",
    "        self.filename = filename\n",
    "    \n",
    "    @property\n",
    "    def filename(self):\n",
    "        return self._filename\n",
    "    \n",
    "    @filename.setter\n",
    "    def filename(self, filename: str):\n",
    "        assert isinstance(filename, str), f'{value} not a valid filename string.'\n",
    "        #make sure that model_dir is never none and filename always is a filename, not an absolute path\n",
    "        model_dir, filename = os.path.split(filename)\n",
    "        if len(model_dir) > 0:\n",
    "            if isinstance(self.model_dir, str) and len(self.model_dir) > 0:\n",
    "                log.warning(f'Overwriting model_dir with path from filename')\n",
    "            self.model_dir = model_dir\n",
    "        self._filename = filename\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return self._model_dir\n",
    "    \n",
    "    @model_dir.setter\n",
    "    def model_dir(self, model_dir: str):\n",
    "        if not isinstance(model_dir, str):\n",
    "            self._model_dir = \"get_default_chkpt_folder()\"\n",
    "            log.warning(f'invalid type for model_dir, assuming default checkpoint {self.model_dir}')\n",
    "        elif not os.path.isabs(model_dir):\n",
    "            #outputs are stored in <current directory>/outputs/<checkpoint_dir>\n",
    "            try:\n",
    "                model_dir =  os.path.join(\"hydra\", \"outputs\", model_dir) #os.path.join(hydra.core.hydra_config.HydraConfig.get().runtime.cwd, \"outputs\", model_dir)\n",
    "            except:\n",
    "                log.debug(f'Could not get cwd from hydra. Reason: ', exc_info=True)\n",
    "                log.debug(f'Using os.getcwd')\n",
    "                model_dir = os.getcwd()\n",
    "        else:\n",
    "            self._model_dir = model_dir\n",
    "\n",
    "    def get_filepath(self) -> str:\n",
    "        return os.path.join(self.model_dir, self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid type for model_dir, assuming default checkpoint get_default_chkpt_folder()\n",
      "Overwriting model_dir with path from filename\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FromFile(filename='None_roneneldan__TinyStories_2024-04-23_10:17:50__epoch:2', model_dir='/home/mabot004/qtransform/utils/checkpoint_dir')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FromFile(model_dir= None, filename=\"/home/mabot004/qtransform/utils/checkpoint_dir/None_roneneldan__TinyStories_2024-04-23_10:17:50__epoch:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qtransform.utils.helper import FromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FromFile(filename='something.txt', model_dir='/home/mabot004/')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FromFile(filename=\"something.txt\", model_dir=\"/home/mabot004/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FromFile(filename=\"something.txt\", model_dir=\"/home/mabot004/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"/home/mabot004/logging_filename.txt\", level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def create_leaf_module(func: Callable) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Function that creates a torch Module from a torch operator (such as neg, sum, pow etc.) in order to avoid Proxy\n",
    "    errors when trying to symbolically trace a module for PTQ. Every model that should be used for PTQ needs to have this function instead of\n",
    "    calling the torch operators directly.\n",
    "\n",
    "    Arguments:\n",
    "        func: torch function\n",
    "    Returns:\n",
    "        torch.nn.Module wrapping the function\n",
    "    \"\"\"\n",
    "    assert getattr(torch, func.__name__, None) is not None, f'Function {func.__name__} is not a torch function.'\n",
    "    leaf_module: torch.nn.Module = type(\n",
    "                        \"Torch\" + func.__name__ + \"LeafModule\",\n",
    "                        (torch.nn.Module, ), \n",
    "                        {\"_is_leaf_module\": True}\n",
    "                    )()\n",
    "    \n",
    "    def forward(x):\n",
    "        return func(x)\n",
    "    #add forward function\n",
    "    setattr(leaf_module, \"forward\", func)\n",
    "    return leaf_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summe = create_leaf_module(torch.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.9414)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summe(torch.randint(10, (2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
