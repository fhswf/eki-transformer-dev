{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General testing notebook for qtransform and quantization\n",
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from logging import getLogger\n",
    "import os\n",
    "from omegaconf import DictConfig\n",
    "from brevitas import nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with dataclasses and python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod, ABC\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\"\n",
    "\n",
    "class Foo(ABC):\n",
    "    def __init__(self, encoding: str):\n",
    "        self.metadata: Metadata = Metadata(encoding)\n",
    "\n",
    "\n",
    "    def load_metadata():\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def test(self, file: str):\n",
    "        file += \"   padding\"\n",
    "\n",
    "class Bar(Foo):\n",
    "    def __init__(self, encoding: str):\n",
    "        super().__init__()\n",
    "        self.metadata: BarMetadata\n",
    "\n",
    "    def test(self, file: str):\n",
    "        super().test(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarMetadata(encoding='gpt2', other='Bruh')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BarMetadata(encoding=\"gpt2\", other=\"ok\")\n",
    "import dataclasses\n",
    "dataclasses.replace(test, **{\"other\": \"Bruh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.BarMetadata() argument after ** must be a mapping, not Metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test: Metadata \u001b[39m=\u001b[39m Metadata(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test: BarMetadata \u001b[39m=\u001b[39m BarMetadata(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest, other\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.BarMetadata() argument after ** must be a mapping, not Metadata"
     ]
    }
   ],
   "source": [
    "test: Metadata = Metadata(\"gpt2\")\n",
    "test: BarMetadata = BarMetadata(**test, other=\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = test\n",
    "params = set(inspect.signature(Metadata.__init__).parameters.keys()) - set(['self'])\n",
    "{x:getattr(obj, x) for x in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, \n",
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#test if inner functions can access member attributes\n",
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def function(self):\n",
    "        def other():\n",
    "            print(self.a)\n",
    "        other()\n",
    "\n",
    "Foo().function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# padding does not get appended to the parameter as it is a seperate function\n",
    "Bar().test(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tests with torch framework to gain familiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b,c,e = 4, 5,6\n",
    "tensor_3d = torch.arange(b*c*e).reshape(b,c,e)\n",
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch has 5 rows, only want 3 \n",
    "index = torch.tile(torch.arange(3).reshape(3,1), (b,1,e))\n",
    "#you only consider the first batch\n",
    "torch.gather(tensor_3d, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 0,  1,  2,  3,  4]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [30, 31, 32, 33, 34]],\n",
       "\n",
       "        [[60, 61, 62, 63, 64],\n",
       "         [60, 61, 62, 63, 64]],\n",
       "\n",
       "        [[90, 91, 92, 93, 94],\n",
       "         [90, 91, 92, 93, 94]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#objective: retrieve first rows of tensor_3d -> if we specify dim=1, we collapse along the rows (we perform indexing for each row)\n",
    "#b,c,e = 4,5,6\n",
    "#i always want the first row -> specify by row, dim=1\n",
    "#how do i reduce the amount of rows if the index tensor has to be of the same dimension?\n",
    "#dimension has to be the same but not the shape\n",
    "#torch.zeros(4,1,6) gets the first row of the tensor, but it is problematic if i want multiple rows as i \n",
    "#then use the same index (0) while having the output shape that i want\n",
    "#solution: arange\n",
    "#index=torch.zeros(4,1,6) -> if we use 5 instead of 6, each row has 5 columns\n",
    "#meaning: we need a row containing the same index \n",
    "tensor_3d.gather(dim=1, index=torch.zeros(4,2,6, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ]\n",
    "   ])\n",
    "#size is: 3, 4, 3. if you collapse in the first dimension (dim=0), the result tensor becomes of size 4,3. if you collapse it in the second dimension, you get a tensor of size 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)\n",
    "#in transformers, we usually have tensors of shape b,c,e (batch_size, context, embedding_dimension).\n",
    "#if we specify dim=0, we perform the operation along the entire batch, in dim=1 along the context and in dim=2 along the embedding dimension.\n",
    "#if we were to sum the tensors together, sum(dim=1) will yield the sum of the embeddings of each word.\n",
    "#think of it as squishing a dimension together so that it is of size 1, meaning that we have to squeeze in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "#test if torch.tile and tensor.repeat are the same\n",
    "c = 2 #simulate two words\n",
    "a = torch.arange(c).reshape((c,1)).repeat((3,1,4))\n",
    "b = torch.tile(torch.arange(c).reshape((c,1)), (3,1,4))\n",
    "print(a.equal(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 7],\n",
       "        [23]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"experiments with torch.gather\"\n",
    "M = torch.tensor([[1,2,3], [4,7,18], [19,9,23]])\n",
    "#if there is more than one value inside of the last dimension, continue along current index\n",
    "#meaning at dim=1:\n",
    "#[1,1,1] -> 2,7,9\n",
    "#[0,0,0] -> 1,4,19\n",
    "#increments along the current dimension\n",
    "#at new row, reset counter ->\n",
    "#[1] -> 2\n",
    "#[1] -> 2\n",
    "indexes = torch.tensor([1,1,2]).view(-1,1) \n",
    "\n",
    "dimension = 0 #2d, meaning dim=0 along rows, dim=1 along columns\n",
    "out = M.gather(dimension ,indexes) #dim=0: , dim=1: tensor([[ 2],[ 7],[23]])\n",
    "M.gather(1, torch.Tensor([[1],[1],[2]]).to(dtype=torch.long)) #counter along the current dimension for the dimension of index\n",
    "#M.gather(1, torch.tensor([[0,0,0],[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BatchNorm with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are: same\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding\n",
    "\"test if padding does not lower values\"\n",
    "#first word of each batch -> gather by column\n",
    "#result tensor: (3, 1, 64)\n",
    "#retrieving an index from the dimension increases the counter along index of said dimension by one\n",
    "#e.g. indexing 0 twice will retrieve two different values\n",
    "FEATURES = 16\n",
    "EMBEDDINGS = 64\n",
    "BATCH_SIZE = 3\n",
    "bn = torch.nn.BatchNorm1d(FEATURES)\n",
    "#get first word embeddings of three batches\n",
    "embedding_layer = torch.nn.Embedding(FEATURES, EMBEDDINGS)\n",
    "batch = embedding_layer(torch.randint(16, (BATCH_SIZE, FEATURES)))\n",
    "index = torch.arange(1).repeat(BATCH_SIZE,1,EMBEDDINGS).to(dtype=torch.long)\n",
    "embd_first_word = torch.gather(batch, index=index, dim=1)\n",
    "padding_bn = BatchNormWithPadding(FEATURES,bias=True)\n",
    "norm_padding = padding_bn(embd_first_word)\n",
    "norm = bn(batch)\n",
    "#check if values are the same\n",
    "print(f'Values are: {\"same\" if torch.gather(norm, index=index, dim=1).equal(norm_padding) else \"different\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#test if huggingface datasets can be created from text files\n",
    "import datasets\n",
    "\n",
    "BASEDIR = '/home/mabot004/.qtransform/datasets/files/shakespeare/untokenized/'\n",
    "#number of rows depends on the amount of files\n",
    "files = [os.path.join(BASEDIR, 'shakespeare.txt'), os.path.join(BASEDIR, 'shakespeare_2.txt')]\n",
    "#does the same as huggingface mapping but now with files\n",
    "def gen_text():\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as file:\n",
    "            yield {\"text\": file.read()}\n",
    "\n",
    "#chunk size from config, default 100\n",
    "def chunk_examples(examples):\n",
    "                #splits the text of each row into chunks of length chunk_length. currently it is only used\n",
    "                #for character tokenization to avoid feeding large samples to the tokenizer\n",
    "    chunk_length = 100\n",
    "                #perform tokenization on a handful of characters at a time\n",
    "                #from: https://huggingface.co/docs/datasets/process#split-long-examples            \n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in examples[\"text\"]:\n",
    "        new_chunks = [sentence[i:i + chunk_length] for i in range(0, len(sentence), chunk_length)]\n",
    "        chunks.extend(new_chunks)\n",
    "    return {\"chunks\": chunks}\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2 examples [00:00, 32.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "chunks = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "rotten_tomatoes = datasets.load_dataset('rotten_tomatoes')\n",
    "rotten_tomatoes[\"train\"].shard(num_shards=1000, index=0, contiguous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok: 100%|██████████| 100/100 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# status bar like huggingface dataset map process\n",
    "from tqdm import tqdm\n",
    "msg = 'ok'\n",
    "for i in tqdm(range(100), desc=f'{msg}'):\n",
    "    msg = str(i)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i, data in tqdm(enumerate(range(10)), desc='test progress bar and other stdout stuff'):\n",
    "    print(data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error occurs because the splits have more than one feature and this function changes the amount of samples in each split of one feature without changint the other\n",
    "#so: 5 samples, 2 features. after mapping: text has 10 samples, other feature still has 5 features\n",
    "#from: https://github.com/huggingface/datasets/issues/1817#issuecomment-774066254\n",
    "rt_chunks = datasets.concatenate_datasets(rotten_tomatoes.select_columns(\"text\").map(chunk_examples, batched=True, remove_columns = \"text\").values())\n",
    "print(rt_chunks)\n",
    "#tokenize\n",
    "rt_chunks = rt_chunks.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")\n",
    "rt_chunks.save_to_disk('/home/mabot004/custom_hf_datasets/')\n",
    "\"test if tokenizing is correct\"\n",
    "tokenizer.decode(rt_chunks[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "shakespeare = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "shakespeare = shakespeare.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.concatenate(shakespeare[:3][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memmap(memmap, start, end, data):\n",
    "    memmap[start:end] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test generating huggingface datasets from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 163 examples [00:00, 31912.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def gen_text():\n",
    "    for i in range(163):\n",
    "        yield {\"text\": i}\n",
    "\n",
    "test_threading = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 163\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.rename_column(\"text\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.shard(num_shards=30, index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "num_threads = 3 #os.cpu_count // 2\n",
    "batch_size = 30\n",
    "num_samples = len(test_threading)\n",
    "# 163 // 30 shards\n",
    "# -> 3 threads, each having a batch size of 30 samples\n",
    "# dataset has 163 samples -> each thread should have around 50-60 samples max\n",
    "# -> divide samples of dataset with num_threads\n",
    "# -> each thread should have the entire dataset as an arg, but split differently\n",
    "# range of splitting should be specified as an arg in thread -> index arg in parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why should you use multithreading? the writing process is I/O based\n",
    "#if anything, the amount of write requests increases with the amount of threads\n",
    "memmap = np.memmap('test', mode='w+', shape=(163,), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: 'abcd'\n"
     ]
    }
   ],
   "source": [
    "#playing around with error messages\n",
    "try:\n",
    "    int(\"abcd\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test memory usage in worst case scenarios\n",
    "\"\"\"\n",
    "\n",
    "#no high memory usage as memmap values are lazily loaded, only overhead is the pages (around 5MB per memmap )\n",
    "memmap = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap2 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap3 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap4 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "import psutil\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709600997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform.dataset import MemmapDataset\n",
    "#token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0\n",
    "memmap_ds = MemmapDataset(\n",
    "    token_file='/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin',\n",
    "    dtype=np.float32,\n",
    "    block_size=64,\n",
    "    start=0.0,\n",
    "    end=0.3\n",
    ")\n",
    "len(memmap_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(memmap_ds, batch_size=12, num_workers=8)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    input, labels = data\n",
    "    print(f'{input.size()}, {labels.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing batchnorm quant\n",
    "#https://github.com/Xilinx/brevitas/issues/542\n",
    "#https://github.com/Xilinx/brevitas/issues/363\n",
    "#test merge_bn from qtransform\n",
    "from qtransform.model.modules import CausalSelfAttention\n",
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding, MLP\n",
    "from qtransform.model.gpt import GPTConfig\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import utils as qutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from brevitas.quant import scaled_int\n",
    "#simulate values from embedding, skip positional encoding\n",
    "wte = torch.nn.Embedding(16,64)\n",
    "tokens = torch.randint(16, (3,16))\n",
    "embeddings = wte(tokens)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if quantized layers having return_quant_tensor set to True are compatible with torch operations \n",
    "quant_tensor_linear = qnn.QuantLinear(1,1,True,return_quant_tensor=True)\n",
    "quant_tensor_linear(torch.Tensor(8,1)) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug loading quantized checkpoint\n",
    "CHECKPOINT = '/home/mabot004/eki-transformer-dev/qtransform/outputs/models/GPT_2024-01-17_08:30:49__epoch:1'\n",
    "#doesnt work since qtransform.dataset cannot be found\n",
    "#but module info about tokenizers is not saved in checkpoint, only their names\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.layer.0.attn.attn_mask',\n",
       " 'transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.attn.attn_mask',\n",
       " 'transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if info about quant params are even saved within checkpoint\n",
    "import re\n",
    "keys = checkpoint[\"model_state_dict\"].keys()\n",
    "#quant param that exists within checkpoint: \n",
    "#transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value \n",
    "weights_and_biases = list(filter(lambda x: re.search(r'.+\\.(weight|bias)$', x), keys))\n",
    "def find(x):\n",
    "    if not re.search(r'.+\\.(weight|bias)$', x):\n",
    "        return x\n",
    "other_keys = list(filter(find, keys))\n",
    "len(keys) == len(weights_and_biases) # not only weights and biases in state dict\n",
    "#only scaling_impl is saved in state dict\n",
    "#no multiheadattention though\n",
    "#in gpt quant config, every single layer has a quantizer (most commonly Int8WeightPerTensorFloat)\n",
    "#that quantizer has ScalingImplType STATS\n",
    "#the layers with scaling_impl had an activation quantizer named Int8ActPerTensorFloat\n",
    "#it had the ScalingImplType PARAMETER_FROM_STATS\n",
    "other_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6414)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if qparam is not one \n",
    "checkpoint[\"model_state_dict\"][\"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(3.1415, requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if scaling_impl params exist within model\n",
    "test_mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=256)\n",
    "#simulate some learning steps for param\n",
    "print(test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value = torch.nn.Parameter(torch.tensor(3.1415))\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_mha.state_dict(), 'mha.chpt')\n",
    "#v_quant etc. not appearing within state_dict\n",
    "test_mha.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[0.9874]])), ('bias', tensor([-0.8623]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if brevitas layers relevant for Transformers return qparams in state_dict\n",
    "print(qnn.QuantLinear(1,1,True,input_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantIdentity(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantReLU(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 5), match='allo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?!hallo|welt).*$', \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if storing checkpoints of quantized models even is working\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.ModuleDict(dict(\n",
    "            wte = qnn.QuantEmbedding(32, 128),\n",
    "            pos = qnn.QuantEmbedding(16, 128),\n",
    "            logic = nn.ModuleDict(dict(\n",
    "                layer1 = qnn.QuantLinear(128, 16, True),\n",
    "                layer2 = qnn.QuantLinear(16,1, True))\n",
    "            )\n",
    "        ))\n",
    "    def forward(self, x):\n",
    "        embd = self.network.wte(x)\n",
    "        b,t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "        pos = self.network.pos(pos)\n",
    "        output = embd + pos\n",
    "        for name, layer in self.network.logic.items():\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3944],\n",
       "         [ 0.2287],\n",
       "         [-0.5937],\n",
       "         [-0.8445],\n",
       "         [ 0.4049],\n",
       "         [-0.1961],\n",
       "         [ 0.2558],\n",
       "         [ 0.5325],\n",
       "         [-0.2270],\n",
       "         [ 0.0485],\n",
       "         [-0.5637],\n",
       "         [ 0.1862],\n",
       "         [ 0.7595],\n",
       "         [-0.2511],\n",
       "         [ 0.1841],\n",
       "         [-0.3207]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(torch.randint(32, (1,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#doesnt work, Quantizer cannot be found in brevitas.inject\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#why are they being searched for in inject if they are in brevitas.quant.scaled_int\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m'\u001b[39;49m\u001b[39mquantized_test\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:831\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    829\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    830\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 831\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    832\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    833\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed"
     ]
    }
   ],
   "source": [
    "#doesnt work, Quantizer cannot be found in brevitas.inject\n",
    "#why are they being searched for in inject if they are in brevitas.quant.scaled_int\n",
    "torch.save(model, 'quantized_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform import DeviceSingleton\n",
    "#check if value from class is set in object\n",
    "DeviceSingleton.device = 'cuda'\n",
    "singleton = DeviceSingleton()\n",
    "singleton.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Batchnorm and Conv merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.49767026\n"
     ]
    }
   ],
   "source": [
    "#from: \n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv1d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) )\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\tfusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )\n",
    "\t#\n",
    "\t# we're done\n",
    "\treturn fusedconv\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = (16, 64, 256)\n",
    "x = torch.randn(16, 64, 256)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=(256,256)),\n",
    "    torch.nn.BatchNorm1d(64)\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy.load_state_dict(cv1.state_dict())\n",
    "bn1 = torch.nn.BatchNorm1d(5)\n",
    "qnn.utils.merge_bn(cv1, bn1)\n",
    "input = torch.Tensor(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 is cv1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9793e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9794e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is the same without batchnorm, why?\n",
    "output = cv1_copy(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272],\n",
       "        [ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bn1(input)\n",
    "cv1_copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(30, (3,5,20)).to(dtype=torch.float32) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5470e+00,  1.5270e-01, -2.5464e-01,  2.9082e-01,  1.2001e+00,\n",
       "           6.7265e-01,  7.3291e-01, -1.8250e-01, -1.2105e+00, -1.0728e+00,\n",
       "           3.1590e-01,  2.5334e+00, -1.3411e-01,  2.5901e+00,  6.9120e-02,\n",
       "          -5.1725e-01,  1.9493e-01,  2.2703e+00],\n",
       "         [-3.4492e-01, -9.6099e-01, -9.2788e-01, -5.6099e-01, -2.0823e+00,\n",
       "           8.8297e-01,  4.6034e-01,  9.3609e-01,  1.8312e+00, -8.3214e-01,\n",
       "          -1.0253e+00, -1.3361e+00, -1.3721e+00,  4.9575e-01, -6.1378e-01,\n",
       "           3.7313e-01, -1.6607e+00, -7.7247e-01],\n",
       "         [ 1.8919e+00,  8.4047e-01,  5.7258e-01,  3.1601e-01,  1.4367e-01,\n",
       "          -4.8590e-01,  7.8809e-01,  5.6231e-01, -9.9302e-01, -5.6623e-01,\n",
       "          -2.1978e-01, -8.2209e-01, -2.8324e-02,  9.5371e-01,  9.6952e-02,\n",
       "          -6.7169e-01, -8.7423e-02, -4.8495e-02],\n",
       "         [-2.3595e+00, -4.5535e-01,  1.1663e+00,  1.6639e+00,  5.8315e-01,\n",
       "          -4.0086e-01, -4.8103e-01, -2.0247e+00,  4.6665e-01,  2.1141e-01,\n",
       "           2.7884e-02,  1.7325e+00,  2.3958e+00,  7.5227e-01,  6.7972e-02,\n",
       "           5.4808e-01,  1.2512e+00,  8.5656e-01],\n",
       "         [-6.4959e-01,  6.6814e-01,  4.8005e-01, -2.0198e+00,  4.6459e-01,\n",
       "          -8.1610e-01, -2.8203e-01,  3.0454e-01,  1.0447e+00,  4.4882e-02,\n",
       "          -9.1393e-01,  1.3076e+00, -2.0745e+00, -3.1045e-01,  6.2427e-01,\n",
       "           5.3256e-01,  4.6315e-01, -1.1006e+00]],\n",
       "\n",
       "        [[ 1.9405e-01,  9.4450e-01, -2.1945e-01, -1.0464e-01, -2.1462e+00,\n",
       "          -1.8074e-01, -5.1549e-01,  2.3760e-01, -4.5039e-01,  2.9090e-02,\n",
       "          -2.0757e-01,  1.0382e+00, -1.5610e-01,  5.3232e-01, -9.5934e-01,\n",
       "           2.4156e-01,  1.1666e+00, -3.6109e-01],\n",
       "         [ 1.9643e+00, -3.5404e-02,  2.8985e-01,  1.2767e+00,  1.5938e+00,\n",
       "           2.8329e-01,  1.3378e-01, -5.0787e-01,  1.2574e+00, -6.1935e-01,\n",
       "          -8.2427e-01,  4.4821e-01, -2.4590e-01, -8.7280e-01,  1.4246e+00,\n",
       "           8.9188e-02,  1.6821e-01, -2.1289e+00],\n",
       "         [-6.4067e-02, -3.6310e-01, -5.1032e-01,  4.1864e-01,  5.1348e-01,\n",
       "          -2.7373e+00,  2.9017e-01,  2.4135e+00, -7.8918e-01, -1.9854e-01,\n",
       "           2.0203e+00, -3.6310e-01, -5.2306e-01,  8.9957e-01, -1.4630e+00,\n",
       "           1.2373e-02, -1.2238e+00,  9.9299e-01],\n",
       "         [ 9.1042e-01,  1.4595e-01, -1.4738e+00, -1.5985e+00, -1.7431e+00,\n",
       "           3.1330e-02,  4.3972e-01, -7.1936e-01,  5.5935e-01,  3.6048e-01,\n",
       "           6.7992e-01,  1.2452e+00, -1.1293e+00, -6.2311e-02,  7.0498e-01,\n",
       "           1.4889e+00,  8.2691e-02, -1.0297e+00],\n",
       "         [-2.0543e-01,  5.3759e-01, -9.8586e-01,  2.0672e-01,  6.3649e-01,\n",
       "           2.3649e+00, -1.7379e+00, -6.4168e-01,  1.0911e+00,  3.2361e-01,\n",
       "          -2.0871e+00,  1.4637e+00, -1.4250e+00, -1.5364e-01,  4.2754e-01,\n",
       "          -4.6797e-01,  1.4504e+00, -5.4709e-01]],\n",
       "\n",
       "        [[-8.9680e-02, -1.4670e+00,  1.2023e+00,  6.6737e-01, -1.5057e+00,\n",
       "          -4.8822e-01,  2.4024e-01, -1.5994e+00,  1.7873e+00, -5.9907e-01,\n",
       "          -8.3309e-01,  5.0197e-01, -7.8162e-01,  1.8217e-01, -4.5251e-02,\n",
       "          -1.7203e+00, -8.5721e-02, -5.5332e-01],\n",
       "         [ 1.1663e+00, -1.0131e+00,  8.3614e-02,  6.0756e-01,  1.2145e+00,\n",
       "           1.1872e+00,  1.1804e-01, -2.2524e-01,  3.7509e-01, -2.6787e-01,\n",
       "           4.7772e-01,  2.2624e-01, -1.5147e-01,  6.2887e-01,  1.1935e-01,\n",
       "           1.5600e+00, -8.1634e-02, -2.2099e+00],\n",
       "         [ 5.1029e-01, -1.4743e+00,  9.7388e-01,  3.3264e-01,  2.0862e+00,\n",
       "          -1.2319e+00, -2.1481e+00,  7.9234e-01, -7.6193e-01,  1.6490e-01,\n",
       "           8.2065e-01,  6.9478e-04,  4.6818e-01, -1.6127e+00, -4.3211e-01,\n",
       "           4.9885e-02, -7.4565e-01,  6.3875e-01],\n",
       "         [-4.3844e-01,  2.0107e-01, -7.5047e-03,  1.8040e-01, -1.4647e+00,\n",
       "           7.5665e-01,  1.1131e+00, -6.5673e-01, -2.1358e-01, -8.7125e-01,\n",
       "          -5.8282e-01,  1.0587e-01, -1.6796e+00, -2.1013e-01, -6.5641e-01,\n",
       "          -8.1833e-01, -2.7934e-01,  6.2668e-01],\n",
       "         [ 1.6464e+00,  4.3653e-01, -1.2452e+00, -5.8335e-02, -3.4749e-01,\n",
       "           4.8509e-01,  3.7072e-01, -3.7842e-01,  3.8690e-01,  8.0157e-01,\n",
       "          -8.9019e-01, -1.9134e-02, -1.5724e-01,  2.2794e-01, -6.8663e-01,\n",
       "           8.4329e-01,  2.0301e+00, -1.4638e+00]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.nn.BatchNorm1d(5)(qnn.QuantConv1d(5,5,kernel_size=3)(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5805,  1.6874,  1.4232,  0.4953,  0.3392,  1.6921,  0.4289,\n",
       "           0.6056,  1.4143, -0.3349,  0.7054,  1.2688,  0.7755,  0.8200,\n",
       "           0.8781,  1.1979,  0.5057,  0.3852],\n",
       "         [-0.6557,  0.1111,  0.1168,  0.1569, -0.1044, -0.7017, -0.4960,\n",
       "          -0.0382, -0.2186,  0.1632,  0.3991, -0.2319, -0.1412, -1.0671,\n",
       "          -0.2227, -0.4349,  0.0598, -0.0862],\n",
       "         [-1.8093, -1.3581, -1.4218, -1.9729, -2.4638, -1.5891, -2.7596,\n",
       "          -1.0707, -0.9489, -1.2964, -0.4576, -2.2601, -2.3817, -2.9702,\n",
       "          -2.1696, -1.4183, -1.6867, -1.5476],\n",
       "         [ 0.0087,  0.9669,  0.2861, -0.0195,  1.7805, -0.1940,  1.5801,\n",
       "           0.4090,  0.1231, -0.3474,  0.4677,  0.5724,  0.8208,  1.0850,\n",
       "           1.3689,  0.1611,  0.5700, -0.0545],\n",
       "         [-1.7327, -0.0812, -0.3896, -1.2272, -0.7673, -1.1279, -1.3454,\n",
       "          -0.1006,  0.1854, -0.1570, -0.0988, -0.2189, -0.5590, -1.7284,\n",
       "          -0.0418, -0.0416,  0.0663, -0.0783]],\n",
       "\n",
       "        [[ 1.6639,  0.7003,  1.2008,  0.9851,  0.8164,  1.4518,  1.4647,\n",
       "          -0.1788,  2.0859,  0.5120,  0.8852,  0.7612,  0.6098,  0.6913,\n",
       "           0.9692,  0.6834,  1.3616,  0.9310],\n",
       "         [-0.8825, -0.3148, -0.2431, -0.5726, -0.2170, -0.3808, -0.1197,\n",
       "          -0.2470, -0.1361, -0.0082,  0.8303, -0.6241,  0.2627,  0.3186,\n",
       "          -0.6388, -0.2168, -0.9436,  0.2259],\n",
       "         [-1.2239, -2.1880, -1.9660, -2.3704, -0.7485, -0.4421, -1.5090,\n",
       "          -2.1500, -0.3124, -1.8201, -1.7527, -2.3098, -1.7470, -2.1069,\n",
       "          -1.3791, -1.5358, -2.2366, -2.0583],\n",
       "         [ 0.6760,  1.1154,  0.6760,  1.2122,  0.3283,  0.1619, -0.1343,\n",
       "           0.7418,  0.9900,  0.0270,  0.7077,  0.9982,  0.1400,  1.7570,\n",
       "          -0.0549,  1.0549,  0.4207,  0.7455],\n",
       "         [-1.1979, -1.6723, -1.1881, -1.0570, -0.1860,  0.5241, -1.5507,\n",
       "          -0.7745, -0.1433, -0.6879, -0.4526, -0.6777, -1.1168, -0.2085,\n",
       "          -0.0175, -0.5790, -1.0503, -0.2322]],\n",
       "\n",
       "        [[ 1.5133,  1.3983,  1.5074,  1.0723,  0.7788,  0.7588,  0.6164,\n",
       "           1.0027,  1.2222,  1.5164,  1.4553,  0.7161,  0.7796,  1.6876,\n",
       "           0.4413,  2.1108,  1.6400,  0.9259],\n",
       "         [-0.9756,  0.0696, -0.2441, -0.4958,  0.0807,  0.5572,  0.0222,\n",
       "          -0.4592, -0.5887, -0.3081, -0.0168, -0.1240,  0.1828, -0.1130,\n",
       "          -0.5912, -1.0130, -0.6067,  0.7823],\n",
       "         [-1.9029, -1.1918, -1.7631, -2.7777, -1.8128, -0.9612, -1.9680,\n",
       "          -2.2855, -1.8218, -1.8851, -1.3642, -2.3375, -1.5278, -1.6818,\n",
       "          -2.4809, -1.1818, -1.5407, -1.7131],\n",
       "         [ 0.4436,  0.8576,  0.8286,  1.5920,  0.6813,  1.2457,  0.7904,\n",
       "           0.3165,  0.7779,  1.4541, -0.0767,  1.1432,  0.8329,  0.7933,\n",
       "           0.9158,  1.3503, -0.0775,  0.6725],\n",
       "         [-0.9279, -0.1795, -1.8110, -1.0357, -0.6544,  0.1664, -0.4564,\n",
       "          -0.7024, -0.6100, -1.0599, -1.0029, -1.0985, -0.9784, -0.8995,\n",
       "          -1.0086, -0.7555, -0.4560, -0.3629]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if QuantMHA is learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug QuantMultiheadAttention and merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_bn_mha(layer, bn, output_channel_dim=0):\n",
    "    \n",
    "    #retrieve learnable parameters from batchnorm (scale + bias)\n",
    "    mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "        bn_mean=bn.running_mean,\n",
    "        bn_var=bn.running_var,\n",
    "        bn_eps=bn.eps,\n",
    "        bn_weight=bn.weight.data.clone(),\n",
    "        bn_bias=bn.bias.data.clone())\n",
    "    #out_proj is QuantLinear(in_features=embd_dim, out_features=embd_dim)\n",
    "    out_ch_weight_shape = qutils.compute_channel_view_shape(layer.weight, output_channel_dim)\n",
    "    #apply batchnorm during after forward pass of layer, before returning result\n",
    "    \n",
    "    #!!\n",
    "    layer.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "    #!!\n",
    "    \n",
    "    if layer.out_proj.bias is not None:\n",
    "        out_ch_bias_shape = qutils.compute_channel_view_shape(layer.out_proj.bias, channel_dim=0)\n",
    "        layer.out_proj.bias.data.mul_(mul_factor.view(out_ch_bias_shape))\n",
    "        layer.out_proj.bias.data.add_(add_factor.view(out_ch_bias_shape))\n",
    "    else:\n",
    "        layer.out_proj.bias = nn.Parameter(add_factor)\n",
    "    if (hasattr(layer, 'out_proj_weight_quant') and\n",
    "            isinstance(layer.out_proj_weight_quant, WeightQuantProxyFromInjector)):\n",
    "        layer.out_proj_weight_quant.init_tensor_quant()\n",
    "    if (hasattr(layer, 'out_proj_bias_quant') and isinstance(layer.out_proj_bias_quant, BiasQuantProxyFromInjector)):\n",
    "        layer.out_proj_bias_quant.init_tensor_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_linear = torch.nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "test_linear.weight = torch.nn.parameter.Parameter(torch.ones((embed_dim, embed_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         [ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         ...,\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         [ -8.1817,  -8.1817,  -8.1817,  ...,  -8.1817,  -8.1817,  -8.1817],\n",
       "         [  1.8297,   1.8297,   1.8297,  ...,   1.8297,   1.8297,   1.8297]],\n",
       "\n",
       "        [[-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670],\n",
       "         [ -3.0146,  -3.0146,  -3.0146,  ...,  -3.0146,  -3.0146,  -3.0146],\n",
       "         [  2.3900,   2.3900,   2.3900,  ...,   2.3900,   2.3900,   2.3900],\n",
       "         ...,\n",
       "         [  9.5870,   9.5870,   9.5870,  ...,   9.5870,   9.5870,   9.5870],\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [ -5.1709,  -5.1709,  -5.1709,  ...,  -5.1709,  -5.1709,  -5.1709]],\n",
       "\n",
       "        [[ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [ -0.0878,  -0.0878,  -0.0878,  ...,  -0.0878,  -0.0878,  -0.0878],\n",
       "         [ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         ...,\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [  4.6854,   4.6854,   4.6854,  ...,   4.6854,   4.6854,   4.6854],\n",
       "         [-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670]]],\n",
       "       grad_fn=<UnsafeViewBackward0>, names=('L', 'N', None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "heads = 2\n",
    "embed_dim = 64\n",
    "context = 16\n",
    "quant_mha = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "#pass the same input tensor to merged and unmerged mha + batchnorm and compare results\n",
    "quant_mha_merged = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "from brevitas import config\n",
    "#qparams not imported from state dict\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "quant_mha_merged.load_state_dict(quant_mha.state_dict())\n",
    "#feature length not critical\n",
    "bn = torch.nn.BatchNorm1d(context)\n",
    "#test if quantmha works\n",
    "assert quant_mha(embeddings, embeddings, embeddings)[0].size() == embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLinear(\n",
       "  in_features=64, out_features=64, bias=True\n",
       "  (input_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "      (activation_impl): Identity()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "          (stats_input_view_shape_impl): OverTensorView()\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsPercentile()\n",
       "          )\n",
       "          (restrict_scaling): _RestrictValue(\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (clamp_scaling): _ClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "          )\n",
       "          (restrict_inplace_preprocess): Identity()\n",
       "          (restrict_preprocess): Identity()\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (weight_quant): WeightQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): RescalingIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClampSte()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (scaling_impl): StatsFromParameterScaling(\n",
       "        (parameter_list_stats): _ParameterListStats(\n",
       "          (first_tracked_param): _ViewParameterWrapper(\n",
       "            (view_shape_impl): OverTensorView()\n",
       "          )\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsMax()\n",
       "          )\n",
       "        )\n",
       "        (stats_scaling_impl): _StatsScaling(\n",
       "          (affine_rescaling): Identity()\n",
       "          (restrict_clamp_scaling): _RestrictClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (restrict_scaling_pre): Identity()\n",
       "        )\n",
       "      )\n",
       "      (int_scaling_impl): IntScaling()\n",
       "      (zero_point_impl): ZeroZeroPoint(\n",
       "        (zero_point): StatelessBuffer()\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bias_quant): BiasQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): PrescaledRestrictIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClamp()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "      (zero_point): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_mha.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0926,  0.1662,  0.0606,  ...,  0.0211,  0.0027,  0.6270],\n",
       "          [-0.4505,  0.2013,  0.0885,  ..., -0.1597, -0.3558,  0.4523],\n",
       "          [-0.3425, -0.0519,  0.0775,  ..., -0.1996, -0.2760, -0.2787],\n",
       "          ...,\n",
       "          [-0.0925,  0.2803, -0.1011,  ...,  0.2038, -0.0426,  0.4598],\n",
       "          [ 0.1152,  0.0099, -0.0458,  ...,  0.0279, -0.2678, -0.3036],\n",
       "          [ 0.0462,  0.4055, -0.4288,  ...,  1.0150,  0.1507, -0.0658]],\n",
       " \n",
       "         [[-0.1684,  0.2740, -0.0567,  ...,  0.1945, -0.1586,  0.4784],\n",
       "          [-0.2414,  0.2263,  0.0706,  ..., -0.2693, -0.4626,  0.4440],\n",
       "          [-0.3649,  0.0976, -0.0294,  ..., -0.0971, -0.1957, -0.1036],\n",
       "          ...,\n",
       "          [-0.0461,  0.1246, -0.1975,  ...,  0.0336,  0.2553,  0.3853],\n",
       "          [ 0.0460,  0.0541, -0.0979,  ..., -0.0303, -0.3825, -0.3351],\n",
       "          [-0.0157,  0.2822, -0.4208,  ...,  0.5638,  0.3149,  0.0858]],\n",
       " \n",
       "         [[-0.0181,  0.1725, -0.0047,  ...,  0.1487,  0.1976,  0.5633],\n",
       "          [-0.1927,  0.1515, -0.0845,  ..., -0.2242, -0.4402,  0.5122],\n",
       "          [-0.1058, -0.1061,  0.0588,  ..., -0.2105, -0.3280, -0.3857],\n",
       "          ...,\n",
       "          [-0.0667,  0.1150, -0.3250,  ..., -0.0771,  0.1500,  0.4351],\n",
       "          [ 0.0051,  0.1560, -0.1228,  ...,  0.3762, -0.2658,  0.0824],\n",
       "          [ 0.0911,  0.2581, -0.2653,  ...,  0.3346,  0.2105,  0.3753]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.2267, 0.3770, 0.3974],\n",
       "          [0.2486, 0.5163, 0.2363],\n",
       "          [0.2704, 0.3674, 0.3619]],\n",
       " \n",
       "         [[0.4780, 0.2718, 0.2513],\n",
       "          [0.2445, 0.2800, 0.4767],\n",
       "          [0.3401, 0.3811, 0.2773]],\n",
       " \n",
       "         [[0.3292, 0.3783, 0.2923],\n",
       "          [0.2991, 0.5053, 0.1953],\n",
       "          [0.4384, 0.2035, 0.3578]],\n",
       " \n",
       "         [[0.2322, 0.4357, 0.3333],\n",
       "          [0.2759, 0.4958, 0.2281],\n",
       "          [0.2827, 0.2062, 0.5122]],\n",
       " \n",
       "         [[0.3565, 0.4220, 0.2199],\n",
       "          [0.4971, 0.1079, 0.3933],\n",
       "          [0.3510, 0.3333, 0.3155]],\n",
       " \n",
       "         [[0.3292, 0.2636, 0.4070],\n",
       "          [0.2868, 0.3141, 0.3988],\n",
       "          [0.3059, 0.3414, 0.3537]],\n",
       " \n",
       "         [[0.2759, 0.2800, 0.4439],\n",
       "          [0.3701, 0.2035, 0.4248],\n",
       "          [0.2144, 0.4534, 0.3319]],\n",
       " \n",
       "         [[0.3414, 0.4220, 0.2376],\n",
       "          [0.2213, 0.4070, 0.3715],\n",
       "          [0.2581, 0.3974, 0.3442]],\n",
       " \n",
       "         [[0.4644, 0.2827, 0.2540],\n",
       "          [0.4138, 0.0970, 0.4889],\n",
       "          [0.3005, 0.3360, 0.3633]],\n",
       " \n",
       "         [[0.3578, 0.2144, 0.4275],\n",
       "          [0.2295, 0.5545, 0.2172],\n",
       "          [0.3947, 0.1980, 0.4084]],\n",
       " \n",
       "         [[0.3865, 0.2103, 0.4029],\n",
       "          [0.2827, 0.5286, 0.1885],\n",
       "          [0.3906, 0.4398, 0.1694]],\n",
       " \n",
       "         [[0.3169, 0.3169, 0.3674],\n",
       "          [0.3169, 0.3169, 0.3674],\n",
       "          [0.3974, 0.3974, 0.2049]],\n",
       " \n",
       "         [[0.3251, 0.3496, 0.3251],\n",
       "          [0.3428, 0.3155, 0.3428],\n",
       "          [0.3251, 0.3496, 0.3251]],\n",
       " \n",
       "         [[0.4999, 0.2240, 0.2759],\n",
       "          [0.3770, 0.4097, 0.2131],\n",
       "          [0.4439, 0.3797, 0.1762]],\n",
       " \n",
       "         [[0.3578, 0.4275, 0.2144],\n",
       "          [0.3947, 0.4084, 0.1980],\n",
       "          [0.2295, 0.2172, 0.5545]],\n",
       " \n",
       "         [[0.0819, 0.4111, 0.5053],\n",
       "          [0.3046, 0.3387, 0.3565],\n",
       "          [0.3920, 0.3988, 0.2090]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#added a print statement after out_proj to see if the shape is changed afterwards\n",
    "quant_mha(embeddings,embeddings,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test if qparams are the same without copying them. they should have value 1 \n",
    "MISSING_QPARAMS =  [\"in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]\n",
    "for missing_qparam in MISSING_QPARAMS:\n",
    "    #omit \".value\" to get submodule \n",
    "    submodule = missing_qparam[:-len(\".value\")]\n",
    "    assert quant_mha.get_submodule(submodule).value == quant_mha_merged.get_submodule(submodule).value, f'qparam {missing_qparam} is different' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0486e+05, 4.5555e-41, 1.2967e-05],\n",
      "        [3.0866e-41, 4.4842e-44, 0.0000e+00]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(test(tensor))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'"
     ]
    }
   ],
   "source": [
    "import brevitas\n",
    "test = brevitas.core.quant.IntQuant(True, True)\n",
    "tensor = torch.Tensor(2,3)\n",
    "print(tensor)\n",
    "print(test(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if creating a custom activation can simulate batchnorm\n",
    "#### Alternative: Implement custom Quantizer which performs normalization based on a scale and add factor which can be passed in its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "def QuantIdentityWithWeights(qnn.QuantIdentity):\n",
    "    def __init__(self,\n",
    "            act_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        super().__init__(self,\n",
    "            act_quant = act_quant\n",
    "            return_quant_tensor = return_quant_tensor,\n",
    "            **kwargs):\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        return super().forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from brevitas.nn.quant_layer import ActQuantType\n",
    "from torch import Tensor\n",
    "from brevitas.quant.scaled_int import *\n",
    "from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL\n",
    "\n",
    "class QuantCustom(QuantNLAL):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mul_factor: Tensor,\n",
    "            add_factor: Tensor,\n",
    "            act_quant: Optional[ActQuantType] = Uint8ActPerTensorFloat,\n",
    "            input_quant: Optional[ActQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        QuantNLAL.__init__(\n",
    "            self,\n",
    "            act_impl=None,\n",
    "            passthrough_act=True,\n",
    "            input_quant=input_quant,\n",
    "            act_quant=act_quant,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "        self.mul_factor = mul_factor\n",
    "        self.add_factor = add_factor\n",
    "    \n",
    "\n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        input = self.unpack_input(input)\n",
    "        quant_input = self.input_quant(input)\n",
    "        # shortcut execution through the export impl during export\n",
    "        if self.export_mode:\n",
    "            out = self.export_handler(quant_input.value)\n",
    "            self._set_global_is_quant_layer(False)\n",
    "            return out\n",
    "        out = self.act_quant(quant_input)\n",
    "        \n",
    "        out = self.pack_output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL\n",
    "from torch.nn import Module as TorchModule\n",
    "from brevitas.nn.mixin import * #WeightQuantType, BiasQuantType\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat\n",
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "import torch\n",
    "#test if a quantized layer can be implemented which basically scales the values along a tensor and adds a bias, thereby simulating batch normalization\n",
    "class QuantBatchnorm1d(QuantWBIOL, TorchModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
    "            bias_quant: Optional[BiasQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs) -> None:\n",
    "        TorchModule.__init__(self)\n",
    "        if not isinstance(num_features, int) or num_features <= 0:\n",
    "            raise AttributeError()\n",
    "        #do the same as quantidentity\n",
    "        self.weight = torch.ones(num_features)\n",
    "        self.bias = torch.zeros(num_features)\n",
    "        QuantWBIOL.__init__(\n",
    "            self,\n",
    "            weight_quant=weight_quant,\n",
    "            bias_quant=bias_quant,\n",
    "            input_quant=None,\n",
    "            output_quant=None,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "    \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:\n",
    "        return self.forward_impl(input)\n",
    "    \n",
    "    def inner_forward_impl(self, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n",
    "        #inner_forward_impl is apparently the actual forward pass of the layer \n",
    "        return x * quant_weight[:,None] + quant_bias[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test functionality of custom Quantized layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.]), tensor([0., 0., 0., 0.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "#mul_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "#add_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "batch, context, embeds = 2,4,8\n",
    "#should do nothing as weights are 1 and biases are 0\n",
    "custom_quant_layer = QuantBatchnorm1d(num_features=context)\n",
    "print(f'{custom_quant_layer.weight}, {custom_quant_layer.bias}\\n')\n",
    "input = torch.randint(30, (batch, context, embeds))\n",
    "output = custom_quant_layer(input)\n",
    "assert input.equal(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### compare results of custom Batchnorm with regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True, False,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True, False, False,  True,  True,  True, False,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False, False,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True, False,  True, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True, False, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False,  True,  True, False,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embeddings = torch.randn(batch, sentence_length, embedding_dim)\n",
    "custom_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "regular_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "#simulate some forward passes to change the mean and standard deviation\n",
    "for _ in range(10):\n",
    "    regular_bn(embeddings)\n",
    "#test if merge_bn works\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=regular_bn.running_mean,\n",
    "    bn_var=regular_bn.running_var,\n",
    "    bn_eps=regular_bn.eps,\n",
    "    bn_weight=regular_bn.weight.data.clone(),\n",
    "    bn_bias=regular_bn.bias.data.clone())\n",
    "assert custom_quant_bn.weight.size() == mul_factor.size()\n",
    "assert custom_quant_bn.bias.size() == add_factor.size()\n",
    "#change weight and bias of quantized batchnorm\n",
    "custom_quant_bn.weight = mul_factor\n",
    "custom_quant_bn.bias = add_factor\n",
    "#test if results are at least somewhat similiar\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_bn.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "qtransform_quant_bn = merge_quant_bn(regular_bn)\n",
    "out_qtransform_quant_bn = qtransform_quant_bn(embeddings)\n",
    "print((out_regular_bn - out_qtransform_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True, False,  True,  True,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False,  True,  True,  True,  True, False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False, False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn.utils import merge_bn\n",
    "custom_merged_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "merge_bn(layer=custom_merged_quant_bn, bn = regular_bn)\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy pasted from brevitas.nn.utils\n",
    "def compute_channel_view_shape(tensor: torch.Tensor, channel_dim: int):\n",
    "    #create a list containing ones with length of the tensor dimension (for mha: always length of 3)\n",
    "    broadcast_shape = [1] * len(tensor.size())\n",
    "    #why is that important\n",
    "    broadcast_shape[channel_dim] = -1\n",
    "    return tuple(broadcast_shape)\n",
    "\n",
    "def mul_add_from_bn(bn_mean, bn_var, bn_eps, bn_weight, bn_bias):\n",
    "    denom = torch.sqrt(bn_var + bn_eps)\n",
    "    mul_factor = bn_weight / denom\n",
    "    add_factor = -bn_mean * mul_factor + bn_bias\n",
    "    return mul_factor, add_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layernorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = embeddings.rename(None)\n",
    "for i in range(10):\n",
    "    layernorm(tensor)\n",
    "    bn(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerNorm' object has no attribute 'running_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test if mul_add_from_bn works for layernorm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layernorm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embed_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m ln_mul, ln_add \u001b[38;5;241m=\u001b[39m qutils\u001b[38;5;241m.\u001b[39mmul_add_from_bn(\n\u001b[0;32m----> 4\u001b[0m     bn_mean\u001b[38;5;241m=\u001b[39m\u001b[43mlayernorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m,\n\u001b[1;32m      5\u001b[0m     bn_var\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mrunning_var,\n\u001b[1;32m      6\u001b[0m     bn_eps\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m      7\u001b[0m     bn_weight\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m      8\u001b[0m     bn_bias\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerNorm' object has no attribute 'running_mean'"
     ]
    }
   ],
   "source": [
    "# test if mul_add_from_bn works for layernorm\n",
    "layernorm = torch.nn.LayerNorm(embed_dim, bias=True)\n",
    "ln_mul, ln_add = qutils.mul_add_from_bn(\n",
    "    bn_mean=layernorm.running_mean,\n",
    "    bn_var=layernorm.running_var,\n",
    "    bn_eps=layernorm.eps,\n",
    "    bn_weight=layernorm.weight.data.clone(),\n",
    "    bn_bias=layernorm.bias.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, -1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basically a list of ones with a -1 at index channel_dim\n",
    "compute_channel_view_shape(torch.Tensor(3,3,3,3,3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quant_mha_merged.out_proj.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (-1, 1), mul_factor view: torch.Size([16, 1])\n",
      "quant_mha_merged.out_proj.weight.data shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#disassemble functionality of merge_bn_mha\n",
    "#one dimensional tensor to scale all values of a batch with a corresponding scalar\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=bn.running_mean,\n",
    "    bn_var=bn.running_var,\n",
    "    bn_eps=bn.eps,\n",
    "    bn_weight=bn.weight.data.clone(),\n",
    "    bn_bias=bn.bias.data.clone())\n",
    "assert mul_factor.size() == add_factor.size()\n",
    "assert mul_factor.size()[0] == add_factor.size()[0] == context\n",
    "output_channel_dim = 0\n",
    "#out_proj is QuantLinear -> 2d Tensor, [-1, 1]\n",
    "#meaning: reverse shape of mul_factor tensor\n",
    "#currently: [1,context] now: [context, 1]\n",
    "out_ch_weight_shape = qutils.compute_channel_view_shape(quant_mha_merged.out_proj.weight, output_channel_dim)\n",
    "#out_proj is a linear layer applying a scaling factor to quantize outputs -> inputs: n_embd, outputs: n_embd\n",
    "#batchnorm applied normalization along second dimension, linear layer along third\n",
    "#-> could work if batchnorm normalizes along embeddings\n",
    "assert mul_factor.view(out_ch_weight_shape).size() != quant_mha_merged.out_proj.weight.data.size()\n",
    "print(f'shape: {out_ch_weight_shape}, mul_factor view: {mul_factor.view(out_ch_weight_shape).size()}')\n",
    "print(f'quant_mha_merged.out_proj.weight.data shape: {quant_mha_merged.out_proj.weight.data.size()}')\n",
    "#quant_mha_merged.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "#merge params of bn in quant_mha_merged\n",
    "#merge_bn_mha(quant_mha_merged, bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_factor: torch.Size([16]), add_factor: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#each value of mul_factor is the scale with which the embedding of one word is multiplied with\n",
    "#in total: context amount of words\n",
    "#linear layer calculates the sum of each embedding of a word with a weight\n",
    "#problem: weights are the same for every word\n",
    "print(f'mul_factor: {mul_factor.size()}, add_factor: {add_factor.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qutils.merge_bn(qnn.QuantLinear(context, context, True), bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <p>Rows: Context, Columns: Embeddings</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Row</th>\n",
       "            <th>Embedding 1</th>\n",
       "            <th>Embedding 2</th>\n",
       "            <th>Embedding 3</th>\n",
       "            <th>Embedding 4</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>-0.6182</td>\n",
       "            <td>0.6397</td>\n",
       "            <td>-0.6141</td>\n",
       "            <td>0.8668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>0.4140</td>\n",
       "            <td>0.1806</td>\n",
       "            <td>-1.1200</td>\n",
       "            <td>-0.3160</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\n",
    "    \"\"\"<body>\n",
    "    <p>Rows: Context, Columns: Embeddings</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Row</th>\n",
    "            <th>Embedding 1</th>\n",
    "            <th>Embedding 2</th>\n",
    "            <th>Embedding 3</th>\n",
    "            <th>Embedding 4</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>-0.6182</td>\n",
    "            <td>0.6397</td>\n",
    "            <td>-0.6141</td>\n",
    "            <td>0.8668</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>0.4140</td>\n",
    "            <td>0.1806</td>\n",
    "            <td>-1.1200</td>\n",
    "            <td>-0.3160</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each row of a prompt needs to be multiplied with the same scalar from mul_factor, the position of the row determines the index of mul_factor\\\n",
    "basically: row 1 * mul_factor[0], row 2 * mul_factor[2] etc.\\\n",
    "remember the bit_width value from quant config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6182,  0.6397, -0.6141,  0.1633,  0.1303,  0.9678, -0.0135, -0.3592,\n",
      "         1.0851,  1.2883, -1.4943, -1.3554, -1.2857,  0.5534, -0.9053, -0.2538,\n",
      "         2.0112,  1.5106, -0.5143,  0.0181, -1.1853, -0.1291,  1.1889, -0.2304,\n",
      "        -0.1677, -1.0456, -0.1630,  0.8798,  0.4793,  1.3267,  0.9272,  0.4181,\n",
      "        -1.6796, -0.2393,  0.7780, -1.7058, -1.1486, -1.5907,  0.9055, -0.6892,\n",
      "        -0.3182,  1.7268,  1.3576, -0.0698,  0.5315, -0.9513,  0.0850, -0.0770,\n",
      "         0.6108, -0.9660, -0.2021, -0.8171, -0.4134, -0.9940, -1.1997,  1.1844,\n",
      "         2.1950, -0.9969, -0.6415,  2.3817, -0.1636,  0.8668, -1.2963,  0.2591],\n",
      "       grad_fn=<SelectBackward0>, names=('E',)), \n",
      "tensor([-0.6476,  0.6723, -0.6434,  0.1723,  0.1378,  1.0165, -0.0132, -0.3759,\n",
      "         1.1397,  1.3528, -1.5670, -1.4212, -1.3480,  0.5817, -0.9490, -0.2653,\n",
      "         2.1113,  1.5861, -0.5386,  0.0200, -1.2427, -0.1344,  1.2486, -0.2408,\n",
      "        -0.1750, -1.0961, -0.1700,  0.9242,  0.5039,  1.3931,  0.9740,  0.4398,\n",
      "        -1.7614, -0.2501,  0.8174, -1.7889, -1.2042, -1.6681,  0.9512, -0.7222,\n",
      "        -0.3328,  1.8130,  1.4256, -0.0722,  0.5587, -0.9972,  0.0902, -0.0798,\n",
      "         0.6419, -1.0126, -0.2110, -0.8563, -0.4327, -1.0420, -1.2578,  1.2439,\n",
      "         2.3043, -1.0450, -0.6721,  2.5002, -0.1707,  0.9105, -1.3592,  0.2729],\n",
      "       grad_fn=<AddBackward0>, names=('E',))\n"
     ]
    }
   ],
   "source": [
    "#perform normalization like so:\n",
    "print(f'{embeddings[0][0]}, \\n{mul_factor[0] * embeddings[0][0] + add_factor[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n",
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_no_merge \u001b[39m=\u001b[39m bn(output_no_merge)\n\u001b[1;32m      4\u001b[0m output_merge \u001b[39m=\u001b[39m quant_mha_merged(q,k,v)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m output_no_merge\u001b[39m.\u001b[39mequal(output_merge)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q,k,v = [embeddings for _ in range(3)]\n",
    "output_no_merge = quant_mha(q,k,v)[0]\n",
    "output_no_merge = bn(output_no_merge)\n",
    "output_merge = quant_mha_merged(q,k,v)[0]\n",
    "assert output_no_merge.equal(output_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test what happens if values are transposed after merging QuantLinear with BatchNorm\n",
    "quant_linear = qnn.QuantLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if transposing inputs changes values for batchnorm with two different feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "transposing does not work",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m out_context \u001b[39m=\u001b[39m bn_context(\u001b[39minput\u001b[39m)\n\u001b[1;32m      6\u001b[0m out_embedding \u001b[39m=\u001b[39m bn_embedding(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39massert\u001b[39;00m out_context\u001b[39m.\u001b[39mequal(out_embedding), \u001b[39m\"\u001b[39m\u001b[39mtransposing does not work\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: transposing does not work"
     ]
    }
   ],
   "source": [
    "bn_context = torch.nn.BatchNorm1d(context)\n",
    "bn_embedding = torch.nn.BatchNorm1d(embed_dim)\n",
    "input = embeddings\n",
    "input = input.rename(None)\n",
    "out_context = bn_context(input)\n",
    "out_embedding = bn_embedding(input.transpose(-1,-2))\n",
    "#after transposing, normalize along embeddings instead of along words\n",
    "assert out_context.equal(out_embedding), \"transposing does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if linear transformation can simulate the functionality of batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9493,  2.7542,  9.5925,  7.8767,  1.7580,  3.1383,  6.2312,  5.3196,\n",
       "          7.8936,  1.0332],\n",
       "        [12.1436, 11.1988, 15.5672, 13.1937,  5.2797,  7.7320, 12.6711,  8.1652,\n",
       "         13.6740,  6.0436],\n",
       "        [ 4.6590,  9.2941,  0.4616, -0.8165,  1.4461,  7.0240, -2.8188,  0.2568,\n",
       "          0.8149,  6.8677]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randint(30, (3,10)).to(dtype=torch.float) * 0.7\n",
    "linear = torch.nn.Linear(3, 3)\n",
    "linear(test.transpose(-1,0)).transpose(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before attention calculation, q,k,v are quantized\n",
    "#shape: n_embd * 3 as q,k,v are the same and of shape n_embd\n",
    "quant_mha_merged.in_proj.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64]), torch.Size([3, 16, 64]), torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "#code copied from forward pass of QuantMultiheadAttention\n",
    "#function is called if in_proj quantization has been set\n",
    "#TODO: find out what it does\n",
    "def chunk(x, num=3, dim=-1):\n",
    "    _len, _bsz, _dim = x.shape\n",
    "    x = x.reshape(_len, _bsz, num, dim)\n",
    "    return x[:, :, 0, :], x[:, :, 1, :], x[:, :, 2, :]\n",
    "assert attn_no_batchfirst.in_proj is not None\n",
    "from brevitas.nn.utils import check_tensors_same_ptr\n",
    "#no idea what it does, it has to be True or else an Exception will be thrown\n",
    "assert check_tensors_same_ptr([embeddings, embeddings, embeddings]) == True\n",
    "torch._C._get_tracing_state()\n",
    "\n",
    "query = embeddings\n",
    "query.rename_('L', 'N', 'E')\n",
    "#no idea why q,k,v are infered from the query and params key and value are still used\n",
    "#this is an issue if no in_proj is specified i think\n",
    "q,k,v = chunk(attn_no_batchfirst.in_proj(query))\n",
    "print(f'{q.size()}, {k.size()}, {v.size()}')\n",
    "#issue with wrong shapes could be that batch size is transposed instead of embedding dimension\n",
    "#q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]), \n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(9).reshape(3,3)\n",
    "#columns become rows, rows become columns\n",
    "print(f'{tensor}, \\n{tensor.transpose(1,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "small_attn_cpy = CausalSelfAttention(GPTConfig(block_size=16, n_embd=64, n_head=2))\n",
    "#if batchnorm and mha are merged together, padding should not be necessary for inference\n",
    "small_attn_cpy.mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=64)\n",
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True #copy state dict does not return brevitas qparams\n",
    "small_attn_cpy.load_state_dict(small_attn.state_dict())\n",
    "#qparams from state dict are set to 1 at first\n",
    "print(small_attn.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "print(small_attn_cpy.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "idea from: https://github.com/Xilinx/brevitas/issues/542#issuecomment-1446338490\n",
    "merge_bn does not delete current batchnorm, meaning that one model has to be initialiized without bn and the parameters from the trained model\n",
    "have to be copied to the model without bn\n",
    "TODO: find more ressource efficient ways\n",
    "\"\"\"\n",
    "#at one step in merge_bn_mha, layer.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape)) is performed\n",
    "#weight is of shape (embd_dim, embd_dim), mul_factor is of (shape features, 1)\n",
    "#meaning that batchnorm probably normalizes along the embeddings instead of each sentence\n",
    "\"bn_alt feature length is 64 (embedding dimension)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     88\u001b[0m out_ch_weight_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mweight, output_channel_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     out_ch_bias_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias, channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#merge_bn_mha appends batchnorm to mha, TODO: prepend it (maybe use input_quant_tensor or something)\n",
    "#problem: merged and unmerged outputs are not the same, possibly since feature length is different\n",
    "no_merge_attn_output = small_attn(embeddings)\n",
    "no_merge_bn_output = bn(no_merge_attn_output)\n",
    "try:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=0)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=1)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=2)\n",
    "merge_attn_output = small_attn(embeddings)\n",
    "assert torch.equal(no_merge_bn_output, merge_attn_output) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mul_:\n",
      "\n",
      "mul_(...) method of torch.Tensor instance\n",
      "    mul_(value) -> Tensor\n",
      "    \n",
      "    In-place version of :meth:`~Tensor.mul`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer.out_proj.weight.data.mul_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a.mul_(tensor) basically is a = a * tensor\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.mul_(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_attn(torch.Tensor(3,16,64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function brevitas.nn.utils.merge_bn(layer, bn, output_channel_dim=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv1d and batchnorm1d merge\n",
    "\n",
    "qnn.quant_layer.merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9917, 0.4984, 0.6176, 0.5039, 0.8158, 0.8521, 0.0155, 0.1858,\n",
       "          0.8048],\n",
       "         [0.1621, 0.4298, 0.3947, 0.5427, 0.8238, 0.9419, 0.7478, 0.4333,\n",
       "          0.0647],\n",
       "         [0.0897, 0.2927, 0.9780, 0.6710, 0.0377, 0.8199, 0.1301, 0.8592,\n",
       "          0.8216],\n",
       "         [0.2074, 0.6790, 0.2042, 0.7838, 0.5414, 0.5088, 0.8481, 0.2490,\n",
       "          0.1760],\n",
       "         [0.0197, 0.6737, 0.1897, 0.2794, 0.4024, 0.3306, 0.8610, 0.8641,\n",
       "          0.6871],\n",
       "         [0.7651, 0.4413, 0.9831, 0.4328, 0.2344, 0.0799, 0.4901, 0.1151,\n",
       "          0.9380]],\n",
       "\n",
       "        [[0.4503, 0.5180, 0.3012, 0.7354, 0.2637, 0.9073, 0.9226, 0.7925,\n",
       "          0.0674],\n",
       "         [0.9067, 0.1654, 0.9186, 0.1072, 0.0438, 0.4049, 0.1374, 0.3990,\n",
       "          0.6381],\n",
       "         [0.3767, 0.8549, 0.5588, 0.2489, 0.2599, 0.6461, 0.5800, 0.1559,\n",
       "          0.0832],\n",
       "         [0.9381, 0.2192, 0.7259, 0.7615, 0.1411, 0.1472, 0.9268, 0.6733,\n",
       "          0.9049],\n",
       "         [0.1468, 0.8668, 0.3151, 0.5401, 0.4347, 0.5541, 0.0995, 0.6333,\n",
       "          0.1252],\n",
       "         [0.4964, 0.4591, 0.3443, 0.2972, 0.6705, 0.2664, 0.4867, 0.5302,\n",
       "          0.6139]],\n",
       "\n",
       "        [[0.3803, 0.7252, 0.5246, 0.4232, 0.7195, 0.7118, 0.8266, 0.7990,\n",
       "          0.3631],\n",
       "         [0.1904, 0.0903, 0.8097, 0.7286, 0.2548, 0.3355, 0.7833, 0.9820,\n",
       "          0.1257],\n",
       "         [0.6124, 0.5454, 0.4477, 0.6442, 0.5862, 0.4324, 0.3639, 0.6780,\n",
       "          0.3984],\n",
       "         [0.4506, 0.1190, 0.4191, 0.3696, 0.6122, 0.7606, 0.1218, 0.3204,\n",
       "          0.6428],\n",
       "         [0.0765, 0.5835, 0.6852, 0.1305, 0.3852, 0.2372, 0.3551, 0.0528,\n",
       "          0.6343],\n",
       "         [0.0852, 0.0078, 0.7411, 0.6070, 0.8316, 0.9041, 0.3005, 0.5442,\n",
       "          0.2225]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand((3,6,9))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5546, -0.3177,  0.1346, -0.2969,  0.8870,  1.0246, -2.1506,\n",
       "          -1.5043,  0.8454],\n",
       "         [-0.9784, -0.1143, -0.2279,  0.2499,  1.1573,  1.5384,  0.9118,\n",
       "          -0.1030, -1.2930],\n",
       "         [-1.5225, -0.7464,  1.8741,  0.7002, -1.7215,  1.2696, -1.3680,\n",
       "           1.4198,  1.2762],\n",
       "         [-1.0571,  0.6570, -1.0687,  1.0382,  0.1571,  0.0385,  1.2718,\n",
       "          -0.9059, -1.1712],\n",
       "         [-1.5085,  0.9969, -0.8575, -0.5138, -0.0426, -0.3174,  1.7148,\n",
       "           1.7266,  1.0484],\n",
       "         [ 1.0773, -0.1349,  1.8936, -0.1669, -0.9097, -1.4882,  0.0477,\n",
       "          -1.3565,  1.7249]],\n",
       "\n",
       "        [[-0.5003, -0.2434, -1.0663,  0.5820, -1.2088,  1.2342,  1.2923,\n",
       "           0.7987, -1.9539],\n",
       "         [ 1.4249, -0.9680,  1.4631, -1.1558, -1.3605, -0.1948, -1.0582,\n",
       "          -0.2139,  0.5578],\n",
       "         [-0.4253,  1.4034,  0.2712, -0.9140, -0.8717,  0.6049,  0.3522,\n",
       "          -1.2693, -1.5475],\n",
       "         [ 1.5987, -1.0141,  0.8276,  0.9569, -1.2980, -1.2758,  1.5576,\n",
       "           0.6364,  1.4780],\n",
       "         [-1.0216,  1.7369, -0.3769,  0.4850,  0.0815,  0.5390, -1.2028,\n",
       "           0.8424, -1.1045],\n",
       "         [ 0.0714, -0.0682, -0.4982, -0.6744,  0.7233, -0.7897,  0.0349,\n",
       "           0.1978,  0.5111]],\n",
       "\n",
       "        [[-0.7660,  0.5431, -0.2185, -0.6032,  0.5214,  0.4922,  0.9280,\n",
       "           0.8232, -0.8312],\n",
       "         [-0.8871, -1.2103,  1.1116,  0.8499, -0.6794, -0.4189,  1.0267,\n",
       "           1.6678, -1.0960],\n",
       "         [ 0.4760,  0.2200, -0.1538,  0.5978,  0.3758, -0.2121, -0.4741,\n",
       "           0.7270, -0.3422],\n",
       "         [-0.1729, -1.3784, -0.2876, -0.4673,  0.4144,  0.9536, -1.3683,\n",
       "          -0.6463,  0.5256],\n",
       "         [-1.2911,  0.6514,  1.0409, -1.0840, -0.1082, -0.6752, -0.2235,\n",
       "          -1.3819,  0.8459],\n",
       "         [-1.4683, -1.7582,  0.9877,  0.4856,  1.3264,  1.5978, -0.6620,\n",
       "           0.2501, -0.9544]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized values along second dimension, meaning: along sentences\n",
    "#are \n",
    "torch.nn.BatchNorm1d(6)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor retains size, batchnorm essentially is a linear transformation to shift values to have a mean of 0 and a standard deviation of 1\n",
    "torch.nn.BatchNorm1d(10)(torch.Tensor(3,10,16)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = qnn.QuantIdentity()\n",
    "tensor = torch.Tensor(12,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1617e+35,  3.0907e-41, -1.5597e+37,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.1351e-43,  0.0000e+00],\n",
      "         [-1.5597e+37,  3.0907e-41, -3.0176e+34,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test if quantidentity is a simple wrapper around a tensor that does nothing\n",
    "#if so, it could be useful for merging with batchnorm\n",
    "tensor = torch.Tensor(2,3,4)\n",
    "print(tensor)\n",
    "print(\"\\n\" + 30* \"-\" + \"\\n\")\n",
    "print(qnn.QuantIdentity()(tensor).isclose(tensor).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = identity(tensor)\n",
    "output.size == tensor.size\n",
    "output == tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test custom batchnorm and merging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2659,  0.6091,  1.4836,  0.9439,  1.2065, -0.9223, -0.5282, -0.8455,\n",
      "         -0.6554, -1.1303],\n",
      "        [-1.3342, -0.6483, -1.0222,  1.5021, -0.2206,  0.6714, -0.7772,  1.1717,\n",
      "          1.8188, -0.7320],\n",
      "        [-0.2430, -2.0209,  0.5107, -1.3762,  0.5122, -0.6174, -0.2816, -0.9368,\n",
      "          0.0110, -0.6685],\n",
      "        [ 1.0439,  0.5637,  0.0682,  0.5662, -0.7288, -0.8823,  1.3416,  0.7040,\n",
      "         -0.4979,  1.0843],\n",
      "        [-0.8410, -0.4107,  1.9964, -0.3908, -1.1501, -1.4927, -1.0785, -0.1600,\n",
      "         -0.2757, -2.0370]], grad_fn=<SelectBackward0>)\n",
      "------------------------------ unquantized: tensor([[-1.2628,  0.6076,  1.4799,  0.9416,  1.2035, -0.9200, -0.5269, -0.8434,\n",
      "         -0.6538, -1.1275],\n",
      "        [-1.3309, -0.6467, -1.0197,  1.4984, -0.2201,  0.6698, -0.7753,  1.1687,\n",
      "          1.8142, -0.7302],\n",
      "        [-0.2424, -2.0158,  0.5095, -1.3728,  0.5109, -0.6158, -0.2809, -0.9344,\n",
      "          0.0109, -0.6668],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8389, -0.4097,  1.9914, -0.3899, -1.1472, -1.4889, -1.0758, -0.1596,\n",
      "         -0.2750, -2.0318]])\n",
      "------------------------------ quantized: tensor([[-1.2582,  0.6056,  1.4749,  0.9384,  1.1995, -0.9166, -0.5249, -0.8402,\n",
      "         -0.6513, -1.1234],\n",
      "        [-1.3332, -0.6480, -1.0215,  1.5007, -0.2206,  0.6707, -0.7768,  1.1705,\n",
      "          1.8171, -0.7316],\n",
      "        [-0.2421, -2.0135,  0.5089, -1.3712,  0.5104, -0.6151, -0.2805, -0.9333,\n",
      "          0.0110, -0.6660],\n",
      "        [ 1.0413,  0.5623,  0.0680,  0.5647, -0.7270, -0.8801,  1.3382,  0.7022,\n",
      "         -0.4966,  1.0816],\n",
      "        [-0.8408, -0.4106,  1.9961, -0.3907, -1.1498, -1.4923, -1.0783, -0.1599,\n",
      "         -0.2755, -2.0365]])\n"
     ]
    }
   ],
   "source": [
    "from qtransform.quantization.quant_bn import *\n",
    "import torch\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "normal_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "unquantized_custom_bn = CustomBatchNorm1d(sentence_length)\n",
    "#simulate some forward passes, without updating learnable scale and bias\n",
    "for _ in range(100):\n",
    "    normal_bn(embedding)\n",
    "#before merging, do nothing\n",
    "assert embedding.equal(unquantized_custom_bn(embedding))\n",
    "unquantized_custom_bn = replace_bn(normal_bn, unquantized_custom_bn)\n",
    "out_normal_bn = normal_bn(embedding)\n",
    "out_unquantized_custom_bn = unquantized_custom_bn(embedding)\n",
    "print(out_normal_bn[0])\n",
    "print(f'{30*\"-\"} unquantized: {out_unquantized_custom_bn[0]}')\n",
    "quantized_custom_bn = QuantBatchnorm1d(sentence_length)\n",
    "assert embedding.equal(quantized_custom_bn(embedding))\n",
    "#replace\n",
    "quantized_custom_bn = replace_bn(normal_bn, quantized_custom_bn)\n",
    "out_quantized_custom_bn = quantized_custom_bn(embedding)\n",
    "print(f'{30*\"-\"} quantized: {out_quantized_custom_bn[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x TransformerBlock(\n",
      "        (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (mha): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (active): ReLU()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n",
      "ModuleDict(\n",
      "  (wte): Embedding(50304, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): CausalSelfAttention(\n",
      "        (mha): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (active): ReLU()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_out): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Embedding(50304, 768)\n",
      "Embedding(1024, 768)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "ModuleList(\n",
      "  (0-11): 12 x TransformerBlock(\n",
      "    (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attn): CausalSelfAttention(\n",
      "      (mha): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (active): ReLU()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "TransformerBlock(\n",
      "  (ln_1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn): CausalSelfAttention(\n",
      "    (mha): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (active): ReLU()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "CausalSelfAttention(\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2304, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (active): ReLU()\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=768, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for mn, module in gpt.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4834, -0.1027, -2.2402,  0.4352,  0.2077,  1.0181,  0.6446, -0.6509,\n",
      "          0.9757,  0.8973, -0.7394,  0.0084,  1.1972, -1.3001,  0.8528,  0.6930,\n",
      "          1.2027, -0.3247,  0.2913, -0.3543, -1.0235,  1.0952,  0.9958, -0.4298,\n",
      "          0.6690, -0.7751,  1.2495,  1.1023, -0.6781,  0.4847, -1.4741, -0.9532]])\n",
      "------------------------------\n",
      "tensor([[-0.4549, -0.9629, -2.8159, -0.4967, -0.6939,  0.0086, -0.3151, -1.4382,\n",
      "         -0.0281, -0.0961, -1.5149, -0.8666,  0.1639, -2.0010, -0.1346, -0.2732,\n",
      "          0.1687, -1.1554, -0.6214, -1.1810, -1.7612,  0.0755, -0.0107, -1.2465,\n",
      "         -0.2940, -1.5459,  0.2093,  0.0817, -1.4618, -0.4537, -2.1518, -1.7002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.quantization import quant_bn\n",
    "#one word with 32 embeddings\n",
    "input = torch.randn(1,32)\n",
    "#context is max. of 8 words\n",
    "weight, bias = torch.randn(2, 8)\n",
    "output = quant_bn.custom_bn1d(input, weight, bias)\n",
    "#input and output are the same, why?\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug custom_bn1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_shapes(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Checks if a tensor is of shape [C], [N,C] or [C,N] with N = 1 and C >= 1.\n",
    "    If tensor is of a different shape, a ValueError will be thrown.\n",
    "    The returning tensor will be of shape [C, 1].\n",
    "    \"\"\"\n",
    "    shape_tensor = tensor.size()\n",
    "    if len(shape_tensor) == 1:\n",
    "        tensor = tensor[:,None]\n",
    "    if len(shape_tensor) == 2:\n",
    "        if shape_tensor[0] > 1 and shape_tensor[1] > 1:\n",
    "            raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "        elif shape_tensor[0] == 1 and shape_tensor[1] > 1:\n",
    "            tensor = tensor.transpose(0,1)\n",
    "    elif len(shape_tensor) > 2:\n",
    "        raise ValueError(f'Too many values to unpack for tensor {shape_tensor}.')\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def custom_bn1d(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass of custom BatchNorm implementation. It expects a Tensor x of size [N,C] or [N,C,L]\n",
    "    and both a weight and bias Tensor, each of size [C, 1] or of size [1,C] / [C].\n",
    "    Each row/ embedding of a sentence (dimension C) will be multiplied with one value from the index of the corresponding\n",
    "    weight tensor and added with the value of the bias tensor.\n",
    "\n",
    "    Output: tensor of shape [N,C] or [N,C,L], basically of the same size as the input tensor.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor) :\n",
    "        raise TypeError('Input is not a tensor')\n",
    "    elif not isinstance(weight, torch.Tensor):\n",
    "        raise TypeError('Weight is not a tensor')\n",
    "    elif not isinstance(bias, torch.Tensor):\n",
    "        raise TypeError('Bias is not a Tensor')\n",
    "    #make sure that weights and biases are of shape [C,1]\n",
    "    print(weight)\n",
    "    print(10*\"#\")\n",
    "    weight = check_shapes(weight)\n",
    "    bias = check_shapes(bias)\n",
    "    C_x = x.size()[0] if len(x.size()) == 2 else x.size()[1]\n",
    "    print(f'ok: {weight[:C_x]}')\n",
    "    out = x * weight[:C_x] + bias[:C_x]\n",
    "    #only return the first C_x rows of output tensor\n",
    "    return out[:,None:C_x] if len(x.size()) == 3 else out[:C_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "tensor([ 0,  8,  9, 14, 13,  3, 14,  9, 11,  8, 10,  6, 17, 15,  9,  3])\n",
      "tensor([[ 0, 16,  3,  2,  9, 19, 13,  7],\n",
      "        [10,  4,  1, 18, 13,  7,  8,  5],\n",
      "        [17, 19,  3,  5, 12, 19, 18, 11]])\n",
      "------------------------------\n",
      "tensor([ 6, 10, 10, 13, 17, 18,  9,  7,  9, 11, 19,  0,  7, 16, 12,  3])\n",
      "##########\n",
      "ok: tensor([[ 6],\n",
      "        [10],\n",
      "        [10]])\n",
      "tensor([[  0,  96,  18,  12,  54, 114,  78,  42],\n",
      "        [108,  48,  18, 188, 138,  78,  88,  58],\n",
      "        [179, 199,  39,  59, 129, 199, 189, 119]])\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.randint(20, (16,))\n",
    "bias = torch.randint(20, (16,))\n",
    "input = torch.randint(20, (3,8))\n",
    "print(weight)\n",
    "print(bias)\n",
    "print(input)\n",
    "print(30*\"-\")\n",
    "out = custom_bn1d(input,weight,bias)\n",
    "print(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(3,3,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17],\n",
       "        [18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(27).reshape(9,3)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:,None:2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(input.size())\n",
    "output = custom_bn1d(input, weight, bias)\n",
    "custom_bn1d(torch.randn(3,1,32), weight, bias)\n",
    "output.size()\n",
    "assert output.equal(input * weight[0] + bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.4805e-10, 6.3011e-10, 6.6376e-07, 6.7212e-04, 1.7340e-07,\n",
       "          1.6594e-07, 6.4097e-10, 1.4580e-19, 1.1495e+24, 3.0956e-18,\n",
       "          5.8981e-10, 3.2506e+21, 1.0528e-11, 2.7625e-06, 6.4103e-10,\n",
       "          2.1744e+23, 1.2794e+22, 2.1574e-04, 3.3980e+21, 3.0818e-18,\n",
       "          3.1360e+27, 7.0800e+31, 3.1095e-18, 4.7851e+22, 2.8826e+32,\n",
       "          4.4248e+30, 7.2442e+22, 2.3086e-12, 7.1760e+22, 7.2250e+28,\n",
       "          1.5766e-19, 2.7447e-06]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.8091e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00, 9.1844e-41,\n",
       "          1.1551e-40, 4.5919e-41, 8.2957e-43, 2.9147e-43, 0.0000e+00,\n",
       "          6.7262e-44, 0.0000e+00]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((tensor,torch.Tensor(1,1,32)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MergeBatchNorm class from brevitas.graph.quantize.preprocess_for_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "from qtransform.model.gpt import GPT, GPTConfig\n",
    "import torch\n",
    "\n",
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1150\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:140\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdropout(tok_emb)\u001b[38;5;66;03m# + pos_emb)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_out(x)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:184\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_size:\n\u001b[0;32m--> 184\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    185\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:40\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#dirty workaround to avoid runtimeerrors by adding a padding if the input is smaller than the feature length\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#padding does not artificially lower mean as normalization is performed along the word embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     n,c,l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m#input tensor should always be three dimensional\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m-\u001b[39m c, l)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:437\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/proxy.py:300\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "#initialization of normalization layer dependent on whether batchnorm or layernorm is used\n",
    "#\n",
    "torch.fx.symbolic_trace(gpt, concrete_args= {'idx': torch.Tensor(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tracer' object has no attribute 'unpack_arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getLogger\n\u001b[1;32m      3\u001b[0m log \u001b[38;5;241m=\u001b[39m getLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m other_model \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_for_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixed_point\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MergeBatchNorm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model needs a graph attribute from torch.fx.symbolic_trace\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/quantize.py:272\u001b[0m, in \u001b[0;36mpreprocess_for_quantize\u001b[0;34m(model, trace_model, relu6_to_relu, equalize_iters, equalize_merge_bias, merge_bn, equalize_bias_shrinkage, equalize_scale_computation)\u001b[0m\n\u001b[1;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_model:\n\u001b[0;32m--> 272\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchFunctionalToModule()\u001b[38;5;241m.\u001b[39mapply(model)\n\u001b[1;32m    274\u001b[0m model \u001b[38;5;241m=\u001b[39m DuplicateSharedStatelessModule()\u001b[38;5;241m.\u001b[39mapply(model)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:133\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, block_size, targets)\u001b[0m\n\u001b[1;32m    131\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#assert block_size <= self.config.block_size, f\"Cannot forward sequence of length {block_size}, block size is only {self.config.block_size}\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    136\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx) \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:53\u001b[0m, in \u001b[0;36m_gen_torch_fn_patches.<locals>.new_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tracers\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m orig_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_arg\u001b[49m(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer\u001b[38;5;241m.\u001b[39munpack_arg(kwargs))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnsetValueException:\n\u001b[1;32m     55\u001b[0m     value \u001b[38;5;241m=\u001b[39m _UNSET\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tracer' object has no attribute 'unpack_arg'"
     ]
    }
   ],
   "source": [
    "#preprocess_for_quantize needs access to a graph representation of the model\n",
    "from logging import getLogger\n",
    "log = getLogger(__name__)\n",
    "other_model = preprocess_for_quantize(gpt)\n",
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "#model needs a graph attribute from torch.fx.symbolic_trace\n",
    "#the purpose of that probably is the same as in https://github.com/pytorch/examples/blob/main/fx/replace_op.py\n",
    "\"\"\"\n",
    "it seems that control flow depending on arguments leads to this error\n",
    "https://discuss.tvm.apache.org/t/torch-fx-symbolic-trace-fails-for-most-encoder-decoder-nlp-models/16004\n",
    "\"\"\"\n",
    "try:\n",
    "    MergeBatchNorm().apply(gpt)\n",
    "except Exception as e:\n",
    "    log.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'dummy_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;66;03m# is being used with transformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/transformers/utils/fx.py:1241\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(model, input_names, disable_check, tracer_cls)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03mPerforms symbolic tracing on the model.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     input_names \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1243\u001b[0m input_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_names)\n\u001b[1;32m   1244\u001b[0m concrete_args \u001b[38;5;241m=\u001b[39m get_concrete_args(model, input_names)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'dummy_inputs'"
     ]
    }
   ],
   "source": [
    "from transformers.utils.fx import symbolic_trace # is being used with transformers\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MinstConv(nn.Module):\n",
    "    def __init__(self, param = 10):\n",
    "        super(MinstConv, self).__init__()\n",
    "        #each model needs nn.module for quantization to work\n",
    "        self.model = nn.ModuleDict(dict(\n",
    "            conv1 = nn.Conv2d(1, 32, 3, 1),\n",
    "            relu1 = nn.ReLU(),\n",
    "            conv2 = nn.Conv2d(32, 64, 3, 1),\n",
    "            relu2 = nn.ReLU(),\n",
    "            maxpool2d = nn.MaxPool2d(kernel_size=2),\n",
    "            dropout1 = nn.Dropout(0.25),\n",
    "            flatten = nn.Flatten(),\n",
    "            fc1 = nn.Linear(9216, 128),\n",
    "            relu3 = nn.ReLU(),\n",
    "            dropout2 = nn.Dropout(0.5),\n",
    "            fc2 = nn.Linear(128, 10)\n",
    "        ))\n",
    "        #check symbolic traceability\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        #no exception\n",
    "        assert self.param > 0\n",
    "        #exception, meaning param checking during forward pass not possible\n",
    "        assert x.size()[-1] > 0\n",
    "        output = x\n",
    "        for layer_name, layer in self.model.items():\n",
    "            output = layer(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinstConv(\n",
       "  (model): Module(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout1): Dropout(p=0.25, inplace=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (relu3): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fx.symbolic_trace(MinstConv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace\n\u001b[0;32m----> 2\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbolic_trace\u001b[39m(root, concrete_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_symbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/fx/brevitas_tracer.py:117\u001b[0m, in \u001b[0;36m_symbolic_trace\u001b[0;34m(tracer, root, concrete_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[1;32m    116\u001b[0m             stack\u001b[38;5;241m.\u001b[39menter_context(patch)\n\u001b[0;32m--> 117\u001b[0m         graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, Module) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/_symbolic_trace.py:789\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    785\u001b[0m             _autowrap_check(patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 789\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    790\u001b[0m             {},\n\u001b[1;32m    791\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/gpt.py:130\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    128\u001b[0m b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#print(f'{idx}----------{idx.size()}')\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot forward sequence of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, block size is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, t, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:495\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/backport/fx/proxy.py:352\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "from brevitas.fx.brevitas_tracer import symbolic_trace\n",
    "symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     l \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(l, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m traced \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1150\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1150\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(x):\n\u001b[1;32m     13\u001b[0m     l \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch.fx as fx\n",
    "\"\"\"\n",
    "using values from param leads to errors\n",
    "\"\"\"\n",
    "try:\n",
    "    fx.symbolic_trace(model)\n",
    "except:\n",
    "    pass\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "def test(x):\n",
    "    l = x.size(1)\n",
    "    return torch.arange(l, dtype=torch.long, device='cuda')\n",
    "traced = symbolic_trace(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, size: int = 10):\n",
    "        super().__init__()\n",
    "        #attributes can be param checked during symbolic tracing\n",
    "        self.size = size\n",
    "        self.mul = MultiplyModule()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #param checking fails for symbolic tracing, unless modules are leaf modules which are not traced\n",
    "        #assert isinstance(x,torch.Tensor), 'x is not a tensor'\n",
    "        #assert x.size()[-1] < 10, f'Size of input tensor {x.size()} not compatible with size {self.size}'\n",
    "        #custom modules for operations only necessary when param checking is performed\n",
    "        return self.mul.forward(x)\n",
    "    \n",
    "class MultiplyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "model = torch.fx.symbolic_trace(SimpleModel())\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "model = preprocess_for_quantize(model, trace_model = False)\n",
    "#forward pass works for model after preprocess_model_for_quantize, but not for gpt\n",
    "assert model(10) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleModel()]\n",
      "[SimpleModel(\n",
      "  (mul): MultiplyModule()\n",
      "), MultiplyModule()]\n"
     ]
    }
   ],
   "source": [
    "#submodules disappear form module list after preprocess_for_quantize\n",
    "print(list(model.modules()))\n",
    "print(list(SimpleModel().modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    size = x.size(1)\n",
      "    arange = self.arange(size);  size = None\n",
      "    getattr_1 = x.device;  x = None\n",
      "    to = arange.to(dtype = torch.int64, device = getattr_1);  arange = getattr_1 = None\n",
      "    return to\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#from: https://github.com/pytorch/pytorch/issues/51803#issuecomment-1104634592\n",
    "#experiments to make fx graphing work with transformers\n",
    "import torch\n",
    "\n",
    "from torch.fx import Tracer\n",
    "from torch.fx import symbolic_trace\n",
    "from torch.fx.graph_module import GraphModule\n",
    "\n",
    "\n",
    "class CustomedTracer(Tracer):\n",
    "    \"\"\"\n",
    "    ``Tracer`` is the class that implements the symbolic tracing functionality\n",
    "    of ``torch.fx.symbolic_trace``. A call to ``symbolic_trace(m)`` is equivalent\n",
    "    to ``Tracer().trace(m)``.\n",
    "    This Tracer override the ``is_leaf_module`` function to make symbolic trace\n",
    "    right in some cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, customed_leaf_module=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.customed_leaf_module = customed_leaf_module\n",
    "\n",
    "    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name : str) -> bool:\n",
    "        \"\"\"\n",
    "        A method to specify whether a given ``nn.Module`` is a \"leaf\" module.\n",
    "        Leaf modules are the atomic units that appear in\n",
    "        the IR, referenced by ``call_module`` calls. By default,\n",
    "        Modules in the PyTorch standard library namespace (torch.nn)\n",
    "        are leaf modules. All other modules are traced through and\n",
    "        their constituent ops are recorded, unless specified otherwise\n",
    "        via this parameter.\n",
    "        Args:\n",
    "            m (Module): The module being queried about\n",
    "            module_qualified_name (str): The path to root of this module. For example,\n",
    "                if you have a module hierarchy where submodule ``foo`` contains\n",
    "                submodule ``bar``, which contains submodule ``baz``, that module will\n",
    "                appear with the qualified name ``foo.bar.baz`` here.\n",
    "        \"\"\"\n",
    "        if self.customed_leaf_module and isinstance(m, self.customed_leaf_module):\n",
    "            return True\n",
    "        return m.__module__.startswith('torch.nn') and not isinstance(m, torch.nn.Sequential)\n",
    "\n",
    "\n",
    "\n",
    "class ArangeForFx(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.arange(x)\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.arange = ArangeForFx()\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = x.size(1)\n",
    "        return self.arange(l).to(dtype=torch.long, device=x.device)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "#all ops which need a param from a symbolically traced variable (from the function parameter) should be a leaf module\n",
    "#no idea why it fixes it though\n",
    "tracer = CustomedTracer(customed_leaf_module=(ArangeForFx,))\n",
    "graph = tracer.trace(model)\n",
    "#graph = symbolic_trace(model)\n",
    "name = model.__class__.__name__ if isinstance(model, torch.nn.Module) else model.__name__\n",
    "traced = GraphModule(tracer.root, graph, name)\n",
    "\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input idx to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qtransform.model.modules import BatchNorm, LayerNorm\n",
    "#https://pytorch.org/docs/stable/fx.html#leaf-modules\n",
    "#leaf modules are not being traced through\n",
    "#could be problematic as the output of batch/layer norm are being passed into the attention and mlp layer\n",
    "tracer_gpt = CustomedTracer(customed_leaf_module=(BatchNorm,LayerNorm))\n",
    "from qtransform.model.gpt import GPT as qGPT, GPTConfig\n",
    "gpt = qGPT(GPTConfig())\n",
    "tokens = torch.randint(50304, (2, 1024))\n",
    "#assert statements should be nested inside of a custom module\n",
    "graph_gpt = tracer_gpt.trace(gpt, {\"idx\": tokens})\n",
    "name = gpt.__class__.__name__ if isinstance(gpt, torch.nn.Module) else gpt.__name__\n",
    "traced_gpt = GraphModule(tracer_gpt.root, graph_gpt, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preceding layer is merged with batchnorm layer\n",
    "#batchnorm, conv, linear and their quantized versions can be merged\n",
    "#if merging occurs before quantization, how can the learnable parameters from batchnorm be merged?\n",
    "#\n",
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "processed_gpt = preprocess_for_quantize(traced_gpt, trace_model = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=1024, vocab_size=50304, n_layer=12, n_head=12, n_embd=768, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU', norm_layer='BatchNorm', single_output=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_no_symbolic_tracing = gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %idx_1 : [num_users=0] = placeholder[target=idx_1]\n",
      "    %targets : [num_users=1] = placeholder[target=targets](default=None)\n",
      "    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]\n",
      "    %transformer_wte : [num_users=1] = call_module[target=transformer.wte](args = (%_tensor_constant0,), kwargs = {})\n",
      "    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]\n",
      "    %transformer_wpe : [num_users=1] = call_module[target=transformer.wpe](args = (%_tensor_constant1,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%transformer_wte, %transformer_wpe), kwargs = {})\n",
      "    %transformer_dropout : [num_users=2] = call_module[target=transformer.dropout](args = (%add,), kwargs = {})\n",
      "    %transformer_layer_0_ln_1 : [num_users=1] = call_module[target=transformer.layer.0.ln_1](args = (%transformer_dropout,), kwargs = {})\n",
      "    %transformer_layer_0_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.0.attn.attn_mask]\n",
      "    %transformer_layer_0_attn_mha : [num_users=2] = call_module[target=transformer.layer.0.attn.mha](args = (%transformer_layer_0_ln_1, %transformer_layer_0_ln_1, %transformer_layer_0_ln_1), kwargs = {attn_mask: %transformer_layer_0_attn_attn_mask, need_weights: False})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 0), kwargs = {})\n",
      "    %getitem_1 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_0_attn_mha, 1), kwargs = {})\n",
      "    %add_1 : [num_users=2] = call_function[target=operator.add](args = (%transformer_dropout, %getitem), kwargs = {})\n",
      "    %transformer_layer_0_ln_2 : [num_users=1] = call_module[target=transformer.layer.0.ln_2](args = (%add_1,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_fc](args = (%transformer_layer_0_ln_2,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_active : [num_users=1] = call_module[target=transformer.layer.0.mlp.active](args = (%transformer_layer_0_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.0.mlp.c_proj](args = (%transformer_layer_0_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_0_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.0.mlp.dropout](args = (%transformer_layer_0_mlp_c_proj,), kwargs = {})\n",
      "    %add_2 : [num_users=2] = call_function[target=operator.add](args = (%add_1, %transformer_layer_0_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_1_ln_1 : [num_users=1] = call_module[target=transformer.layer.1.ln_1](args = (%add_2,), kwargs = {})\n",
      "    %transformer_layer_1_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.1.attn.attn_mask]\n",
      "    %transformer_layer_1_attn_mha : [num_users=2] = call_module[target=transformer.layer.1.attn.mha](args = (%transformer_layer_1_ln_1, %transformer_layer_1_ln_1, %transformer_layer_1_ln_1), kwargs = {attn_mask: %transformer_layer_1_attn_attn_mask, need_weights: False})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 0), kwargs = {})\n",
      "    %getitem_3 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_1_attn_mha, 1), kwargs = {})\n",
      "    %add_3 : [num_users=2] = call_function[target=operator.add](args = (%add_2, %getitem_2), kwargs = {})\n",
      "    %transformer_layer_1_ln_2 : [num_users=1] = call_module[target=transformer.layer.1.ln_2](args = (%add_3,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_fc](args = (%transformer_layer_1_ln_2,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_active : [num_users=1] = call_module[target=transformer.layer.1.mlp.active](args = (%transformer_layer_1_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.1.mlp.c_proj](args = (%transformer_layer_1_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_1_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.1.mlp.dropout](args = (%transformer_layer_1_mlp_c_proj,), kwargs = {})\n",
      "    %add_4 : [num_users=2] = call_function[target=operator.add](args = (%add_3, %transformer_layer_1_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_2_ln_1 : [num_users=1] = call_module[target=transformer.layer.2.ln_1](args = (%add_4,), kwargs = {})\n",
      "    %transformer_layer_2_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.2.attn.attn_mask]\n",
      "    %transformer_layer_2_attn_mha : [num_users=2] = call_module[target=transformer.layer.2.attn.mha](args = (%transformer_layer_2_ln_1, %transformer_layer_2_ln_1, %transformer_layer_2_ln_1), kwargs = {attn_mask: %transformer_layer_2_attn_attn_mask, need_weights: False})\n",
      "    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_2_attn_mha, 1), kwargs = {})\n",
      "    %add_5 : [num_users=2] = call_function[target=operator.add](args = (%add_4, %getitem_4), kwargs = {})\n",
      "    %transformer_layer_2_ln_2 : [num_users=1] = call_module[target=transformer.layer.2.ln_2](args = (%add_5,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_fc](args = (%transformer_layer_2_ln_2,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_active : [num_users=1] = call_module[target=transformer.layer.2.mlp.active](args = (%transformer_layer_2_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.2.mlp.c_proj](args = (%transformer_layer_2_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_2_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.2.mlp.dropout](args = (%transformer_layer_2_mlp_c_proj,), kwargs = {})\n",
      "    %add_6 : [num_users=2] = call_function[target=operator.add](args = (%add_5, %transformer_layer_2_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_3_ln_1 : [num_users=1] = call_module[target=transformer.layer.3.ln_1](args = (%add_6,), kwargs = {})\n",
      "    %transformer_layer_3_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.3.attn.attn_mask]\n",
      "    %transformer_layer_3_attn_mha : [num_users=2] = call_module[target=transformer.layer.3.attn.mha](args = (%transformer_layer_3_ln_1, %transformer_layer_3_ln_1, %transformer_layer_3_ln_1), kwargs = {attn_mask: %transformer_layer_3_attn_attn_mask, need_weights: False})\n",
      "    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 0), kwargs = {})\n",
      "    %getitem_7 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_3_attn_mha, 1), kwargs = {})\n",
      "    %add_7 : [num_users=2] = call_function[target=operator.add](args = (%add_6, %getitem_6), kwargs = {})\n",
      "    %transformer_layer_3_ln_2 : [num_users=1] = call_module[target=transformer.layer.3.ln_2](args = (%add_7,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_fc](args = (%transformer_layer_3_ln_2,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_active : [num_users=1] = call_module[target=transformer.layer.3.mlp.active](args = (%transformer_layer_3_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.3.mlp.c_proj](args = (%transformer_layer_3_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_3_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.3.mlp.dropout](args = (%transformer_layer_3_mlp_c_proj,), kwargs = {})\n",
      "    %add_8 : [num_users=2] = call_function[target=operator.add](args = (%add_7, %transformer_layer_3_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_4_ln_1 : [num_users=1] = call_module[target=transformer.layer.4.ln_1](args = (%add_8,), kwargs = {})\n",
      "    %transformer_layer_4_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.4.attn.attn_mask]\n",
      "    %transformer_layer_4_attn_mha : [num_users=2] = call_module[target=transformer.layer.4.attn.mha](args = (%transformer_layer_4_ln_1, %transformer_layer_4_ln_1, %transformer_layer_4_ln_1), kwargs = {attn_mask: %transformer_layer_4_attn_attn_mask, need_weights: False})\n",
      "    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 0), kwargs = {})\n",
      "    %getitem_9 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_4_attn_mha, 1), kwargs = {})\n",
      "    %add_9 : [num_users=2] = call_function[target=operator.add](args = (%add_8, %getitem_8), kwargs = {})\n",
      "    %transformer_layer_4_ln_2 : [num_users=1] = call_module[target=transformer.layer.4.ln_2](args = (%add_9,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_fc](args = (%transformer_layer_4_ln_2,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_active : [num_users=1] = call_module[target=transformer.layer.4.mlp.active](args = (%transformer_layer_4_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.4.mlp.c_proj](args = (%transformer_layer_4_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_4_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.4.mlp.dropout](args = (%transformer_layer_4_mlp_c_proj,), kwargs = {})\n",
      "    %add_10 : [num_users=2] = call_function[target=operator.add](args = (%add_9, %transformer_layer_4_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_5_ln_1 : [num_users=1] = call_module[target=transformer.layer.5.ln_1](args = (%add_10,), kwargs = {})\n",
      "    %transformer_layer_5_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.5.attn.attn_mask]\n",
      "    %transformer_layer_5_attn_mha : [num_users=2] = call_module[target=transformer.layer.5.attn.mha](args = (%transformer_layer_5_ln_1, %transformer_layer_5_ln_1, %transformer_layer_5_ln_1), kwargs = {attn_mask: %transformer_layer_5_attn_attn_mask, need_weights: False})\n",
      "    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 0), kwargs = {})\n",
      "    %getitem_11 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_5_attn_mha, 1), kwargs = {})\n",
      "    %add_11 : [num_users=2] = call_function[target=operator.add](args = (%add_10, %getitem_10), kwargs = {})\n",
      "    %transformer_layer_5_ln_2 : [num_users=1] = call_module[target=transformer.layer.5.ln_2](args = (%add_11,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_fc](args = (%transformer_layer_5_ln_2,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_active : [num_users=1] = call_module[target=transformer.layer.5.mlp.active](args = (%transformer_layer_5_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.5.mlp.c_proj](args = (%transformer_layer_5_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_5_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.5.mlp.dropout](args = (%transformer_layer_5_mlp_c_proj,), kwargs = {})\n",
      "    %add_12 : [num_users=2] = call_function[target=operator.add](args = (%add_11, %transformer_layer_5_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_6_ln_1 : [num_users=1] = call_module[target=transformer.layer.6.ln_1](args = (%add_12,), kwargs = {})\n",
      "    %transformer_layer_6_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.6.attn.attn_mask]\n",
      "    %transformer_layer_6_attn_mha : [num_users=2] = call_module[target=transformer.layer.6.attn.mha](args = (%transformer_layer_6_ln_1, %transformer_layer_6_ln_1, %transformer_layer_6_ln_1), kwargs = {attn_mask: %transformer_layer_6_attn_attn_mask, need_weights: False})\n",
      "    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 0), kwargs = {})\n",
      "    %getitem_13 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_6_attn_mha, 1), kwargs = {})\n",
      "    %add_13 : [num_users=2] = call_function[target=operator.add](args = (%add_12, %getitem_12), kwargs = {})\n",
      "    %transformer_layer_6_ln_2 : [num_users=1] = call_module[target=transformer.layer.6.ln_2](args = (%add_13,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_fc](args = (%transformer_layer_6_ln_2,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_active : [num_users=1] = call_module[target=transformer.layer.6.mlp.active](args = (%transformer_layer_6_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.6.mlp.c_proj](args = (%transformer_layer_6_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_6_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.6.mlp.dropout](args = (%transformer_layer_6_mlp_c_proj,), kwargs = {})\n",
      "    %add_14 : [num_users=2] = call_function[target=operator.add](args = (%add_13, %transformer_layer_6_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_7_ln_1 : [num_users=1] = call_module[target=transformer.layer.7.ln_1](args = (%add_14,), kwargs = {})\n",
      "    %transformer_layer_7_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.7.attn.attn_mask]\n",
      "    %transformer_layer_7_attn_mha : [num_users=2] = call_module[target=transformer.layer.7.attn.mha](args = (%transformer_layer_7_ln_1, %transformer_layer_7_ln_1, %transformer_layer_7_ln_1), kwargs = {attn_mask: %transformer_layer_7_attn_attn_mask, need_weights: False})\n",
      "    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 0), kwargs = {})\n",
      "    %getitem_15 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_7_attn_mha, 1), kwargs = {})\n",
      "    %add_15 : [num_users=2] = call_function[target=operator.add](args = (%add_14, %getitem_14), kwargs = {})\n",
      "    %transformer_layer_7_ln_2 : [num_users=1] = call_module[target=transformer.layer.7.ln_2](args = (%add_15,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_fc](args = (%transformer_layer_7_ln_2,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_active : [num_users=1] = call_module[target=transformer.layer.7.mlp.active](args = (%transformer_layer_7_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.7.mlp.c_proj](args = (%transformer_layer_7_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_7_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.7.mlp.dropout](args = (%transformer_layer_7_mlp_c_proj,), kwargs = {})\n",
      "    %add_16 : [num_users=2] = call_function[target=operator.add](args = (%add_15, %transformer_layer_7_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_8_ln_1 : [num_users=1] = call_module[target=transformer.layer.8.ln_1](args = (%add_16,), kwargs = {})\n",
      "    %transformer_layer_8_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.8.attn.attn_mask]\n",
      "    %transformer_layer_8_attn_mha : [num_users=2] = call_module[target=transformer.layer.8.attn.mha](args = (%transformer_layer_8_ln_1, %transformer_layer_8_ln_1, %transformer_layer_8_ln_1), kwargs = {attn_mask: %transformer_layer_8_attn_attn_mask, need_weights: False})\n",
      "    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 0), kwargs = {})\n",
      "    %getitem_17 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_8_attn_mha, 1), kwargs = {})\n",
      "    %add_17 : [num_users=2] = call_function[target=operator.add](args = (%add_16, %getitem_16), kwargs = {})\n",
      "    %transformer_layer_8_ln_2 : [num_users=1] = call_module[target=transformer.layer.8.ln_2](args = (%add_17,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_fc](args = (%transformer_layer_8_ln_2,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_active : [num_users=1] = call_module[target=transformer.layer.8.mlp.active](args = (%transformer_layer_8_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.8.mlp.c_proj](args = (%transformer_layer_8_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_8_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.8.mlp.dropout](args = (%transformer_layer_8_mlp_c_proj,), kwargs = {})\n",
      "    %add_18 : [num_users=2] = call_function[target=operator.add](args = (%add_17, %transformer_layer_8_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_9_ln_1 : [num_users=1] = call_module[target=transformer.layer.9.ln_1](args = (%add_18,), kwargs = {})\n",
      "    %transformer_layer_9_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.9.attn.attn_mask]\n",
      "    %transformer_layer_9_attn_mha : [num_users=2] = call_module[target=transformer.layer.9.attn.mha](args = (%transformer_layer_9_ln_1, %transformer_layer_9_ln_1, %transformer_layer_9_ln_1), kwargs = {attn_mask: %transformer_layer_9_attn_attn_mask, need_weights: False})\n",
      "    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 0), kwargs = {})\n",
      "    %getitem_19 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_9_attn_mha, 1), kwargs = {})\n",
      "    %add_19 : [num_users=2] = call_function[target=operator.add](args = (%add_18, %getitem_18), kwargs = {})\n",
      "    %transformer_layer_9_ln_2 : [num_users=1] = call_module[target=transformer.layer.9.ln_2](args = (%add_19,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_fc](args = (%transformer_layer_9_ln_2,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_active : [num_users=1] = call_module[target=transformer.layer.9.mlp.active](args = (%transformer_layer_9_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.9.mlp.c_proj](args = (%transformer_layer_9_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_9_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.9.mlp.dropout](args = (%transformer_layer_9_mlp_c_proj,), kwargs = {})\n",
      "    %add_20 : [num_users=2] = call_function[target=operator.add](args = (%add_19, %transformer_layer_9_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_10_ln_1 : [num_users=1] = call_module[target=transformer.layer.10.ln_1](args = (%add_20,), kwargs = {})\n",
      "    %transformer_layer_10_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.10.attn.attn_mask]\n",
      "    %transformer_layer_10_attn_mha : [num_users=2] = call_module[target=transformer.layer.10.attn.mha](args = (%transformer_layer_10_ln_1, %transformer_layer_10_ln_1, %transformer_layer_10_ln_1), kwargs = {attn_mask: %transformer_layer_10_attn_attn_mask, need_weights: False})\n",
      "    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 0), kwargs = {})\n",
      "    %getitem_21 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_10_attn_mha, 1), kwargs = {})\n",
      "    %add_21 : [num_users=2] = call_function[target=operator.add](args = (%add_20, %getitem_20), kwargs = {})\n",
      "    %transformer_layer_10_ln_2 : [num_users=1] = call_module[target=transformer.layer.10.ln_2](args = (%add_21,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_fc](args = (%transformer_layer_10_ln_2,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_active : [num_users=1] = call_module[target=transformer.layer.10.mlp.active](args = (%transformer_layer_10_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.10.mlp.c_proj](args = (%transformer_layer_10_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_10_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.10.mlp.dropout](args = (%transformer_layer_10_mlp_c_proj,), kwargs = {})\n",
      "    %add_22 : [num_users=2] = call_function[target=operator.add](args = (%add_21, %transformer_layer_10_mlp_dropout), kwargs = {})\n",
      "    %transformer_layer_11_ln_1 : [num_users=1] = call_module[target=transformer.layer.11.ln_1](args = (%add_22,), kwargs = {})\n",
      "    %transformer_layer_11_attn_attn_mask : [num_users=1] = get_attr[target=transformer.layer.11.attn.attn_mask]\n",
      "    %transformer_layer_11_attn_mha : [num_users=2] = call_module[target=transformer.layer.11.attn.mha](args = (%transformer_layer_11_ln_1, %transformer_layer_11_ln_1, %transformer_layer_11_ln_1), kwargs = {attn_mask: %transformer_layer_11_attn_attn_mask, need_weights: False})\n",
      "    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 0), kwargs = {})\n",
      "    %getitem_23 : [num_users=0] = call_function[target=operator.getitem](args = (%transformer_layer_11_attn_mha, 1), kwargs = {})\n",
      "    %add_23 : [num_users=2] = call_function[target=operator.add](args = (%add_22, %getitem_22), kwargs = {})\n",
      "    %transformer_layer_11_ln_2 : [num_users=1] = call_module[target=transformer.layer.11.ln_2](args = (%add_23,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_fc : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_fc](args = (%transformer_layer_11_ln_2,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_active : [num_users=1] = call_module[target=transformer.layer.11.mlp.active](args = (%transformer_layer_11_mlp_c_fc,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_c_proj : [num_users=1] = call_module[target=transformer.layer.11.mlp.c_proj](args = (%transformer_layer_11_mlp_active,), kwargs = {})\n",
      "    %transformer_layer_11_mlp_dropout : [num_users=1] = call_module[target=transformer.layer.11.mlp.dropout](args = (%transformer_layer_11_mlp_c_proj,), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=operator.add](args = (%add_23, %transformer_layer_11_mlp_dropout), kwargs = {})\n",
      "    %transformer_ln_out : [num_users=1] = call_module[target=transformer.ln_out](args = (%add_24,), kwargs = {})\n",
      "    %linear_out : [num_users=3] = call_module[target=linear_out](args = (%transformer_ln_out,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%linear_out, -1), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%linear_out, -1, %size), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_method[target=view](args = (%targets, -1), kwargs = {})\n",
      "    %cross_entropy : [num_users=1] = call_function[target=torch.nn.functional.cross_entropy](args = (%view, %view_1), kwargs = {weight: None, size_average: None, ignore_index: -1, reduce: None, reduction: mean, label_smoothing: 0.0})\n",
      "    return (linear_out, cross_entropy)\n"
     ]
    }
   ],
   "source": [
    "print(processed_gpt.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 274, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"<eval_with_key>.21 from /home/mabot004/eki-transformer-dev/qtransform/qtransform/model/gpt.py:126 in forward\", line 159, in forward\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "AttributeError: 'NoneType' object has no attribute 'view'\n",
      "\n",
      "Call using an FX-traced Module, line 159 of the traced Module's generated forward function:\n",
      "    view = linear_out.view(-1, size);  size = None\n",
      "    view_1 = targets.view(-1);  targets = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    cross_entropy = torch.nn.functional.cross_entropy(view, view_1, weight = None, size_average = None, ignore_index = -1, reduce = None, reduction = 'mean', label_smoothing = 0.0);  view = view_1 = None\n",
      "\n",
      "    return (linear_out, cross_entropy)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocessed_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:678\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/fx/graph_module.py:282\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_with_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m topmost_framesummary\u001b[38;5;241m.\u001b[39mfilename:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_WrappedCall\u001b[38;5;241m.\u001b[39m_generate_error_message(topmost_framesummary),\n\u001b[1;32m    281\u001b[0m           file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "#normalization function receives a 2d tensor, during training a 3d tensor is forwarded. \n",
    "processed_gpt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mini transformer test\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm1d(64)\n",
    "        self.linear = torch.nn.Linear(256,256)\n",
    "        self.mha = torch.nn.MultiheadAttention(256, 2, batch_first = True)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        x, = self.mha(x,x,x, need_weights = False)\n",
    "        return self.softmax(x)\n",
    "#maybe the order of layers being called in the forward pass\n",
    "nodes = list(torch.fx.symbolic_trace(Layer()).graph.nodes)\n",
    "#nodes[1].args.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrevitas_tracer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_trace \u001b[38;5;28;01mas\u001b[39;00m brevitas_symbolic_trace\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m brevitas_symbolic_trace(Layer())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mMergeBatchNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/base.py:64\u001b[0m, in \u001b[0;36mUntilFixedPointGraphTransform.apply\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_model: GraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_converged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_model\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_model\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/graph/fixed_point.py:112\u001b[0m, in \u001b[0;36mMergeBatchNorm.is_converged\u001b[0;34m(self, graph_model)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m layer \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[0;32m--> 112\u001b[0m bn \u001b[38;5;241m=\u001b[39m named_modules[node\u001b[38;5;241m.\u001b[39mtarget]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#merging happens here\u001b[39;00m\n\u001b[1;32m    115\u001b[0m merge_bn(layer, bn, get_output_channel_dim(layer))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/brevitas/nn/utils.py:37\u001b[0m, in \u001b[0;36mmerge_bn\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     35\u001b[0m mul_factor, add_factor \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     36\u001b[0m out_ch_weight_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mweight, output_channel_dim)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmul_factor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ch_weight_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     out_ch_bias_shape \u001b[38;5;241m=\u001b[39m compute_channel_view_shape(layer\u001b[38;5;241m.\u001b[39mbias, channel_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from brevitas.graph.fixed_point import MergeBatchNorm\n",
    "from brevitas.fx.brevitas_tracer import symbolic_trace as brevitas_symbolic_trace\n",
    "model = brevitas_symbolic_trace(Layer())\n",
    "MergeBatchNorm().apply(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.graph.utils import matches_module_pattern\n",
    "def is_converged(graph_model: GraphModule):\n",
    "        named_modules = dict(graph_model.named_modules())\n",
    "        for node in graph_model.graph.nodes:\n",
    "            for pattern in MergeBatchNorm.DEFAULT_PATTERNS:\n",
    "                if matches_module_pattern(pattern, node, named_modules):\n",
    "                    #potential error since node.args is a list containing tuples\n",
    "                    if len(node.args[0].users) > 1:\n",
    "                        continue\n",
    "                    layer = named_modules[node.args[0].target]\n",
    "                    bn = named_modules[node.target]\n",
    "                    #!!!! check if batchnorm is merged\n",
    "                    print(f'{layer}\\n{bn}')\n",
    "                    return -1\n",
    "                    #merging happens here\n",
    "                    merge_bn(layer, bn, get_output_channel_dim(layer))\n",
    "                    \n",
    "                    \n",
    "                    node.replace_all_uses_with(node.args[0])\n",
    "                    graph_model.graph.erase_node(node)\n",
    "                    del_module(graph_model, node.target)\n",
    "        graph_model.recompile()\n",
    "        graph_model.graph.lint()\n",
    "        return graph_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#not merged as it does not fit the patterns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m is_converged(model)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#not merged as it does not fit the patterns\n",
    "#BatchNorm can be merged into Conv Layers, BatchNorm layers and linear layers\n",
    "out = is_converged(model)\n",
    "assert out == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(64)\n",
    "for i in range(200):\n",
    "    bn(torch.randn(12,64,256))\n",
    "bn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = bn.running_mean\n",
    "bn(torch.randn(12,64,256))\n",
    "assert bn.running_mean.equal(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
