{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General testing notebook for qtransform and quantization\n",
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from logging import getLogger\n",
    "import os\n",
    "from omegaconf import DictConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with dataclasses and python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod, ABC\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\"\n",
    "\n",
    "class Foo(ABC):\n",
    "    def __init__(self, encoding: str):\n",
    "        self.metadata: Metadata = Metadata(encoding)\n",
    "\n",
    "\n",
    "    def load_metadata():\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def test(self, file: str):\n",
    "        file += \"   padding\"\n",
    "\n",
    "class Bar(Foo):\n",
    "    def __init__(self, encoding: str):\n",
    "        super().__init__()\n",
    "        self.metadata: BarMetadata\n",
    "\n",
    "    def test(self, file: str):\n",
    "        super().test(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Metadata():\n",
    "    encoding: str\n",
    "\n",
    "@dataclass\n",
    "class BarMetadata(Metadata):\n",
    "    other: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarMetadata(encoding='gpt2', other='Bruh')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BarMetadata(encoding=\"gpt2\", other=\"ok\")\n",
    "import dataclasses\n",
    "dataclasses.replace(test, **{\"other\": \"Bruh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.BarMetadata() argument after ** must be a mapping, not Metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test: Metadata \u001b[39m=\u001b[39m Metadata(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test: BarMetadata \u001b[39m=\u001b[39m BarMetadata(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest, other\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.BarMetadata() argument after ** must be a mapping, not Metadata"
     ]
    }
   ],
   "source": [
    "test: Metadata = Metadata(\"gpt2\")\n",
    "test: BarMetadata = BarMetadata(**test, other=\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = test\n",
    "params = set(inspect.signature(Metadata.__init__).parameters.keys()) - set(['self'])\n",
    "{x:getattr(obj, x) for x in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'gpt2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, \n",
    "asdict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#test if inner functions can access member attributes\n",
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def function(self):\n",
    "        def other():\n",
    "            print(self.a)\n",
    "        other()\n",
    "\n",
    "Foo().function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# padding does not get appended to the parameter as it is a seperate function\n",
    "Bar().test(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tests with torch framework to gain familiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b,c,e = 4, 5,6\n",
    "tensor_3d = torch.arange(b*c*e).reshape(b,c,e)\n",
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5],\n",
       "         [  6,   7,   8,   9,  10,  11],\n",
       "         [ 12,  13,  14,  15,  16,  17]],\n",
       "\n",
       "        [[ 30,  31,  32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39,  40,  41],\n",
       "         [ 42,  43,  44,  45,  46,  47]],\n",
       "\n",
       "        [[ 60,  61,  62,  63,  64,  65],\n",
       "         [ 66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77]],\n",
       "\n",
       "        [[ 90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101],\n",
       "         [102, 103, 104, 105, 106, 107]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch has 5 rows, only want 3 \n",
    "index = torch.tile(torch.arange(3).reshape(3,1), (b,1,e))\n",
    "#you only consider the first batch\n",
    "torch.gather(tensor_3d, dim=1, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 0,  1,  2,  3,  4]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [30, 31, 32, 33, 34]],\n",
       "\n",
       "        [[60, 61, 62, 63, 64],\n",
       "         [60, 61, 62, 63, 64]],\n",
       "\n",
       "        [[90, 91, 92, 93, 94],\n",
       "         [90, 91, 92, 93, 94]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#objective: retrieve first rows of tensor_3d -> if we specify dim=1, we collapse along the rows (we perform indexing for each row)\n",
    "#b,c,e = 4,5,6\n",
    "#i always want the first row -> specify by row, dim=1\n",
    "#how do i reduce the amount of rows if the index tensor has to be of the same dimension?\n",
    "#dimension has to be the same but not the shape\n",
    "#torch.zeros(4,1,6) gets the first row of the tensor, but it is problematic if i want multiple rows as i \n",
    "#then use the same index (0) while having the output shape that i want\n",
    "#solution: arange\n",
    "#index=torch.zeros(4,1,6) -> if we use 5 instead of 6, each row has 5 columns\n",
    "#meaning: we need a row containing the same index \n",
    "tensor_3d.gather(dim=1, index=torch.zeros(4,2,6, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2).reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6],\n",
    "       [0, 0, 0],\n",
    "       [0, 0, 0]\n",
    "     ]\n",
    "   ])\n",
    "#size is: 3, 4, 3. if you collapse in the first dimension (dim=0), the result tensor becomes of size 4,3. if you collapse it in the second dimension, you get a tensor of size 3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)\n",
    "#in transformers, we usually have tensors of shape b,c,e (batch_size, context, embedding_dimension).\n",
    "#if we specify dim=0, we perform the operation along the entire batch, in dim=1 along the context and in dim=2 along the embedding dimension.\n",
    "#if we were to sum the tensors together, sum(dim=1) will yield the sum of the embeddings of each word.\n",
    "#think of it as squishing a dimension together so that it is of size 1, meaning that we have to squeeze in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "#test if torch.tile and tensor.repeat are the same\n",
    "c = 2 #simulate two words\n",
    "a = torch.arange(c).reshape((c,1)).repeat((3,1,4))\n",
    "b = torch.tile(torch.arange(c).reshape((c,1)), (3,1,4))\n",
    "print(a.equal(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 7],\n",
       "        [23]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"experiments with torch.gather\"\n",
    "M = torch.tensor([[1,2,3], [4,7,18], [19,9,23]])\n",
    "#if there is more than one value inside of the last dimension, continue along current index\n",
    "#meaning at dim=1:\n",
    "#[1,1,1] -> 2,7,9\n",
    "#[0,0,0] -> 1,4,19\n",
    "#increments along the current dimension\n",
    "#at new row, reset counter ->\n",
    "#[1] -> 2\n",
    "#[1] -> 2\n",
    "indexes = torch.tensor([1,1,2]).view(-1,1) \n",
    "\n",
    "dimension = 0 #2d, meaning dim=0 along rows, dim=1 along columns\n",
    "out = M.gather(dimension ,indexes) #dim=0: , dim=1: tensor([[ 2],[ 7],[23]])\n",
    "M.gather(1, torch.Tensor([[1],[1],[2]]).to(dtype=torch.long)) #counter along the current dimension for the dimension of index\n",
    "#M.gather(1, torch.tensor([[0,0,0],[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BatchNorm with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are: same\n"
     ]
    }
   ],
   "source": [
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding\n",
    "\"test if padding does not lower values\"\n",
    "#first word of each batch -> gather by column\n",
    "#result tensor: (3, 1, 64)\n",
    "#retrieving an index from the dimension increases the counter along index of said dimension by one\n",
    "#e.g. indexing 0 twice will retrieve two different values\n",
    "FEATURES = 16\n",
    "EMBEDDINGS = 64\n",
    "BATCH_SIZE = 3\n",
    "bn = torch.nn.BatchNorm1d(FEATURES)\n",
    "#get first word embeddings of three batches\n",
    "embedding_layer = torch.nn.Embedding(FEATURES, EMBEDDINGS)\n",
    "batch = embedding_layer(torch.randint(16, (BATCH_SIZE, FEATURES)))\n",
    "index = torch.arange(1).repeat(BATCH_SIZE,1,EMBEDDINGS).to(dtype=torch.long)\n",
    "embd_first_word = torch.gather(batch, index=index, dim=1)\n",
    "padding_bn = BatchNormWithPadding(FEATURES,bias=True)\n",
    "norm_padding = padding_bn(embd_first_word)\n",
    "norm = bn(batch)\n",
    "#check if values are the same\n",
    "print(f'Values are: {\"same\" if torch.gather(norm, index=index, dim=1).equal(norm_padding) else \"different\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test huggingface dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#test if huggingface datasets can be created from text files\n",
    "import datasets\n",
    "\n",
    "BASEDIR = '/home/mabot004/.qtransform/datasets/files/shakespeare/untokenized/'\n",
    "#number of rows depends on the amount of files\n",
    "files = [os.path.join(BASEDIR, 'shakespeare.txt'), os.path.join(BASEDIR, 'shakespeare_2.txt')]\n",
    "#does the same as huggingface mapping but now with files\n",
    "def gen_text():\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as file:\n",
    "            yield {\"text\": file.read()}\n",
    "\n",
    "#chunk size from config, default 100\n",
    "def chunk_examples(examples):\n",
    "                #splits the text of each row into chunks of length chunk_length. currently it is only used\n",
    "                #for character tokenization to avoid feeding large samples to the tokenizer\n",
    "    chunk_length = 100\n",
    "                #perform tokenization on a handful of characters at a time\n",
    "                #from: https://huggingface.co/docs/datasets/process#split-long-examples            \n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in examples[\"text\"]:\n",
    "        new_chunks = [sentence[i:i + chunk_length] for i in range(0, len(sentence), chunk_length)]\n",
    "        chunks.extend(new_chunks)\n",
    "    return {\"chunks\": chunks}\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2 examples [00:00, 32.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "chunks = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "rotten_tomatoes = datasets.load_dataset('rotten_tomatoes')\n",
    "rotten_tomatoes[\"train\"].shard(num_shards=1000, index=0, contiguous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok: 100%|██████████| 100/100 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# status bar like huggingface dataset map process\n",
    "from tqdm import tqdm\n",
    "msg = 'ok'\n",
    "for i in tqdm(range(100), desc=f'{msg}'):\n",
    "    msg = str(i)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i, data in tqdm(enumerate(range(10)), desc='test progress bar and other stdout stuff'):\n",
    "    print(data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error occurs because the splits have more than one feature and this function changes the amount of samples in each split of one feature without changint the other\n",
    "#so: 5 samples, 2 features. after mapping: text has 10 samples, other feature still has 5 features\n",
    "#from: https://github.com/huggingface/datasets/issues/1817#issuecomment-774066254\n",
    "rt_chunks = datasets.concatenate_datasets(rotten_tomatoes.select_columns(\"text\").map(chunk_examples, batched=True, remove_columns = \"text\").values())\n",
    "print(rt_chunks)\n",
    "#tokenize\n",
    "rt_chunks = rt_chunks.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")\n",
    "rt_chunks.save_to_disk('/home/mabot004/custom_hf_datasets/')\n",
    "\"test if tokenizing is correct\"\n",
    "tokenizer.decode(rt_chunks[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://huggingface.co/docs/datasets/create_dataset#from-local-files\n",
    "shakespeare = datasets.Dataset.from_generator(gen_text)\n",
    "shakespeare = shakespeare.map(chunk_examples, batched=True, remove_columns = \"text\")\n",
    "shakespeare = shakespeare.map(\n",
    "    #map function expects dictionary or dataset object, tokenize function returns list of tokens (integers)\n",
    "    lambda batch: {\"input_ids\": [tokenizer.encode(x) for x in batch[\"chunks\"]]}, \n",
    "    batched=True, \n",
    "    remove_columns = \"chunks\",\n",
    "    #num_proc=os.cpu_count()//2 if cfg.encoding != 'character' else 1 \n",
    "    desc=\"tokenizing the dataset from chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.concatenate(shakespeare[:3][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memmap(memmap, start, end, data):\n",
    "    memmap[start:end] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test generating huggingface datasets from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 163 examples [00:00, 31912.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def gen_text():\n",
    "    for i in range(163):\n",
    "        yield {\"text\": i}\n",
    "\n",
    "test_threading = datasets.Dataset.from_generator(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunks'],\n",
       "    num_rows: 163\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.rename_column(\"text\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_threading.shard(num_shards=30, index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "num_threads = 3 #os.cpu_count // 2\n",
    "batch_size = 30\n",
    "num_samples = len(test_threading)\n",
    "# 163 // 30 shards\n",
    "# -> 3 threads, each having a batch size of 30 samples\n",
    "# dataset has 163 samples -> each thread should have around 50-60 samples max\n",
    "# -> divide samples of dataset with num_threads\n",
    "# -> each thread should have the entire dataset as an arg, but split differently\n",
    "# range of splitting should be specified as an arg in thread -> index arg in parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why should you use multithreading? the writing process is I/O based\n",
    "#if anything, the amount of write requests increases with the amount of threads\n",
    "memmap = np.memmap('test', mode='w+', shape=(163,), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: 'abcd'\n"
     ]
    }
   ],
   "source": [
    "#playing around with error messages\n",
    "try:\n",
    "    int(\"abcd\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test memory usage in worst case scenarios\n",
    "\"\"\"\n",
    "\n",
    "#no high memory usage as memmap values are lazily loaded, only overhead is the pages (around 5MB per memmap )\n",
    "memmap = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap2 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap3 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "memmap4 = np.memmap('/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin', dtype=np.float32, mode='r')\n",
    "import psutil\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709600997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform.dataset import MemmapDataset\n",
    "#token_file: str, dtype: np.dtype, block_size: int, start: float=0.0, end: float = 1.0\n",
    "memmap_ds = MemmapDataset(\n",
    "    token_file='/home/mabot004/.qtransform/datasets/huggingface/openwebtext/tokenized/gpt2/openwebtext-float32.bin',\n",
    "    dtype=np.float32,\n",
    "    block_size=64,\n",
    "    start=0.0,\n",
    "    end=0.3\n",
    ")\n",
    "len(memmap_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(memmap_ds, batch_size=12, num_workers=8)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n",
      "torch.Size([12, 64]), torch.Size([12, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "        break\n",
    "    input, labels = data\n",
    "    print(f'{input.size()}, {labels.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing batchnorm quant\n",
    "#https://github.com/Xilinx/brevitas/issues/542\n",
    "#https://github.com/Xilinx/brevitas/issues/363\n",
    "#test merge_bn from qtransform\n",
    "from qtransform.model.modules import merge_bn_mha, CausalSelfAttention\n",
    "from qtransform.model.modules import BatchNorm as BatchNormWithPadding, MLP\n",
    "from qtransform.model.gpt import GPTConfig\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.nn import utils as qutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from brevitas.quant import scaled_int\n",
    "#simulate values from embedding, skip positional encoding\n",
    "wte = torch.nn.Embedding(16,64)\n",
    "tokens = torch.randint(16, (3,16))\n",
    "embeddings = wte(tokens)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if quantized layers having return_quant_tensor set to True are compatible with torch operations \n",
    "quant_tensor_linear = qnn.QuantLinear(1,1,True,return_quant_tensor=True)\n",
    "quant_tensor_linear(torch.Tensor(8,1)) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'model_cfg', 'tokenizer_cfg', 'metrics'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug loading quantized checkpoint\n",
    "CHECKPOINT = '/home/mabot004/eki-transformer-dev/qtransform/outputs/models/GPT_2024-01-17_08:30:49__epoch:1'\n",
    "#doesnt work since qtransform.dataset cannot be found\n",
    "#but module info about tokenizers is not saved in checkpoint, only their names\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.layer.0.attn.attn_mask',\n",
       " 'transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.attn.attn_mask',\n",
       " 'transformer.layer.1.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value',\n",
       " 'transformer.layer.1.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if info about quant params are even saved within checkpoint\n",
    "import re\n",
    "keys = checkpoint[\"model_state_dict\"].keys()\n",
    "#quant param that exists within checkpoint: \n",
    "#transformer.layer.0.mlp.active.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value \n",
    "weights_and_biases = list(filter(lambda x: re.search(r'.+\\.(weight|bias)$', x), keys))\n",
    "def find(x):\n",
    "    if not re.search(r'.+\\.(weight|bias)$', x):\n",
    "        return x\n",
    "other_keys = list(filter(find, keys))\n",
    "len(keys) == len(weights_and_biases) # not only weights and biases in state dict\n",
    "#only scaling_impl is saved in state dict\n",
    "#no multiheadattention though\n",
    "#in gpt quant config, every single layer has a quantizer (most commonly Int8WeightPerTensorFloat)\n",
    "#that quantizer has ScalingImplType STATS\n",
    "#the layers with scaling_impl had an activation quantizer named Int8ActPerTensorFloat\n",
    "#it had the ScalingImplType PARAMETER_FROM_STATS\n",
    "other_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6414)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if qparam is not one \n",
    "checkpoint[\"model_state_dict\"][\"transformer.layer.0.mlp.active.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(3.1415, requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if scaling_impl params exist within model\n",
    "test_mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=256)\n",
    "#simulate some learning steps for param\n",
    "print(test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value = torch.nn.Parameter(torch.tensor(3.1415))\n",
    "test_mha.v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_mha.state_dict(), 'mha.chpt')\n",
    "#v_quant etc. not appearing within state_dict\n",
    "test_mha.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[0.9874]])), ('bias', tensor([-0.8623]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if brevitas layers relevant for Transformers return qparams in state_dict\n",
    "print(qnn.QuantLinear(1,1,True,input_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantIdentity(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())\n",
    "print(qnn.QuantReLU(act_quant=scaled_int.Int8ActPerTensorFloat).state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 5), match='allo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?!hallo|welt).*$', \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if storing checkpoints of quantized models even is working\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.ModuleDict(dict(\n",
    "            wte = qnn.QuantEmbedding(32, 128),\n",
    "            pos = qnn.QuantEmbedding(16, 128),\n",
    "            logic = nn.ModuleDict(dict(\n",
    "                layer1 = qnn.QuantLinear(128, 16, True),\n",
    "                layer2 = qnn.QuantLinear(16,1, True))\n",
    "            )\n",
    "        ))\n",
    "    def forward(self, x):\n",
    "        embd = self.network.wte(x)\n",
    "        b,t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "        pos = self.network.pos(pos)\n",
    "        output = embd + pos\n",
    "        for name, layer in self.network.logic.items():\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabot004/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3944],\n",
       "         [ 0.2287],\n",
       "         [-0.5937],\n",
       "         [-0.8445],\n",
       "         [ 0.4049],\n",
       "         [-0.1961],\n",
       "         [ 0.2558],\n",
       "         [ 0.5325],\n",
       "         [-0.2270],\n",
       "         [ 0.0485],\n",
       "         [-0.5637],\n",
       "         [ 0.1862],\n",
       "         [ 0.7595],\n",
       "         [-0.2511],\n",
       "         [ 0.1841],\n",
       "         [-0.3207]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(torch.randint(32, (1,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#doesnt work, Quantizer cannot be found in brevitas.inject\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#why are they being searched for in inject if they are in brevitas.quant.scaled_int\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39m'\u001b[39;49m\u001b[39mquantized_test\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/serialization.py:831\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    829\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    830\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 831\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    832\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    833\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'brevitas.inject.Int8WeightPerTensorFloat'>: attribute lookup Int8WeightPerTensorFloat on brevitas.inject failed"
     ]
    }
   ],
   "source": [
    "#doesnt work, Quantizer cannot be found in brevitas.inject\n",
    "#why are they being searched for in inject if they are in brevitas.quant.scaled_int\n",
    "torch.save(model, 'quantized_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qtransform import DeviceSingleton\n",
    "#check if value from class is set in object\n",
    "DeviceSingleton.device = 'cuda'\n",
    "singleton = DeviceSingleton()\n",
    "singleton.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Batchnorm and Conv merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.49767026\n"
     ]
    }
   ],
   "source": [
    "#from: \n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv1d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) )\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\tfusedconv.bias.copy_( torch.matmul(w_bn, b_conv) + b_bn )\n",
    "\t#\n",
    "\t# we're done\n",
    "\treturn fusedconv\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "batch_size = (16, 64, 256)\n",
    "x = torch.randn(16, 64, 256)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=(256,256)),\n",
    "    torch.nn.BatchNorm1d(64)\n",
    ")\n",
    "y1 = net.forward(x)\n",
    "fusedconv = fuse_conv_and_bn(net[0], net[1])\n",
    "y2 = fusedconv.forward(x)\n",
    "d = (y1 - y2).norm().div(y1.norm()).item()\n",
    "print(\"error: %.8f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy = qnn.QuantLinear(5,5,bias=True)\n",
    "cv1_copy.load_state_dict(cv1.state_dict())\n",
    "bn1 = torch.nn.BatchNorm1d(5)\n",
    "qnn.utils.merge_bn(cv1, bn1)\n",
    "input = torch.Tensor(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 is cv1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9793e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0883e-01, -9.2026e-03,  3.9794e-01,  3.7391e-01,  4.2723e-01],\n",
       "        [-3.9785e+20,  2.8513e+20, -5.6362e+19,  2.4866e+20,  3.8127e+20]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is the same without batchnorm, why?\n",
    "output = cv1_copy(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272],\n",
       "        [ 0.3088, -0.0092,  0.3979,  0.3739,  0.4272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bn1(input)\n",
    "cv1_copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(30, (3,5,20)).to(dtype=torch.float32) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5470e+00,  1.5270e-01, -2.5464e-01,  2.9082e-01,  1.2001e+00,\n",
       "           6.7265e-01,  7.3291e-01, -1.8250e-01, -1.2105e+00, -1.0728e+00,\n",
       "           3.1590e-01,  2.5334e+00, -1.3411e-01,  2.5901e+00,  6.9120e-02,\n",
       "          -5.1725e-01,  1.9493e-01,  2.2703e+00],\n",
       "         [-3.4492e-01, -9.6099e-01, -9.2788e-01, -5.6099e-01, -2.0823e+00,\n",
       "           8.8297e-01,  4.6034e-01,  9.3609e-01,  1.8312e+00, -8.3214e-01,\n",
       "          -1.0253e+00, -1.3361e+00, -1.3721e+00,  4.9575e-01, -6.1378e-01,\n",
       "           3.7313e-01, -1.6607e+00, -7.7247e-01],\n",
       "         [ 1.8919e+00,  8.4047e-01,  5.7258e-01,  3.1601e-01,  1.4367e-01,\n",
       "          -4.8590e-01,  7.8809e-01,  5.6231e-01, -9.9302e-01, -5.6623e-01,\n",
       "          -2.1978e-01, -8.2209e-01, -2.8324e-02,  9.5371e-01,  9.6952e-02,\n",
       "          -6.7169e-01, -8.7423e-02, -4.8495e-02],\n",
       "         [-2.3595e+00, -4.5535e-01,  1.1663e+00,  1.6639e+00,  5.8315e-01,\n",
       "          -4.0086e-01, -4.8103e-01, -2.0247e+00,  4.6665e-01,  2.1141e-01,\n",
       "           2.7884e-02,  1.7325e+00,  2.3958e+00,  7.5227e-01,  6.7972e-02,\n",
       "           5.4808e-01,  1.2512e+00,  8.5656e-01],\n",
       "         [-6.4959e-01,  6.6814e-01,  4.8005e-01, -2.0198e+00,  4.6459e-01,\n",
       "          -8.1610e-01, -2.8203e-01,  3.0454e-01,  1.0447e+00,  4.4882e-02,\n",
       "          -9.1393e-01,  1.3076e+00, -2.0745e+00, -3.1045e-01,  6.2427e-01,\n",
       "           5.3256e-01,  4.6315e-01, -1.1006e+00]],\n",
       "\n",
       "        [[ 1.9405e-01,  9.4450e-01, -2.1945e-01, -1.0464e-01, -2.1462e+00,\n",
       "          -1.8074e-01, -5.1549e-01,  2.3760e-01, -4.5039e-01,  2.9090e-02,\n",
       "          -2.0757e-01,  1.0382e+00, -1.5610e-01,  5.3232e-01, -9.5934e-01,\n",
       "           2.4156e-01,  1.1666e+00, -3.6109e-01],\n",
       "         [ 1.9643e+00, -3.5404e-02,  2.8985e-01,  1.2767e+00,  1.5938e+00,\n",
       "           2.8329e-01,  1.3378e-01, -5.0787e-01,  1.2574e+00, -6.1935e-01,\n",
       "          -8.2427e-01,  4.4821e-01, -2.4590e-01, -8.7280e-01,  1.4246e+00,\n",
       "           8.9188e-02,  1.6821e-01, -2.1289e+00],\n",
       "         [-6.4067e-02, -3.6310e-01, -5.1032e-01,  4.1864e-01,  5.1348e-01,\n",
       "          -2.7373e+00,  2.9017e-01,  2.4135e+00, -7.8918e-01, -1.9854e-01,\n",
       "           2.0203e+00, -3.6310e-01, -5.2306e-01,  8.9957e-01, -1.4630e+00,\n",
       "           1.2373e-02, -1.2238e+00,  9.9299e-01],\n",
       "         [ 9.1042e-01,  1.4595e-01, -1.4738e+00, -1.5985e+00, -1.7431e+00,\n",
       "           3.1330e-02,  4.3972e-01, -7.1936e-01,  5.5935e-01,  3.6048e-01,\n",
       "           6.7992e-01,  1.2452e+00, -1.1293e+00, -6.2311e-02,  7.0498e-01,\n",
       "           1.4889e+00,  8.2691e-02, -1.0297e+00],\n",
       "         [-2.0543e-01,  5.3759e-01, -9.8586e-01,  2.0672e-01,  6.3649e-01,\n",
       "           2.3649e+00, -1.7379e+00, -6.4168e-01,  1.0911e+00,  3.2361e-01,\n",
       "          -2.0871e+00,  1.4637e+00, -1.4250e+00, -1.5364e-01,  4.2754e-01,\n",
       "          -4.6797e-01,  1.4504e+00, -5.4709e-01]],\n",
       "\n",
       "        [[-8.9680e-02, -1.4670e+00,  1.2023e+00,  6.6737e-01, -1.5057e+00,\n",
       "          -4.8822e-01,  2.4024e-01, -1.5994e+00,  1.7873e+00, -5.9907e-01,\n",
       "          -8.3309e-01,  5.0197e-01, -7.8162e-01,  1.8217e-01, -4.5251e-02,\n",
       "          -1.7203e+00, -8.5721e-02, -5.5332e-01],\n",
       "         [ 1.1663e+00, -1.0131e+00,  8.3614e-02,  6.0756e-01,  1.2145e+00,\n",
       "           1.1872e+00,  1.1804e-01, -2.2524e-01,  3.7509e-01, -2.6787e-01,\n",
       "           4.7772e-01,  2.2624e-01, -1.5147e-01,  6.2887e-01,  1.1935e-01,\n",
       "           1.5600e+00, -8.1634e-02, -2.2099e+00],\n",
       "         [ 5.1029e-01, -1.4743e+00,  9.7388e-01,  3.3264e-01,  2.0862e+00,\n",
       "          -1.2319e+00, -2.1481e+00,  7.9234e-01, -7.6193e-01,  1.6490e-01,\n",
       "           8.2065e-01,  6.9478e-04,  4.6818e-01, -1.6127e+00, -4.3211e-01,\n",
       "           4.9885e-02, -7.4565e-01,  6.3875e-01],\n",
       "         [-4.3844e-01,  2.0107e-01, -7.5047e-03,  1.8040e-01, -1.4647e+00,\n",
       "           7.5665e-01,  1.1131e+00, -6.5673e-01, -2.1358e-01, -8.7125e-01,\n",
       "          -5.8282e-01,  1.0587e-01, -1.6796e+00, -2.1013e-01, -6.5641e-01,\n",
       "          -8.1833e-01, -2.7934e-01,  6.2668e-01],\n",
       "         [ 1.6464e+00,  4.3653e-01, -1.2452e+00, -5.8335e-02, -3.4749e-01,\n",
       "           4.8509e-01,  3.7072e-01, -3.7842e-01,  3.8690e-01,  8.0157e-01,\n",
       "          -8.9019e-01, -1.9134e-02, -1.5724e-01,  2.2794e-01, -6.8663e-01,\n",
       "           8.4329e-01,  2.0301e+00, -1.4638e+00]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.torch.nn.BatchNorm1d(5)(qnn.QuantConv1d(5,5,kernel_size=3)(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5805,  1.6874,  1.4232,  0.4953,  0.3392,  1.6921,  0.4289,\n",
       "           0.6056,  1.4143, -0.3349,  0.7054,  1.2688,  0.7755,  0.8200,\n",
       "           0.8781,  1.1979,  0.5057,  0.3852],\n",
       "         [-0.6557,  0.1111,  0.1168,  0.1569, -0.1044, -0.7017, -0.4960,\n",
       "          -0.0382, -0.2186,  0.1632,  0.3991, -0.2319, -0.1412, -1.0671,\n",
       "          -0.2227, -0.4349,  0.0598, -0.0862],\n",
       "         [-1.8093, -1.3581, -1.4218, -1.9729, -2.4638, -1.5891, -2.7596,\n",
       "          -1.0707, -0.9489, -1.2964, -0.4576, -2.2601, -2.3817, -2.9702,\n",
       "          -2.1696, -1.4183, -1.6867, -1.5476],\n",
       "         [ 0.0087,  0.9669,  0.2861, -0.0195,  1.7805, -0.1940,  1.5801,\n",
       "           0.4090,  0.1231, -0.3474,  0.4677,  0.5724,  0.8208,  1.0850,\n",
       "           1.3689,  0.1611,  0.5700, -0.0545],\n",
       "         [-1.7327, -0.0812, -0.3896, -1.2272, -0.7673, -1.1279, -1.3454,\n",
       "          -0.1006,  0.1854, -0.1570, -0.0988, -0.2189, -0.5590, -1.7284,\n",
       "          -0.0418, -0.0416,  0.0663, -0.0783]],\n",
       "\n",
       "        [[ 1.6639,  0.7003,  1.2008,  0.9851,  0.8164,  1.4518,  1.4647,\n",
       "          -0.1788,  2.0859,  0.5120,  0.8852,  0.7612,  0.6098,  0.6913,\n",
       "           0.9692,  0.6834,  1.3616,  0.9310],\n",
       "         [-0.8825, -0.3148, -0.2431, -0.5726, -0.2170, -0.3808, -0.1197,\n",
       "          -0.2470, -0.1361, -0.0082,  0.8303, -0.6241,  0.2627,  0.3186,\n",
       "          -0.6388, -0.2168, -0.9436,  0.2259],\n",
       "         [-1.2239, -2.1880, -1.9660, -2.3704, -0.7485, -0.4421, -1.5090,\n",
       "          -2.1500, -0.3124, -1.8201, -1.7527, -2.3098, -1.7470, -2.1069,\n",
       "          -1.3791, -1.5358, -2.2366, -2.0583],\n",
       "         [ 0.6760,  1.1154,  0.6760,  1.2122,  0.3283,  0.1619, -0.1343,\n",
       "           0.7418,  0.9900,  0.0270,  0.7077,  0.9982,  0.1400,  1.7570,\n",
       "          -0.0549,  1.0549,  0.4207,  0.7455],\n",
       "         [-1.1979, -1.6723, -1.1881, -1.0570, -0.1860,  0.5241, -1.5507,\n",
       "          -0.7745, -0.1433, -0.6879, -0.4526, -0.6777, -1.1168, -0.2085,\n",
       "          -0.0175, -0.5790, -1.0503, -0.2322]],\n",
       "\n",
       "        [[ 1.5133,  1.3983,  1.5074,  1.0723,  0.7788,  0.7588,  0.6164,\n",
       "           1.0027,  1.2222,  1.5164,  1.4553,  0.7161,  0.7796,  1.6876,\n",
       "           0.4413,  2.1108,  1.6400,  0.9259],\n",
       "         [-0.9756,  0.0696, -0.2441, -0.4958,  0.0807,  0.5572,  0.0222,\n",
       "          -0.4592, -0.5887, -0.3081, -0.0168, -0.1240,  0.1828, -0.1130,\n",
       "          -0.5912, -1.0130, -0.6067,  0.7823],\n",
       "         [-1.9029, -1.1918, -1.7631, -2.7777, -1.8128, -0.9612, -1.9680,\n",
       "          -2.2855, -1.8218, -1.8851, -1.3642, -2.3375, -1.5278, -1.6818,\n",
       "          -2.4809, -1.1818, -1.5407, -1.7131],\n",
       "         [ 0.4436,  0.8576,  0.8286,  1.5920,  0.6813,  1.2457,  0.7904,\n",
       "           0.3165,  0.7779,  1.4541, -0.0767,  1.1432,  0.8329,  0.7933,\n",
       "           0.9158,  1.3503, -0.0775,  0.6725],\n",
       "         [-0.9279, -0.1795, -1.8110, -1.0357, -0.6544,  0.1664, -0.4564,\n",
       "          -0.7024, -0.6100, -1.0599, -1.0029, -1.0985, -0.9784, -0.8995,\n",
       "          -1.0086, -0.7555, -0.4560, -0.3629]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if QuantMHA is learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug QuantMultiheadAttention and merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_bn_mha(layer, bn, output_channel_dim=0):\n",
    "    \n",
    "    #retrieve learnable parameters from batchnorm (scale + bias)\n",
    "    mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "        bn_mean=bn.running_mean,\n",
    "        bn_var=bn.running_var,\n",
    "        bn_eps=bn.eps,\n",
    "        bn_weight=bn.weight.data.clone(),\n",
    "        bn_bias=bn.bias.data.clone())\n",
    "    #out_proj is QuantLinear(in_features=embd_dim, out_features=embd_dim)\n",
    "    out_ch_weight_shape = qutils.compute_channel_view_shape(layer.weight, output_channel_dim)\n",
    "    #apply batchnorm during after forward pass of layer, before returning result\n",
    "    \n",
    "    #!!\n",
    "    layer.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "    #!!\n",
    "    \n",
    "    if layer.out_proj.bias is not None:\n",
    "        out_ch_bias_shape = qutils.compute_channel_view_shape(layer.out_proj.bias, channel_dim=0)\n",
    "        layer.out_proj.bias.data.mul_(mul_factor.view(out_ch_bias_shape))\n",
    "        layer.out_proj.bias.data.add_(add_factor.view(out_ch_bias_shape))\n",
    "    else:\n",
    "        layer.out_proj.bias = nn.Parameter(add_factor)\n",
    "    if (hasattr(layer, 'out_proj_weight_quant') and\n",
    "            isinstance(layer.out_proj_weight_quant, WeightQuantProxyFromInjector)):\n",
    "        layer.out_proj_weight_quant.init_tensor_quant()\n",
    "    if (hasattr(layer, 'out_proj_bias_quant') and isinstance(layer.out_proj_bias_quant, BiasQuantProxyFromInjector)):\n",
    "        layer.out_proj_bias_quant.init_tensor_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_linear = torch.nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "test_linear.weight = torch.nn.parameter.Parameter(torch.ones((embed_dim, embed_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         [ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         ...,\n",
       "         [  4.0272,   4.0272,   4.0272,  ...,   4.0272,   4.0272,   4.0272],\n",
       "         [ -8.1817,  -8.1817,  -8.1817,  ...,  -8.1817,  -8.1817,  -8.1817],\n",
       "         [  1.8297,   1.8297,   1.8297,  ...,   1.8297,   1.8297,   1.8297]],\n",
       "\n",
       "        [[-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670],\n",
       "         [ -3.0146,  -3.0146,  -3.0146,  ...,  -3.0146,  -3.0146,  -3.0146],\n",
       "         [  2.3900,   2.3900,   2.3900,  ...,   2.3900,   2.3900,   2.3900],\n",
       "         ...,\n",
       "         [  9.5870,   9.5870,   9.5870,  ...,   9.5870,   9.5870,   9.5870],\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [ -5.1709,  -5.1709,  -5.1709,  ...,  -5.1709,  -5.1709,  -5.1709]],\n",
       "\n",
       "        [[ 10.6637,  10.6637,  10.6637,  ...,  10.6637,  10.6637,  10.6637],\n",
       "         [ -0.0878,  -0.0878,  -0.0878,  ...,  -0.0878,  -0.0878,  -0.0878],\n",
       "         [ -0.8011,  -0.8011,  -0.8011,  ...,  -0.8011,  -0.8011,  -0.8011],\n",
       "         ...,\n",
       "         [ -6.9218,  -6.9218,  -6.9218,  ...,  -6.9218,  -6.9218,  -6.9218],\n",
       "         [  4.6854,   4.6854,   4.6854,  ...,   4.6854,   4.6854,   4.6854],\n",
       "         [-11.1670, -11.1670, -11.1670,  ..., -11.1670, -11.1670, -11.1670]]],\n",
       "       grad_fn=<UnsafeViewBackward0>, names=('L', 'N', None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "heads = 2\n",
    "embed_dim = 64\n",
    "context = 16\n",
    "quant_mha = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "#pass the same input tensor to merged and unmerged mha + batchnorm and compare results\n",
    "quant_mha_merged = qnn.QuantMultiheadAttention(num_heads=heads, embed_dim=embed_dim)\n",
    "from brevitas import config\n",
    "#qparams not imported from state dict\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "quant_mha_merged.load_state_dict(quant_mha.state_dict())\n",
    "#feature length not critical\n",
    "bn = torch.nn.BatchNorm1d(context)\n",
    "#test if quantmha works\n",
    "assert quant_mha(embeddings, embeddings, embeddings)[0].size() == embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLinear(\n",
       "  in_features=64, out_features=64, bias=True\n",
       "  (input_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "      (activation_impl): Identity()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "          (stats_input_view_shape_impl): OverTensorView()\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsPercentile()\n",
       "          )\n",
       "          (restrict_scaling): _RestrictValue(\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (clamp_scaling): _ClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "          )\n",
       "          (restrict_inplace_preprocess): Identity()\n",
       "          (restrict_preprocess): Identity()\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (weight_quant): WeightQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): RescalingIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClampSte()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (scaling_impl): StatsFromParameterScaling(\n",
       "        (parameter_list_stats): _ParameterListStats(\n",
       "          (first_tracked_param): _ViewParameterWrapper(\n",
       "            (view_shape_impl): OverTensorView()\n",
       "          )\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsMax()\n",
       "          )\n",
       "        )\n",
       "        (stats_scaling_impl): _StatsScaling(\n",
       "          (affine_rescaling): Identity()\n",
       "          (restrict_clamp_scaling): _RestrictClampValue(\n",
       "            (clamp_min_ste): ScalarClampMinSte()\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (restrict_scaling_pre): Identity()\n",
       "        )\n",
       "      )\n",
       "      (int_scaling_impl): IntScaling()\n",
       "      (zero_point_impl): ZeroZeroPoint(\n",
       "        (zero_point): StatelessBuffer()\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bias_quant): BiasQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): PrescaledRestrictIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClamp()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "      (zero_point): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_mha.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0926,  0.1662,  0.0606,  ...,  0.0211,  0.0027,  0.6270],\n",
       "          [-0.4505,  0.2013,  0.0885,  ..., -0.1597, -0.3558,  0.4523],\n",
       "          [-0.3425, -0.0519,  0.0775,  ..., -0.1996, -0.2760, -0.2787],\n",
       "          ...,\n",
       "          [-0.0925,  0.2803, -0.1011,  ...,  0.2038, -0.0426,  0.4598],\n",
       "          [ 0.1152,  0.0099, -0.0458,  ...,  0.0279, -0.2678, -0.3036],\n",
       "          [ 0.0462,  0.4055, -0.4288,  ...,  1.0150,  0.1507, -0.0658]],\n",
       " \n",
       "         [[-0.1684,  0.2740, -0.0567,  ...,  0.1945, -0.1586,  0.4784],\n",
       "          [-0.2414,  0.2263,  0.0706,  ..., -0.2693, -0.4626,  0.4440],\n",
       "          [-0.3649,  0.0976, -0.0294,  ..., -0.0971, -0.1957, -0.1036],\n",
       "          ...,\n",
       "          [-0.0461,  0.1246, -0.1975,  ...,  0.0336,  0.2553,  0.3853],\n",
       "          [ 0.0460,  0.0541, -0.0979,  ..., -0.0303, -0.3825, -0.3351],\n",
       "          [-0.0157,  0.2822, -0.4208,  ...,  0.5638,  0.3149,  0.0858]],\n",
       " \n",
       "         [[-0.0181,  0.1725, -0.0047,  ...,  0.1487,  0.1976,  0.5633],\n",
       "          [-0.1927,  0.1515, -0.0845,  ..., -0.2242, -0.4402,  0.5122],\n",
       "          [-0.1058, -0.1061,  0.0588,  ..., -0.2105, -0.3280, -0.3857],\n",
       "          ...,\n",
       "          [-0.0667,  0.1150, -0.3250,  ..., -0.0771,  0.1500,  0.4351],\n",
       "          [ 0.0051,  0.1560, -0.1228,  ...,  0.3762, -0.2658,  0.0824],\n",
       "          [ 0.0911,  0.2581, -0.2653,  ...,  0.3346,  0.2105,  0.3753]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.2267, 0.3770, 0.3974],\n",
       "          [0.2486, 0.5163, 0.2363],\n",
       "          [0.2704, 0.3674, 0.3619]],\n",
       " \n",
       "         [[0.4780, 0.2718, 0.2513],\n",
       "          [0.2445, 0.2800, 0.4767],\n",
       "          [0.3401, 0.3811, 0.2773]],\n",
       " \n",
       "         [[0.3292, 0.3783, 0.2923],\n",
       "          [0.2991, 0.5053, 0.1953],\n",
       "          [0.4384, 0.2035, 0.3578]],\n",
       " \n",
       "         [[0.2322, 0.4357, 0.3333],\n",
       "          [0.2759, 0.4958, 0.2281],\n",
       "          [0.2827, 0.2062, 0.5122]],\n",
       " \n",
       "         [[0.3565, 0.4220, 0.2199],\n",
       "          [0.4971, 0.1079, 0.3933],\n",
       "          [0.3510, 0.3333, 0.3155]],\n",
       " \n",
       "         [[0.3292, 0.2636, 0.4070],\n",
       "          [0.2868, 0.3141, 0.3988],\n",
       "          [0.3059, 0.3414, 0.3537]],\n",
       " \n",
       "         [[0.2759, 0.2800, 0.4439],\n",
       "          [0.3701, 0.2035, 0.4248],\n",
       "          [0.2144, 0.4534, 0.3319]],\n",
       " \n",
       "         [[0.3414, 0.4220, 0.2376],\n",
       "          [0.2213, 0.4070, 0.3715],\n",
       "          [0.2581, 0.3974, 0.3442]],\n",
       " \n",
       "         [[0.4644, 0.2827, 0.2540],\n",
       "          [0.4138, 0.0970, 0.4889],\n",
       "          [0.3005, 0.3360, 0.3633]],\n",
       " \n",
       "         [[0.3578, 0.2144, 0.4275],\n",
       "          [0.2295, 0.5545, 0.2172],\n",
       "          [0.3947, 0.1980, 0.4084]],\n",
       " \n",
       "         [[0.3865, 0.2103, 0.4029],\n",
       "          [0.2827, 0.5286, 0.1885],\n",
       "          [0.3906, 0.4398, 0.1694]],\n",
       " \n",
       "         [[0.3169, 0.3169, 0.3674],\n",
       "          [0.3169, 0.3169, 0.3674],\n",
       "          [0.3974, 0.3974, 0.2049]],\n",
       " \n",
       "         [[0.3251, 0.3496, 0.3251],\n",
       "          [0.3428, 0.3155, 0.3428],\n",
       "          [0.3251, 0.3496, 0.3251]],\n",
       " \n",
       "         [[0.4999, 0.2240, 0.2759],\n",
       "          [0.3770, 0.4097, 0.2131],\n",
       "          [0.4439, 0.3797, 0.1762]],\n",
       " \n",
       "         [[0.3578, 0.4275, 0.2144],\n",
       "          [0.3947, 0.4084, 0.1980],\n",
       "          [0.2295, 0.2172, 0.5545]],\n",
       " \n",
       "         [[0.0819, 0.4111, 0.5053],\n",
       "          [0.3046, 0.3387, 0.3565],\n",
       "          [0.3920, 0.3988, 0.2090]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#added a print statement after out_proj to see if the shape is changed afterwards\n",
    "quant_mha(embeddings,embeddings,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test if qparams are the same without copying them. they should have value 1 \n",
    "MISSING_QPARAMS =  [\"in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"out_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"attn_output_weights_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"q_scaled_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"k_transposed_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\", \"v_quant.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\"]\n",
    "for missing_qparam in MISSING_QPARAMS:\n",
    "    #omit \".value\" to get submodule \n",
    "    submodule = missing_qparam[:-len(\".value\")]\n",
    "    assert quant_mha.get_submodule(submodule).value == quant_mha_merged.get_submodule(submodule).value, f'qparam {missing_qparam} is different' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0486e+05, 4.5555e-41, 1.2967e-05],\n",
      "        [3.0866e-41, 4.4842e-44, 0.0000e+00]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(test(tensor))\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: IntQuant.forward() missing 3 required positional arguments: 'zero_point', 'bit_width', and 'x'"
     ]
    }
   ],
   "source": [
    "import brevitas\n",
    "test = brevitas.core.quant.IntQuant(True, True)\n",
    "tensor = torch.Tensor(2,3)\n",
    "print(tensor)\n",
    "print(test(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if creating a custom activation can simulate batchnorm\n",
    "#### Alternative: Implement custom Quantizer which performs normalization based on a scale and add factor which can be passed in its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "def QuantIdentityWithWeights(qnn.QuantIdentity):\n",
    "    def __init__(self,\n",
    "            act_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        super().__init__(self,\n",
    "            act_quant = act_quant\n",
    "            return_quant_tensor = return_quant_tensor,\n",
    "            **kwargs):\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        return super().forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from brevitas.nn.quant_layer import ActQuantType\n",
    "from torch import Tensor\n",
    "from brevitas.quant.scaled_int import *\n",
    "from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL\n",
    "\n",
    "class QuantCustom(QuantNLAL):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mul_factor: Tensor,\n",
    "            add_factor: Tensor,\n",
    "            act_quant: Optional[ActQuantType] = Uint8ActPerTensorFloat,\n",
    "            input_quant: Optional[ActQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs):\n",
    "        QuantNLAL.__init__(\n",
    "            self,\n",
    "            act_impl=None,\n",
    "            passthrough_act=True,\n",
    "            input_quant=input_quant,\n",
    "            act_quant=act_quant,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "        self.mul_factor = mul_factor\n",
    "        self.add_factor = add_factor\n",
    "    \n",
    "\n",
    "    def forward(self, input: Union[Tensor, QuantTensor]):\n",
    "        input = self.unpack_input(input)\n",
    "        quant_input = self.input_quant(input)\n",
    "        # shortcut execution through the export impl during export\n",
    "        if self.export_mode:\n",
    "            out = self.export_handler(quant_input.value)\n",
    "            self._set_global_is_quant_layer(False)\n",
    "            return out\n",
    "        out = self.act_quant(quant_input)\n",
    "        \n",
    "        out = self.pack_output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL\n",
    "from torch.nn import Module as TorchModule\n",
    "from brevitas.nn.mixin import * #WeightQuantType, BiasQuantType\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat\n",
    "from typing import Optional, Union\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "from torch import Tensor\n",
    "#test if a quantized layer can be implemented which basically scales the values along a tensor and adds a bias, thereby simulating batch normalization\n",
    "class QuantBatchnorm1d(QuantWBIOL, TorchModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
    "            bias_quant: Optional[BiasQuantType] = None,\n",
    "            return_quant_tensor: bool = False,\n",
    "            **kwargs) -> None:\n",
    "        TorchModule.__init__(self)\n",
    "        if not isinstance(num_features, int) or num_features <= 0:\n",
    "            raise AttributeError()\n",
    "        #do the same as quantidentity\n",
    "        self.weight = torch.ones(num_features)\n",
    "        self.bias = torch.zeros(num_features)\n",
    "        QuantWBIOL.__init__(\n",
    "            self,\n",
    "            weight_quant=weight_quant,\n",
    "            bias_quant=bias_quant,\n",
    "            input_quant=None,\n",
    "            output_quant=None,\n",
    "            return_quant_tensor=return_quant_tensor,\n",
    "            **kwargs)\n",
    "    \n",
    "    def forward(self, input: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:\n",
    "        return self.forward_impl(input)\n",
    "    \n",
    "    def inner_forward_impl(self, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n",
    "        #inner_forward_impl is apparently the actual forward pass of the layer \n",
    "        return x * quant_weight[:,None] + quant_bias[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test functionality of custom Quantized layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.]), tensor([0., 0., 0., 0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mul_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "#add_factor = torch.randint(15, (context,)).to(dtype=torch.float) / 6.3\n",
    "batch, context, embeds = 2,4,8\n",
    "#should do nothing as weights are 1 and biases are 0\n",
    "custom_quant_layer = QuantBatchnorm1d(num_features=context)\n",
    "print(f'{custom_quant_layer.weight}, {custom_quant_layer.bias}\\n')\n",
    "input = torch.randint(30, (batch, context, embeds))\n",
    "output = custom_quant_layer(input)\n",
    "assert input.equal(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### compare results of custom Batchnorm with regular batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True, False,  True,  True, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False,  True,  True,  True, False,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False,  True,  True,  True, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False,  True,  True, False,  True, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embeddings = torch.randn(batch, sentence_length, embedding_dim)\n",
    "custom_quant_bn = QuantBatchnorm1d(sentence_length)\n",
    "regular_bn = torch.nn.BatchNorm1d(sentence_length)\n",
    "#simulate some forward passes to change the mean and standard deviation\n",
    "for _ in range(10):\n",
    "    regular_bn(embeddings)\n",
    "#test if merge_bn works\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=regular_bn.running_mean,\n",
    "    bn_var=regular_bn.running_var,\n",
    "    bn_eps=regular_bn.eps,\n",
    "    bn_weight=regular_bn.weight.data.clone(),\n",
    "    bn_bias=regular_bn.bias.data.clone())\n",
    "assert custom_quant_bn.weight.size() == mul_factor.size()\n",
    "assert custom_quant_bn.bias.size() == add_factor.size()\n",
    "#change weight and bias of quantized batchnorm\n",
    "custom_quant_bn.weight = mul_factor\n",
    "custom_quant_bn.bias = add_factor\n",
    "#test if results are at least somewhat similiar\n",
    "out_regular_bn = regular_bn(embeddings)\n",
    "out_quant_bn = custom_quant_bn(embeddings)\n",
    "#loss is around 0.05 for each normalized embedding\n",
    "print((out_regular_bn - out_quant_bn) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy pasted from brevitas.nn.utils\n",
    "def compute_channel_view_shape(tensor: torch.Tensor, channel_dim: int):\n",
    "    #create a list containing ones with length of the tensor dimension (for mha: always length of 3)\n",
    "    broadcast_shape = [1] * len(tensor.size())\n",
    "    #why is that important\n",
    "    broadcast_shape[channel_dim] = -1\n",
    "    return tuple(broadcast_shape)\n",
    "\n",
    "def mul_add_from_bn(bn_mean, bn_var, bn_eps, bn_weight, bn_bias):\n",
    "    denom = torch.sqrt(bn_var + bn_eps)\n",
    "    mul_factor = bn_weight / denom\n",
    "    add_factor = -bn_mean * mul_factor + bn_bias\n",
    "    return mul_factor, add_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layernorm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = embeddings.rename(None)\n",
    "for i in range(10):\n",
    "    layernorm(tensor)\n",
    "    bn(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerNorm' object has no attribute 'running_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test if mul_add_from_bn works for layernorm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layernorm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(embed_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m ln_mul, ln_add \u001b[38;5;241m=\u001b[39m qutils\u001b[38;5;241m.\u001b[39mmul_add_from_bn(\n\u001b[0;32m----> 4\u001b[0m     bn_mean\u001b[38;5;241m=\u001b[39m\u001b[43mlayernorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m,\n\u001b[1;32m      5\u001b[0m     bn_var\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mrunning_var,\n\u001b[1;32m      6\u001b[0m     bn_eps\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m      7\u001b[0m     bn_weight\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m      8\u001b[0m     bn_bias\u001b[38;5;241m=\u001b[39mlayernorm\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/eki/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerNorm' object has no attribute 'running_mean'"
     ]
    }
   ],
   "source": [
    "# test if mul_add_from_bn works for layernorm\n",
    "layernorm = torch.nn.LayerNorm(embed_dim, bias=True)\n",
    "ln_mul, ln_add = qutils.mul_add_from_bn(\n",
    "    bn_mean=layernorm.running_mean,\n",
    "    bn_var=layernorm.running_var,\n",
    "    bn_eps=layernorm.eps,\n",
    "    bn_weight=layernorm.weight.data.clone(),\n",
    "    bn_bias=layernorm.bias.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, -1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basically a list of ones with a -1 at index channel_dim\n",
    "compute_channel_view_shape(torch.Tensor(3,3,3,3,3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quant_mha_merged.out_proj.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (-1, 1), mul_factor view: torch.Size([16, 1])\n",
      "quant_mha_merged.out_proj.weight.data shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#disassemble functionality of merge_bn_mha\n",
    "#one dimensional tensor to scale all values of a batch with a corresponding scalar\n",
    "mul_factor, add_factor = qutils.mul_add_from_bn(\n",
    "    bn_mean=bn.running_mean,\n",
    "    bn_var=bn.running_var,\n",
    "    bn_eps=bn.eps,\n",
    "    bn_weight=bn.weight.data.clone(),\n",
    "    bn_bias=bn.bias.data.clone())\n",
    "assert mul_factor.size() == add_factor.size()\n",
    "assert mul_factor.size()[0] == add_factor.size()[0] == context\n",
    "output_channel_dim = 0\n",
    "#out_proj is QuantLinear -> 2d Tensor, [-1, 1]\n",
    "#meaning: reverse shape of mul_factor tensor\n",
    "#currently: [1,context] now: [context, 1]\n",
    "out_ch_weight_shape = qutils.compute_channel_view_shape(quant_mha_merged.out_proj.weight, output_channel_dim)\n",
    "#out_proj is a linear layer applying a scaling factor to quantize outputs -> inputs: n_embd, outputs: n_embd\n",
    "#batchnorm applied normalization along second dimension, linear layer along third\n",
    "#-> could work if batchnorm normalizes along embeddings\n",
    "assert mul_factor.view(out_ch_weight_shape).size() != quant_mha_merged.out_proj.weight.data.size()\n",
    "print(f'shape: {out_ch_weight_shape}, mul_factor view: {mul_factor.view(out_ch_weight_shape).size()}')\n",
    "print(f'quant_mha_merged.out_proj.weight.data shape: {quant_mha_merged.out_proj.weight.data.size()}')\n",
    "#quant_mha_merged.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape))\n",
    "#merge params of bn in quant_mha_merged\n",
    "#merge_bn_mha(quant_mha_merged, bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_factor: torch.Size([16]), add_factor: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#each value of mul_factor is the scale with which the embedding of one word is multiplied with\n",
    "#in total: context amount of words\n",
    "#linear layer calculates the sum of each embedding of a word with a weight\n",
    "#problem: weights are the same for every word\n",
    "print(f'mul_factor: {mul_factor.size()}, add_factor: {add_factor.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qutils.merge_bn(qnn.QuantLinear(context, context, True), bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <p>Rows: Context, Columns: Embeddings</p>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Row</th>\n",
       "            <th>Embedding 1</th>\n",
       "            <th>Embedding 2</th>\n",
       "            <th>Embedding 3</th>\n",
       "            <th>Embedding 4</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>-0.6182</td>\n",
       "            <td>0.6397</td>\n",
       "            <td>-0.6141</td>\n",
       "            <td>0.8668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>0.4140</td>\n",
       "            <td>0.1806</td>\n",
       "            <td>-1.1200</td>\n",
       "            <td>-0.3160</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\n",
    "    \"\"\"<body>\n",
    "    <p>Rows: Context, Columns: Embeddings</p>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Row</th>\n",
    "            <th>Embedding 1</th>\n",
    "            <th>Embedding 2</th>\n",
    "            <th>Embedding 3</th>\n",
    "            <th>Embedding 4</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>-0.6182</td>\n",
    "            <td>0.6397</td>\n",
    "            <td>-0.6141</td>\n",
    "            <td>0.8668</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>0.4140</td>\n",
    "            <td>0.1806</td>\n",
    "            <td>-1.1200</td>\n",
    "            <td>-0.3160</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each row of a prompt needs to be multiplied with the same scalar from mul_factor, the position of the row determines the index of mul_factor\\\n",
    "basically: row 1 * mul_factor[0], row 2 * mul_factor[2] etc.\\\n",
    "remember the bit_width value from quant config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6182,  0.6397, -0.6141,  0.1633,  0.1303,  0.9678, -0.0135, -0.3592,\n",
      "         1.0851,  1.2883, -1.4943, -1.3554, -1.2857,  0.5534, -0.9053, -0.2538,\n",
      "         2.0112,  1.5106, -0.5143,  0.0181, -1.1853, -0.1291,  1.1889, -0.2304,\n",
      "        -0.1677, -1.0456, -0.1630,  0.8798,  0.4793,  1.3267,  0.9272,  0.4181,\n",
      "        -1.6796, -0.2393,  0.7780, -1.7058, -1.1486, -1.5907,  0.9055, -0.6892,\n",
      "        -0.3182,  1.7268,  1.3576, -0.0698,  0.5315, -0.9513,  0.0850, -0.0770,\n",
      "         0.6108, -0.9660, -0.2021, -0.8171, -0.4134, -0.9940, -1.1997,  1.1844,\n",
      "         2.1950, -0.9969, -0.6415,  2.3817, -0.1636,  0.8668, -1.2963,  0.2591],\n",
      "       grad_fn=<SelectBackward0>, names=('E',)), \n",
      "tensor([-0.6476,  0.6723, -0.6434,  0.1723,  0.1378,  1.0165, -0.0132, -0.3759,\n",
      "         1.1397,  1.3528, -1.5670, -1.4212, -1.3480,  0.5817, -0.9490, -0.2653,\n",
      "         2.1113,  1.5861, -0.5386,  0.0200, -1.2427, -0.1344,  1.2486, -0.2408,\n",
      "        -0.1750, -1.0961, -0.1700,  0.9242,  0.5039,  1.3931,  0.9740,  0.4398,\n",
      "        -1.7614, -0.2501,  0.8174, -1.7889, -1.2042, -1.6681,  0.9512, -0.7222,\n",
      "        -0.3328,  1.8130,  1.4256, -0.0722,  0.5587, -0.9972,  0.0902, -0.0798,\n",
      "         0.6419, -1.0126, -0.2110, -0.8563, -0.4327, -1.0420, -1.2578,  1.2439,\n",
      "         2.3043, -1.0450, -0.6721,  2.5002, -0.1707,  0.9105, -1.3592,  0.2729],\n",
      "       grad_fn=<AddBackward0>, names=('E',))\n"
     ]
    }
   ],
   "source": [
    "#perform normalization like so:\n",
    "print(f'{embeddings[0][0]}, \\n{mul_factor[0] * embeddings[0][0] + add_factor[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64])\n",
      "torch.Size([3, 16, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_no_merge \u001b[39m=\u001b[39m bn(output_no_merge)\n\u001b[1;32m      4\u001b[0m output_merge \u001b[39m=\u001b[39m quant_mha_merged(q,k,v)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[39massert\u001b[39;00m output_no_merge\u001b[39m.\u001b[39mequal(output_merge)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q,k,v = [embeddings for _ in range(3)]\n",
    "output_no_merge = quant_mha(q,k,v)[0]\n",
    "output_no_merge = bn(output_no_merge)\n",
    "output_merge = quant_mha_merged(q,k,v)[0]\n",
    "assert output_no_merge.equal(output_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test what happens if values are transposed after merging QuantLinear with BatchNorm\n",
    "quant_linear = qnn.QuantLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if transposing inputs changes values for batchnorm with two different feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "transposing does not work",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m out_context \u001b[39m=\u001b[39m bn_context(\u001b[39minput\u001b[39m)\n\u001b[1;32m      6\u001b[0m out_embedding \u001b[39m=\u001b[39m bn_embedding(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39massert\u001b[39;00m out_context\u001b[39m.\u001b[39mequal(out_embedding), \u001b[39m\"\u001b[39m\u001b[39mtransposing does not work\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: transposing does not work"
     ]
    }
   ],
   "source": [
    "bn_context = torch.nn.BatchNorm1d(context)\n",
    "bn_embedding = torch.nn.BatchNorm1d(embed_dim)\n",
    "input = embeddings\n",
    "input = input.rename(None)\n",
    "out_context = bn_context(input)\n",
    "out_embedding = bn_embedding(input.transpose(-1,-2))\n",
    "#after transposing, normalize along embeddings instead of along words\n",
    "assert out_context.equal(out_embedding), \"transposing does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if linear transformation can simulate the functionality of batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9493,  2.7542,  9.5925,  7.8767,  1.7580,  3.1383,  6.2312,  5.3196,\n",
       "          7.8936,  1.0332],\n",
       "        [12.1436, 11.1988, 15.5672, 13.1937,  5.2797,  7.7320, 12.6711,  8.1652,\n",
       "         13.6740,  6.0436],\n",
       "        [ 4.6590,  9.2941,  0.4616, -0.8165,  1.4461,  7.0240, -2.8188,  0.2568,\n",
       "          0.8149,  6.8677]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randint(30, (3,10)).to(dtype=torch.float) * 0.7\n",
    "linear = torch.nn.Linear(3, 3)\n",
    "linear(test.transpose(-1,0)).transpose(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before attention calculation, q,k,v are quantized\n",
    "#shape: n_embd * 3 as q,k,v are the same and of shape n_embd\n",
    "quant_mha_merged.in_proj.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 64]), torch.Size([3, 16, 64]), torch.Size([3, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "#code copied from forward pass of QuantMultiheadAttention\n",
    "#function is called if in_proj quantization has been set\n",
    "#TODO: find out what it does\n",
    "def chunk(x, num=3, dim=-1):\n",
    "    _len, _bsz, _dim = x.shape\n",
    "    x = x.reshape(_len, _bsz, num, dim)\n",
    "    return x[:, :, 0, :], x[:, :, 1, :], x[:, :, 2, :]\n",
    "assert attn_no_batchfirst.in_proj is not None\n",
    "from brevitas.nn.utils import check_tensors_same_ptr\n",
    "#no idea what it does, it has to be True or else an Exception will be thrown\n",
    "assert check_tensors_same_ptr([embeddings, embeddings, embeddings]) == True\n",
    "torch._C._get_tracing_state()\n",
    "\n",
    "query = embeddings\n",
    "query.rename_('L', 'N', 'E')\n",
    "#no idea why q,k,v are infered from the query and params key and value are still used\n",
    "#this is an issue if no in_proj is specified i think\n",
    "q,k,v = chunk(attn_no_batchfirst.in_proj(query))\n",
    "print(f'{q.size()}, {k.size()}, {v.size()}')\n",
    "#issue with wrong shapes could be that batch size is transposed instead of embedding dimension\n",
    "#q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]), \n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(9).reshape(3,3)\n",
    "#columns become rows, rows become columns\n",
    "print(f'{tensor}, \\n{tensor.transpose(1,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "small_attn_cpy = CausalSelfAttention(GPTConfig(block_size=16, n_embd=64, n_head=2))\n",
    "#if batchnorm and mha are merged together, padding should not be necessary for inference\n",
    "small_attn_cpy.mha = qnn.QuantMultiheadAttention(num_heads=2, embed_dim=64)\n",
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True #copy state dict does not return brevitas qparams\n",
    "small_attn_cpy.load_state_dict(small_attn.state_dict())\n",
    "#qparams from state dict are set to 1 at first\n",
    "print(small_attn.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "print(small_attn_cpy.mha.in_proj.input_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "idea from: https://github.com/Xilinx/brevitas/issues/542#issuecomment-1446338490\n",
    "merge_bn does not delete current batchnorm, meaning that one model has to be initialiized without bn and the parameters from the trained model\n",
    "have to be copied to the model without bn\n",
    "TODO: find more ressource efficient ways\n",
    "\"\"\"\n",
    "#at one step in merge_bn_mha, layer.out_proj.weight.data.mul_(mul_factor.view(out_ch_weight_shape)) is performed\n",
    "#weight is of shape (embd_dim, embd_dim), mul_factor is of (shape features, 1)\n",
    "#meaning that batchnorm probably normalizes along the embeddings instead of each sentence\n",
    "\"bn_alt feature length is 64 (embedding dimension)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39;49mmha, bn, output_channel_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     merge_bn_mha(small_attn\u001b[39m.\u001b[39mmha, bn, output_channel_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/eki-transformer-dev/qtransform/qtransform/model/modules/__init__.py:90\u001b[0m, in \u001b[0;36mmerge_bn_mha\u001b[0;34m(layer, bn, output_channel_dim)\u001b[0m\n\u001b[1;32m     88\u001b[0m out_ch_weight_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mweight, output_channel_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39m#apply batchnorm during after forward pass of layer, before returning result\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m layer\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmul_(mul_factor\u001b[39m.\u001b[39;49mview(out_ch_weight_shape))\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     out_ch_bias_shape \u001b[39m=\u001b[39m qutils\u001b[39m.\u001b[39mcompute_channel_view_shape(layer\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39mbias, channel_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#merge_bn_mha appends batchnorm to mha, TODO: prepend it (maybe use input_quant_tensor or something)\n",
    "#problem: merged and unmerged outputs are not the same, possibly since feature length is different\n",
    "no_merge_attn_output = small_attn(embeddings)\n",
    "no_merge_bn_output = bn(no_merge_attn_output)\n",
    "try:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=0)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=1)\n",
    "except Exception:\n",
    "    merge_bn_mha(small_attn.mha, bn, output_channel_dim=2)\n",
    "merge_attn_output = small_attn(embeddings)\n",
    "assert torch.equal(no_merge_bn_output, merge_attn_output) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mul_:\n",
      "\n",
      "mul_(...) method of torch.Tensor instance\n",
      "    mul_(value) -> Tensor\n",
      "    \n",
      "    In-place version of :meth:`~Tensor.mul`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer.out_proj.weight.data.mul_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a.mul_(tensor) basically is a = a * tensor\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.mul_(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_attn(torch.Tensor(3,16,64)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function brevitas.nn.utils.merge_bn(layer, bn, output_channel_dim=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv1d and batchnorm1d merge\n",
    "\n",
    "qnn.quant_layer.merge_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9917, 0.4984, 0.6176, 0.5039, 0.8158, 0.8521, 0.0155, 0.1858,\n",
       "          0.8048],\n",
       "         [0.1621, 0.4298, 0.3947, 0.5427, 0.8238, 0.9419, 0.7478, 0.4333,\n",
       "          0.0647],\n",
       "         [0.0897, 0.2927, 0.9780, 0.6710, 0.0377, 0.8199, 0.1301, 0.8592,\n",
       "          0.8216],\n",
       "         [0.2074, 0.6790, 0.2042, 0.7838, 0.5414, 0.5088, 0.8481, 0.2490,\n",
       "          0.1760],\n",
       "         [0.0197, 0.6737, 0.1897, 0.2794, 0.4024, 0.3306, 0.8610, 0.8641,\n",
       "          0.6871],\n",
       "         [0.7651, 0.4413, 0.9831, 0.4328, 0.2344, 0.0799, 0.4901, 0.1151,\n",
       "          0.9380]],\n",
       "\n",
       "        [[0.4503, 0.5180, 0.3012, 0.7354, 0.2637, 0.9073, 0.9226, 0.7925,\n",
       "          0.0674],\n",
       "         [0.9067, 0.1654, 0.9186, 0.1072, 0.0438, 0.4049, 0.1374, 0.3990,\n",
       "          0.6381],\n",
       "         [0.3767, 0.8549, 0.5588, 0.2489, 0.2599, 0.6461, 0.5800, 0.1559,\n",
       "          0.0832],\n",
       "         [0.9381, 0.2192, 0.7259, 0.7615, 0.1411, 0.1472, 0.9268, 0.6733,\n",
       "          0.9049],\n",
       "         [0.1468, 0.8668, 0.3151, 0.5401, 0.4347, 0.5541, 0.0995, 0.6333,\n",
       "          0.1252],\n",
       "         [0.4964, 0.4591, 0.3443, 0.2972, 0.6705, 0.2664, 0.4867, 0.5302,\n",
       "          0.6139]],\n",
       "\n",
       "        [[0.3803, 0.7252, 0.5246, 0.4232, 0.7195, 0.7118, 0.8266, 0.7990,\n",
       "          0.3631],\n",
       "         [0.1904, 0.0903, 0.8097, 0.7286, 0.2548, 0.3355, 0.7833, 0.9820,\n",
       "          0.1257],\n",
       "         [0.6124, 0.5454, 0.4477, 0.6442, 0.5862, 0.4324, 0.3639, 0.6780,\n",
       "          0.3984],\n",
       "         [0.4506, 0.1190, 0.4191, 0.3696, 0.6122, 0.7606, 0.1218, 0.3204,\n",
       "          0.6428],\n",
       "         [0.0765, 0.5835, 0.6852, 0.1305, 0.3852, 0.2372, 0.3551, 0.0528,\n",
       "          0.6343],\n",
       "         [0.0852, 0.0078, 0.7411, 0.6070, 0.8316, 0.9041, 0.3005, 0.5442,\n",
       "          0.2225]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand((3,6,9))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5546, -0.3177,  0.1346, -0.2969,  0.8870,  1.0246, -2.1506,\n",
       "          -1.5043,  0.8454],\n",
       "         [-0.9784, -0.1143, -0.2279,  0.2499,  1.1573,  1.5384,  0.9118,\n",
       "          -0.1030, -1.2930],\n",
       "         [-1.5225, -0.7464,  1.8741,  0.7002, -1.7215,  1.2696, -1.3680,\n",
       "           1.4198,  1.2762],\n",
       "         [-1.0571,  0.6570, -1.0687,  1.0382,  0.1571,  0.0385,  1.2718,\n",
       "          -0.9059, -1.1712],\n",
       "         [-1.5085,  0.9969, -0.8575, -0.5138, -0.0426, -0.3174,  1.7148,\n",
       "           1.7266,  1.0484],\n",
       "         [ 1.0773, -0.1349,  1.8936, -0.1669, -0.9097, -1.4882,  0.0477,\n",
       "          -1.3565,  1.7249]],\n",
       "\n",
       "        [[-0.5003, -0.2434, -1.0663,  0.5820, -1.2088,  1.2342,  1.2923,\n",
       "           0.7987, -1.9539],\n",
       "         [ 1.4249, -0.9680,  1.4631, -1.1558, -1.3605, -0.1948, -1.0582,\n",
       "          -0.2139,  0.5578],\n",
       "         [-0.4253,  1.4034,  0.2712, -0.9140, -0.8717,  0.6049,  0.3522,\n",
       "          -1.2693, -1.5475],\n",
       "         [ 1.5987, -1.0141,  0.8276,  0.9569, -1.2980, -1.2758,  1.5576,\n",
       "           0.6364,  1.4780],\n",
       "         [-1.0216,  1.7369, -0.3769,  0.4850,  0.0815,  0.5390, -1.2028,\n",
       "           0.8424, -1.1045],\n",
       "         [ 0.0714, -0.0682, -0.4982, -0.6744,  0.7233, -0.7897,  0.0349,\n",
       "           0.1978,  0.5111]],\n",
       "\n",
       "        [[-0.7660,  0.5431, -0.2185, -0.6032,  0.5214,  0.4922,  0.9280,\n",
       "           0.8232, -0.8312],\n",
       "         [-0.8871, -1.2103,  1.1116,  0.8499, -0.6794, -0.4189,  1.0267,\n",
       "           1.6678, -1.0960],\n",
       "         [ 0.4760,  0.2200, -0.1538,  0.5978,  0.3758, -0.2121, -0.4741,\n",
       "           0.7270, -0.3422],\n",
       "         [-0.1729, -1.3784, -0.2876, -0.4673,  0.4144,  0.9536, -1.3683,\n",
       "          -0.6463,  0.5256],\n",
       "         [-1.2911,  0.6514,  1.0409, -1.0840, -0.1082, -0.6752, -0.2235,\n",
       "          -1.3819,  0.8459],\n",
       "         [-1.4683, -1.7582,  0.9877,  0.4856,  1.3264,  1.5978, -0.6620,\n",
       "           0.2501, -0.9544]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized values along second dimension, meaning: along sentences\n",
    "#are \n",
    "torch.nn.BatchNorm1d(6)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5873)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor retains size, batchnorm essentially is a linear transformation to shift values to have a mean of 0 and a standard deviation of 1\n",
    "torch.nn.BatchNorm1d(10)(torch.Tensor(3,10,16)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = qnn.QuantIdentity()\n",
    "tensor = torch.Tensor(12,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1617e+35,  3.0907e-41, -1.5597e+37,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.1351e-43,  0.0000e+00],\n",
      "         [-1.5597e+37,  3.0907e-41, -3.0176e+34,  3.0907e-41],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test if quantidentity is a simple wrapper around a tensor that does nothing\n",
    "#if so, it could be useful for merging with batchnorm\n",
    "tensor = torch.Tensor(2,3,4)\n",
    "print(tensor)\n",
    "print(\"\\n\" + 30* \"-\" + \"\\n\")\n",
    "print(qnn.QuantIdentity()(tensor).isclose(tensor).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = identity(tensor)\n",
    "output.size == tensor.size\n",
    "output == tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eki",
   "language": "python",
   "name": "eki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
