{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015aa5ab-928b-4d8e-8a29-0d3f390a9c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in case it is not been done before, install the package:\n",
    "#!pip install -e .\n",
    "# alternativly via git:\n",
    "#!pip install git+https://github.com/fhswf/eki-transformer-dev.git@develop#subdirectory=qtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d87df4-a4c0-4d0a-9dbc-39f9d7806798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076401d2-93b4-4969-9cea-9994bfdd52ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import qtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d302a961-db64-4b6b-8d91-d6aa7092cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/makuh001/eki/eki-transformer-dev/qtransform/qtransform/conf\n"
     ]
    }
   ],
   "source": [
    "# Manually load some logging conf\n",
    "config_path = qtransform.get_module_config_path()\n",
    "print(config_path)\n",
    "import logging.config\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(config_path, 'hydra','job_logging', 'custom.yaml'), 'r') as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "\n",
    "logging.config.dictConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2058296-a377-4ad2-bc30-814a26532404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## compute config and run qtransform.main\n",
    "\n",
    "#with initialize_config_dir(version_base=None, config_dir=qtransform.get_module_config_path()):\n",
    "#    args = [\n",
    "#        \"run=train\", \n",
    "#        \"model=gpt_2_small\",\n",
    "#        \"dataset=huggingface\", \n",
    "#        \"dataset.name=tiny_shakespeare\",\n",
    "#        \"+export=True\",\n",
    "#        \"run.epochs=1\",\n",
    "#        \"run.max_iters=300\",\n",
    "#        \"dataset/tokenizer=tiktoken\",\n",
    "#        \"dataset.tokenizer.encoding=gpt2\"\n",
    "#    ]\n",
    "#    cfg = compose(config_name=\"config.yaml\", overrides=args)\n",
    "#    print(\"Hydra Config:\", cfg)\n",
    "#    qtransform.run(cfg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65354c98-0e04-4217-b10f-bf95ef8f35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[36m2023-12-20 11:45:48,747 \u001b[0m][\u001b[2;37mhydra.core.utils\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mSetting JobRuntime:name=app\u001b[0m\n",
      "{'data': {'dtype': 'float32'}, 'device': 'cuda', 'debug': True, 'dataset': {'wrapper': 'HuggingfaceDatasetWrapper', 'module': 'huggingface', 'name': 'tiny_shakespeare', 'root_path': '~/.qtransform/datasets', 'dataset_dir': ['${dataset.root_path}', '${dataset.module}', '${dataset.name}'], 'sizes': {'train': 0.3, 'eval': 0.05, 'test': 0.05, 'bench': 0.3}, 'tokenizer': {'dataset_dir': ['${dataset.root_path}', '${dataset.name}'], 'name': '${dataset.name}', 'dtype': '${data.dtype}', 'wrapper': 'TikTokenizer', 'encoding': 'gpt2'}, 'dataloader': {'shuffle': True, 'num_workers': 2, 'batch_size': 12}, 'type': 'huggingface', 'args': {'block_size': '${model.args.block_size}', 'cache_dir': None, 'data_column_name': 'text', 'batches': 1000}}, 'seed': 1234567890, 'model': {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'flash': False}}, 'quantization': {'quantize': False, 'device': '${device}'}, 'optim': {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}, 'run': {'command': 'train', 'always_save_checkpoint': True, 'init_from': 'scratch', 'model_dir': 'models', 'epochs': 1, 'gradient_accumulation_steps': '5 * 8', 'flash': False, 'export': True, 'max_iters': 300, 'save_epoch_interval': 1, 'log_steps_interval': 10, 'grad_clip': 0.7, 'validation_epoch_interval': 1000, 'validation_iters': 200, 'export_fn': 'export_qonnx', 'opset_version': 16, 'do_constant_folding': True}, 'export': True}\n",
      "[ \u001b[36m2023-12-20 11:45:48,925 \u001b[0m][\u001b[2;37mqtransform.qtransform.__main__\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mDEBUG ENABLED\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,929 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,931 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mRunning Training\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,934 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,938 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2023-12-20_11:45:48\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,940 \u001b[0m][\u001b[2;37mqtransform\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mUsing device: cuda\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,943 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of torch dataloader: 2\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,945 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,947 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:48,951 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50304, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU')\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,043 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,058 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,310 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.99M\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,324 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.dataset.HuggingfaceDatasetWrapper(parent: <class 'qtransform.dataset.DatasetWrapper'>)\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,333 \u001b[0m][\u001b[2;37mqtransform.dataset.huggingface\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading dataset: tiny_shakespeare, with encoding: gpt2 and dtype: float32\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,337 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,340 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.3\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,343 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,347 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 101408 tokens.\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,350 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,353 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 16900, start is 0.05, end is 0.1\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,354 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 16900\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,357 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 33802 tokens.\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,359 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,361 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 0, start is 0.0, end is 0.05\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,363 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 0\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,365 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 16901 tokens.\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,367 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mAttempting to retrieve tokenized dataset under \"/home/makuh001/.qtransform/datasets/huggingface/tiny_shakespeare/tokenized/gpt2/tiny_shakespeare-float32.bin\"\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,369 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mOffset is 321124, start is 0.95, end is 1.0\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,370 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mTokenized file has 338027.0 tokens of datatype: float32. Attempting to start at token: 321124\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,373 \u001b[0m][\u001b[2;37mqtransform.dataset.files\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoaded data has 257746 tokens.\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,376 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,380 \u001b[0m][\u001b[2;37mqtransform.dataset\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_loader config: {'shuffle': True, 'num_workers': 2, 'batch_size': 12, 'pin_memory': True}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,381 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34moptim config: {'optimizer': 'AdamW', 'args': {'learning_rate': 0.00015, 'weight_decay': 0.1, 'betas': [0.9, 0.95]}}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,384 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class torch.optim.AdamW(parent: <class 'torch.optim.optimizer.Optimizer'>)\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,388 \u001b[0m][\u001b[2;37mqtransform.optim\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigurable optimizer args: {'weight_decay', 'lr', 'betas'}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,390 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mConfigured optimizer (<class 'torch.optim._multi_tensor.partialclass.<locals>.NewCls'>): NewCls (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.95]\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: True\n",
      "    fused: None\n",
      "    lr: 0.00015\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,392 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mStarting new training\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,393 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mEPOCH: 1/1\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,535 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 0 loss: 1.0834431648254395\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,635 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 10 loss: 10.130547142028808\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,738 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 20 loss: 9.609923267364502\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,845 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 30 loss: 9.115524291992188\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:49,953 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 40 loss: 8.577453899383546\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,057 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 50 loss: 8.062648916244507\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,161 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 60 loss: 7.6793200969696045\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,265 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 70 loss: 7.199911880493164\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,370 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 80 loss: 6.910511302947998\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,473 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 90 loss: 6.6476966381073\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,576 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 100 loss: 6.407506656646729\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,683 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 110 loss: 6.290319395065308\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,783 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 120 loss: 6.146818351745606\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,884 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 130 loss: 6.140860223770142\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:50,987 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 140 loss: 6.007913446426391\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,091 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 150 loss: 5.857836437225342\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,195 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 160 loss: 5.894076204299926\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,300 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 170 loss: 5.847245216369629\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,403 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 180 loss: 5.695031356811524\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,505 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 190 loss: 5.680656480789184\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,612 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 200 loss: 5.591442537307739\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,715 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 210 loss: 5.427318668365478\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,818 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 220 loss: 5.531098651885986\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:51,924 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 230 loss: 5.421172666549682\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,026 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 240 loss: 5.474373245239258\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,129 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 250 loss: 5.332010793685913\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,231 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 260 loss: 5.472705888748169\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,335 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 270 loss: 5.1835325241088865\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,438 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 280 loss: 5.1897180557250975\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,542 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 290 loss: 5.017619228363037\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,646 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m  batch 300 loss: 5.23262996673584\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:52,746 \u001b[0m][\u001b[2;37mqtransform.run.train\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m5.23262996673584\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,004 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel checkpoint saved to /home/makuh001/eki/eki-transformer-dev/qtransform/GPT_2023-12-20_11:45:48__epoch:1\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,008 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,010 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mExporting Model\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,012 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m================\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,014 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mtime is: 2023-12-20_11:45:53\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,016 \u001b[0m][\u001b[2;37mqtransform.utils.helper\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mLoading checkpoint from /home/makuh001/eki/eki-transformer-dev/qtransform/GPT_2023-12-20_11:45:48__epoch:1\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,162 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mget_model config: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,164 \u001b[0m][\u001b[2;37mqtransform.model\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mLoading class qtransform.model.GPT(parent: <class 'torch.nn.modules.module.Module'>)\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,169 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mModel config: GPTConfig(block_size=64, vocab_size=50304, n_layer=2, n_head=2, n_embd=256, dropout=0.0, bias=True, flash=False, transformer_active_func='ReLU')\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,263 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,279 \u001b[0m][\u001b[2;37mqtransform.model.modules\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.2\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,525 \u001b[0m][\u001b[2;37mqtransform.model.gpt\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32mnumber of parameters: 14.99M\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,555 \u001b[0m][\u001b[2;37mqtransform.run.export\u001b[0m][\u001b[34mDEBUG\u001b[0m] - \u001b[34mModel config from checkpoint: {'calc_loss_in_model': True, 'cls': 'GPT', 'args': {'n_layer': 2, 'n_head': 2, 'n_embd': 256, 'dropout': 0.0, 'bias': True, 'block_size': 64, 'vocab_size': 50304, 'transformer_active_func': 'ReLU', 'flash': False}}\u001b[0m\n",
      "[ \u001b[36m2023-12-20 11:45:53,560 \u001b[0m][\u001b[2;37mpy.warnings\u001b[0m][\u001b[33mWARNING\u001b[0m] - \u001b[33m/home/makuh001/eki/eki-transformer-dev/qtransform/qtransform/run/export.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  export_qonnx(model, torch.tensor(sample_tensor), export_path=f\"qonnx_{str(input_dim)}_\" + filename, **kwargs)\n",
      "\u001b[0m\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "        \"run=train\", \n",
    "        \"model=gpt_2_small\",\n",
    "        \"dataset=huggingface\", \n",
    "        \"debug=True\",\n",
    "        \"dataset.name=tiny_shakespeare\",\n",
    "        \"+export=True\",\n",
    "        \"run.epochs=1\",\n",
    "        \"run.max_iters=300\",\n",
    "        \"dataset/tokenizer=tiktoken\",\n",
    "        \"dataset.tokenizer.encoding=gpt2\"\n",
    "    ]\n",
    "qtransform.notebook_run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada9928-1ab5-4848-9706-a06124124146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
